<!DOCTYPE html><html lang="[&quot;zh-CN&quot;,&quot;en&quot;]" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>LoRA, DPO, KTO ä¸ SFT æŠ€æœ¯è¯¦è§£ | é»‘å¤´å‘†é±¼è¿›åŒ–ä¹‹æ—…</title><meta name="author" content="Huiyu Chen"><meta name="copyright" content="Huiyu Chen"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="LoRA, DPO, KTO ä¸ SFT æŠ€æœ¯è¯¦è§£æœ¬ç¯‡æ–‡æ¡£å°†è¯¦ç»†ä»‹ç»å‡ ç§åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ LLAMA3ï¼‰å¾®è°ƒå’Œä¼˜åŒ–ä¸­çš„é‡è¦æŠ€æœ¯ï¼ŒåŒ…æ‹¬ SFTï¼ˆSupervised Fine-Tuningï¼‰ã€LoRAï¼ˆLow-Rank Adaptationï¼‰ã€Alignment æŠ€æœ¯ã€KTOï¼ˆKahneman-Tversky Optimizationï¼‰ å’Œ DPOï¼ˆDirect Preference Optimi">
<meta property="og:type" content="article">
<meta property="og:title" content="LoRA, DPO, KTO ä¸ SFT æŠ€æœ¯è¯¦è§£">
<meta property="og:url" content="https://chenhuiyu.github.io/2024/10/23/NLP%20Insights/LLM%E8%AE%AD%E7%BB%83%E6%9C%AF%E8%AF%AD%E4%BB%8B%E7%BB%8D%EF%BC%9ASFT%E3%80%81LoRA%E3%80%81Alignment%E3%80%81KTO%E5%92%8CDPO/index.html">
<meta property="og:site_name" content="é»‘å¤´å‘†é±¼è¿›åŒ–ä¹‹æ—…">
<meta property="og:description" content="LoRA, DPO, KTO ä¸ SFT æŠ€æœ¯è¯¦è§£æœ¬ç¯‡æ–‡æ¡£å°†è¯¦ç»†ä»‹ç»å‡ ç§åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ LLAMA3ï¼‰å¾®è°ƒå’Œä¼˜åŒ–ä¸­çš„é‡è¦æŠ€æœ¯ï¼ŒåŒ…æ‹¬ SFTï¼ˆSupervised Fine-Tuningï¼‰ã€LoRAï¼ˆLow-Rank Adaptationï¼‰ã€Alignment æŠ€æœ¯ã€KTOï¼ˆKahneman-Tversky Optimizationï¼‰ å’Œ DPOï¼ˆDirect Preference Optimi">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://chenhuiyu.github.io/img/butterfly-icon.png">
<meta property="article:published_time" content="2024-10-23T08:26:29.000Z">
<meta property="article:modified_time" content="2026-02-20T21:51:54.078Z">
<meta property="article:author" content="Huiyu Chen">
<meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://chenhuiyu.github.io/img/butterfly-icon.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "LoRA, DPO, KTO ä¸ SFT æŠ€æœ¯è¯¦è§£",
  "url": "https://chenhuiyu.github.io/2024/10/23/NLP%20Insights/LLM%E8%AE%AD%E7%BB%83%E6%9C%AF%E8%AF%AD%E4%BB%8B%E7%BB%8D%EF%BC%9ASFT%E3%80%81LoRA%E3%80%81Alignment%E3%80%81KTO%E5%92%8CDPO/",
  "image": "https://chenhuiyu.github.io/img/butterfly-icon.png",
  "datePublished": "2024-10-23T08:26:29.000Z",
  "dateModified": "2026-02-20T21:51:54.078Z",
  "author": [
    {
      "@type": "Person",
      "name": "Huiyu Chen",
      "url": "https://chenhuiyu.github.io"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://chenhuiyu.github.io/2024/10/23/NLP%20Insights/LLM%E8%AE%AD%E7%BB%83%E6%9C%AF%E8%AF%AD%E4%BB%8B%E7%BB%8D%EF%BC%9ASFT%E3%80%81LoRA%E3%80%81Alignment%E3%80%81KTO%E5%92%8CDPO/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=5.5.4"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: 'å¤åˆ¶æˆåŠŸ',
    error: 'å¤åˆ¶å¤±è´¥',
    noSupport: 'æµè§ˆå™¨ä¸æ”¯æŒ'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'åˆšåˆš',
    min: 'åˆ†é’Ÿå‰',
    hour: 'å°æ—¶å‰',
    day: 'å¤©å‰',
    month: 'ä¸ªæœˆå‰'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.13.0/dist/infinitegrid.min.js',
    buttonText: 'åŠ è½½æ›´å¤š'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'LoRA, DPO, KTO ä¸ SFT æŠ€æœ¯è¯¦è§£',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 6.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="é»‘å¤´å‘†é±¼è¿›åŒ–ä¹‹æ—…" type="application/atom+xml">
</head><body><div class="bg-animation" id="web_bg" style="background-image: url(/img/site-bg.jpg);"></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">é»‘å¤´å‘†é±¼è¿›åŒ–ä¹‹æ—…</span></a><a class="nav-page-title" href="/"><span class="site-name">LoRA, DPO, KTO ä¸ SFT æŠ€æœ¯è¯¦è§£</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  è¿”å›é¦–é¡µ</span></span></a></span><div id="menus"></div></nav><div id="post-info"><h1 class="post-title">LoRA, DPO, KTO ä¸ SFT æŠ€æœ¯è¯¦è§£</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">å‘è¡¨äº</span><time class="post-meta-date-created" datetime="2024-10-23T08:26:29.000Z" title="å‘è¡¨äº 2024-10-23 16:26:29">2024-10-23</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">æ›´æ–°äº</span><time class="post-meta-date-updated" datetime="2026-02-20T21:51:54.078Z" title="æ›´æ–°äº 2026-02-21 05:51:54">2026-02-21</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/NLP-Insights/">NLP Insights</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">æµè§ˆé‡:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h3 id="LoRA-DPO-KTO-ä¸-SFT-æŠ€æœ¯è¯¦è§£"><a href="#LoRA-DPO-KTO-ä¸-SFT-æŠ€æœ¯è¯¦è§£" class="headerlink" title="LoRA, DPO, KTO ä¸ SFT æŠ€æœ¯è¯¦è§£"></a><strong>LoRA, DPO, KTO ä¸ SFT æŠ€æœ¯è¯¦è§£</strong></h3><p>æœ¬ç¯‡æ–‡æ¡£å°†è¯¦ç»†ä»‹ç»å‡ ç§åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ LLAMA3ï¼‰å¾®è°ƒå’Œä¼˜åŒ–ä¸­çš„é‡è¦æŠ€æœ¯ï¼ŒåŒ…æ‹¬ <strong>SFTï¼ˆSupervised Fine-Tuningï¼‰</strong>ã€<strong>LoRAï¼ˆLow-Rank Adaptationï¼‰</strong>ã€<strong>Alignment</strong> æŠ€æœ¯ã€<strong>KTOï¼ˆKahneman-Tversky Optimizationï¼‰</strong> å’Œ <strong>DPOï¼ˆDirect Preference Optimizationï¼‰</strong>ã€‚æ–‡ä¸­è¿˜å°†è¯¦ç»†é˜è¿°æ¯ç§æŠ€æœ¯çš„åŸç†ã€å…·ä½“å®ç°æ–¹æ³•ä»¥åŠç›¸åº”çš„æŸå¤±å‡½æ•°ä¸ä¼˜åŒ–å™¨é€‰æ‹©ã€‚</p>
<hr>
<h2 id="1-SFTï¼ˆSupervised-Fine-Tuningï¼‰"><a href="#1-SFTï¼ˆSupervised-Fine-Tuningï¼‰" class="headerlink" title="1. SFTï¼ˆSupervised Fine-Tuningï¼‰"></a>1. <strong>SFTï¼ˆSupervised Fine-Tuningï¼‰</strong></h2><h3 id="1-1-åŸç†"><a href="#1-1-åŸç†" class="headerlink" title="1.1 åŸç†"></a>1.1 <strong>åŸç†</strong></h3><p>SFT æ˜¯ä¸€ç§ä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•ï¼Œé€šè¿‡ç›‘ç£å­¦ä¹ å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œè°ƒæ•´æ¨¡å‹çš„å‚æ•°ä½¿å…¶åœ¨ç‰¹å®šä»»åŠ¡ä¸Šè¡¨ç°æ›´å¥½ã€‚SFT é€šå¸¸ç”¨äºé’ˆå¯¹ç‰¹å®šçš„æ ‡æ³¨æ•°æ®è¿›è¡Œæ¨¡å‹å¾®è°ƒï¼Œè®­ç»ƒçš„è¿‡ç¨‹ç±»ä¼¼äºå¸¸è§„çš„ç›‘ç£å­¦ä¹ ã€‚</p>
<h3 id="1-2-å®ç°æ–¹æ³•"><a href="#1-2-å®ç°æ–¹æ³•" class="headerlink" title="1.2 å®ç°æ–¹æ³•"></a>1.2 <strong>å®ç°æ–¹æ³•</strong></h3><ul>
<li><strong>é€‰æ‹©é¢„è®­ç»ƒæ¨¡å‹</strong>ï¼šå¦‚ GPTã€BERT ç­‰è¯­è¨€æ¨¡å‹ã€‚</li>
<li><strong>å‡†å¤‡æ ‡æ³¨æ•°æ®é›†</strong>ï¼šæ•°æ®é›†åŒ…å«è¾“å…¥å’Œè¾“å‡ºå¯¹ã€‚</li>
<li><strong>è®­ç»ƒæ¨¡å‹</strong>ï¼šä½¿ç”¨æ ‡å‡†çš„äº¤å‰ç†µæŸå¤±å‡½æ•°å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œé€šè¿‡æ¢¯åº¦ä¸‹é™ä¼˜åŒ–å‚æ•°ã€‚</li>
</ul>
<h3 id="1-3-æ ¸å¿ƒä»£ç "><a href="#1-3-æ ¸å¿ƒä»£ç " class="headerlink" title="1.3 æ ¸å¿ƒä»£ç "></a>1.3 <strong>æ ¸å¿ƒä»£ç </strong></h3><p>ä½¿ç”¨ Hugging Face çš„ <code>Trainer</code> æ¥å£è¿›è¡Œ SFTï¼š</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Trainer, TrainingArguments, AutoModelForSeq2SeqLM, AutoTokenizer</span><br><span class="line"></span><br><span class="line">model_name = <span class="string">"gpt2"</span></span><br><span class="line">model = AutoModelForSeq2SeqLM.from_pretrained(model_name)</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_name)</span><br><span class="line"></span><br><span class="line">train_dataset = load_dataset(<span class="string">"my_dataset"</span>, split=<span class="string">"train"</span>)</span><br><span class="line"></span><br><span class="line">training_args = TrainingArguments(</span><br><span class="line">    output_dir=<span class="string">"./results"</span>,</span><br><span class="line">    evaluation_strategy=<span class="string">"epoch"</span>,</span><br><span class="line">    learning_rate=<span class="number">2e-5</span>,</span><br><span class="line">    per_device_train_batch_size=<span class="number">16</span>,</span><br><span class="line">    num_train_epochs=<span class="number">3</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model=model,</span><br><span class="line">    args=training_args,</span><br><span class="line">    train_dataset=train_dataset,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer.train()</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h2 id="2-LoRAï¼ˆLow-Rank-Adaptationï¼‰"><a href="#2-LoRAï¼ˆLow-Rank-Adaptationï¼‰" class="headerlink" title="2. LoRAï¼ˆLow-Rank Adaptationï¼‰"></a>2. <strong>LoRAï¼ˆLow-Rank Adaptationï¼‰</strong></h2><h3 id="2-1-åŸç†"><a href="#2-1-åŸç†" class="headerlink" title="2.1 åŸç†"></a>2.1 <strong>åŸç†</strong></h3><p>LoRA æ˜¯ä¸€ç§å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæŠ€æœ¯ï¼Œé€šè¿‡å¯¹å¤§æ¨¡å‹ä¸­çš„æƒé‡çŸ©é˜µè¿›è¡Œä½ç§©åˆ†è§£ï¼Œå°†åŸå§‹æƒé‡çŸ©é˜µ $W$ åˆ†è§£ä¸ºä¸¤ä¸ªä½ç§©çŸ©é˜µ $B$ å’Œ $A$ï¼Œå¹¶ä»…å¯¹è¿™äº›ä½ç§©çŸ©é˜µè¿›è¡Œå¾®è°ƒã€‚LoRA çš„è®¾è®¡ç›®æ ‡æ˜¯å‡å°‘å¾®è°ƒå‚æ•°çš„æ•°é‡ï¼Œåœ¨ä¿ç•™é¢„è®­ç»ƒæ¨¡å‹æƒé‡çš„åŒæ—¶ï¼Œé€šè¿‡è°ƒæ•´ä½ç§©çŸ©é˜µæ¥ä¼˜åŒ–æ¨¡å‹è¡¨ç°ã€‚</p>
<h3 id="2-2-å®ç°æ–¹æ³•"><a href="#2-2-å®ç°æ–¹æ³•" class="headerlink" title="2.2 å®ç°æ–¹æ³•"></a>2.2 <strong>å®ç°æ–¹æ³•</strong></h3><ul>
<li><strong>æƒé‡åˆ†è§£</strong>ï¼šå¯¹äºæ¨¡å‹çš„çº¿æ€§å±‚ï¼ˆå¦‚æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„ <code>q_proj</code> å’Œ <code>v_proj</code> å±‚ï¼‰ï¼Œå°†æƒé‡çŸ©é˜µåˆ†è§£ä¸ºä¸¤ä¸ªä½ç§©çŸ©é˜µ $B$ å’Œ $A$ã€‚</li>
<li><strong>å¾®è°ƒç‰¹å®šå±‚</strong>ï¼šä»…å¯¹è¿™äº›ç‰¹å®šçš„çº¿æ€§å±‚åº”ç”¨ LoRAï¼Œè€Œæ¨¡å‹ä¸­çš„å…¶ä»–å±‚ä¿æŒä¸å˜ã€‚</li>
</ul>
<h3 id="2-3-å¯å¾®è°ƒçš„å±‚ä¸ä¸å˜çš„å±‚"><a href="#2-3-å¯å¾®è°ƒçš„å±‚ä¸ä¸å˜çš„å±‚" class="headerlink" title="2.3 å¯å¾®è°ƒçš„å±‚ä¸ä¸å˜çš„å±‚"></a>2.3 <strong>å¯å¾®è°ƒçš„å±‚ä¸ä¸å˜çš„å±‚</strong></h3><h4 id="å¯å¾®è°ƒçš„å±‚"><a href="#å¯å¾®è°ƒçš„å±‚" class="headerlink" title="å¯å¾®è°ƒçš„å±‚"></a><strong>å¯å¾®è°ƒçš„å±‚</strong></h4><p>LoRA é€šå¸¸åº”ç”¨äº Transformer æ¨¡å‹ä¸­çš„çº¿æ€§æŠ•å½±å±‚ï¼Œå°¤å…¶æ˜¯å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„å‡ ä¸ªå…³é”®å±‚ï¼š</p>
<ul>
<li><strong>q_proj</strong>ï¼ˆQuery æŠ•å½±å±‚ï¼‰</li>
<li><strong>k_proj</strong>ï¼ˆKey æŠ•å½±å±‚ï¼‰</li>
<li><strong>v_proj</strong>ï¼ˆValue æŠ•å½±å±‚ï¼‰</li>
<li><strong>o_proj</strong>ï¼ˆOutput æŠ•å½±å±‚ï¼‰</li>
<li><strong>ffn_up_proj</strong> å’Œ <strong>ffn_down_proj</strong>ï¼ˆå‰é¦ˆç¥ç»ç½‘ç»œçš„ä¸Šä¸‹æŠ•å½±å±‚ï¼‰</li>
</ul>
<h4 id="ä¸å˜çš„å±‚"><a href="#ä¸å˜çš„å±‚" class="headerlink" title="ä¸å˜çš„å±‚"></a><strong>ä¸å˜çš„å±‚</strong></h4><ul>
<li><strong>Embedding å±‚</strong>ï¼šè´Ÿè´£è¾“å…¥å’Œè¾“å‡ºçš„ç¼–ç ï¼Œé€šå¸¸ä¸éœ€è¦å¾®è°ƒã€‚</li>
<li><strong>LayerNorm å±‚</strong>ï¼šè¿™äº›å±‚ä¸»è¦ç”¨äºå½’ä¸€åŒ–ï¼Œä¸å«å¤§é‡å‚æ•°ï¼Œé€šå¸¸ä¿æŒä¸å˜ã€‚</li>
<li><strong>æ¿€æ´»å‡½æ•°å±‚</strong>ï¼šå¦‚ ReLU æˆ– GELU ç­‰éçº¿æ€§æ¿€æ´»å‡½æ•°ä¸æ¶‰åŠå‚æ•°ï¼Œä¸éœ€è¦è¿›è¡Œå¾®è°ƒã€‚</li>
</ul>
<h3 id="2-4-æŸå¤±å‡½æ•°"><a href="#2-4-æŸå¤±å‡½æ•°" class="headerlink" title="2.4 æŸå¤±å‡½æ•°"></a>2.4 <strong>æŸå¤±å‡½æ•°</strong></h3><p>LoRA çš„æŸå¤±å‡½æ•°é€šå¸¸ä¸å…·ä½“ä»»åŠ¡ç›¸å…³ã€‚åœ¨è¯­è¨€ç”Ÿæˆä»»åŠ¡ä¸­ï¼ŒLoRA ä½¿ç”¨<strong>äº¤å‰ç†µæŸå¤±</strong>æ¥åº¦é‡ç”Ÿæˆæ–‡æœ¬å’Œç›®æ ‡æ–‡æœ¬ä¹‹é—´çš„å·®å¼‚ï¼š</p>
<p>$$<br>\mathcal{L}<em>{\text{LoRA}} = - \sum</em>{i} y_i \log(\hat{y}_i)<br>$$</p>
<p>å…¶ä¸­ $y_i$ æ˜¯çœŸå®æ ‡ç­¾ï¼Œ$\hat{y}_i$ æ˜¯æ¨¡å‹çš„è¾“å‡ºæ¦‚ç‡ã€‚</p>
<h3 id="2-5-ä¼˜åŒ–å™¨"><a href="#2-5-ä¼˜åŒ–å™¨" class="headerlink" title="2.5 ä¼˜åŒ–å™¨"></a>2.5 <strong>ä¼˜åŒ–å™¨</strong></h3><p>LoRA å¾®è°ƒé€šå¸¸ä½¿ç”¨ <strong>AdamW</strong> ä¼˜åŒ–å™¨ï¼Œå…·ä½“ä»£ç å¦‚ä¸‹ï¼š</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.AdamW(lora_model.parameters(), lr=<span class="number">5e-5</span>)</span><br></pre></td></tr></tbody></table></figure>

<h3 id="2-6-æ ¸å¿ƒä»£ç "><a href="#2-6-æ ¸å¿ƒä»£ç " class="headerlink" title="2.6 æ ¸å¿ƒä»£ç "></a>2.6 <strong>æ ¸å¿ƒä»£ç </strong></h3><p>ä½¿ç”¨ <code>peft</code> åº“å®ç° LoRAï¼š</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> peft <span class="keyword">import</span> LoraConfig, get_peft_model</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForSeq2SeqLM</span><br><span class="line"></span><br><span class="line">model_name = <span class="string">"gpt2"</span></span><br><span class="line">model = AutoModelForSeq2SeqLM.from_pretrained(model_name)</span><br><span class="line"></span><br><span class="line">lora_config = LoraConfig(</span><br><span class="line">    r=<span class="number">8</span>,                </span><br><span class="line">    lora_alpha=<span class="number">32</span>,      </span><br><span class="line">    target_modules=[<span class="string">"q_proj"</span>, <span class="string">"v_proj"</span>],  </span><br><span class="line">    lora_dropout=<span class="number">0.1</span>,   </span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">lora_model = get_peft_model(model, lora_config)</span><br><span class="line">lora_model.train()</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h2 id="3-Alignmentï¼ˆå¯¹é½æŠ€æœ¯ï¼‰"><a href="#3-Alignmentï¼ˆå¯¹é½æŠ€æœ¯ï¼‰" class="headerlink" title="3. Alignmentï¼ˆå¯¹é½æŠ€æœ¯ï¼‰"></a>3. <strong>Alignmentï¼ˆå¯¹é½æŠ€æœ¯ï¼‰</strong></h2><p>åœ¨å¼•å…¥KLæ•£åº¦ä¹‹å‰ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦æ˜ç¡®LLMå¯¹é½ï¼ˆAlignmentï¼‰æ˜¯å¦‚ä½•å®ç°çš„ï¼Œä»¥åŠèƒŒåçš„åŸç†å’Œæ•°å­¦å…¬å¼ã€‚</p>
<h3 id="1-ä»€ä¹ˆæ˜¯æ¨¡å‹å¯¹é½ï¼ˆAlignmentï¼‰ï¼Ÿ"><a href="#1-ä»€ä¹ˆæ˜¯æ¨¡å‹å¯¹é½ï¼ˆAlignmentï¼‰ï¼Ÿ" class="headerlink" title="1. ä»€ä¹ˆæ˜¯æ¨¡å‹å¯¹é½ï¼ˆAlignmentï¼‰ï¼Ÿ"></a><strong>1. ä»€ä¹ˆæ˜¯æ¨¡å‹å¯¹é½ï¼ˆAlignmentï¼‰ï¼Ÿ</strong></h3><p>æ¨¡å‹å¯¹é½çš„æ ¸å¿ƒç›®æ ‡æ˜¯è®©è¯­è¨€æ¨¡å‹çš„è¾“å‡ºç¬¦åˆäººç±»çš„æœŸæœ›æˆ–åå¥½ã€‚é€šå¸¸ï¼Œæ¨¡å‹æœ€åˆé€šè¿‡å¤§è§„æ¨¡ç›‘ç£å­¦ä¹ ï¼ˆSFTï¼ŒSupervised Fine-Tuningï¼‰è®­ç»ƒï¼Œç”Ÿæˆå…·æœ‰åŸºç¡€èƒ½åŠ›çš„æ¨¡å‹ã€‚æ¥ä¸‹æ¥ï¼Œé€šè¿‡å¯¹é½æŠ€æœ¯ï¼Œè¿›ä¸€æ­¥è°ƒæ•´æ¨¡å‹ï¼Œä½¿å…¶ç”Ÿæˆçš„å†…å®¹æ›´ç¬¦åˆäººç±»åå¥½æˆ–é¿å…äº§ç”Ÿæœ‰å®³ã€é”™è¯¯çš„ä¿¡æ¯ã€‚</p>
<p><strong>å¯¹é½çš„æ ¸å¿ƒæœºåˆ¶</strong>ï¼š</p>
<ul>
<li><strong>æ­£æ ·æœ¬</strong>ï¼šç¬¦åˆäººç±»é¢„æœŸçš„è¾“å‡ºï¼ˆå¦‚æ­£ç¡®å›ç­”ï¼‰ã€‚</li>
<li><strong>è´Ÿæ ·æœ¬</strong>ï¼šä¸ç¬¦åˆäººç±»é¢„æœŸçš„è¾“å‡ºï¼ˆå¦‚é”™è¯¯å›ç­”ï¼‰ã€‚</li>
</ul>
<p>é€šè¿‡ä½¿ç”¨æˆå¯¹åå¥½æ•°æ®æˆ–æ ‡ç­¾ï¼ˆæ­£ç¡®/é”™è¯¯ï¼‰ï¼Œå¯¹æ¨¡å‹çš„è¾“å‡ºè¿›è¡Œè¿›ä¸€æ­¥å¾®è°ƒï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿç”Ÿæˆæ›´å¤šçš„æ­£æ ·æœ¬ï¼ŒåŒæ—¶å‡å°‘è´Ÿæ ·æœ¬çš„ç”Ÿæˆæ¦‚ç‡ã€‚</p>
<hr>
<h3 id="2-æ¨¡å‹å¯¹é½çš„æ•°å­¦åŸç†"><a href="#2-æ¨¡å‹å¯¹é½çš„æ•°å­¦åŸç†" class="headerlink" title="2. æ¨¡å‹å¯¹é½çš„æ•°å­¦åŸç†"></a><strong>2. æ¨¡å‹å¯¹é½çš„æ•°å­¦åŸç†</strong></h3><p>åœ¨å¯¹é½è¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹ä¼šé€šè¿‡<strong>ç­–ç•¥æ¨¡å‹</strong>ï¼ˆPolicy Modelï¼‰æ¥ç”Ÿæˆè¾“å‡ºï¼Œç­–ç•¥æ¨¡å‹é€šå¸¸æ˜¯ç»è¿‡SFTè®­ç»ƒçš„è¯­è¨€æ¨¡å‹ï¼Œç”¨æ¥åœ¨ç»™å®šè¾“å…¥ä¸‹ç”Ÿæˆè¾“å‡ºã€‚ä¸ºäº†ä¼˜åŒ–æ¨¡å‹çš„è¾“å‡ºï¼Œä½¿å…¶æ›´åŠ ç¬¦åˆäººç±»åå¥½ï¼Œå¸¸å¸¸ä½¿ç”¨ä»¥ä¸‹æŸå¤±å‡½æ•°å’Œä¼˜åŒ–æ–¹æ³•ï¼š</p>
<h4 id="2-1-ç­–ç•¥æ¨¡å‹"><a href="#2-1-ç­–ç•¥æ¨¡å‹" class="headerlink" title="2.1 ç­–ç•¥æ¨¡å‹"></a><strong>2.1 ç­–ç•¥æ¨¡å‹</strong></h4><p>å‡è®¾å½“å‰æ¨¡å‹çš„ç­–ç•¥ä¸º $\pi_\theta$ï¼Œå®ƒè¡¨ç¤ºåœ¨ç»™å®šè¾“å…¥ $x$ æ—¶ï¼Œæ¨¡å‹ç”Ÿæˆè¾“å‡º $y$ çš„æ¦‚ç‡ï¼š<br>$$<br>\pi_\theta(y|x)<br>$$<br>ç­–ç•¥æ¨¡å‹çš„ç›®æ ‡æ˜¯é€šè¿‡è°ƒæ•´å‚æ•° $\theta$ï¼Œæé«˜ç”Ÿæˆæ­£ç¡®è¾“å‡ºï¼ˆæ­£æ ·æœ¬ï¼‰çš„æ¦‚ç‡ï¼Œé™ä½ç”Ÿæˆé”™è¯¯è¾“å‡ºï¼ˆè´Ÿæ ·æœ¬ï¼‰çš„æ¦‚ç‡ã€‚</p>
<h4 id="2-2-æé«˜æ­£æ ·æœ¬æ¦‚ç‡ä¸é™ä½è´Ÿæ ·æœ¬æ¦‚ç‡çš„æœºåˆ¶"><a href="#2-2-æé«˜æ­£æ ·æœ¬æ¦‚ç‡ä¸é™ä½è´Ÿæ ·æœ¬æ¦‚ç‡çš„æœºåˆ¶" class="headerlink" title="2.2 æé«˜æ­£æ ·æœ¬æ¦‚ç‡ä¸é™ä½è´Ÿæ ·æœ¬æ¦‚ç‡çš„æœºåˆ¶"></a><strong>2.2 æé«˜æ­£æ ·æœ¬æ¦‚ç‡ä¸é™ä½è´Ÿæ ·æœ¬æ¦‚ç‡çš„æœºåˆ¶</strong></h4><p>ä¸ºäº†å®ç°è¿™ä¸ªç›®æ ‡ï¼Œé€šå¸¸ä½¿ç”¨å¸¦æœ‰åå¥½æ¯”è¾ƒæˆ–æ ‡ç­¾çš„æŸå¤±å‡½æ•°è¿›è¡Œä¼˜åŒ–ï¼š</p>
<ol>
<li><p><strong>æ­£æ ·æœ¬çš„ä¼˜åŒ–</strong>ï¼šé€šè¿‡å¢åŠ æ­£æ ·æœ¬çš„æŸå¤±æƒé‡ï¼Œä½¿å¾—æ¨¡å‹ç”Ÿæˆæ­£æ ·æœ¬çš„æ¦‚ç‡æ›´é«˜ã€‚</p>
<ul>
<li>æ­£æ ·æœ¬çš„æŸå¤±å‡½æ•°ä¼šå¼•å¯¼æ¨¡å‹åœ¨é¢å¯¹ç›¸åŒé—®é¢˜æ—¶ï¼Œç”Ÿæˆæ›´å¤šç¬¦åˆäººç±»æœŸæœ›çš„ç­”æ¡ˆã€‚</li>
</ul>
</li>
<li><p><strong>è´Ÿæ ·æœ¬çš„æƒ©ç½š</strong>ï¼šå¯¹è´Ÿæ ·æœ¬æ–½åŠ æ›´é«˜çš„æŸå¤±æƒé‡ï¼Œæ¨¡å‹ä¼šå­¦ä¹ åˆ°å‡å°‘è¿™äº›é”™è¯¯è¾“å‡ºçš„æ¦‚ç‡ã€‚</p>
<ul>
<li>è´Ÿæ ·æœ¬çš„æŸå¤±å‡½æ•°æ—¨åœ¨è®©æ¨¡å‹åœ¨ç”Ÿæˆé”™è¯¯ç­”æ¡ˆæ—¶æ„ŸçŸ¥åˆ°æ›´å¤§çš„æƒ©ç½šï¼Œä»è€Œå‡å°‘è¿™äº›è¾“å‡ºçš„ç”Ÿæˆã€‚</li>
</ul>
</li>
</ol>
<p>åœ¨æŸäº›æ–¹æ³•ä¸­ï¼Œä¾‹å¦‚DPOå’ŒKTOï¼Œè¿˜ä¼šé€šè¿‡è®¡ç®—å½“å‰ç­–ç•¥æ¨¡å‹ä¸å‚è€ƒæ¨¡å‹ä¹‹é—´çš„<strong>KLæ•£åº¦</strong>ï¼Œæ¥é˜²æ­¢æ¨¡å‹åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­è¿‡åº¦åç¦»åŸå§‹é¢„è®­ç»ƒæ¨¡å‹ã€‚</p>
<hr>
<h3 id="3-æŸå¤±å‡½æ•°ä¸KLæ•£åº¦çš„ä½œç”¨"><a href="#3-æŸå¤±å‡½æ•°ä¸KLæ•£åº¦çš„ä½œç”¨" class="headerlink" title="3. æŸå¤±å‡½æ•°ä¸KLæ•£åº¦çš„ä½œç”¨"></a><strong>3. æŸå¤±å‡½æ•°ä¸KLæ•£åº¦çš„ä½œç”¨</strong></h3><p>åœ¨æ¨¡å‹å¯¹é½çš„è¿‡ç¨‹ä¸­ï¼ŒæŸå¤±å‡½æ•°é€šå¸¸åŒ…å«ä¸¤éƒ¨åˆ†ï¼š</p>
<ol>
<li><strong>åå¥½æŸå¤±</strong>æˆ–<strong>æ ‡ç­¾æŸå¤±</strong>ï¼Œç”¨äºä¼˜åŒ–æ¨¡å‹ç”Ÿæˆç¬¦åˆäººç±»æœŸæœ›çš„è¾“å‡ºã€‚</li>
<li><strong>KLæ•£åº¦</strong>ï¼Œç”¨äºçº¦æŸæ¨¡å‹ä¸è¦åç¦»å‚è€ƒæ¨¡å‹ã€‚</li>
</ol>
<h4 id="3-1-KLæ•£åº¦çš„ä½œç”¨"><a href="#3-1-KLæ•£åº¦çš„ä½œç”¨" class="headerlink" title="3.1 KLæ•£åº¦çš„ä½œç”¨"></a><strong>3.1 KLæ•£åº¦çš„ä½œç”¨</strong></h4><p>KLæ•£åº¦ï¼ˆKullback-Leibler Divergenceï¼‰è¡¡é‡çš„æ˜¯ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„å·®å¼‚ã€‚åœ¨æ¨¡å‹å¯¹é½ä¸­ï¼ŒKLæ•£åº¦ç”¨äºé™åˆ¶å½“å‰æ¨¡å‹ \(\pi_\theta\) å’Œå‚è€ƒæ¨¡å‹ \(\pi_{\text{ref}}\) çš„åˆ†å¸ƒå·®å¼‚ï¼Œç¡®ä¿åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­æ¨¡å‹çš„è¾“å‡ºä¸ä¼šè¿‡åº¦åç¦»é¢„è®­ç»ƒæ¨¡å‹ã€‚å…·ä½“å…¬å¼ä¸ºï¼š<br>$$<br>\text{KL}(\pi_\theta(y|x) | \pi_{\text{ref}}(y|x)) = \sum_y \pi_\theta(y|x) \log \frac{\pi_\theta(y|x)}{\pi_{\text{ref}}(y|x)}<br>$$</p>
<ul>
<li>å¦‚æœKLæ•£åº¦è¾ƒå¤§ï¼Œè¡¨ç¤ºå½“å‰æ¨¡å‹ç”Ÿæˆçš„åˆ†å¸ƒä¸å‚è€ƒæ¨¡å‹æœ‰è¾ƒå¤§çš„å·®å¼‚ï¼Œè¿™å¯èƒ½æ„å‘³ç€æ¨¡å‹ç”Ÿæˆäº†ä¸åˆç†çš„è¾“å‡ºã€‚</li>
<li>é€šè¿‡æœ€å°åŒ–KLæ•£åº¦ï¼Œæ¨¡å‹èƒ½å¤Ÿåœ¨ä¿è¯è¾“å‡ºåˆç†æ€§çš„åŸºç¡€ä¸Šï¼Œè¿›è¡Œè¿›ä¸€æ­¥çš„ä¼˜åŒ–ã€‚</li>
</ul>
<h4 id="3-2-æŸå¤±å‡½æ•°å…¬å¼"><a href="#3-2-æŸå¤±å‡½æ•°å…¬å¼" class="headerlink" title="3.2 æŸå¤±å‡½æ•°å…¬å¼"></a><strong>3.2 æŸå¤±å‡½æ•°å…¬å¼</strong></h4><p>æ ¹æ®åå¥½æˆ–æ ‡ç­¾ï¼Œæ¨¡å‹çš„æŸå¤±å‡½æ•°å¯ä»¥è¡¨è¾¾ä¸ºä»¥ä¸‹å½¢å¼ï¼š</p>
<h5 id="DPOä¸­çš„æŸå¤±å‡½æ•°ï¼š"><a href="#DPOä¸­çš„æŸå¤±å‡½æ•°ï¼š" class="headerlink" title="DPOä¸­çš„æŸå¤±å‡½æ•°ï¼š"></a><strong>DPOä¸­çš„æŸå¤±å‡½æ•°</strong>ï¼š</h5><p>$$<br>L_{DPO} = -\mathbb{E}_{x, y_w, y_l \sim D} [\log \sigma(\beta (\log(\pi_\theta(y_w|x)) - \log(\pi_\theta(y_l|x))))]<br>$$</p>
<ul>
<li>$y_w$ï¼šåå¥½è¾ƒé«˜çš„ç­”æ¡ˆã€‚</li>
<li>$y_l$ï¼šåå¥½è¾ƒä½çš„ç­”æ¡ˆã€‚</li>
</ul>
<p>DPOä¸­å¯ä»¥å¼•å…¥KLæ•£åº¦ä½œä¸ºæ­£åˆ™åŒ–é¡¹ï¼š<br>$$<br>L_{DPO} = -\log \sigma(\log(\pi_\theta(y_w|x)) - \log(\pi_\theta(y_l|x)) + \beta \cdot \text{KL}(\pi_\theta | \pi_{\text{ref}}))<br>$$<br>é€šè¿‡æ§åˆ¶KLæ•£åº¦ï¼Œæ¨¡å‹çš„è¾“å‡ºä¸ä¼šåç¦»å‚è€ƒæ¨¡å‹å¤ªå¤šã€‚</p>
<h5 id="KTOä¸­çš„æŸå¤±å‡½æ•°ï¼š"><a href="#KTOä¸­çš„æŸå¤±å‡½æ•°ï¼š" class="headerlink" title="KTOä¸­çš„æŸå¤±å‡½æ•°ï¼š"></a><strong>KTOä¸­çš„æŸå¤±å‡½æ•°</strong>ï¼š</h5><p>KTOçš„æŸå¤±å‡½æ•°åŸºäºå‰æ™¯ç†è®ºï¼Œå¹¶å°†KLæ•£åº¦ä½œä¸ºæ ¸å¿ƒéƒ¨åˆ†ï¼Œè¡¨è¾¾ä¸ºï¼š<br>$$<br>L_{KTO} = \lambda_U \cdot \sigma(\beta \cdot (\text{KL}(\pi_\theta(\text{Answer 2}) | \pi_{\text{ref}}) - r_{\theta}(\text{Answer 2})))<br>$$</p>
<ul>
<li>$r_{\theta}(x, y)$ï¼šå½“å‰ç­–ç•¥å¯¹è´Ÿæ ·æœ¬ï¼ˆé”™è¯¯ç­”æ¡ˆï¼‰çš„ç½®ä¿¡åº¦ã€‚</li>
<li>KLæ•£åº¦ç”¨äºè¡¡é‡å½“å‰æ¨¡å‹ä¸å‚è€ƒæ¨¡å‹çš„å·®å¼‚ï¼Œç¡®ä¿æ¨¡å‹åœ¨å‡å°‘è´Ÿæ ·æœ¬ç”Ÿæˆçš„åŒæ—¶ï¼Œä¸åç¦»åŸå§‹å‚è€ƒæ¨¡å‹ã€‚</li>
</ul>
<p>é€šè¿‡å¢åŠ è´Ÿæ ·æœ¬çš„æŸå¤±ï¼ˆå³å¢åŠ  $\lambda_U$ çš„å€¼ï¼‰ï¼Œæ¨¡å‹ä¼šé™ä½è´Ÿæ ·æœ¬çš„ç½®ä¿¡åº¦ï¼Œä½¿æœªæ¥ç”Ÿæˆç±»ä¼¼é”™è¯¯ç­”æ¡ˆçš„æ¦‚ç‡å˜å°ã€‚</p>
<hr>
<h3 id="4-å¦‚ä½•ä¼˜åŒ–æ¨¡å‹"><a href="#4-å¦‚ä½•ä¼˜åŒ–æ¨¡å‹" class="headerlink" title="4. å¦‚ä½•ä¼˜åŒ–æ¨¡å‹"></a><strong>4. å¦‚ä½•ä¼˜åŒ–æ¨¡å‹</strong></h3><p>é€šè¿‡ä¸Šé¢ä»‹ç»çš„æŸå¤±å‡½æ•°ï¼Œæ¨¡å‹çš„ä¼˜åŒ–é€šå¸¸æ˜¯é€šè¿‡<strong>æ¢¯åº¦ä¸‹é™</strong>ï¼ˆGradient Descentï¼‰æ¥å®Œæˆçš„ã€‚æŸå¤±å‡½æ•°çš„æ¢¯åº¦åæ˜ äº†æ¨¡å‹è¾“å‡ºä¸æœŸæœ›è¾“å‡ºä¹‹é—´çš„å·®å¼‚ï¼Œä¼˜åŒ–ç›®æ ‡æ˜¯æœ€å°åŒ–æŸå¤±å‡½æ•°ã€‚</p>
<p><strong>æ¢¯åº¦æ›´æ–°å…¬å¼</strong>ï¼š<br>$$<br>\theta_{\text{new}} = \theta_{\text{old}} - \eta \nabla_{\theta} L<br>$$<br>å…¶ä¸­ï¼š</p>
<ul>
<li>$\eta$ æ˜¯å­¦ä¹ ç‡ï¼Œå†³å®šæ¯æ¬¡å‚æ•°æ›´æ–°çš„æ­¥é•¿ã€‚</li>
<li>$\nabla_{\theta} L$ æ˜¯æŸå¤±å‡½æ•°å¯¹æ¨¡å‹å‚æ•°çš„æ¢¯åº¦ï¼Œè¡¨ç¤ºå½“å‰å‚æ•°å¯¹æŸå¤±çš„è´¡çŒ®ã€‚</li>
</ul>
<p>é€šè¿‡ä¸æ–­è¿­ä»£ï¼Œæ¨¡å‹ä¼šé€æ¸æé«˜ç”Ÿæˆæ­£æ ·æœ¬çš„æ¦‚ç‡ï¼Œå‡å°‘è´Ÿæ ·æœ¬çš„ç”Ÿæˆæ¦‚ç‡ï¼Œæœ€ç»ˆå®ç°æ¨¡å‹å¯¹é½ã€‚</p>
<ul>
<li>æ¨¡å‹å¯¹é½ï¼ˆAlignmentï¼‰çš„æ ¸å¿ƒç›®æ ‡æ˜¯é€šè¿‡åå¥½æˆ–æ ‡ç­¾æ•°æ®ï¼Œä¼˜åŒ–æ¨¡å‹çš„è¾“å‡ºï¼Œä½¿å…¶ç¬¦åˆäººç±»æœŸæœ›ã€‚</li>
<li><strong>ç­–ç•¥æ¨¡å‹</strong>ï¼ˆ$\pi_\theta$ï¼‰ç”Ÿæˆè¾“å‡ºï¼ŒKLæ•£åº¦ç”¨äºæ§åˆ¶æ¨¡å‹ä¸å‚è€ƒæ¨¡å‹çš„åç¦»ç¨‹åº¦ï¼Œé¿å…æ¨¡å‹åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­äº§ç”Ÿä¸åˆç†çš„åå·®ã€‚</li>
<li><strong>æ­£æ ·æœ¬çš„æ¦‚ç‡</strong>é€šè¿‡æŸå¤±å‡½æ•°çš„ä¼˜åŒ–é€æ­¥æå‡ï¼Œ<strong>è´Ÿæ ·æœ¬çš„æ¦‚ç‡</strong>é€šè¿‡å¢åŠ æŸå¤±æƒé‡å’Œé™ä½ç½®ä¿¡åº¦æ¥å‡å°‘ã€‚</li>
<li>æ¢¯åº¦ä¸‹é™ç”¨äºæ›´æ–°æ¨¡å‹å‚æ•°ï¼Œæœ€ç»ˆå®ç°æ¨¡å‹å¯¹é½</li>
</ul>
<hr>
<h2 id="4-DPOï¼ˆDirect-Preference-Optimizationï¼‰"><a href="#4-DPOï¼ˆDirect-Preference-Optimizationï¼‰" class="headerlink" title="4. DPOï¼ˆDirect Preference Optimizationï¼‰"></a>4. <strong>DPOï¼ˆDirect Preference Optimizationï¼‰</strong></h2><h3 id="4-1-åŸç†"><a href="#4-1-åŸç†" class="headerlink" title="4.1 åŸç†"></a>4.1 <strong>åŸç†</strong></h3><p>DPO é€šè¿‡ç›´æ¥ä¼˜åŒ–æ¨¡å‹è¾“å‡ºçš„åå¥½å‡½æ•°ï¼Œä½¿æ¨¡å‹çš„è¾“å‡ºæ›´åŠ ç¬¦åˆäººç±»åå¥½ã€‚å®ƒæ¯”è¾ƒæ¨¡å‹çš„ä¸åŒè¾“å‡ºï¼Œå¹¶é€šè¿‡åå¥½å‡½æ•°è¯„ä¼°è¿™ä¸¤ä¸ªè¾“å‡ºå“ªä¸ªæ›´å¥½ï¼Œä»è€ŒæŒ‡å¯¼æ¨¡å‹å‚æ•°çš„ä¼˜åŒ–ã€‚</p>
<h3 id="4-2-æŸå¤±å‡½æ•°"><a href="#4-2-æŸå¤±å‡½æ•°" class="headerlink" title="4.2 æŸå¤±å‡½æ•°"></a>4.2 <strong>æŸå¤±å‡½æ•°</strong></h3><p>DPO ä½¿ç”¨åå¥½æŸå¤±å‡½æ•°ï¼ˆPreference Lossï¼‰ï¼Œç”¨äºæ¯”è¾ƒä¸¤ä¸ªè¾“å‡ºçš„ä¼˜åŠ£ï¼š</p>
<p>$$<br>\mathcal{L}_{\text{DPO}} = \log(1 + \exp(-\sigma \cdot (\hat{y}_a - \hat{y}_b) \cdot p))<br>$$</p>
<ul>
<li>$ \hat{y}_a $ å’Œ $ \hat{y}_b $ æ˜¯æ¨¡å‹å¯¹ä¸¤ä¸ªæ ·æœ¬çš„é¢„æµ‹å€¼ã€‚</li>
<li>$ p $ æ˜¯äººç±»åå¥½ï¼ˆ1 è¡¨ç¤ºåå¥½ $a$ï¼Œ-1 è¡¨ç¤ºåå¥½ $b$ï¼‰ã€‚</li>
<li>$ \sigma $ æ˜¯å¹³æ»‘å‚æ•°ã€‚</li>
</ul>
<h3 id="4-3-ä¼˜åŒ–å™¨"><a href="#4-3-ä¼˜åŒ–å™¨" class="headerlink" title="4.3 ä¼˜åŒ–å™¨"></a>4.3 <strong>ä¼˜åŒ–å™¨</strong></h3><p>DPO é€šå¸¸ä½¿ç”¨ <strong>AdamW</strong> ä¼˜åŒ–å™¨ï¼Œé€‚ç”¨äºå¤§è§„æ¨¡å‚æ•°æ¨¡å‹çš„ä¼˜åŒ–ï¼Œä»£ç å¦‚ä¸‹ï¼š</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.AdamW(model.parameters(), lr=<span class="number">1e-5</span>)</span><br></pre></td></tr></tbody></table></figure>

<h3 id="4-4-æ ¸å¿ƒä»£ç "><a href="#4-4-æ ¸å¿ƒä»£ç " class="headerlink" title="4.4 æ ¸å¿ƒä»£ç "></a>4.4 <strong>æ ¸å¿ƒä»£ç </strong></h3><p>ä»¥ä¸‹æ˜¯ DPO çš„è®­ç»ƒæ­¥éª¤ï¼š</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preference_loss</span>(<span class="params">output_a, output_b, human_preference</span>):</span><br><span class="line">    preference = human_preference(output_a, output_b)</span><br><span class="line">    loss = torch.log(<span class="number">1</span> + torch.exp(-preference * (output_a - output_b)))</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dpo_training_step</span>(<span class="params">model, data_a, data_b, human_preference</span>):</span><br><span class="line">    output_a = model(data_a)</span><br><span class="line">    output_b = model(data_b)</span><br><span class="line">    </span><br><span class="line">    loss = preference_loss(output_a, output_b, human_preference)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> batch_a, batch_b <span class="keyword">in</span> training_data:</span><br><span class="line">    dpo_training_step(model, batch_a, batch_b, human_preference_function)</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h2 id="5-KTOï¼ˆKahneman-Tversky-Optimizationï¼‰"><a href="#5-KTOï¼ˆKahneman-Tversky-Optimizationï¼‰" class="headerlink" title="5. KTOï¼ˆKahneman-Tversky Optimizationï¼‰"></a>5. <strong>KTOï¼ˆKahneman-Tversky Optimizationï¼‰</strong></h2><h3 id="5-1-åŸç†"><a href="#5-1-åŸç†" class="headerlink" title="5.1 åŸç†"></a>5.1 <strong>åŸç†</strong></h3><p>KTO åŸºäº Kahneman å’Œ Tversky çš„å‰æ™¯ç†è®ºï¼ˆProspect Theoryï¼‰ï¼Œé€šè¿‡éå¯¹ç§°æ•ˆç”¨å‡½æ•°è¡¡é‡æ¨¡å‹çš„å¢ç›Šå’ŒæŸå¤±ï¼Œæ—¨åœ¨ä¼˜åŒ–æ¨¡å‹çš„è¡¨ç°ï¼Œå°¤å…¶åœ¨é£é™©å’Œæ”¶ç›Šä¸å¯¹ç§°çš„åœºæ™¯ä¸‹ã€‚æ•ˆç”¨å‡½æ•°å®šä¹‰å¦‚ä¸‹ï¼š</p>
<p>$$<br>\mathcal{U}(x) =<br>\begin{cases}<br>x^{\alpha}, &amp; x \geq 0 \<br>-\lambda (-x)^{\alpha}, &amp; x &lt; 0<br>\end{cases}<br>$$</p>
<ul>
<li>$x$ æ˜¯æ¨¡å‹é¢„æµ‹ä¸çœŸå®å€¼çš„å·®å¼‚ã€‚</li>
<li>$\alpha$ æ˜¯éçº¿æ€§ç³»æ•°ï¼Œé€šå¸¸ä¸º 0</li>
</ul>
<p>.88ã€‚</p>
<ul>
<li>$\lambda$ æ˜¯æŸå¤±çš„æƒ©ç½šæƒé‡ï¼Œé€šå¸¸ä¸º 2.25ã€‚</li>
</ul>
<h3 id="5-2-æŸå¤±å‡½æ•°"><a href="#5-2-æŸå¤±å‡½æ•°" class="headerlink" title="5.2 æŸå¤±å‡½æ•°"></a>5.2 <strong>æŸå¤±å‡½æ•°</strong></h3><p>KTO çš„æŸå¤±å‡½æ•°åŸºäºå‰æ™¯ç†è®ºçš„æ•ˆç”¨å‡½æ•°ï¼Œç”¨äºæƒ©ç½šæ¨¡å‹çš„é¢„æµ‹è¯¯å·®ï¼š</p>
<p>$$<br>\mathcal{L}<em>{\text{KTO}} = -\mathbb{E}[\mathcal{U}(y</em>{\text{pred}} - y_{\text{true}})]<br>$$</p>
<h3 id="5-3-ä¼˜åŒ–å™¨"><a href="#5-3-ä¼˜åŒ–å™¨" class="headerlink" title="5.3 ä¼˜åŒ–å™¨"></a>5.3 <strong>ä¼˜åŒ–å™¨</strong></h3><p>KTO å¸¸ä½¿ç”¨ <strong>AdamW</strong> ä¼˜åŒ–å™¨ï¼Œä»¥ç¡®ä¿è®­ç»ƒè¿‡ç¨‹çš„ç¨³å®šæ€§ï¼š</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.AdamW(model.parameters(), lr=<span class="number">3e-5</span>)</span><br></pre></td></tr></tbody></table></figure>

<h3 id="5-4-æ ¸å¿ƒä»£ç "><a href="#5-4-æ ¸å¿ƒä»£ç " class="headerlink" title="5.4 æ ¸å¿ƒä»£ç "></a>5.4 <strong>æ ¸å¿ƒä»£ç </strong></h3><p>ä»¥ä¸‹æ˜¯ KTO æŸå¤±å‡½æ•°çš„è®¡ç®—ä»£ç ï¼š</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">prospect_utility</span>(<span class="params">value, alpha=<span class="number">0.88</span>, lambda_=<span class="number">2.25</span></span>):</span><br><span class="line">    <span class="keyword">if</span> value &gt;= <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> torch.<span class="built_in">pow</span>(value, alpha)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> -lambda_ * torch.<span class="built_in">pow</span>(-value, alpha)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">kto_loss</span>(<span class="params">predictions, targets</span>):</span><br><span class="line">    value = predictions - targets</span><br><span class="line">    utility = prospect_utility(value)</span><br><span class="line">    loss = -torch.mean(utility)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">    predictions = model(data)</span><br><span class="line">    loss = kto_loss(predictions, targets)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h3 id="æ€»ç»“"><a href="#æ€»ç»“" class="headerlink" title="æ€»ç»“"></a><strong>æ€»ç»“</strong></h3><table>
<thead>
<tr>
<th>æ–¹æ³•</th>
<th>æŸå¤±å‡½æ•°</th>
<th>ä¼˜åŒ–å™¨</th>
</tr>
</thead>
<tbody><tr>
<td><strong>SFT</strong></td>
<td>äº¤å‰ç†µæŸå¤±</td>
<td>AdamWï¼ŒRMSpropï¼ŒSGD</td>
</tr>
<tr>
<td><strong>LoRA</strong></td>
<td>äº¤å‰ç†µæŸå¤±</td>
<td>AdamWï¼ŒRMSpropï¼ŒSGD</td>
</tr>
<tr>
<td><strong>DPO</strong></td>
<td>åå¥½æŸå¤±å‡½æ•°ï¼š $\log(1 + \exp(-\sigma (\hat{y}_a - \hat{y}_b)p))$</td>
<td>AdamW</td>
</tr>
<tr>
<td><strong>KTO</strong></td>
<td>å‰æ™¯ç†è®ºæ•ˆç”¨å‡½æ•°ï¼š $-\mathbb{E}[\mathcal{U}(y_{\text{pred}} - y_{\text{true}})]$</td>
<td>AdamW</td>
</tr>
</tbody></table>
<p>é€šè¿‡æœ¬æ–‡æ¡£çš„æ•´ç†ï¼Œè¯»è€…èƒ½å¤Ÿæ¸…æ™°ç†è§£ SFTã€LoRAã€DPO å’Œ KTO ç­‰æŠ€æœ¯çš„åŸç†ã€å…·ä½“å®ç°æ­¥éª¤ã€æŸå¤±å‡½æ•°è®¾è®¡å’Œä¼˜åŒ–å™¨é€‰æ‹©ï¼Œç‰¹åˆ«æ˜¯åœ¨ LLAMA3 è¿™ç§å¤§è§„æ¨¡é¢„è®­ç»ƒæ¨¡å‹çš„å¾®è°ƒåœºæ™¯ä¸‹çš„å®é™…åº”ç”¨ã€‚</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>æ–‡ç« ä½œè€…: </span><span class="post-copyright-info"><a href="https://chenhuiyu.github.io">Huiyu Chen</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>æ–‡ç« é“¾æ¥: </span><span class="post-copyright-info"><a href="https://chenhuiyu.github.io/2024/10/23/NLP%20Insights/LLM%E8%AE%AD%E7%BB%83%E6%9C%AF%E8%AF%AD%E4%BB%8B%E7%BB%8D%EF%BC%9ASFT%E3%80%81LoRA%E3%80%81Alignment%E3%80%81KTO%E5%92%8CDPO/">https://chenhuiyu.github.io/2024/10/23/NLP%20Insights/LLM%E8%AE%AD%E7%BB%83%E6%9C%AF%E8%AF%AD%E4%BB%8B%E7%BB%8D%EF%BC%9ASFT%E3%80%81LoRA%E3%80%81Alignment%E3%80%81KTO%E5%92%8CDPO/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>ç‰ˆæƒå£°æ˜: </span><span class="post-copyright-info">æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ«å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨ <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº <a href="https://chenhuiyu.github.io" target="_blank">é»‘å¤´å‘†é±¼è¿›åŒ–ä¹‹æ—…</a>ï¼</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/LLM/">LLM</a></div><div class="post-share"><div class="social-share" data-image="/img/butterfly-icon.png" data-sites="facebook,x,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2024/10/23/NLP%20Insights/Introduction%20to%20LLM%20Training%20Terminology:%20LoRA,%20DPO,%20KTO,%20and%20SFT%20Technologies.en/" title="Detailed Explanation of LoRA, DPO, KTO, and SFT Technologies"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">ä¸Šä¸€ç¯‡</div><div class="info-item-2">Detailed Explanation of LoRA, DPO, KTO, and SFT Technologies</div></div><div class="info-2"><div class="info-item-1">Introduction to LLM Training Terminology:LoRA, DPO, KTO, and SFT TechnologiesThis document provides a detailed introduction to several important techniques used in fine-tuning and optimizing large language models (such as LLAMA3), including SFT (Supervised Fine-Tuning), LoRA (Low-Rank Adaptation), Alignment technologies, KTO (Kahneman-Tversky Optimization), and DPO (Direct Preference Optimization). The document also elaborates on the principles of each technique, specific implementation metho...</div></div></div></a><a class="pagination-related" href="/2024/10/23/NLP%20Insights/LLM%E8%AE%AD%E7%BB%83%E6%9C%AF%E8%AF%AD%E4%BB%8B%E7%BB%8D%EF%BC%9ASFT%E3%80%81LoRA%E3%80%81Alignment%E3%80%81KTO%E5%92%8CDPO.en/" title="LoRA, DPO, KTO ä¸ SFT æŠ€æœ¯è¯¦è§£"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">ä¸‹ä¸€ç¯‡</div><div class="info-item-2">LoRA, DPO, KTO ä¸ SFT æŠ€æœ¯è¯¦è§£</div></div><div class="info-2"><div class="info-item-1">LoRA, DPO, KTO ä¸ SFT æŠ€æœ¯è¯¦è§£æœ¬ç¯‡æ–‡æ¡£å°†è¯¦ç»†ä»‹ç»å‡ ç§åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ LLAMA3ï¼‰å¾®è°ƒå’Œä¼˜åŒ–ä¸­çš„é‡è¦æŠ€æœ¯ï¼ŒåŒ…æ‹¬ SFTï¼ˆSupervised Fine-Tuningï¼‰ã€LoRAï¼ˆLow-Rank Adaptationï¼‰ã€Alignment æŠ€æœ¯ã€KTOï¼ˆKahneman-Tversky Optimizationï¼‰ å’Œ DPOï¼ˆDirect Preference Optimizationï¼‰ã€‚æ–‡ä¸­è¿˜å°†è¯¦ç»†é˜è¿°æ¯ç§æŠ€æœ¯çš„åŸç†ã€å…·ä½“å®ç°æ–¹æ³•ä»¥åŠç›¸åº”çš„æŸå¤±å‡½æ•°ä¸ä¼˜åŒ–å™¨é€‰æ‹©ã€‚  1. SFTï¼ˆSupervised Fine-Tuningï¼‰1.1 åŸç†SFT æ˜¯ä¸€ç§ä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•ï¼Œé€šè¿‡ç›‘ç£å­¦ä¹ å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œè°ƒæ•´æ¨¡å‹çš„å‚æ•°ä½¿å…¶åœ¨ç‰¹å®šä»»åŠ¡ä¸Šè¡¨ç°æ›´å¥½ã€‚SFT é€šå¸¸ç”¨äºé’ˆå¯¹ç‰¹å®šçš„æ ‡æ³¨æ•°æ®è¿›è¡Œæ¨¡å‹å¾®è°ƒï¼Œè®­ç»ƒçš„è¿‡ç¨‹ç±»ä¼¼äºå¸¸è§„çš„ç›‘ç£å­¦ä¹ ã€‚ 1.2 å®ç°æ–¹æ³• é€‰æ‹©é¢„è®­ç»ƒæ¨¡å‹ï¼šå¦‚ GPTã€BERT ç­‰è¯­è¨€æ¨¡å‹ã€‚ å‡†å¤‡æ ‡æ³¨æ•°æ®é›†ï¼šæ•°æ®é›†åŒ…å«è¾“å…¥å’Œè¾“å‡ºå¯¹ã€‚ è®­ç»ƒæ¨¡å‹ï¼šä½¿ç”¨æ ‡å‡†çš„äº¤å‰ç†µæŸå¤±å‡½æ•°å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œé€šè¿‡æ¢¯åº¦ä¸‹é™ä¼˜åŒ–å‚æ•°ã€‚  1.3 æ ¸å¿ƒä»£ç ä½¿ç”¨ Hugging Face çš„ ...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>ç›¸å…³æ¨è</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1.en/" title="æ¨ç† LLM çš„å¯è§†åŒ–æŒ‡å—ï¼šæ¢ç´¢æ¨ç†æ—¶è®¡ç®—æŠ€æœ¯ä¸ DeepSeek-R1"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-02-11</div><div class="info-item-2">æ¨ç† LLM çš„å¯è§†åŒ–æŒ‡å—ï¼šæ¢ç´¢æ¨ç†æ—¶è®¡ç®—æŠ€æœ¯ä¸ DeepSeek-R1</div></div><div class="info-2"><div class="info-item-1">æ¨ç† LLM çš„å¯è§†åŒ–æŒ‡å—ï¼šæ¢ç´¢æ¨ç†æ—¶è®¡ç®—æŠ€æœ¯ä¸ DeepSeek-R1åŸæ–‡åœ°å€ï¼šA Visual Guide to Reasoning LLMs ğŸ“… ä½œè€…ï¼šMaarten Grootendorst ğŸ“† æ—¥æœŸï¼š2025 å¹´ 2 æœˆ 3 æ—¥  ğŸ“Œ å¼•è¨€DeepSeek-R1ã€OpenAI o3-mini å’Œ Google Gemini 2.0 Flash Thinking æ˜¯å¦‚ä½•é€šè¿‡â€œæ¨ç†â€æ¡†æ¶å°† LLMï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹, Large Language Modelsï¼‰ æ‰©å±•åˆ°æ–°é«˜åº¦çš„å…¸å‹ç¤ºä¾‹ã€‚ å®ƒä»¬æ ‡å¿—ç€ä» æ‰©å±•è®­ç»ƒæ—¶è®¡ç®—ï¼ˆtrain-time computeï¼‰ åˆ° æ‰©å±•æ¨ç†æ—¶è®¡ç®—ï¼ˆtest-time computeï¼‰ çš„èŒƒå¼è½¬å˜ã€‚ åœ¨æœ¬ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬æä¾›äº† è¶…è¿‡ 40 å¼ å®šåˆ¶å¯è§†åŒ–å›¾è¡¨ï¼Œå¸¦ä½ æ·±å…¥æ¢ç´¢ï¼š  æ¨ç† LLMï¼ˆReasoning LLMsï¼‰ é¢†åŸŸ æ¨ç†æ—¶è®¡ç®—ï¼ˆTest-Time Computeï¼‰ æœºåˆ¶ DeepSeek-R1 çš„æ ¸å¿ƒæ€æƒ³  æˆ‘ä»¬å°†é€æ­¥ä»‹ç»ç›¸å…³æ¦‚å¿µï¼Œå¸®åŠ©ä½ å»ºç«‹å¯¹è¿™ä¸€æ–°èŒƒå¼çš„ç›´è§‰ç†è§£ã€‚    ğŸ“– ä»€ä¹ˆæ˜¯æ¨ç† LLMï¼Ÿä¸æ™®é€š LLMï¼ˆLarge Langu...</div></div></div></a><a class="pagination-related" href="/2025/03/06/NLP%20Insights/Differences%20in%20Padding%20Strategies%20Between%20Decoder-only%20and%20Encoder-only%20Models.en/" title="Differences in Padding Strategies Between Decoder-only and Encoder-only Models"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-06</div><div class="info-item-2">Differences in Padding Strategies Between Decoder-only and Encoder-only Models</div></div><div class="info-2"><div class="info-item-1">ğŸ“Œ What is Padding?In Large Language Models (LLMs), padding is a method used to standardize sequence lengths for batch processing. For example: 12Sentence 1: "I love NLP"Sentence 2: "Padding is useful in LLM training"  Using the &lt;pad&gt; token for alignment: 12"I love NLP &lt;pad&gt; &lt;pad&gt; &lt;pad&gt;""Padding is useful in LLM training"   ğŸ“Œ Padding Positioning: Left vs RightThere are two common padding strategies:  Right padding: 1"I love NLP &lt;pad&gt; &lt;pad&gt;"  Left padding: ...</div></div></div></a><a class="pagination-related" href="/2024/02/27/NLP%20Insights/FastChat%20Training%20Script%20Code%20Analysis%20-%20Train.py%20%E3%80%90FastChat%20Series%20Part%201%E3%80%91/" title="FastChat Training Script Code Analysis - Train.py ã€FastChat Series Part 1ã€‘"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-27</div><div class="info-item-2">FastChat Training Script Code Analysis - Train.py ã€FastChat Series Part 1ã€‘</div></div><div class="info-2"><div class="info-item-1">FastChat Training Script Code Analysis - Train.py ã€FastChat Series Part 1ã€‘In this article, we delve into the train.py script of FastChat (https://github.com/lm-sys/FastChat) (https://github.com/lm-sys/FastChat/blob/main/fastchat/train/train.py), a key component for training and optimizing large language models (LLMs). FastChat is an advanced open-source platform focused on developing, deploying, and evaluating chatbots based on LLMs. The platform not only supports top-tier models like Vicuna ...</div></div></div></a><a class="pagination-related" href="/2025/03/06/NLP%20Insights/Decoder%E6%A8%A1%E5%9E%8B%E5%92%8CEncoder%E6%A8%A1%E5%9E%8B%E5%9C%A8Padding%E4%B8%AD%E7%9A%84%E4%B8%8D%E5%90%8C.en/" title="Decoder-onlyä¸Encoder-onlyæ¨¡å‹Paddingç­–ç•¥çš„å·®å¼‚"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-06</div><div class="info-item-2">Decoder-onlyä¸Encoder-onlyæ¨¡å‹Paddingç­–ç•¥çš„å·®å¼‚</div></div><div class="info-2"><div class="info-item-1">ğŸ“Œ Padding çš„å«ä¹‰åœ¨å¤§æ¨¡å‹ (LLM) ä¸­ï¼Œpadding æ˜¯ç”¨äºå°†ä¸åŒé•¿åº¦çš„åºåˆ—è°ƒæ•´ä¸ºåŒä¸€é•¿åº¦çš„æ–¹æ³•ï¼Œä»¥ä¾¿äºæ‰¹é‡ (batch) å¤„ç†ã€‚ ä¾‹å¦‚ï¼š 12å¥å­1: "I love NLP"å¥å­2: "Padding is useful in LLM training"  ä½¿ç”¨ &lt;pad&gt; token è¿›è¡Œå¯¹é½ï¼š 12"I love NLP &lt;pad&gt; &lt;pad&gt; &lt;pad&gt;""Padding is useful in LLM training"   ğŸ“Œ Padding ä½ç½®çš„é€‰æ‹©ï¼šLeft vs RightPadding æœ‰ä¸¤ç§å¸¸è§æ–¹å¼ï¼š  Right paddingï¼ˆå³å¡«å……ï¼‰ï¼š 1"I love NLP &lt;pad&gt; &lt;pad&gt;"  Left paddingï¼ˆå·¦å¡«å……ï¼‰ï¼š 1"&lt;pad&gt; &lt;pad&gt; I love NLP"  é€šå¸¸ï¼š  Decoder-only æ¨¡å‹ï¼ˆå¦‚ GPT, Llamaï¼‰ï¼šé‡‡ç”¨ Left padding Encoder-only æ¨¡å‹ï¼ˆå¦‚ BERTï¼‰ï¼šé‡‡ç”¨...</div></div></div></a><a class="pagination-related" href="/2024/02/27/NLP%20Insights/FastChat%20Training%20Script%20Code%20Analysis%20-%20Train.py%20%E3%80%90FastChat%20Series%20Part%201%E3%80%91.en/" title="FastChat Training Script Code Analysis - Train.py ã€FastChat Series Part 1ã€‘"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-27</div><div class="info-item-2">FastChat Training Script Code Analysis - Train.py ã€FastChat Series Part 1ã€‘</div></div><div class="info-2"><div class="info-item-1">FastChat Training Script Code Analysis - Train.py ã€FastChat Series Part 1ã€‘In this article, we delve into the train.py script of FastChat (https://github.com/lm-sys/FastChat) (https://github.com/lm-sys/FastChat/blob/main/fastchat/train/train.py), a key component for training and optimizing large language models (LLMs). FastChat is an advanced open-source platform focused on developing, deploying, and evaluating chatbots based on LLMs. The platform not only supports top-tier models like Vicuna ...</div></div></div></a><a class="pagination-related" href="/2024/02/19/NLP%20Insights/%E7%90%86%E8%A7%A3%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%ADFine-tuning%E5%92%8CFurther%20Pretraining%E7%9A%84%E5%8C%BA%E5%88%AB.en/" title="ç†è§£å¤§å‹è¯­è¨€æ¨¡å‹ä¸­Fine-tuningå’ŒFurther Pretrainingçš„åŒºåˆ«"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-19</div><div class="info-item-2">ç†è§£å¤§å‹è¯­è¨€æ¨¡å‹ä¸­Fine-tuningå’ŒFurther Pretrainingçš„åŒºåˆ«</div></div><div class="info-2"><div class="info-item-1">ç†è§£å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ Fine-tuning å’Œ Further Pretraining çš„åŒºåˆ«åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¦‚ GPT å’Œ BERT çš„å‡ºç°ï¼Œå½»åº•æ”¹å˜äº†æˆ‘ä»¬å¤„ç†æ–‡æœ¬åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æå’Œé—®ç­”ç­‰ä»»åŠ¡çš„æ–¹å¼ã€‚åœ¨è¿™äº›æ¨¡å‹çš„åº”ç”¨ä¸­ï¼ŒFine-tuningï¼ˆå¾®è°ƒï¼‰å’Œ Further Pretrainingï¼ˆè¿›ä¸€æ­¥é¢„è®­ç»ƒï¼‰æ˜¯ä¸¤ç§å…³é”®æŠ€æœ¯ã€‚è™½ç„¶å®ƒä»¬çœ‹èµ·æ¥ç›¸ä¼¼ï¼Œä½†å®é™…ä¸ŠæœåŠ¡äº NLP æµç¨‹ä¸­çš„ä¸åŒéœ€æ±‚å’Œåœºæ™¯ã€‚ ä»€ä¹ˆæ˜¯ Fine-tuningï¼ŸFine-tuning æ˜¯æŒ‡åœ¨ç‰¹å®šä»»åŠ¡çš„æ•°æ®é›†ä¸Šè¿›ä¸€æ­¥è®­ç»ƒï¼ˆæˆ–â€œå¾®è°ƒâ€ï¼‰ä¸€ä¸ªé¢„è®­ç»ƒå¥½çš„æ¨¡å‹çš„è¿‡ç¨‹ã€‚è¿™ç§æ–¹æ³•åœ¨æ•°æ®é›†ç›¸å¯¹è¾ƒå°ä½†æ ‡æ³¨è‰¯å¥½çš„æƒ…å†µä¸‹ç‰¹åˆ«æœ‰æ•ˆã€‚ ç¤ºä¾‹åœºæ™¯ï¼šæƒ…æ„Ÿåˆ†æå‡è®¾ä½ æœ‰ä¸€ç»„ç”µå½±è¯„è®ºæ•°æ®ï¼Œæ¯æ¡è¯„è®ºéƒ½æ ‡è®°äº†æ­£é¢æˆ–è´Ÿé¢æƒ…æ„Ÿã€‚ä½ æƒ³åˆ›å»ºä¸€ä¸ªæ¨¡å‹æ¥é¢„æµ‹è¯„è®ºçš„æƒ…æ„Ÿã€‚ Python ä»£ç ç¤ºä¾‹ï¼ˆä½¿ç”¨ PyTorch å’Œ HuggingFace çš„ Transformersï¼‰This notebook demonstrates the fine-tuning of a BERT model on the IMDB dataset for sen...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Huiyu Chen</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">æ–‡ç« </div><div class="length-num">124</div></a><a href="/tags/"><div class="headline">æ ‡ç­¾</div><div class="length-num">46</div></a><a href="/categories/"><div class="headline">åˆ†ç±»</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>å…¬å‘Š</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>ç›®å½•</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#LoRA-DPO-KTO-%E4%B8%8E-SFT-%E6%8A%80%E6%9C%AF%E8%AF%A6%E8%A7%A3"><span class="toc-number">1.</span> <span class="toc-text">LoRA, DPO, KTO ä¸ SFT æŠ€æœ¯è¯¦è§£</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-SFT%EF%BC%88Supervised-Fine-Tuning%EF%BC%89"><span class="toc-number"></span> <span class="toc-text">1. SFTï¼ˆSupervised Fine-Tuningï¼‰</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-%E5%8E%9F%E7%90%86"><span class="toc-number">1.</span> <span class="toc-text">1.1 åŸç†</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-%E5%AE%9E%E7%8E%B0%E6%96%B9%E6%B3%95"><span class="toc-number">2.</span> <span class="toc-text">1.2 å®ç°æ–¹æ³•</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-%E6%A0%B8%E5%BF%83%E4%BB%A3%E7%A0%81"><span class="toc-number">3.</span> <span class="toc-text">1.3 æ ¸å¿ƒä»£ç </span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-LoRA%EF%BC%88Low-Rank-Adaptation%EF%BC%89"><span class="toc-number"></span> <span class="toc-text">2. LoRAï¼ˆLow-Rank Adaptationï¼‰</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E5%8E%9F%E7%90%86"><span class="toc-number">1.</span> <span class="toc-text">2.1 åŸç†</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E5%AE%9E%E7%8E%B0%E6%96%B9%E6%B3%95"><span class="toc-number">2.</span> <span class="toc-text">2.2 å®ç°æ–¹æ³•</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E5%8F%AF%E5%BE%AE%E8%B0%83%E7%9A%84%E5%B1%82%E4%B8%8E%E4%B8%8D%E5%8F%98%E7%9A%84%E5%B1%82"><span class="toc-number">3.</span> <span class="toc-text">2.3 å¯å¾®è°ƒçš„å±‚ä¸ä¸å˜çš„å±‚</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%AF%E5%BE%AE%E8%B0%83%E7%9A%84%E5%B1%82"><span class="toc-number">3.1.</span> <span class="toc-text">å¯å¾®è°ƒçš„å±‚</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%8D%E5%8F%98%E7%9A%84%E5%B1%82"><span class="toc-number">3.2.</span> <span class="toc-text">ä¸å˜çš„å±‚</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">4.</span> <span class="toc-text">2.4 æŸå¤±å‡½æ•°</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-5-%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-number">5.</span> <span class="toc-text">2.5 ä¼˜åŒ–å™¨</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-6-%E6%A0%B8%E5%BF%83%E4%BB%A3%E7%A0%81"><span class="toc-number">6.</span> <span class="toc-text">2.6 æ ¸å¿ƒä»£ç </span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Alignment%EF%BC%88%E5%AF%B9%E9%BD%90%E6%8A%80%E6%9C%AF%EF%BC%89"><span class="toc-number"></span> <span class="toc-text">3. Alignmentï¼ˆå¯¹é½æŠ€æœ¯ï¼‰</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E4%BB%80%E4%B9%88%E6%98%AF%E6%A8%A1%E5%9E%8B%E5%AF%B9%E9%BD%90%EF%BC%88Alignment%EF%BC%89%EF%BC%9F"><span class="toc-number">1.</span> <span class="toc-text">1. ä»€ä¹ˆæ˜¯æ¨¡å‹å¯¹é½ï¼ˆAlignmentï¼‰ï¼Ÿ</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%A8%A1%E5%9E%8B%E5%AF%B9%E9%BD%90%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86"><span class="toc-number">2.</span> <span class="toc-text">2. æ¨¡å‹å¯¹é½çš„æ•°å­¦åŸç†</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-%E7%AD%96%E7%95%A5%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.1.</span> <span class="toc-text">2.1 ç­–ç•¥æ¨¡å‹</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-%E6%8F%90%E9%AB%98%E6%AD%A3%E6%A0%B7%E6%9C%AC%E6%A6%82%E7%8E%87%E4%B8%8E%E9%99%8D%E4%BD%8E%E8%B4%9F%E6%A0%B7%E6%9C%AC%E6%A6%82%E7%8E%87%E7%9A%84%E6%9C%BA%E5%88%B6"><span class="toc-number">2.2.</span> <span class="toc-text">2.2 æé«˜æ­£æ ·æœ¬æ¦‚ç‡ä¸é™ä½è´Ÿæ ·æœ¬æ¦‚ç‡çš„æœºåˆ¶</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8EKL%E6%95%A3%E5%BA%A6%E7%9A%84%E4%BD%9C%E7%94%A8"><span class="toc-number">3.</span> <span class="toc-text">3. æŸå¤±å‡½æ•°ä¸KLæ•£åº¦çš„ä½œç”¨</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-KL%E6%95%A3%E5%BA%A6%E7%9A%84%E4%BD%9C%E7%94%A8"><span class="toc-number">3.1.</span> <span class="toc-text">3.1 KLæ•£åº¦çš„ä½œç”¨</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%85%AC%E5%BC%8F"><span class="toc-number">3.2.</span> <span class="toc-text">3.2 æŸå¤±å‡½æ•°å…¬å¼</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#DPO%E4%B8%AD%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%EF%BC%9A"><span class="toc-number">3.2.1.</span> <span class="toc-text">DPOä¸­çš„æŸå¤±å‡½æ•°ï¼š</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#KTO%E4%B8%AD%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%EF%BC%9A"><span class="toc-number">3.2.2.</span> <span class="toc-text">KTOä¸­çš„æŸå¤±å‡½æ•°ï¼š</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E5%A6%82%E4%BD%95%E4%BC%98%E5%8C%96%E6%A8%A1%E5%9E%8B"><span class="toc-number">4.</span> <span class="toc-text">4. å¦‚ä½•ä¼˜åŒ–æ¨¡å‹</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-DPO%EF%BC%88Direct-Preference-Optimization%EF%BC%89"><span class="toc-number"></span> <span class="toc-text">4. DPOï¼ˆDirect Preference Optimizationï¼‰</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E5%8E%9F%E7%90%86"><span class="toc-number">1.</span> <span class="toc-text">4.1 åŸç†</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">2.</span> <span class="toc-text">4.2 æŸå¤±å‡½æ•°</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-number">3.</span> <span class="toc-text">4.3 ä¼˜åŒ–å™¨</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-%E6%A0%B8%E5%BF%83%E4%BB%A3%E7%A0%81"><span class="toc-number">4.</span> <span class="toc-text">4.4 æ ¸å¿ƒä»£ç </span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-KTO%EF%BC%88Kahneman-Tversky-Optimization%EF%BC%89"><span class="toc-number"></span> <span class="toc-text">5. KTOï¼ˆKahneman-Tversky Optimizationï¼‰</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-%E5%8E%9F%E7%90%86"><span class="toc-number">1.</span> <span class="toc-text">5.1 åŸç†</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">2.</span> <span class="toc-text">5.2 æŸå¤±å‡½æ•°</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-number">3.</span> <span class="toc-text">5.3 ä¼˜åŒ–å™¨</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-4-%E6%A0%B8%E5%BF%83%E4%BB%A3%E7%A0%81"><span class="toc-number">4.</span> <span class="toc-text">5.4 æ ¸å¿ƒä»£ç </span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">5.</span> <span class="toc-text">æ€»ç»“</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>æœ€æ–°æ–‡ç« </span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/21/NLP%20Insights/Evaluation%20of%20Generation-Based%20Large%20Language%20Models%20(LLMs):%20Opportunities%20and%20Challenges%20from%20Generation%20to%20Judgment.en/" title="Evaluation of Generation-Based Large Language Models (LLMs): Opportunities and Challenges from Generation to Judgment">Evaluation of Generation-Based Large Language Models (LLMs): Opportunities and Challenges from Generation to Judgment</a><time datetime="2026-02-20T22:00:00.000Z" title="å‘è¡¨äº 2026-02-21 06:00:00">2026-02-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/21/NLP%20Insights/Evaluation%20of%20Generation-Based%20Large%20Language%20Models%20(LLMs):%20Opportunities%20and%20Challenges%20from%20Generation%20to%20Judgment/" title="Evaluation of Generation-Based Large Language Models (LLMs): Opportunities and Challenges from Generation to Judgment">Evaluation of Generation-Based Large Language Models (LLMs): Opportunities and Challenges from Generation to Judgment</a><time datetime="2026-02-20T22:00:00.000Z" title="å‘è¡¨äº 2026-02-21 06:00:00">2026-02-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/24/NLP%20Insights/SeCom%20Redefining%20Memory%20Management%20in%20Conversational%20AI.en/" title="SeCom: Redefining Memory Management in Conversational AI">SeCom: Redefining Memory Management in Conversational AI</a><time datetime="2025-06-24T08:00:00.000Z" title="å‘è¡¨äº 2025-06-24 16:00:00">2025-06-24</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/24/NLP%20Insights/SeCom%20Redefining%20Memory%20Management%20in%20Conversational%20AI/" title="SeCom: Redefining Memory Management in Conversational AI">SeCom: Redefining Memory Management in Conversational AI</a><time datetime="2025-06-24T08:00:00.000Z" title="å‘è¡¨äº 2025-06-24 16:00:00">2025-06-24</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/24/NLP%20Insights/SeCom:%20%E9%87%8D%E6%96%B0%E5%AE%9A%E4%B9%89%E5%AF%B9%E8%AF%9DAI%E7%9A%84%E8%AE%B0%E5%BF%86%E7%AE%A1%E7%90%86.en/" title="SeCom: é‡æ–°å®šä¹‰å¯¹è¯AIçš„è®°å¿†ç®¡ç†">SeCom: é‡æ–°å®šä¹‰å¯¹è¯AIçš„è®°å¿†ç®¡ç†</a><time datetime="2025-06-24T08:00:00.000Z" title="å‘è¡¨äº 2025-06-24 16:00:00">2025-06-24</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 - 2026 By Huiyu Chen</span><span class="framework-info"><span>æ¡†æ¶ </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 6.3.0</a><span class="footer-separator">|</span><span>ä¸»é¢˜ </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.5.4</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="é˜…è¯»æ¨¡å¼"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="æ—¥é—´å’Œå¤œé—´æ¨¡å¼åˆ‡æ¢"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="å•æ å’ŒåŒæ åˆ‡æ¢"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="è®¾ç½®"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="ç›®å½•"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="å›åˆ°é¡¶éƒ¨"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.4"></script><script src="/js/main.js?v=5.5.4"></script><div class="js-pjax"></div><script src="/js/lang-switch.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>