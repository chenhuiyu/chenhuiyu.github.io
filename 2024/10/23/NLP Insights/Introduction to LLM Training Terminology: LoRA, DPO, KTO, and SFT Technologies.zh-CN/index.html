<!DOCTYPE html><html lang="[&quot;zh-CN&quot;,&quot;en&quot;]" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Detailed Explanation of LoRA, DPO, KTO, and SFT Technologies | é»‘å¤´å‘†é±¼è¿›åŒ–ä¹‹æ—…</title><meta name="author" content="Huiyu Chen"><meta name="copyright" content="Huiyu Chen"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Introduction to LLM Training Terminology:LoRA, DPO, KTO, and SFT TechnologiesThis document provides a detailed introduction to several important techniques used in fine-tuning and optimizing large lan">
<meta property="og:type" content="article">
<meta property="og:title" content="Detailed Explanation of LoRA, DPO, KTO, and SFT Technologies">
<meta property="og:url" content="https://chenhuiyu.github.io/2024/10/23/NLP%20Insights/Introduction%20to%20LLM%20Training%20Terminology:%20LoRA,%20DPO,%20KTO,%20and%20SFT%20Technologies.zh-CN/index.html">
<meta property="og:site_name" content="é»‘å¤´å‘†é±¼è¿›åŒ–ä¹‹æ—…">
<meta property="og:description" content="Introduction to LLM Training Terminology:LoRA, DPO, KTO, and SFT TechnologiesThis document provides a detailed introduction to several important techniques used in fine-tuning and optimizing large lan">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://chenhuiyu.github.io/img/butterfly-icon.png">
<meta property="article:published_time" content="2024-10-23T08:26:29.000Z">
<meta property="article:modified_time" content="2026-02-20T22:13:37.929Z">
<meta property="article:author" content="Huiyu Chen">
<meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://chenhuiyu.github.io/img/butterfly-icon.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Detailed Explanation of LoRA, DPO, KTO, and SFT Technologies",
  "url": "https://chenhuiyu.github.io/2024/10/23/NLP%20Insights/Introduction%20to%20LLM%20Training%20Terminology:%20LoRA,%20DPO,%20KTO,%20and%20SFT%20Technologies.zh-CN/",
  "image": "https://chenhuiyu.github.io/img/butterfly-icon.png",
  "datePublished": "2024-10-23T08:26:29.000Z",
  "dateModified": "2026-02-20T22:13:37.929Z",
  "author": [
    {
      "@type": "Person",
      "name": "Huiyu Chen",
      "url": "https://chenhuiyu.github.io"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://chenhuiyu.github.io/2024/10/23/NLP%20Insights/Introduction%20to%20LLM%20Training%20Terminology:%20LoRA,%20DPO,%20KTO,%20and%20SFT%20Technologies.zh-CN/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=5.5.4"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: 'å¤åˆ¶æˆåŠŸ',
    error: 'å¤åˆ¶å¤±è´¥',
    noSupport: 'æµè§ˆå™¨ä¸æ”¯æŒ'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'åˆšåˆš',
    min: 'åˆ†é’Ÿå‰',
    hour: 'å°æ—¶å‰',
    day: 'å¤©å‰',
    month: 'ä¸ªæœˆå‰'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.13.0/dist/infinitegrid.min.js',
    buttonText: 'åŠ è½½æ›´å¤š'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Detailed Explanation of LoRA, DPO, KTO, and SFT Technologies',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 6.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="é»‘å¤´å‘†é±¼è¿›åŒ–ä¹‹æ—…" type="application/atom+xml">
</head><body><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">é»‘å¤´å‘†é±¼è¿›åŒ–ä¹‹æ—…</span></a><a class="nav-page-title" href="/"><span class="site-name">Detailed Explanation of LoRA, DPO, KTO, and SFT Technologies</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  è¿”å›é¦–é¡µ</span></span></a></span><div id="menus"></div></nav><div id="post-info"><h1 class="post-title">Detailed Explanation of LoRA, DPO, KTO, and SFT Technologies</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">å‘è¡¨äº</span><time class="post-meta-date-created" datetime="2024-10-23T08:26:29.000Z" title="å‘è¡¨äº 2024-10-23 16:26:29">2024-10-23</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">æ›´æ–°äº</span><time class="post-meta-date-updated" datetime="2026-02-20T22:13:37.929Z" title="æ›´æ–°äº 2026-02-21 06:13:37">2026-02-21</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/NLP-Insights/">NLP Insights</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">æµè§ˆé‡:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h3 id="Introduction-to-LLM-Training-Terminology-LoRA-DPO-KTO-and-SFT-Technologies"><a href="#Introduction-to-LLM-Training-Terminology-LoRA-DPO-KTO-and-SFT-Technologies" class="headerlink" title="Introduction to LLM Training Terminology:LoRA, DPO, KTO, and SFT Technologies"></a><strong>Introduction to LLM Training Terminology:LoRA, DPO, KTO, and SFT Technologies</strong></h3><p>This document provides a detailed introduction to several important techniques used in fine-tuning and optimizing large language models (such as LLAMA3), including <strong>SFT (Supervised Fine-Tuning)</strong>, <strong>LoRA (Low-Rank Adaptation)</strong>, <strong>Alignment</strong> technologies, <strong>KTO (Kahneman-Tversky Optimization)</strong>, and <strong>DPO (Direct Preference Optimization)</strong>. The document also elaborates on the principles of each technique, specific implementation methods, as well as the selection of corresponding loss functions and optimizers.</p>
<hr>
<h2 id="1-SFT-Supervised-Fine-Tuning"><a href="#1-SFT-Supervised-Fine-Tuning" class="headerlink" title="1. SFT (Supervised Fine-Tuning)"></a>1. <strong>SFT (Supervised Fine-Tuning)</strong></h2><h3 id="1-1-Principle"><a href="#1-1-Principle" class="headerlink" title="1.1 Principle"></a>1.1 <strong>Principle</strong></h3><p>SFT is a traditional fine-tuning method that adjusts the parameters of a pre-trained model through supervised learning to improve its performance on specific tasks. SFT is typically used to fine-tune models on specific labeled datasets, with the training process resembling standard supervised learning.</p>
<h3 id="1-2-Implementation-Method"><a href="#1-2-Implementation-Method" class="headerlink" title="1.2 Implementation Method"></a>1.2 <strong>Implementation Method</strong></h3><ul>
<li><strong>Select a Pre-trained Model</strong>: Such as GPT, BERT, and other language models.</li>
<li><strong>Prepare a Labeled Dataset</strong>: The dataset includes input-output pairs.</li>
<li><strong>Train the Model</strong>: Use a standard cross-entropy loss function to train the model, optimizing parameters through gradient descent.</li>
</ul>
<h3 id="1-3-Core-Code"><a href="#1-3-Core-Code" class="headerlink" title="1.3 Core Code"></a>1.3 <strong>Core Code</strong></h3><p>Using Hugging Faceâ€™s <code>Trainer</code> interface for SFT:</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Trainer, TrainingArguments, AutoModelForSeq2SeqLM, AutoTokenizer</span><br><span class="line"></span><br><span class="line">model_name = <span class="string">"gpt2"</span></span><br><span class="line">model = AutoModelForSeq2SeqLM.from_pretrained(model_name)</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_name)</span><br><span class="line"></span><br><span class="line">train_dataset = load_dataset(<span class="string">"my_dataset"</span>, split=<span class="string">"train"</span>)</span><br><span class="line"></span><br><span class="line">training_args = TrainingArguments(</span><br><span class="line">    output_dir=<span class="string">"./results"</span>,</span><br><span class="line">    evaluation_strategy=<span class="string">"epoch"</span>,</span><br><span class="line">    learning_rate=<span class="number">2e-5</span>,</span><br><span class="line">    per_device_train_batch_size=<span class="number">16</span>,</span><br><span class="line">    num_train_epochs=<span class="number">3</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model=model,</span><br><span class="line">    args=training_args,</span><br><span class="line">    train_dataset=train_dataset,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer.train()</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h2 id="2-LoRA-Low-Rank-Adaptation"><a href="#2-LoRA-Low-Rank-Adaptation" class="headerlink" title="2. LoRA (Low-Rank Adaptation)"></a>2. <strong>LoRA (Low-Rank Adaptation)</strong></h2><h3 id="2-1-Principle"><a href="#2-1-Principle" class="headerlink" title="2.1 Principle"></a>2.1 <strong>Principle</strong></h3><p>LoRA is a parameter-efficient fine-tuning technique that performs low-rank decomposition of the weight matrices in large models. It decomposes the original weight matrix $W$ into two low-rank matrices $B$ and $A$, and only fine-tunes these low-rank matrices. The design goal of LoRA is to reduce the number of fine-tuning parameters while retaining the pre-trained model weights, optimizing model performance by adjusting the low-rank matrices.</p>
<h3 id="2-2-Implementation-Method"><a href="#2-2-Implementation-Method" class="headerlink" title="2.2 Implementation Method"></a>2.2 <strong>Implementation Method</strong></h3><ul>
<li><strong>Weight Decomposition</strong>: For the modelâ€™s linear layers (such as the <code>q_proj</code> and <code>v_proj</code> layers in the attention mechanism), decompose the weight matrix into two low-rank matrices $B$ and $A$.</li>
<li><strong>Fine-Tune Specific Layers</strong>: Apply LoRA only to these specific linear layers, keeping other layers in the model unchanged.</li>
</ul>
<h3 id="2-3-Layers-to-Fine-Tune-vs-Layers-to-Keep-Unchanged"><a href="#2-3-Layers-to-Fine-Tune-vs-Layers-to-Keep-Unchanged" class="headerlink" title="2.3 Layers to Fine-Tune vs. Layers to Keep Unchanged"></a>2.3 <strong>Layers to Fine-Tune vs. Layers to Keep Unchanged</strong></h3><h4 id="Layers-to-Fine-Tune"><a href="#Layers-to-Fine-Tune" class="headerlink" title="Layers to Fine-Tune"></a><strong>Layers to Fine-Tune</strong></h4><p>LoRA is typically applied to the linear projection layers in Transformer models, especially several key layers in the multi-head attention mechanism:</p>
<ul>
<li><strong>q_proj</strong> (Query Projection Layer)</li>
<li><strong>k_proj</strong> (Key Projection Layer)</li>
<li><strong>v_proj</strong> (Value Projection Layer)</li>
<li><strong>o_proj</strong> (Output Projection Layer)</li>
<li><strong>ffn_up_proj</strong> and <strong>ffn_down_proj</strong> (Up and Down Projection Layers of the Feedforward Neural Network)</li>
</ul>
<h4 id="Layers-to-Keep-Unchanged"><a href="#Layers-to-Keep-Unchanged" class="headerlink" title="Layers to Keep Unchanged"></a><strong>Layers to Keep Unchanged</strong></h4><ul>
<li><strong>Embedding Layers</strong>: Responsible for encoding inputs and outputs, usually do not require fine-tuning.</li>
<li><strong>LayerNorm Layers</strong>: These layers are mainly used for normalization, do not contain many parameters, and are typically kept unchanged.</li>
<li><strong>Activation Function Layers</strong>: Non-linear activation functions like ReLU or GELU do not involve parameters and do not require fine-tuning.</li>
</ul>
<h3 id="2-4-Loss-Function"><a href="#2-4-Loss-Function" class="headerlink" title="2.4 Loss Function"></a>2.4 <strong>Loss Function</strong></h3><p>The loss function for LoRA is usually task-specific. In language generation tasks, LoRA uses <strong>cross-entropy loss</strong> to measure the difference between the generated text and the target text:</p>
<p>$$<br>\mathcal{L}<em>{\text{LoRA}} = - \sum</em>{i} y_i \log(\hat{y}_i)<br>$$</p>
<p>where $y_i$ is the true label, and $\hat{y}_i$ is the modelâ€™s output probability.</p>
<h3 id="2-5-Optimizer"><a href="#2-5-Optimizer" class="headerlink" title="2.5 Optimizer"></a>2.5 <strong>Optimizer</strong></h3><p>LoRA fine-tuning typically uses the <strong>AdamW</strong> optimizer, as shown in the following code:</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.AdamW(lora_model.parameters(), lr=<span class="number">5e-5</span>)</span><br></pre></td></tr></tbody></table></figure>

<h3 id="2-6-Core-Code"><a href="#2-6-Core-Code" class="headerlink" title="2.6 Core Code"></a>2.6 <strong>Core Code</strong></h3><p>Implementing LoRA using the <code>peft</code> library:</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> peft <span class="keyword">import</span> LoraConfig, get_peft_model</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForSeq2SeqLM</span><br><span class="line"></span><br><span class="line">model_name = <span class="string">"gpt2"</span></span><br><span class="line">model = AutoModelForSeq2SeqLM.from_pretrained(model_name)</span><br><span class="line"></span><br><span class="line">lora_config = LoraConfig(</span><br><span class="line">    r=<span class="number">8</span>,                </span><br><span class="line">    lora_alpha=<span class="number">32</span>,      </span><br><span class="line">    target_modules=[<span class="string">"q_proj"</span>, <span class="string">"v_proj"</span>],  </span><br><span class="line">    lora_dropout=<span class="number">0.1</span>,   </span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">lora_model = get_peft_model(model, lora_config)</span><br><span class="line">lora_model.train()</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h2 id="3-Alignment-Alignment-Techniques"><a href="#3-Alignment-Alignment-Techniques" class="headerlink" title="3. Alignment (Alignment Techniques)"></a>3. <strong>Alignment (Alignment Techniques)</strong></h2><p>Before introducing KL divergence, we first need to clarify how LLM alignment is achieved, along with the underlying principles and mathematical formulas.</p>
<h3 id="1-What-is-Model-Alignment"><a href="#1-What-is-Model-Alignment" class="headerlink" title="1. What is Model Alignment?"></a><strong>1. What is Model Alignment?</strong></h3><p>The core objective of model alignment is to ensure that the language modelâ€™s outputs meet human expectations or preferences. Typically, the model is initially trained through large-scale supervised learning (SFT, Supervised Fine-Tuning) to generate a model with basic capabilities. Subsequently, through alignment techniques, the model is further adjusted to ensure that its generated content better aligns with human preferences or avoids producing harmful or erroneous information.</p>
<p><strong>Core Mechanism of Alignment</strong>:</p>
<ul>
<li><strong>Positive Samples</strong>: Outputs that meet human expectations (e.g., correct answers).</li>
<li><strong>Negative Samples</strong>: Outputs that do not meet human expectations (e.g., incorrect answers).</li>
</ul>
<p>By using paired preference data or labels (correct/incorrect), the modelâ€™s outputs are further fine-tuned to generate more positive samples while reducing the probability of generating negative samples.</p>
<hr>
<h3 id="2-Mathematical-Principles-of-Model-Alignment"><a href="#2-Mathematical-Principles-of-Model-Alignment" class="headerlink" title="2. Mathematical Principles of Model Alignment"></a><strong>2. Mathematical Principles of Model Alignment</strong></h3><p>During the alignment process, the model generates outputs through a <strong>policy model</strong>, which is typically an SFT-trained language model used to generate outputs given an input. To optimize the modelâ€™s outputs to better align with human preferences, the following loss functions and optimization methods are commonly used:</p>
<h4 id="2-1-Policy-Model"><a href="#2-1-Policy-Model" class="headerlink" title="2.1 Policy Model"></a><strong>2.1 Policy Model</strong></h4><p>Assume the current policy of the model is $\pi_\theta$, which represents the probability of the model generating output $y$ given input $x$:</p>
<p>$$<br>\pi_\theta(y|x)<br>$$</p>
<p>The objective of the policy model is to adjust the parameters $\theta$ to increase the probability of generating correct outputs (positive samples) and decrease the probability of generating incorrect outputs (negative samples).</p>
<h4 id="2-2-Mechanism-for-Increasing-Positive-Sample-Probability-and-Decreasing-Negative-Sample-Probability"><a href="#2-2-Mechanism-for-Increasing-Positive-Sample-Probability-and-Decreasing-Negative-Sample-Probability" class="headerlink" title="2.2 Mechanism for Increasing Positive Sample Probability and Decreasing Negative Sample Probability"></a><strong>2.2 Mechanism for Increasing Positive Sample Probability and Decreasing Negative Sample Probability</strong></h4><p>To achieve this goal, loss functions with preference comparisons or labels are typically used for optimization:</p>
<ol>
<li><p><strong>Optimization of Positive Samples</strong>: By increasing the loss weight of positive samples, the model is guided to generate positive samples with higher probability when faced with the same problem.</p>
<ul>
<li>The loss function for positive samples guides the model to produce more outputs that meet human expectations.</li>
</ul>
</li>
<li><p><strong>Penalty for Negative Samples</strong>: By applying higher loss weights to negative samples, the model learns to reduce the probability of generating these incorrect outputs.</p>
<ul>
<li>The loss function for negative samples aims to penalize the model more when it generates incorrect answers, thereby reducing the likelihood of such outputs.</li>
</ul>
</li>
</ol>
<p>In some methods, such as DPO and KTO, <strong>KL divergence</strong> between the current policy model and a reference model is calculated to prevent the model from deviating excessively from the original pre-trained model during optimization.</p>
<hr>
<h3 id="3-Role-of-Loss-Functions-and-KL-Divergence"><a href="#3-Role-of-Loss-Functions-and-KL-Divergence" class="headerlink" title="3. Role of Loss Functions and KL Divergence"></a><strong>3. Role of Loss Functions and KL Divergence</strong></h3><p>In the model alignment process, the loss function typically consists of two parts:</p>
<ol>
<li><strong>Preference Loss</strong> or <strong>Label Loss</strong>, used to optimize the model to generate outputs that meet human expectations.</li>
<li><strong>KL Divergence</strong>, used to constrain the model from deviating from the reference model.</li>
</ol>
<h4 id="3-1-Role-of-KL-Divergence"><a href="#3-1-Role-of-KL-Divergence" class="headerlink" title="3.1 Role of KL Divergence"></a><strong>3.1 Role of KL Divergence</strong></h4><p>KL divergence (Kullback-Leibler Divergence) measures the difference between two probability distributions. In model alignment, KL divergence is used to limit the distribution difference between the current model $\pi_\theta$ and the reference model $\pi_{\text{ref}}$, ensuring that the modelâ€™s outputs do not deviate excessively from the pre-trained model during optimization. The specific formula is:</p>
<p>$$<br>\text{KL}(\pi_\theta(y|x) | \pi_{\text{ref}}(y|x)) = \sum_y \pi_\theta(y|x) \log \frac{\pi_\theta(y|x)}{\pi_{\text{ref}}(y|x)}<br>$$</p>
<ul>
<li>If the KL divergence is large, it indicates that the current modelâ€™s generated distribution significantly differs from the reference model, which may mean the model is producing unreasonable outputs.</li>
<li>By minimizing KL divergence, the model can be further optimized while ensuring the reasonableness of its outputs.</li>
</ul>
<h4 id="3-2-Loss-Function-Formulas"><a href="#3-2-Loss-Function-Formulas" class="headerlink" title="3.2 Loss Function Formulas"></a><strong>3.2 Loss Function Formulas</strong></h4><p>Based on preferences or labels, the modelâ€™s loss function can be expressed in the following forms:</p>
<h5 id="Loss-Function-in-DPO"><a href="#Loss-Function-in-DPO" class="headerlink" title="Loss Function in DPO:"></a><strong>Loss Function in DPO</strong>:</h5><p>$$<br>L_{DPO} = -\mathbb{E}_{x, y_w, y_l \sim D} [\log \sigma(\beta (\log(\pi_\theta(y_w|x)) - \log(\pi_\theta(y_l|x))))]<br>$$</p>
<ul>
<li>$y_w$: Higher-preference answer.</li>
<li>$y_l$: Lower-preference answer.</li>
</ul>
<p>In DPO, KL divergence can be introduced as a regularization term:<br>$$<br>L_{DPO} = -\log \sigma(\log(\pi_\theta(y_w|x)) - \log(\pi_\theta(y_l|x)) + \beta \cdot \text{KL}(\pi_\theta | \pi_{\text{ref}}))<br>$$<br>By controlling KL divergence, the modelâ€™s outputs do not deviate too much from the reference model.</p>
<h5 id="Loss-Function-in-KTO"><a href="#Loss-Function-in-KTO" class="headerlink" title="Loss Function in KTO:"></a><strong>Loss Function in KTO</strong>:</h5><p>The loss function in KTO is based on prospect theory and incorporates KL divergence as a core component:<br>$$<br>L_{KTO} = \lambda_U \cdot \sigma(\beta \cdot (\text{KL}(\pi_\theta(\text{Answer 2}) | \pi_{\text{ref}}) - r_{\theta}(\text{Answer 2})))<br>$$</p>
<ul>
<li>$r_{\theta}(x, y)$: The current policyâ€™s confidence in negative samples (incorrect answers).</li>
<li>KL divergence is used to measure the difference between the current model and the reference model, ensuring that while reducing the generation of negative samples, the model does not deviate from the original reference model.</li>
</ul>
<p>By increasing the loss for negative samples (i.e., increasing the value of $\lambda_U$), the model reduces the confidence in negative samples, thereby decreasing the probability of generating similar incorrect answers in the future.</p>
<hr>
<h3 id="4-How-to-Optimize-the-Model"><a href="#4-How-to-Optimize-the-Model" class="headerlink" title="4. How to Optimize the Model"></a><strong>4. How to Optimize the Model</strong></h3><p>Through the loss functions introduced above, model optimization is typically performed using <strong>Gradient Descent</strong>. The gradients of the loss function reflect the differences between the modelâ€™s outputs and the expected outputs, and the optimization goal is to minimize the loss function.</p>
<p><strong>Gradient Update Formula</strong>:<br>$$<br>\theta_{\text{new}} = \theta_{\text{old}} - \eta \nabla_{\theta} L<br>$$<br>where:</p>
<ul>
<li>$\eta$ is the learning rate, determining the step size of each parameter update.</li>
<li>$\nabla_{\theta} L$ is the gradient of the loss function with respect to the model parameters, indicating the contribution of the current parameters to the loss.</li>
</ul>
<p>Through continuous iteration, the model gradually increases the probability of generating positive samples and decreases the probability of generating negative samples, ultimately achieving model alignment.</p>
<ul>
<li>The core objective of <strong>Model Alignment</strong> is to optimize the modelâ€™s outputs to meet human expectations through preference or label data.</li>
<li>The <strong>Policy Model</strong> ($\pi_\theta$) generates outputs, and KL divergence is used to control the degree of deviation from the reference model, preventing unreasonable biases during optimization.</li>
<li>The <strong>Probability of Positive Samples</strong> is gradually increased through the optimization of the loss function, while the <strong>Probability of Negative Samples</strong> is reduced by increasing loss weights and lowering confidence.</li>
<li>Gradient descent is used to update model parameters, ultimately achieving model alignment.</li>
</ul>
<hr>
<h2 id="4-DPO-Direct-Preference-Optimization"><a href="#4-DPO-Direct-Preference-Optimization" class="headerlink" title="4. DPO (Direct Preference Optimization)"></a>4. <strong>DPO (Direct Preference Optimization)</strong></h2><h3 id="4-1-Principle"><a href="#4-1-Principle" class="headerlink" title="4.1 Principle"></a>4.1 <strong>Principle</strong></h3><p>DPO directly optimizes the modelâ€™s output preference function to make the modelâ€™s outputs more aligned with human preferences. It compares different outputs generated by the model and uses a preference function to evaluate which of the two outputs is better, thereby guiding the optimization of the model parameters.</p>
<h3 id="4-2-Loss-Function"><a href="#4-2-Loss-Function" class="headerlink" title="4.2 Loss Function"></a>4.2 <strong>Loss Function</strong></h3><p>DPO uses a preference loss function to compare the quality of two outputs:</p>
<p>$$<br>\mathcal{L}_{\text{DPO}} = \log(1 + \exp(-\sigma \cdot (\hat{y}_a - \hat{y}_b) \cdot p))<br>$$</p>
<ul>
<li>$ \hat{y}_a $ and $ \hat{y}_b $ are the modelâ€™s predictions for two samples.</li>
<li>$ p $ is the human preference (1 indicates preference for $a$, -1 indicates preference for $b$).</li>
<li>$ \sigma $ is a smoothing parameter.</li>
</ul>
<h3 id="4-3-Optimizer"><a href="#4-3-Optimizer" class="headerlink" title="4.3 Optimizer"></a>4.3 <strong>Optimizer</strong></h3><p>DPO typically uses the <strong>AdamW</strong> optimizer, which is suitable for optimizing large-scale parameter models. The code is as follows:</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.AdamW(model.parameters(), lr=<span class="number">1e-5</span>)</span><br></pre></td></tr></tbody></table></figure>

<h3 id="4-4-Core-Code"><a href="#4-4-Core-Code" class="headerlink" title="4.4 Core Code"></a>4.4 <strong>Core Code</strong></h3><p>The following are the training steps for DPO:</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preference_loss</span>(<span class="params">output_a, output_b, human_preference</span>):</span><br><span class="line">    preference = human_preference(output_a, output_b)</span><br><span class="line">    loss = torch.log(<span class="number">1</span> + torch.exp(-preference * (output_a - output_b)))</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dpo_training_step</span>(<span class="params">model, data_a, data_b, human_preference</span>):</span><br><span class="line">    output_a = model(data_a)</span><br><span class="line">    output_b = model(data_b)</span><br><span class="line">    </span><br><span class="line">    loss = preference_loss(output_a, output_b, human_preference)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> batch_a, batch_b <span class="keyword">in</span> training_data:</span><br><span class="line">    dpo_training_step(model, batch_a, batch_b, human_preference_function)</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h2 id="5-KTO-Kahneman-Tversky-Optimization"><a href="#5-KTO-Kahneman-Tversky-Optimization" class="headerlink" title="5. KTO (Kahneman-Tversky Optimization)"></a>5. <strong>KTO (Kahneman-Tversky Optimization)</strong></h2><h3 id="5-1-Principle"><a href="#5-1-Principle" class="headerlink" title="5.1 Principle"></a>5.1 <strong>Principle</strong></h3><p>KTO is based on Kahneman and Tverskyâ€™s Prospect Theory, which uses an asymmetric utility function to measure the modelâ€™s gains and losses. It aims to optimize the modelâ€™s performance, especially in scenarios with asymmetric risks and rewards. The utility function is defined as follows:</p>
<p>$$<br>\mathcal{U}(x) =<br>\begin{cases}<br>x^{\alpha}, &amp; x \geq 0 \<br>-\lambda (-x)^{\alpha}, &amp; x &lt; 0<br>\end{cases}<br>$$</p>
<ul>
<li>$x$ is the difference between the modelâ€™s prediction and the true value.</li>
<li>$\alpha$ is the non-linear coefficient, typically 0.88.</li>
<li>$\lambda$ is the loss penalty weight, typically 2.25.</li>
</ul>
<h3 id="5-2-Loss-Function"><a href="#5-2-Loss-Function" class="headerlink" title="5.2 Loss Function"></a>5.2 <strong>Loss Function</strong></h3><p>The loss function for KTO is based on the utility function from Prospect Theory and is used to penalize the modelâ€™s prediction errors:</p>
<p>$$<br>\mathcal{L}<em>{\text{KTO}} = -\mathbb{E}[\mathcal{U}(y</em>{\text{pred}} - y_{\text{true}})]<br>$$</p>
<h3 id="5-3-Optimizer"><a href="#5-3-Optimizer" class="headerlink" title="5.3 Optimizer"></a>5.3 <strong>Optimizer</strong></h3><p>KTO commonly uses the <strong>AdamW</strong> optimizer to ensure stability during the training process:</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.AdamW(model.parameters(), lr=<span class="number">3e-5</span>)</span><br></pre></td></tr></tbody></table></figure>

<h3 id="5-4-Core-Code"><a href="#5-4-Core-Code" class="headerlink" title="5.4 Core Code"></a>5.4 <strong>Core Code</strong></h3><p>The following is the code for calculating the KTO loss function:</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">prospect_utility</span>(<span class="params">value, alpha=<span class="number">0.88</span>, lambda_=<span class="number">2.25</span></span>):</span><br><span class="line">    <span class="keyword">if</span> value &gt;= <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> torch.<span class="built_in">pow</span>(value, alpha)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> -lambda_ * torch.<span class="built_in">pow</span>(-value, alpha)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">kto_loss</span>(<span class="params">predictions, targets</span>):</span><br><span class="line">    value = predictions - targets</span><br><span class="line">    utility = prospect_utility(value)</span><br><span class="line">    loss = -torch.mean(utility)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">    predictions = model(data)</span><br><span class="line">    loss = kto_loss(predictions, targets)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a><strong>Summary</strong></h3><table>
<thead>
<tr>
<th>Method</th>
<th>Loss Function</th>
<th>Optimizer</th>
</tr>
</thead>
<tbody><tr>
<td><strong>SFT</strong></td>
<td>Cross-Entropy Loss</td>
<td>AdamW, RMSprop, SGD</td>
</tr>
<tr>
<td><strong>LoRA</strong></td>
<td>Cross-Entropy Loss</td>
<td>AdamW, RMSprop, SGD</td>
</tr>
<tr>
<td><strong>DPO</strong></td>
<td>Preference Loss Function: $\log(1 + \exp(-\sigma (\hat{y}_a - \hat{y}_b)p))$</td>
<td>AdamW</td>
</tr>
<tr>
<td><strong>KTO</strong></td>
<td>Prospect Theory Utility Function: $-\mathbb{E}[\mathcal{U}(y_{\text{pred}} - y_{\text{true}})]$</td>
<td>AdamW</td>
</tr>
</tbody></table>
<p>Through the organization of this document, readers can clearly understand the principles, specific implementation steps, loss function designs, and optimizer selections for technologies such as SFT, LoRA, DPO, and KTO, especially in the context of fine-tuning large-scale pre-trained models like LLAMA3.</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>æ–‡ç« ä½œè€…: </span><span class="post-copyright-info"><a href="https://chenhuiyu.github.io">Huiyu Chen</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>æ–‡ç« é“¾æ¥: </span><span class="post-copyright-info"><a href="https://chenhuiyu.github.io/2024/10/23/NLP%20Insights/Introduction%20to%20LLM%20Training%20Terminology:%20LoRA,%20DPO,%20KTO,%20and%20SFT%20Technologies.zh-CN/">https://chenhuiyu.github.io/2024/10/23/NLP%20Insights/Introduction%20to%20LLM%20Training%20Terminology:%20LoRA,%20DPO,%20KTO,%20and%20SFT%20Technologies.zh-CN/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>ç‰ˆæƒå£°æ˜: </span><span class="post-copyright-info">æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ«å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨ <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº <a href="https://chenhuiyu.github.io" target="_blank">é»‘å¤´å‘†é±¼è¿›åŒ–ä¹‹æ—…</a>ï¼</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/LLM/">LLM</a></div><div class="post-share"><div class="social-share" data-image="/img/butterfly-icon.png" data-sites="facebook,x,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2024/10/23/NLP%20Insights/LLM%E8%AE%AD%E7%BB%83%E6%9C%AF%E8%AF%AD%E4%BB%8B%E7%BB%8D%EF%BC%9ASFT%E3%80%81LoRA%E3%80%81Alignment%E3%80%81KTO%E5%92%8CDPO.en/" title="LoRA, DPO, KTO ä¸ SFT æŠ€æœ¯è¯¦è§£"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">ä¸Šä¸€ç¯‡</div><div class="info-item-2">LoRA, DPO, KTO ä¸ SFT æŠ€æœ¯è¯¦è§£</div></div><div class="info-2"><div class="info-item-1">LoRA, DPO, KTO ä¸ SFT æŠ€æœ¯è¯¦è§£æœ¬ç¯‡æ–‡æ¡£å°†è¯¦ç»†ä»‹ç»å‡ ç§åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ LLAMA3ï¼‰å¾®è°ƒå’Œä¼˜åŒ–ä¸­çš„é‡è¦æŠ€æœ¯ï¼ŒåŒ…æ‹¬ SFTï¼ˆSupervised Fine-Tuningï¼‰ã€LoRAï¼ˆLow-Rank Adaptationï¼‰ã€Alignment æŠ€æœ¯ã€KTOï¼ˆKahneman-Tversky Optimizationï¼‰ å’Œ DPOï¼ˆDirect Preference Optimizationï¼‰ã€‚æ–‡ä¸­è¿˜å°†è¯¦ç»†é˜è¿°æ¯ç§æŠ€æœ¯çš„åŸç†ã€å…·ä½“å®ç°æ–¹æ³•ä»¥åŠç›¸åº”çš„æŸå¤±å‡½æ•°ä¸ä¼˜åŒ–å™¨é€‰æ‹©ã€‚  1. SFTï¼ˆSupervised Fine-Tuningï¼‰1.1 åŸç†SFT æ˜¯ä¸€ç§ä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•ï¼Œé€šè¿‡ç›‘ç£å­¦ä¹ å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œè°ƒæ•´æ¨¡å‹çš„å‚æ•°ä½¿å…¶åœ¨ç‰¹å®šä»»åŠ¡ä¸Šè¡¨ç°æ›´å¥½ã€‚SFT é€šå¸¸ç”¨äºé’ˆå¯¹ç‰¹å®šçš„æ ‡æ³¨æ•°æ®è¿›è¡Œæ¨¡å‹å¾®è°ƒï¼Œè®­ç»ƒçš„è¿‡ç¨‹ç±»ä¼¼äºå¸¸è§„çš„ç›‘ç£å­¦ä¹ ã€‚ 1.2 å®ç°æ–¹æ³• é€‰æ‹©é¢„è®­ç»ƒæ¨¡å‹ï¼šå¦‚ GPTã€BERT ç­‰è¯­è¨€æ¨¡å‹ã€‚ å‡†å¤‡æ ‡æ³¨æ•°æ®é›†ï¼šæ•°æ®é›†åŒ…å«è¾“å…¥å’Œè¾“å‡ºå¯¹ã€‚ è®­ç»ƒæ¨¡å‹ï¼šä½¿ç”¨æ ‡å‡†çš„äº¤å‰ç†µæŸå¤±å‡½æ•°å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œé€šè¿‡æ¢¯åº¦ä¸‹é™ä¼˜åŒ–å‚æ•°ã€‚  1.3 æ ¸å¿ƒä»£ç ä½¿ç”¨ Hugging Face çš„ ...</div></div></div></a><a class="pagination-related" href="/2024/10/23/NLP%20Insights/Introduction%20to%20LLM%20Training%20Terminology:%20LoRA,%20DPO,%20KTO,%20and%20SFT%20Technologies.en/" title="Detailed Explanation of LoRA, DPO, KTO, and SFT Technologies"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">ä¸‹ä¸€ç¯‡</div><div class="info-item-2">Detailed Explanation of LoRA, DPO, KTO, and SFT Technologies</div></div><div class="info-2"><div class="info-item-1">Introduction to LLM Training Terminology:LoRA, DPO, KTO, and SFT TechnologiesThis document provides a detailed introduction to several important techniques used in fine-tuning and optimizing large language models (such as LLAMA3), including SFT (Supervised Fine-Tuning), LoRA (Low-Rank Adaptation), Alignment technologies, KTO (Kahneman-Tversky Optimization), and DPO (Direct Preference Optimization). The document also elaborates on the principles of each technique, specific implementation metho...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>ç›¸å…³æ¨è</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2024/02/27/NLP%20Insights/FastChat%20%E8%AE%AD%E7%BB%83%E8%84%9A%E6%9C%AC%E4%BB%A3%E7%A0%81%E9%80%90%E8%A1%8C%E8%A7%A3%E6%9E%90-Train.py%20%E3%80%90FastChat%20%E7%B3%BB%E5%88%97%E7%AC%AC%201%20%E7%AF%87%E3%80%91.en/" title="FastChat è®­ç»ƒè„šæœ¬ä»£ç é€è¡Œè§£æ-Train.py ã€FastChat ç³»åˆ—ç¬¬ 1 ç¯‡ã€‘"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-27</div><div class="info-item-2">FastChat è®­ç»ƒè„šæœ¬ä»£ç é€è¡Œè§£æ-Train.py ã€FastChat ç³»åˆ—ç¬¬ 1 ç¯‡ã€‘</div></div><div class="info-2"><div class="info-item-1">FastChat è®­ç»ƒè„šæœ¬ä»£ç é€è¡Œè§£æ-Train.py ã€FastChat ç³»åˆ—ç¬¬ 1 ç¯‡ã€‘åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥æ¢è®¨ FastChat çš„ train.py è„šæœ¬ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè®­ç»ƒå’Œä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹çš„å…³é”®ç»„ä»¶ã€‚FastChat æ˜¯ä¸€ä¸ªå…ˆè¿›çš„å¼€æºå¹³å°ï¼Œä¸“æ³¨äºå¼€å‘ã€éƒ¨ç½²å’Œè¯„ä¼°åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èŠå¤©æœºå™¨äººã€‚è¯¥å¹³å°ä¸ä»…æä¾›å¯¹é¡¶å°–æ¨¡å‹å¦‚ Vicuna å’Œ MT-Bench çš„æ”¯æŒï¼Œè¿˜åŒ…æ‹¬ä¸€ä¸ªåˆ†å¸ƒå¼çš„å¤šæ¨¡å‹æœåŠ¡ç³»ç»Ÿï¼Œé…å¤‡äº† Web UI å’Œä¸ OpenAI å…¼å®¹çš„ RESTful APIï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿé«˜æ•ˆåœ°è®­ç»ƒå’Œè¯„ä¼°ä»–ä»¬çš„æ¨¡å‹ã€‚ æœ¬æ–‡çš„æ·±å…¥åˆ†æå°†èšç„¦äº train.py è„šæœ¬çš„æºä»£ç ã€‚è¿™ä¸ªè„šæœ¬æ˜¯åŸºäº transformers åº“çš„è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹è®­ç»ƒè„šæœ¬ï¼Œæ¶µç›–äº†æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒå’Œä¿å­˜ç­‰å…³é”®æ­¥éª¤ã€‚æˆ‘ä»¬æ—¨åœ¨æä¾›å¯¹ train.py ä¸­æ¯ä¸ªç±»å’Œå‡½æ•°çš„è¯¦ç»†è§£é‡Šï¼ŒåŒ…æ‹¬å®ƒä»¬çš„åŠŸèƒ½å’Œåœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­çš„ä½œç”¨ã€‚ 1. å¯¼å…¥æ¨¡å—1. å†…ç½®æ¨¡å—è¿™äº›æ˜¯ Python è‡ªå¸¦çš„æ ‡å‡†åº“æ¨¡å—ï¼Œæ— éœ€é¢å¤–å®‰è£…ã€‚ 1from dataclasses import dataclass, field  å¯¼å…¥ Pytho...</div></div></div></a><a class="pagination-related" href="/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2.zh-CN/" title="MoEæ¨¡å‹çš„çš„å¯è§†åŒ–æŒ‡å—ï¼šæ­ç§˜ MoE åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„è§’è‰²"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-02-11</div><div class="info-item-2">MoEæ¨¡å‹çš„çš„å¯è§†åŒ–æŒ‡å—ï¼šæ­ç§˜ MoE åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„è§’è‰²</div></div><div class="info-2"><div class="info-item-1">MoE æ¨¡å‹çš„çš„å¯è§†åŒ–æŒ‡å—ï¼šæ­ç§˜ MoE åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„è§’è‰²åŸæ–‡åœ°å€ï¼šA Visual Guide to Mixture of Experts (MoE) ğŸ“… ä½œè€…ï¼šMaarten Grootendorst ğŸ“† æ—¥æœŸï¼š2024 å¹´ 10 æœˆ 7 æ—¥  æ¢ç´¢è¯­è¨€æ¨¡å‹ï¼šæ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMoEï¼‰å¯è§†åŒ–æŒ‡å—ç›®å½• MoE æ¨¡å‹çš„çš„å¯è§†åŒ–æŒ‡å—ï¼šæ­ç§˜ MoE åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„è§’è‰² æ¢ç´¢è¯­è¨€æ¨¡å‹ï¼šæ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMoEï¼‰å¯è§†åŒ–æŒ‡å— ç›®å½• ä»€ä¹ˆæ˜¯æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¨¡å‹ï¼Ÿ Experts Dense Layers Sparse Layers What does an Expert Learn? ä¸“å®¶çš„æ¶æ„ï¼ˆArchitecture of Expertsï¼‰      å½“æˆ‘ä»¬æŸ¥çœ‹æœ€æ–°å‘å¸ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼ŒLarge Language Modelsï¼‰æ—¶ï¼Œå¸¸å¸¸ä¼šåœ¨æ ‡é¢˜ä¸­çœ‹åˆ° â€œMoEâ€ã€‚è¿™ä¸ª â€œMoEâ€ ä»£è¡¨ä»€ä¹ˆï¼Ÿä¸ºä»€ä¹ˆè¿™ä¹ˆå¤š LLM éƒ½åœ¨ä½¿ç”¨å®ƒï¼Ÿ åœ¨è¿™ä»½å¯è§†åŒ–æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬ä¼šé€šè¿‡ 50 å¤šä¸ªå¯è§†åŒ–å›¾ç¤ºï¼Œé€æ­¥æ¢ç´¢è¿™ä¸€å…³é”®ç»„ä»¶ï¼š**Mixture of Experts (MoE)**ã€‚   å›¾ç¤ºå†…...</div></div></div></a><a class="pagination-related" href="/2024/02/19/NLP%20Insights/Understanding%20the%20Differences%20Between%20Fine-tuning%20and%20Further%20Pretraining%20in%20Large%20Language%20Models.en/" title="Understanding the Differences Between Fine-tuning and Further Pretraining in Large Language Models"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-19</div><div class="info-item-2">Understanding the Differences Between Fine-tuning and Further Pretraining in Large Language Models</div></div><div class="info-2"><div class="info-item-1">Understanding the Differences Between Fine-tuning and Further Pretraining in Large Language ModelsIn the world of Natural Language Processing (NLP), the advent of large language models like GPT and BERT has revolutionized how we approach tasks such as text classification, sentiment analysis, and question-answering. Two pivotal techniques in leveraging these models are Fine-tuning and Further Pretraining. While they may seem similar at a glance, they cater to different needs and scenarios in t...</div></div></div></a><a class="pagination-related" href="/2023/07/27/NLP%20Insights/Prompt%20Engineering.zh-CN/" title="Prompt Engineering"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2023-07-27</div><div class="info-item-2">Prompt Engineering</div></div><div class="info-2"><div class="info-item-1">Prompt EngineeringPrompt Engineering, ä¹Ÿè¢«ç§°ä¸ºä¸Šä¸‹æ–‡æç¤ºï¼Œæ˜¯æŒ‡åœ¨ä¸æ›´æ–°æ¨¡å‹æƒé‡çš„æƒ…å†µä¸‹ï¼Œä¸LLMï¼ˆè¯­è¨€æ¨¡å‹ï¼‰è¿›è¡Œäº¤äº’ä»¥å¼•å¯¼å…¶äº§ç”ŸæœŸæœ›è¾“å‡ºçš„æ–¹æ³•ã€‚å®ƒæ˜¯ä¸€é—¨å®è¯ç§‘å­¦ï¼Œæç¤ºå·¥ç¨‹æ–¹æ³•çš„æ•ˆæœåœ¨ä¸åŒæ¨¡å‹ä¹‹é—´å¯èƒ½ä¼šæœ‰å¾ˆå¤§çš„å·®å¼‚ï¼Œå› æ­¤éœ€è¦è¿›è¡Œå¤§é‡çš„å®éªŒå’Œè¯•æ¢ã€‚ æœ¬æ–‡ä»…å…³æ³¨è‡ªå›å½’è¯­è¨€æ¨¡å‹çš„æç¤ºå·¥ç¨‹ï¼Œä¸æ¶‰åŠå¡«ç©ºæµ‹è¯•ã€å›¾åƒç”Ÿæˆæˆ–å¤šæ¨¡æ€æ¨¡å‹ã€‚åœ¨æœ¬è´¨ä¸Šï¼Œæç¤ºå·¥ç¨‹çš„ç›®æ ‡æ˜¯å®ç°æ¨¡å‹çš„å¯¹é½å’Œå¯æ“æ§æ€§ã€‚æ‚¨å¯ä»¥æŸ¥é˜…æˆ‘ä¹‹å‰å…³äºå¯æ§æ–‡æœ¬ç”Ÿæˆçš„å¸–å­ã€‚ åŸºæœ¬æç¤ºæ–¹æ³•zero-shotå­¦ä¹ å’Œfew-shotå­¦ä¹ æ˜¯ä¸¤ç§æœ€åŸºæœ¬çš„æç¤ºæ¨¡å‹æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•ç”±è®¸å¤šLLMè®ºæ–‡é¦–åˆ›ï¼Œå¹¶ä¸”é€šå¸¸ç”¨äºè¯„ä¼°LLMæ€§èƒ½ã€‚ zero-shotå­¦ä¹ zero-shotå­¦ä¹ æ˜¯å°†ä»»åŠ¡æ–‡æœ¬ç›´æ¥è¾“å…¥æ¨¡å‹å¹¶è¦æ±‚è·å¾—ç»“æœã€‚ ï¼ˆæ‰€æœ‰æƒ…æ„Ÿåˆ†æç¤ºä¾‹æ¥è‡ªäºSST-2æ•°æ®é›†ï¼‰ 12Text: i'll bet the video game is a lot more fun than the film.Sentiment: few-shotå­¦ä¹ few-shotå­¦ä¹ é€šè¿‡æä¾›ä¸€ç»„é«˜è´¨é‡çš„ç¤ºä¾‹æ¼”ç¤ºï¼Œæ¯ä¸ªç¤ºä¾‹éƒ½åŒ…å«ç›®æ ‡ä»»åŠ¡çš„è¾“å…¥å’ŒæœŸæœ›è¾“å‡ºã€‚å½“æ¨¡å‹é¦–...</div></div></div></a><a class="pagination-related" href="/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2.en/" title="MoEæ¨¡å‹çš„çš„å¯è§†åŒ–æŒ‡å—ï¼šæ­ç§˜ MoE åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„è§’è‰²"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-02-11</div><div class="info-item-2">MoEæ¨¡å‹çš„çš„å¯è§†åŒ–æŒ‡å—ï¼šæ­ç§˜ MoE åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„è§’è‰²</div></div><div class="info-2"><div class="info-item-1">MoE æ¨¡å‹çš„çš„å¯è§†åŒ–æŒ‡å—ï¼šæ­ç§˜ MoE åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„è§’è‰²åŸæ–‡åœ°å€ï¼šA Visual Guide to Mixture of Experts (MoE) ğŸ“… ä½œè€…ï¼šMaarten Grootendorst ğŸ“† æ—¥æœŸï¼š2024 å¹´ 10 æœˆ 7 æ—¥  æ¢ç´¢è¯­è¨€æ¨¡å‹ï¼šæ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMoEï¼‰å¯è§†åŒ–æŒ‡å—ç›®å½• MoE æ¨¡å‹çš„çš„å¯è§†åŒ–æŒ‡å—ï¼šæ­ç§˜ MoE åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„è§’è‰² æ¢ç´¢è¯­è¨€æ¨¡å‹ï¼šæ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMoEï¼‰å¯è§†åŒ–æŒ‡å— ç›®å½• ä»€ä¹ˆæ˜¯æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¨¡å‹ï¼Ÿ Experts Dense Layers Sparse Layers What does an Expert Learn? ä¸“å®¶çš„æ¶æ„ï¼ˆArchitecture of Expertsï¼‰      å½“æˆ‘ä»¬æŸ¥çœ‹æœ€æ–°å‘å¸ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼ŒLarge Language Modelsï¼‰æ—¶ï¼Œå¸¸å¸¸ä¼šåœ¨æ ‡é¢˜ä¸­çœ‹åˆ° â€œMoEâ€ã€‚è¿™ä¸ª â€œMoEâ€ ä»£è¡¨ä»€ä¹ˆï¼Ÿä¸ºä»€ä¹ˆè¿™ä¹ˆå¤š LLM éƒ½åœ¨ä½¿ç”¨å®ƒï¼Ÿ åœ¨è¿™ä»½å¯è§†åŒ–æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬ä¼šé€šè¿‡ 50 å¤šä¸ªå¯è§†åŒ–å›¾ç¤ºï¼Œé€æ­¥æ¢ç´¢è¿™ä¸€å…³é”®ç»„ä»¶ï¼š**Mixture of Experts (MoE)**ã€‚   å›¾ç¤ºå†…...</div></div></div></a><a class="pagination-related" href="/2023/07/28/NLP%20Insights/LONGNET.en/" title="LONGNET - Scaling Transformers to 1,000,000,000 Tokens"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2023-07-28</div><div class="info-item-2">LONGNET - Scaling Transformers to 1,000,000,000 Tokens</div></div><div class="info-2"><div class="info-item-1">LONGNETï¼šå°†Transformeræ‰©å±•åˆ°10äº¿ä¸ªæ ‡è®°åœ¨æœ¬ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†è¯¦ç»†è®¨è®ºä¸€ä¸ªè¿‘æœŸå‘å¸ƒçš„å…ˆè¿›æ¨¡å‹â€”â€”â€œLongNetâ€ã€‚è¯¥æ¨¡å‹ç”±å¾®è½¯äºšæ´²ç ”ç©¶é™¢ç ”å‘ï¼Œäºå¤§çº¦ä¸¤å‘¨å‰æ­£å¼å…¬å¸ƒã€‚LongNetåŸºäºTransformeræ¨¡å‹æ„å»ºï¼Œå…¶æ ¸å¿ƒç†å¿µåœ¨äºæ‹“å±•Transformerçš„åº”ç”¨è§„æ¨¡ã€‚å€¼å¾—ä¸€æçš„æ˜¯ï¼Œç ”ç©¶å›¢é˜ŸæˆåŠŸåœ°å°†å…¶æ‰©å±•è‡³å¤„ç†10äº¿ä¸ªä»¤ç‰Œçš„è§„æ¨¡ã€‚å¯¹äºç†Ÿæ‚‰è¯­è¨€æ¨¡å‹çš„äººæ¥è¯´ï¼Œä¼šæ˜ç™½åºåˆ—é•¿åº¦å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œå› ä¸ºåºåˆ—é•¿åº¦å†³å®šäº†åœ¨æ‰§è¡Œæ³¨æ„åŠ›æœºåˆ¶æ—¶ï¼Œèƒ½å¤Ÿå…³è”çš„ä»¤ç‰Œæ•°é‡ï¼Œä»è€Œå½±å“æ¨¡å‹å¯ä»¥è·å–çš„ä¸Šä¸‹æ–‡ä¿¡æ¯é•¿åº¦ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¸Œæœ›åƒGPTè¿™æ ·çš„æ¨¡å‹èƒ½æ‹¥æœ‰æ›´é•¿çš„ä¸Šä¸‹æ–‡ï¼Œä½¿å¾—æ¨¡å‹å¯ä»¥å‚è€ƒæ›´ä¹…ä¹‹å‰çš„å•è¯æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªä»¤ç‰Œã€‚è€ŒLongNetå°±æˆåŠŸåœ°å°†è¿™ä¸ªèƒ½åŠ›æ‰©å±•åˆ°äº†10äº¿ä¸ªä»¤ç‰Œã€‚ä»¥ä¸‹å›¾ä¸ºä¾‹ï¼Œå¯ä»¥æ¸…æ™°çœ‹å‡ºï¼ŒGPTçš„åºåˆ—é•¿åº¦ä»…ä¸º512ï¼Œè€ŒPower Transformerçš„åºåˆ—é•¿åº¦å¯æ‰©å±•è‡³12ã€000ã€64ã€262ã€000ã€ç”šè‡³1000ä¸‡ï¼Œç„¶è€ŒLongNetå°†åºåˆ—é•¿åº¦æ‰©å±•è‡³æƒŠäººçš„10äº¿ä¸ªä»¤ç‰Œã€‚è¯•æƒ³ä¸€ä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥å°†æ‰€æœ‰ç»´åŸºç™¾ç§‘çš„æ–‡æœ¬ä¿¡æ¯è¾“å…¥åˆ°æ¨¡å‹ä¸­ï¼Œæ¨¡å‹å¯ä»¥åˆ©ç”¨æ‰€æœ‰è¿™äº›ä»¤ç‰Œè¿›è¡Œæ³¨æ„åŠ›è®¡ç®—ã€‚æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬é¦–å…ˆæ¥äº†è§£ä¸€ä¸‹...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Huiyu Chen</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">æ–‡ç« </div><div class="length-num">125</div></a><a href="/tags/"><div class="headline">æ ‡ç­¾</div><div class="length-num">46</div></a><a href="/categories/"><div class="headline">åˆ†ç±»</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>å…¬å‘Š</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>ç›®å½•</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Introduction-to-LLM-Training-Terminology-LoRA-DPO-KTO-and-SFT-Technologies"><span class="toc-number">1.</span> <span class="toc-text">Introduction to LLM Training Terminology:LoRA, DPO, KTO, and SFT Technologies</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-SFT-Supervised-Fine-Tuning"><span class="toc-number"></span> <span class="toc-text">1. SFT (Supervised Fine-Tuning)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-Principle"><span class="toc-number">1.</span> <span class="toc-text">1.1 Principle</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-Implementation-Method"><span class="toc-number">2.</span> <span class="toc-text">1.2 Implementation Method</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-Core-Code"><span class="toc-number">3.</span> <span class="toc-text">1.3 Core Code</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-LoRA-Low-Rank-Adaptation"><span class="toc-number"></span> <span class="toc-text">2. LoRA (Low-Rank Adaptation)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-Principle"><span class="toc-number">1.</span> <span class="toc-text">2.1 Principle</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-Implementation-Method"><span class="toc-number">2.</span> <span class="toc-text">2.2 Implementation Method</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-Layers-to-Fine-Tune-vs-Layers-to-Keep-Unchanged"><span class="toc-number">3.</span> <span class="toc-text">2.3 Layers to Fine-Tune vs. Layers to Keep Unchanged</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Layers-to-Fine-Tune"><span class="toc-number">3.1.</span> <span class="toc-text">Layers to Fine-Tune</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Layers-to-Keep-Unchanged"><span class="toc-number">3.2.</span> <span class="toc-text">Layers to Keep Unchanged</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-Loss-Function"><span class="toc-number">4.</span> <span class="toc-text">2.4 Loss Function</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-5-Optimizer"><span class="toc-number">5.</span> <span class="toc-text">2.5 Optimizer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-6-Core-Code"><span class="toc-number">6.</span> <span class="toc-text">2.6 Core Code</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Alignment-Alignment-Techniques"><span class="toc-number"></span> <span class="toc-text">3. Alignment (Alignment Techniques)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-What-is-Model-Alignment"><span class="toc-number">1.</span> <span class="toc-text">1. What is Model Alignment?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-Mathematical-Principles-of-Model-Alignment"><span class="toc-number">2.</span> <span class="toc-text">2. Mathematical Principles of Model Alignment</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-Policy-Model"><span class="toc-number">2.1.</span> <span class="toc-text">2.1 Policy Model</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-Mechanism-for-Increasing-Positive-Sample-Probability-and-Decreasing-Negative-Sample-Probability"><span class="toc-number">2.2.</span> <span class="toc-text">2.2 Mechanism for Increasing Positive Sample Probability and Decreasing Negative Sample Probability</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-Role-of-Loss-Functions-and-KL-Divergence"><span class="toc-number">3.</span> <span class="toc-text">3. Role of Loss Functions and KL Divergence</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-Role-of-KL-Divergence"><span class="toc-number">3.1.</span> <span class="toc-text">3.1 Role of KL Divergence</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-Loss-Function-Formulas"><span class="toc-number">3.2.</span> <span class="toc-text">3.2 Loss Function Formulas</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Loss-Function-in-DPO"><span class="toc-number">3.2.1.</span> <span class="toc-text">Loss Function in DPO:</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Loss-Function-in-KTO"><span class="toc-number">3.2.2.</span> <span class="toc-text">Loss Function in KTO:</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-How-to-Optimize-the-Model"><span class="toc-number">4.</span> <span class="toc-text">4. How to Optimize the Model</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-DPO-Direct-Preference-Optimization"><span class="toc-number"></span> <span class="toc-text">4. DPO (Direct Preference Optimization)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-Principle"><span class="toc-number">1.</span> <span class="toc-text">4.1 Principle</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-Loss-Function"><span class="toc-number">2.</span> <span class="toc-text">4.2 Loss Function</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-Optimizer"><span class="toc-number">3.</span> <span class="toc-text">4.3 Optimizer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-Core-Code"><span class="toc-number">4.</span> <span class="toc-text">4.4 Core Code</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-KTO-Kahneman-Tversky-Optimization"><span class="toc-number"></span> <span class="toc-text">5. KTO (Kahneman-Tversky Optimization)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-Principle"><span class="toc-number">1.</span> <span class="toc-text">5.1 Principle</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-Loss-Function"><span class="toc-number">2.</span> <span class="toc-text">5.2 Loss Function</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-Optimizer"><span class="toc-number">3.</span> <span class="toc-text">5.3 Optimizer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-4-Core-Code"><span class="toc-number">4.</span> <span class="toc-text">5.4 Core Code</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Summary"><span class="toc-number">5.</span> <span class="toc-text">Summary</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>æœ€æ–°æ–‡ç« </span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/21/NLP%20Insights/Evaluation%20of%20Generation-Based%20Large%20Language%20Models%20(LLMs):%20Opportunities%20and%20Challenges%20from%20Generation%20to%20Judgment.zh-CN/" title="Untitled">Untitled</a><time datetime="2026-02-20T22:15:00.000Z" title="å‘è¡¨äº 2026-02-21 06:15:00">2026-02-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/21/NLP%20Insights/Evaluation%20of%20Generation-Based%20Large%20Language%20Models%20(LLMs):%20Opportunities%20and%20Challenges%20from%20Generation%20to%20Judgment.en/" title="Untitled">Untitled</a><time datetime="2026-02-20T22:15:00.000Z" title="å‘è¡¨äº 2026-02-21 06:15:00">2026-02-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/24/NLP%20Insights/SeCom%20Redefining%20Memory%20Management%20in%20Conversational%20AI.en/" title="SeCom: Redefining Memory Management in Conversational AI">SeCom: Redefining Memory Management in Conversational AI</a><time datetime="2025-06-24T08:00:00.000Z" title="å‘è¡¨äº 2025-06-24 16:00:00">2025-06-24</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/24/NLP%20Insights/SeCom%20Redefining%20Memory%20Management%20in%20Conversational%20AI.zh-CN/" title="SeCom: Redefining Memory Management in Conversational AI">SeCom: Redefining Memory Management in Conversational AI</a><time datetime="2025-06-24T08:00:00.000Z" title="å‘è¡¨äº 2025-06-24 16:00:00">2025-06-24</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/24/NLP%20Insights/SeCom:%20%E9%87%8D%E6%96%B0%E5%AE%9A%E4%B9%89%E5%AF%B9%E8%AF%9DAI%E7%9A%84%E8%AE%B0%E5%BF%86%E7%AE%A1%E7%90%86.en/" title="SeCom: é‡æ–°å®šä¹‰å¯¹è¯AIçš„è®°å¿†ç®¡ç†">SeCom: é‡æ–°å®šä¹‰å¯¹è¯AIçš„è®°å¿†ç®¡ç†</a><time datetime="2025-06-24T08:00:00.000Z" title="å‘è¡¨äº 2025-06-24 16:00:00">2025-06-24</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 - 2026 By Huiyu Chen</span><span class="framework-info"><span>æ¡†æ¶ </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 6.3.0</a><span class="footer-separator">|</span><span>ä¸»é¢˜ </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.5.4</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="é˜…è¯»æ¨¡å¼"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="æ—¥é—´å’Œå¤œé—´æ¨¡å¼åˆ‡æ¢"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="å•æ å’ŒåŒæ åˆ‡æ¢"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="è®¾ç½®"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="ç›®å½•"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="å›åˆ°é¡¶éƒ¨"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.4"></script><script src="/js/main.js?v=5.5.4"></script><div class="js-pjax"></div><script src="/js/lang-switch.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>