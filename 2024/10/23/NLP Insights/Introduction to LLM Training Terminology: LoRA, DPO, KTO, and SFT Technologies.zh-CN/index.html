<!DOCTYPE html><html class="appearance-auto" lang="zh-CN"><head><meta charset="UTF-8"><title>Detailed Explanation of LoRA, DPO, KTO, and SFT Technologies</title><meta name="description" content="May the Force be with you"><meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no, initial-scale=1"><!-- Google Analytics --><!-- End Google Analytics -->
<!-- Baidu Analytics --><!-- End Baidu Analytics --><link rel="icon" href="/images/favicon.ico"><link rel="stylesheet" href="/style/common/bulma.css"><link rel="stylesheet" href="/style/base.css"><link rel="stylesheet" href="/style/common/helper.css"><script src="/js/common.js"></script><link rel="stylesheet" href="/style/post.css"><link rel="stylesheet" href="/style/themes/highlight-theme-light.css"><link rel="stylesheet" href="/style/common/jquery.fancybox.min.css"><script src="/js/highlight.pack.js"></script><meta name="description" content="Introduction to LLM Training Terminology:LoRA, DPO, KTO, and SFT TechnologiesThis document provides a detailed introduction to several important techniques used in fine-tuning and optimizing large language models (such as LLAMA3), including SFT (Supervised Fine-Tuning), LoRA (Low-Rank Adaptation), Alignment technologies, KTO (Kahneman-Tversky Optimization).."><meta name="generator" content="Hexo 6.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="黑头呆鱼进化之旅" type="application/atom+xml">
</head><body class="is-flex is-flex-direction-column"><header class="header-widget is-flex-shrink-0 is-hidden-mobile"><div class="container is-fullhd is-flex is-justify-content-space-between is-align-items-center is-full-height"><section class="is-hidden-mobile is-flex-shrink-0"><h2><a href="/">user's blog</a></h2></section><h3 class="is-hidden-mobile is-family-serif is-full-height is-flex is-align-items-center is-flex-shrink-0"><div class="is-full-height" id="postTopic"><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">Detailed Explanation of LoRA, DPO, KTO, and SFT Technologies</p><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">点击返回顶部</p></div></h3><aside class="is-flex-shrink-0"><h3 class="is-inline-block"><a href="/">首页</a></h3><h3 class="is-inline-block"><a href="/about">关于</a></h3><h3 class="is-inline-block"><a href="/archives">归档</a></h3></aside></div></header><header class="is-flex header-widget is-flex-shrink-0 is-align-items-center is-justify-content-center is-hidden-tablet"><h3 class="is-inline-block"><a href="/">首页</a></h3><h3 class="is-inline-block"><a href="/about">关于</a></h3><h3 class="is-inline-block"><a href="/archives">归档</a></h3></header><main><main class="container is-max-widescreen content section post-page pt-4 px-4"><div class="columns is-flex-desktop is-justify-content-center is-flex-direction-row-reverse"><div class="column is-3 is-hidden-mobile"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Introduction-to-LLM-Training-Terminology-LoRA-DPO-KTO-and-SFT-Technologies"><span class="toc-text">Introduction to LLM Training Terminology:LoRA, DPO, KTO, and SFT Technologies</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-SFT-Supervised-Fine-Tuning"><span class="toc-text">1. SFT (Supervised Fine-Tuning)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-Principle"><span class="toc-text">1.1 Principle</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-Implementation-Method"><span class="toc-text">1.2 Implementation Method</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-Core-Code"><span class="toc-text">1.3 Core Code</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-LoRA-Low-Rank-Adaptation"><span class="toc-text">2. LoRA (Low-Rank Adaptation)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-Principle"><span class="toc-text">2.1 Principle</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-Implementation-Method"><span class="toc-text">2.2 Implementation Method</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-Layers-to-Fine-Tune-vs-Layers-to-Keep-Unchanged"><span class="toc-text">2.3 Layers to Fine-Tune vs. Layers to Keep Unchanged</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Layers-to-Fine-Tune"><span class="toc-text">Layers to Fine-Tune</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Layers-to-Keep-Unchanged"><span class="toc-text">Layers to Keep Unchanged</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-Loss-Function"><span class="toc-text">2.4 Loss Function</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-5-Optimizer"><span class="toc-text">2.5 Optimizer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-6-Core-Code"><span class="toc-text">2.6 Core Code</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Alignment-Alignment-Techniques"><span class="toc-text">3. Alignment (Alignment Techniques)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-What-is-Model-Alignment"><span class="toc-text">1. What is Model Alignment?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-Mathematical-Principles-of-Model-Alignment"><span class="toc-text">2. Mathematical Principles of Model Alignment</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-Policy-Model"><span class="toc-text">2.1 Policy Model</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-Mechanism-for-Increasing-Positive-Sample-Probability-and-Decreasing-Negative-Sample-Probability"><span class="toc-text">2.2 Mechanism for Increasing Positive Sample Probability and Decreasing Negative Sample Probability</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-Role-of-Loss-Functions-and-KL-Divergence"><span class="toc-text">3. Role of Loss Functions and KL Divergence</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-Role-of-KL-Divergence"><span class="toc-text">3.1 Role of KL Divergence</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-Loss-Function-Formulas"><span class="toc-text">3.2 Loss Function Formulas</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Loss-Function-in-DPO"><span class="toc-text">Loss Function in DPO:</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Loss-Function-in-KTO"><span class="toc-text">Loss Function in KTO:</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-How-to-Optimize-the-Model"><span class="toc-text">4. How to Optimize the Model</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-DPO-Direct-Preference-Optimization"><span class="toc-text">4. DPO (Direct Preference Optimization)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-Principle"><span class="toc-text">4.1 Principle</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-Loss-Function"><span class="toc-text">4.2 Loss Function</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-Optimizer"><span class="toc-text">4.3 Optimizer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-Core-Code"><span class="toc-text">4.4 Core Code</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-KTO-Kahneman-Tversky-Optimization"><span class="toc-text">5. KTO (Kahneman-Tversky Optimization)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-Principle"><span class="toc-text">5.1 Principle</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-Loss-Function"><span class="toc-text">5.2 Loss Function</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-Optimizer"><span class="toc-text">5.3 Optimizer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-4-Core-Code"><span class="toc-text">5.4 Core Code</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Summary"><span class="toc-text">Summary</span></a></li></ol></div><div class="column is-9"><header class="my-4"><a href="/tags/LLM"><i class="tag post-item-tag">LLM</i></a></header><h1 class="mt-0 mb-1 is-family-serif" id="postTitle">Detailed Explanation of LoRA, DPO, KTO, and SFT Technologies</h1><time class="has-text-grey" datetime="2024-10-23T08:26:29.000Z">2024-10-23</time><article class="mt-2 post-content"><h3 id="Introduction-to-LLM-Training-Terminology-LoRA-DPO-KTO-and-SFT-Technologies"><a href="#Introduction-to-LLM-Training-Terminology-LoRA-DPO-KTO-and-SFT-Technologies" class="headerlink" title="Introduction to LLM Training Terminology:LoRA, DPO, KTO, and SFT Technologies"></a><strong>Introduction to LLM Training Terminology:LoRA, DPO, KTO, and SFT Technologies</strong></h3><p>This document provides a detailed introduction to several important techniques used in fine-tuning and optimizing large language models (such as LLAMA3), including <strong>SFT (Supervised Fine-Tuning)</strong>, <strong>LoRA (Low-Rank Adaptation)</strong>, <strong>Alignment</strong> technologies, <strong>KTO (Kahneman-Tversky Optimization)</strong>, and <strong>DPO (Direct Preference Optimization)</strong>. The document also elaborates on the principles of each technique, specific implementation methods, as well as the selection of corresponding loss functions and optimizers.</p>
<hr>
<h2 id="1-SFT-Supervised-Fine-Tuning"><a href="#1-SFT-Supervised-Fine-Tuning" class="headerlink" title="1. SFT (Supervised Fine-Tuning)"></a>1. <strong>SFT (Supervised Fine-Tuning)</strong></h2><h3 id="1-1-Principle"><a href="#1-1-Principle" class="headerlink" title="1.1 Principle"></a>1.1 <strong>Principle</strong></h3><p>SFT is a traditional fine-tuning method that adjusts the parameters of a pre-trained model through supervised learning to improve its performance on specific tasks. SFT is typically used to fine-tune models on specific labeled datasets, with the training process resembling standard supervised learning.</p>
<h3 id="1-2-Implementation-Method"><a href="#1-2-Implementation-Method" class="headerlink" title="1.2 Implementation Method"></a>1.2 <strong>Implementation Method</strong></h3><ul>
<li><strong>Select a Pre-trained Model</strong>: Such as GPT, BERT, and other language models.</li>
<li><strong>Prepare a Labeled Dataset</strong>: The dataset includes input-output pairs.</li>
<li><strong>Train the Model</strong>: Use a standard cross-entropy loss function to train the model, optimizing parameters through gradient descent.</li>
</ul>
<h3 id="1-3-Core-Code"><a href="#1-3-Core-Code" class="headerlink" title="1.3 Core Code"></a>1.3 <strong>Core Code</strong></h3><p>Using Hugging Face’s <code>Trainer</code> interface for SFT:</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Trainer, TrainingArguments, AutoModelForSeq2SeqLM, AutoTokenizer</span><br><span class="line"></span><br><span class="line">model_name = <span class="string">"gpt2"</span></span><br><span class="line">model = AutoModelForSeq2SeqLM.from_pretrained(model_name)</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_name)</span><br><span class="line"></span><br><span class="line">train_dataset = load_dataset(<span class="string">"my_dataset"</span>, split=<span class="string">"train"</span>)</span><br><span class="line"></span><br><span class="line">training_args = TrainingArguments(</span><br><span class="line">    output_dir=<span class="string">"./results"</span>,</span><br><span class="line">    evaluation_strategy=<span class="string">"epoch"</span>,</span><br><span class="line">    learning_rate=<span class="number">2e-5</span>,</span><br><span class="line">    per_device_train_batch_size=<span class="number">16</span>,</span><br><span class="line">    num_train_epochs=<span class="number">3</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model=model,</span><br><span class="line">    args=training_args,</span><br><span class="line">    train_dataset=train_dataset,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer.train()</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h2 id="2-LoRA-Low-Rank-Adaptation"><a href="#2-LoRA-Low-Rank-Adaptation" class="headerlink" title="2. LoRA (Low-Rank Adaptation)"></a>2. <strong>LoRA (Low-Rank Adaptation)</strong></h2><h3 id="2-1-Principle"><a href="#2-1-Principle" class="headerlink" title="2.1 Principle"></a>2.1 <strong>Principle</strong></h3><p>LoRA is a parameter-efficient fine-tuning technique that performs low-rank decomposition of the weight matrices in large models. It decomposes the original weight matrix $W$ into two low-rank matrices $B$ and $A$, and only fine-tunes these low-rank matrices. The design goal of LoRA is to reduce the number of fine-tuning parameters while retaining the pre-trained model weights, optimizing model performance by adjusting the low-rank matrices.</p>
<h3 id="2-2-Implementation-Method"><a href="#2-2-Implementation-Method" class="headerlink" title="2.2 Implementation Method"></a>2.2 <strong>Implementation Method</strong></h3><ul>
<li><strong>Weight Decomposition</strong>: For the model’s linear layers (such as the <code>q_proj</code> and <code>v_proj</code> layers in the attention mechanism), decompose the weight matrix into two low-rank matrices $B$ and $A$.</li>
<li><strong>Fine-Tune Specific Layers</strong>: Apply LoRA only to these specific linear layers, keeping other layers in the model unchanged.</li>
</ul>
<h3 id="2-3-Layers-to-Fine-Tune-vs-Layers-to-Keep-Unchanged"><a href="#2-3-Layers-to-Fine-Tune-vs-Layers-to-Keep-Unchanged" class="headerlink" title="2.3 Layers to Fine-Tune vs. Layers to Keep Unchanged"></a>2.3 <strong>Layers to Fine-Tune vs. Layers to Keep Unchanged</strong></h3><h4 id="Layers-to-Fine-Tune"><a href="#Layers-to-Fine-Tune" class="headerlink" title="Layers to Fine-Tune"></a><strong>Layers to Fine-Tune</strong></h4><p>LoRA is typically applied to the linear projection layers in Transformer models, especially several key layers in the multi-head attention mechanism:</p>
<ul>
<li><strong>q_proj</strong> (Query Projection Layer)</li>
<li><strong>k_proj</strong> (Key Projection Layer)</li>
<li><strong>v_proj</strong> (Value Projection Layer)</li>
<li><strong>o_proj</strong> (Output Projection Layer)</li>
<li><strong>ffn_up_proj</strong> and <strong>ffn_down_proj</strong> (Up and Down Projection Layers of the Feedforward Neural Network)</li>
</ul>
<h4 id="Layers-to-Keep-Unchanged"><a href="#Layers-to-Keep-Unchanged" class="headerlink" title="Layers to Keep Unchanged"></a><strong>Layers to Keep Unchanged</strong></h4><ul>
<li><strong>Embedding Layers</strong>: Responsible for encoding inputs and outputs, usually do not require fine-tuning.</li>
<li><strong>LayerNorm Layers</strong>: These layers are mainly used for normalization, do not contain many parameters, and are typically kept unchanged.</li>
<li><strong>Activation Function Layers</strong>: Non-linear activation functions like ReLU or GELU do not involve parameters and do not require fine-tuning.</li>
</ul>
<h3 id="2-4-Loss-Function"><a href="#2-4-Loss-Function" class="headerlink" title="2.4 Loss Function"></a>2.4 <strong>Loss Function</strong></h3><p>The loss function for LoRA is usually task-specific. In language generation tasks, LoRA uses <strong>cross-entropy loss</strong> to measure the difference between the generated text and the target text:</p>
<p>$$<br>\mathcal{L}<em>{\text{LoRA}} = - \sum</em>{i} y_i \log(\hat{y}_i)<br>$$</p>
<p>where $y_i$ is the true label, and $\hat{y}_i$ is the model’s output probability.</p>
<h3 id="2-5-Optimizer"><a href="#2-5-Optimizer" class="headerlink" title="2.5 Optimizer"></a>2.5 <strong>Optimizer</strong></h3><p>LoRA fine-tuning typically uses the <strong>AdamW</strong> optimizer, as shown in the following code:</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.AdamW(lora_model.parameters(), lr=<span class="number">5e-5</span>)</span><br></pre></td></tr></tbody></table></figure>

<h3 id="2-6-Core-Code"><a href="#2-6-Core-Code" class="headerlink" title="2.6 Core Code"></a>2.6 <strong>Core Code</strong></h3><p>Implementing LoRA using the <code>peft</code> library:</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> peft <span class="keyword">import</span> LoraConfig, get_peft_model</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForSeq2SeqLM</span><br><span class="line"></span><br><span class="line">model_name = <span class="string">"gpt2"</span></span><br><span class="line">model = AutoModelForSeq2SeqLM.from_pretrained(model_name)</span><br><span class="line"></span><br><span class="line">lora_config = LoraConfig(</span><br><span class="line">    r=<span class="number">8</span>,                </span><br><span class="line">    lora_alpha=<span class="number">32</span>,      </span><br><span class="line">    target_modules=[<span class="string">"q_proj"</span>, <span class="string">"v_proj"</span>],  </span><br><span class="line">    lora_dropout=<span class="number">0.1</span>,   </span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">lora_model = get_peft_model(model, lora_config)</span><br><span class="line">lora_model.train()</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h2 id="3-Alignment-Alignment-Techniques"><a href="#3-Alignment-Alignment-Techniques" class="headerlink" title="3. Alignment (Alignment Techniques)"></a>3. <strong>Alignment (Alignment Techniques)</strong></h2><p>Before introducing KL divergence, we first need to clarify how LLM alignment is achieved, along with the underlying principles and mathematical formulas.</p>
<h3 id="1-What-is-Model-Alignment"><a href="#1-What-is-Model-Alignment" class="headerlink" title="1. What is Model Alignment?"></a><strong>1. What is Model Alignment?</strong></h3><p>The core objective of model alignment is to ensure that the language model’s outputs meet human expectations or preferences. Typically, the model is initially trained through large-scale supervised learning (SFT, Supervised Fine-Tuning) to generate a model with basic capabilities. Subsequently, through alignment techniques, the model is further adjusted to ensure that its generated content better aligns with human preferences or avoids producing harmful or erroneous information.</p>
<p><strong>Core Mechanism of Alignment</strong>:</p>
<ul>
<li><strong>Positive Samples</strong>: Outputs that meet human expectations (e.g., correct answers).</li>
<li><strong>Negative Samples</strong>: Outputs that do not meet human expectations (e.g., incorrect answers).</li>
</ul>
<p>By using paired preference data or labels (correct/incorrect), the model’s outputs are further fine-tuned to generate more positive samples while reducing the probability of generating negative samples.</p>
<hr>
<h3 id="2-Mathematical-Principles-of-Model-Alignment"><a href="#2-Mathematical-Principles-of-Model-Alignment" class="headerlink" title="2. Mathematical Principles of Model Alignment"></a><strong>2. Mathematical Principles of Model Alignment</strong></h3><p>During the alignment process, the model generates outputs through a <strong>policy model</strong>, which is typically an SFT-trained language model used to generate outputs given an input. To optimize the model’s outputs to better align with human preferences, the following loss functions and optimization methods are commonly used:</p>
<h4 id="2-1-Policy-Model"><a href="#2-1-Policy-Model" class="headerlink" title="2.1 Policy Model"></a><strong>2.1 Policy Model</strong></h4><p>Assume the current policy of the model is $\pi_\theta$, which represents the probability of the model generating output $y$ given input $x$:</p>
<p>$$<br>\pi_\theta(y|x)<br>$$</p>
<p>The objective of the policy model is to adjust the parameters $\theta$ to increase the probability of generating correct outputs (positive samples) and decrease the probability of generating incorrect outputs (negative samples).</p>
<h4 id="2-2-Mechanism-for-Increasing-Positive-Sample-Probability-and-Decreasing-Negative-Sample-Probability"><a href="#2-2-Mechanism-for-Increasing-Positive-Sample-Probability-and-Decreasing-Negative-Sample-Probability" class="headerlink" title="2.2 Mechanism for Increasing Positive Sample Probability and Decreasing Negative Sample Probability"></a><strong>2.2 Mechanism for Increasing Positive Sample Probability and Decreasing Negative Sample Probability</strong></h4><p>To achieve this goal, loss functions with preference comparisons or labels are typically used for optimization:</p>
<ol>
<li><p><strong>Optimization of Positive Samples</strong>: By increasing the loss weight of positive samples, the model is guided to generate positive samples with higher probability when faced with the same problem.</p>
<ul>
<li>The loss function for positive samples guides the model to produce more outputs that meet human expectations.</li>
</ul>
</li>
<li><p><strong>Penalty for Negative Samples</strong>: By applying higher loss weights to negative samples, the model learns to reduce the probability of generating these incorrect outputs.</p>
<ul>
<li>The loss function for negative samples aims to penalize the model more when it generates incorrect answers, thereby reducing the likelihood of such outputs.</li>
</ul>
</li>
</ol>
<p>In some methods, such as DPO and KTO, <strong>KL divergence</strong> between the current policy model and a reference model is calculated to prevent the model from deviating excessively from the original pre-trained model during optimization.</p>
<hr>
<h3 id="3-Role-of-Loss-Functions-and-KL-Divergence"><a href="#3-Role-of-Loss-Functions-and-KL-Divergence" class="headerlink" title="3. Role of Loss Functions and KL Divergence"></a><strong>3. Role of Loss Functions and KL Divergence</strong></h3><p>In the model alignment process, the loss function typically consists of two parts:</p>
<ol>
<li><strong>Preference Loss</strong> or <strong>Label Loss</strong>, used to optimize the model to generate outputs that meet human expectations.</li>
<li><strong>KL Divergence</strong>, used to constrain the model from deviating from the reference model.</li>
</ol>
<h4 id="3-1-Role-of-KL-Divergence"><a href="#3-1-Role-of-KL-Divergence" class="headerlink" title="3.1 Role of KL Divergence"></a><strong>3.1 Role of KL Divergence</strong></h4><p>KL divergence (Kullback-Leibler Divergence) measures the difference between two probability distributions. In model alignment, KL divergence is used to limit the distribution difference between the current model $\pi_\theta$ and the reference model $\pi_{\text{ref}}$, ensuring that the model’s outputs do not deviate excessively from the pre-trained model during optimization. The specific formula is:</p>
<p>$$<br>\text{KL}(\pi_\theta(y|x) | \pi_{\text{ref}}(y|x)) = \sum_y \pi_\theta(y|x) \log \frac{\pi_\theta(y|x)}{\pi_{\text{ref}}(y|x)}<br>$$</p>
<ul>
<li>If the KL divergence is large, it indicates that the current model’s generated distribution significantly differs from the reference model, which may mean the model is producing unreasonable outputs.</li>
<li>By minimizing KL divergence, the model can be further optimized while ensuring the reasonableness of its outputs.</li>
</ul>
<h4 id="3-2-Loss-Function-Formulas"><a href="#3-2-Loss-Function-Formulas" class="headerlink" title="3.2 Loss Function Formulas"></a><strong>3.2 Loss Function Formulas</strong></h4><p>Based on preferences or labels, the model’s loss function can be expressed in the following forms:</p>
<h5 id="Loss-Function-in-DPO"><a href="#Loss-Function-in-DPO" class="headerlink" title="Loss Function in DPO:"></a><strong>Loss Function in DPO</strong>:</h5><p>$$<br>L_{DPO} = -\mathbb{E}_{x, y_w, y_l \sim D} [\log \sigma(\beta (\log(\pi_\theta(y_w|x)) - \log(\pi_\theta(y_l|x))))]<br>$$</p>
<ul>
<li>$y_w$: Higher-preference answer.</li>
<li>$y_l$: Lower-preference answer.</li>
</ul>
<p>In DPO, KL divergence can be introduced as a regularization term:<br>$$<br>L_{DPO} = -\log \sigma(\log(\pi_\theta(y_w|x)) - \log(\pi_\theta(y_l|x)) + \beta \cdot \text{KL}(\pi_\theta | \pi_{\text{ref}}))<br>$$<br>By controlling KL divergence, the model’s outputs do not deviate too much from the reference model.</p>
<h5 id="Loss-Function-in-KTO"><a href="#Loss-Function-in-KTO" class="headerlink" title="Loss Function in KTO:"></a><strong>Loss Function in KTO</strong>:</h5><p>The loss function in KTO is based on prospect theory and incorporates KL divergence as a core component:<br>$$<br>L_{KTO} = \lambda_U \cdot \sigma(\beta \cdot (\text{KL}(\pi_\theta(\text{Answer 2}) | \pi_{\text{ref}}) - r_{\theta}(\text{Answer 2})))<br>$$</p>
<ul>
<li>$r_{\theta}(x, y)$: The current policy’s confidence in negative samples (incorrect answers).</li>
<li>KL divergence is used to measure the difference between the current model and the reference model, ensuring that while reducing the generation of negative samples, the model does not deviate from the original reference model.</li>
</ul>
<p>By increasing the loss for negative samples (i.e., increasing the value of $\lambda_U$), the model reduces the confidence in negative samples, thereby decreasing the probability of generating similar incorrect answers in the future.</p>
<hr>
<h3 id="4-How-to-Optimize-the-Model"><a href="#4-How-to-Optimize-the-Model" class="headerlink" title="4. How to Optimize the Model"></a><strong>4. How to Optimize the Model</strong></h3><p>Through the loss functions introduced above, model optimization is typically performed using <strong>Gradient Descent</strong>. The gradients of the loss function reflect the differences between the model’s outputs and the expected outputs, and the optimization goal is to minimize the loss function.</p>
<p><strong>Gradient Update Formula</strong>:<br>$$<br>\theta_{\text{new}} = \theta_{\text{old}} - \eta \nabla_{\theta} L<br>$$<br>where:</p>
<ul>
<li>$\eta$ is the learning rate, determining the step size of each parameter update.</li>
<li>$\nabla_{\theta} L$ is the gradient of the loss function with respect to the model parameters, indicating the contribution of the current parameters to the loss.</li>
</ul>
<p>Through continuous iteration, the model gradually increases the probability of generating positive samples and decreases the probability of generating negative samples, ultimately achieving model alignment.</p>
<ul>
<li>The core objective of <strong>Model Alignment</strong> is to optimize the model’s outputs to meet human expectations through preference or label data.</li>
<li>The <strong>Policy Model</strong> ($\pi_\theta$) generates outputs, and KL divergence is used to control the degree of deviation from the reference model, preventing unreasonable biases during optimization.</li>
<li>The <strong>Probability of Positive Samples</strong> is gradually increased through the optimization of the loss function, while the <strong>Probability of Negative Samples</strong> is reduced by increasing loss weights and lowering confidence.</li>
<li>Gradient descent is used to update model parameters, ultimately achieving model alignment.</li>
</ul>
<hr>
<h2 id="4-DPO-Direct-Preference-Optimization"><a href="#4-DPO-Direct-Preference-Optimization" class="headerlink" title="4. DPO (Direct Preference Optimization)"></a>4. <strong>DPO (Direct Preference Optimization)</strong></h2><h3 id="4-1-Principle"><a href="#4-1-Principle" class="headerlink" title="4.1 Principle"></a>4.1 <strong>Principle</strong></h3><p>DPO directly optimizes the model’s output preference function to make the model’s outputs more aligned with human preferences. It compares different outputs generated by the model and uses a preference function to evaluate which of the two outputs is better, thereby guiding the optimization of the model parameters.</p>
<h3 id="4-2-Loss-Function"><a href="#4-2-Loss-Function" class="headerlink" title="4.2 Loss Function"></a>4.2 <strong>Loss Function</strong></h3><p>DPO uses a preference loss function to compare the quality of two outputs:</p>
<p>$$<br>\mathcal{L}_{\text{DPO}} = \log(1 + \exp(-\sigma \cdot (\hat{y}_a - \hat{y}_b) \cdot p))<br>$$</p>
<ul>
<li>$ \hat{y}_a $ and $ \hat{y}_b $ are the model’s predictions for two samples.</li>
<li>$ p $ is the human preference (1 indicates preference for $a$, -1 indicates preference for $b$).</li>
<li>$ \sigma $ is a smoothing parameter.</li>
</ul>
<h3 id="4-3-Optimizer"><a href="#4-3-Optimizer" class="headerlink" title="4.3 Optimizer"></a>4.3 <strong>Optimizer</strong></h3><p>DPO typically uses the <strong>AdamW</strong> optimizer, which is suitable for optimizing large-scale parameter models. The code is as follows:</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.AdamW(model.parameters(), lr=<span class="number">1e-5</span>)</span><br></pre></td></tr></tbody></table></figure>

<h3 id="4-4-Core-Code"><a href="#4-4-Core-Code" class="headerlink" title="4.4 Core Code"></a>4.4 <strong>Core Code</strong></h3><p>The following are the training steps for DPO:</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preference_loss</span>(<span class="params">output_a, output_b, human_preference</span>):</span><br><span class="line">    preference = human_preference(output_a, output_b)</span><br><span class="line">    loss = torch.log(<span class="number">1</span> + torch.exp(-preference * (output_a - output_b)))</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dpo_training_step</span>(<span class="params">model, data_a, data_b, human_preference</span>):</span><br><span class="line">    output_a = model(data_a)</span><br><span class="line">    output_b = model(data_b)</span><br><span class="line">    </span><br><span class="line">    loss = preference_loss(output_a, output_b, human_preference)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> batch_a, batch_b <span class="keyword">in</span> training_data:</span><br><span class="line">    dpo_training_step(model, batch_a, batch_b, human_preference_function)</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h2 id="5-KTO-Kahneman-Tversky-Optimization"><a href="#5-KTO-Kahneman-Tversky-Optimization" class="headerlink" title="5. KTO (Kahneman-Tversky Optimization)"></a>5. <strong>KTO (Kahneman-Tversky Optimization)</strong></h2><h3 id="5-1-Principle"><a href="#5-1-Principle" class="headerlink" title="5.1 Principle"></a>5.1 <strong>Principle</strong></h3><p>KTO is based on Kahneman and Tversky’s Prospect Theory, which uses an asymmetric utility function to measure the model’s gains and losses. It aims to optimize the model’s performance, especially in scenarios with asymmetric risks and rewards. The utility function is defined as follows:</p>
<p>$$<br>\mathcal{U}(x) =<br>\begin{cases}<br>x^{\alpha}, &amp; x \geq 0 \<br>-\lambda (-x)^{\alpha}, &amp; x &lt; 0<br>\end{cases}<br>$$</p>
<ul>
<li>$x$ is the difference between the model’s prediction and the true value.</li>
<li>$\alpha$ is the non-linear coefficient, typically 0.88.</li>
<li>$\lambda$ is the loss penalty weight, typically 2.25.</li>
</ul>
<h3 id="5-2-Loss-Function"><a href="#5-2-Loss-Function" class="headerlink" title="5.2 Loss Function"></a>5.2 <strong>Loss Function</strong></h3><p>The loss function for KTO is based on the utility function from Prospect Theory and is used to penalize the model’s prediction errors:</p>
<p>$$<br>\mathcal{L}<em>{\text{KTO}} = -\mathbb{E}[\mathcal{U}(y</em>{\text{pred}} - y_{\text{true}})]<br>$$</p>
<h3 id="5-3-Optimizer"><a href="#5-3-Optimizer" class="headerlink" title="5.3 Optimizer"></a>5.3 <strong>Optimizer</strong></h3><p>KTO commonly uses the <strong>AdamW</strong> optimizer to ensure stability during the training process:</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.AdamW(model.parameters(), lr=<span class="number">3e-5</span>)</span><br></pre></td></tr></tbody></table></figure>

<h3 id="5-4-Core-Code"><a href="#5-4-Core-Code" class="headerlink" title="5.4 Core Code"></a>5.4 <strong>Core Code</strong></h3><p>The following is the code for calculating the KTO loss function:</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">prospect_utility</span>(<span class="params">value, alpha=<span class="number">0.88</span>, lambda_=<span class="number">2.25</span></span>):</span><br><span class="line">    <span class="keyword">if</span> value &gt;= <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> torch.<span class="built_in">pow</span>(value, alpha)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> -lambda_ * torch.<span class="built_in">pow</span>(-value, alpha)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">kto_loss</span>(<span class="params">predictions, targets</span>):</span><br><span class="line">    value = predictions - targets</span><br><span class="line">    utility = prospect_utility(value)</span><br><span class="line">    loss = -torch.mean(utility)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">    predictions = model(data)</span><br><span class="line">    loss = kto_loss(predictions, targets)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a><strong>Summary</strong></h3><table>
<thead>
<tr>
<th>Method</th>
<th>Loss Function</th>
<th>Optimizer</th>
</tr>
</thead>
<tbody><tr>
<td><strong>SFT</strong></td>
<td>Cross-Entropy Loss</td>
<td>AdamW, RMSprop, SGD</td>
</tr>
<tr>
<td><strong>LoRA</strong></td>
<td>Cross-Entropy Loss</td>
<td>AdamW, RMSprop, SGD</td>
</tr>
<tr>
<td><strong>DPO</strong></td>
<td>Preference Loss Function: $\log(1 + \exp(-\sigma (\hat{y}_a - \hat{y}_b)p))$</td>
<td>AdamW</td>
</tr>
<tr>
<td><strong>KTO</strong></td>
<td>Prospect Theory Utility Function: $-\mathbb{E}[\mathcal{U}(y_{\text{pred}} - y_{\text{true}})]$</td>
<td>AdamW</td>
</tr>
</tbody></table>
<p>Through the organization of this document, readers can clearly understand the principles, specific implementation steps, loss function designs, and optimizer selections for technologies such as SFT, LoRA, DPO, and KTO, especially in the context of fine-tuning large-scale pre-trained models like LLAMA3.</p>
</article><section class="jump-container is-flex is-justify-content-space-between my-6"><!-- em is empty placeholder--><a class="button is-default" href="/2024/10/23/NLP%20Insights/Introduction%20to%20LLM%20Training%20Terminology:%20LoRA,%20DPO,%20KTO,%20and%20SFT%20Technologies.en/" title="Detailed Explanation of LoRA, DPO, KTO, and SFT Technologies"><i class="iconfont icon-prev mr-2 has-text-grey"></i><span class="has-text-weight-semibold">上一页: Detailed Explanation of LoRA, DPO, KTO, and SFT Technologies</span></a><a class="button is-default" href="/2024/10/23/NLP%20Insights/LLM%E8%AE%AD%E7%BB%83%E6%9C%AF%E8%AF%AD%E4%BB%8B%E7%BB%8D%EF%BC%9ASFT%E3%80%81LoRA%E3%80%81Alignment%E3%80%81KTO%E5%92%8CDPO.en/" title="LoRA, DPO, KTO 与 SFT 技术详解"><span class="has-text-weight-semibold">下一页: LoRA, DPO, KTO 与 SFT 技术详解</span><i class="iconfont icon-next ml-2 has-text-grey"></i></a></section><article class="mt-6 comment-container"><script async repo="Haojen/Claudia-theme-blog" src="https://utteranc.es/client.js" issue-term="pathname" theme="preferred-color-scheme"></script></article></div></div></main></main><footer class="is-flex is-flex-direction-column is-align-items-center is-flex-shrink-0 is-family-serif"><section class="sns-container"><a title="twitter" target="_blank" rel="noopener nofollow" href="//twitter.com//"><i class="iconfont icon-twitter"></i></a><!-- Github--><a title="github" target="_blank" rel="noopener nofollow" href="//github.com/haojen"><i class="iconfont icon-github"></i></a><!-- Ins--><a title="instagram" target="_blank" rel="noopener nofollow" href="//www.instagram.com//"><i class="iconfont icon-ins"></i></a><!-- RSS--><!-- 知乎--><!-- 领英--><!-- 脸书--><a title="facebook" target="_blank" rel="noopener nofollow" href="//www.facebook.com//"><i class="iconfont icon-tian7_facebook"></i></a></section><p><span>Copyright ©</span><span> user 2026</span></p><div class="is-flex is-justify-content-center is-flex-wrap-wrap"><p>Powered by Hexo &verbar;&nbsp;</p><p class="is-flex is-justify-content-center"><a title="Hexo theme author" target="_blank" rel="noopener" href="//github.com/haojen">Theme by Haojen&nbsp;</a></p><div style="margin-top: 2px"><a class="github-button" title="github-button" target="_blank" rel="noopener" href="https://github.com/haojen/hexo-theme-Claudia" data-color-scheme="no-preference: light; light: light; dark: dark;" data-show-count="true"></a></div></div><div><span></span></div></footer><script src="/js/lang-switch.js"></script><script async defer src="https://buttons.github.io/buttons.js"></script><script src="/js/jquery-3.6.1.min.js"></script><script src="/js/jquery-fancybox.min.js"></script><script src="/js/img_zoom.js"></script><script src="/js/post.js"></script></body></html>