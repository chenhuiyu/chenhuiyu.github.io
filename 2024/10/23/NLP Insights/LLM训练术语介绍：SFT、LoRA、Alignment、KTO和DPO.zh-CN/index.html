<!DOCTYPE html><html class="appearance-auto" lang="zh-CN"><head><meta charset="UTF-8"><title>LoRA, DPO, KTO 与 SFT 技术详解</title><meta name="description" content="May the Force be with you"><meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no, initial-scale=1"><!-- Google Analytics --><!-- End Google Analytics -->
<!-- Baidu Analytics --><!-- End Baidu Analytics --><link rel="icon" href="/images/favicon.ico"><link rel="stylesheet" href="/style/common/bulma.css"><link rel="stylesheet" href="/style/base.css"><link rel="stylesheet" href="/style/common/helper.css"><script src="/js/common.js"></script><link rel="stylesheet" href="/style/post.css"><link rel="stylesheet" href="/style/themes/highlight-theme-light.css"><link rel="stylesheet" href="/style/common/jquery.fancybox.min.css"><script src="/js/highlight.pack.js"></script><meta name="description" content="LoRA, DPO, KTO 与 SFT 技术详解本篇文档将详细介绍几种在大型语言模型（如 LLAMA3）微调和优化中的重要技术，包括 SFT（Supervised Fine-Tuning）、LoRA（Low-Rank Adaptation）、Alignment 技术、KTO（Kahneman-Tversky Optimization） 和 DPO（Direct Preference Optimization）。文中还将详细阐述每种技术的原理、具体实现方法以及相应的损失函数与优化器选择。

1. SFT（Supervised Fine-Tuning）1.1 原理SFT 是一种传统的微调方法，通过监督学习对预训练模型进行微调，调整模型的参数使其在特定任务上表现更好。SFT 通常用于针对特定的标注数据进行模型微.."><meta name="generator" content="Hexo 6.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="黑头呆鱼进化之旅" type="application/atom+xml">
</head><body class="is-flex is-flex-direction-column"><header class="header-widget is-flex-shrink-0 is-hidden-mobile"><div class="container is-fullhd is-flex is-justify-content-space-between is-align-items-center is-full-height"><section class="is-hidden-mobile is-flex-shrink-0"><h2><a href="/">user's blog</a></h2></section><h3 class="is-hidden-mobile is-family-serif is-full-height is-flex is-align-items-center is-flex-shrink-0"><div class="is-full-height" id="postTopic"><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">LoRA, DPO, KTO 与 SFT 技术详解</p><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">点击返回顶部</p></div></h3><aside class="is-flex-shrink-0"><h3 class="is-inline-block"><a href="/">首页</a></h3><h3 class="is-inline-block"><a href="/about">关于</a></h3><h3 class="is-inline-block"><a href="/archives">归档</a></h3></aside></div></header><header class="is-flex header-widget is-flex-shrink-0 is-align-items-center is-justify-content-center is-hidden-tablet"><h3 class="is-inline-block"><a href="/">首页</a></h3><h3 class="is-inline-block"><a href="/about">关于</a></h3><h3 class="is-inline-block"><a href="/archives">归档</a></h3></header><main><main class="container is-max-widescreen content section post-page pt-4 px-4"><div class="columns is-flex-desktop is-justify-content-center is-flex-direction-row-reverse"><div class="column is-3 is-hidden-mobile"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#LoRA-DPO-KTO-%E4%B8%8E-SFT-%E6%8A%80%E6%9C%AF%E8%AF%A6%E8%A7%A3"><span class="toc-text">LoRA, DPO, KTO 与 SFT 技术详解</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-SFT%EF%BC%88Supervised-Fine-Tuning%EF%BC%89"><span class="toc-text">1. SFT（Supervised Fine-Tuning）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-%E5%8E%9F%E7%90%86"><span class="toc-text">1.1 原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-%E5%AE%9E%E7%8E%B0%E6%96%B9%E6%B3%95"><span class="toc-text">1.2 实现方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-%E6%A0%B8%E5%BF%83%E4%BB%A3%E7%A0%81"><span class="toc-text">1.3 核心代码</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-LoRA%EF%BC%88Low-Rank-Adaptation%EF%BC%89"><span class="toc-text">2. LoRA（Low-Rank Adaptation）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E5%8E%9F%E7%90%86"><span class="toc-text">2.1 原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E5%AE%9E%E7%8E%B0%E6%96%B9%E6%B3%95"><span class="toc-text">2.2 实现方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E5%8F%AF%E5%BE%AE%E8%B0%83%E7%9A%84%E5%B1%82%E4%B8%8E%E4%B8%8D%E5%8F%98%E7%9A%84%E5%B1%82"><span class="toc-text">2.3 可微调的层与不变的层</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%AF%E5%BE%AE%E8%B0%83%E7%9A%84%E5%B1%82"><span class="toc-text">可微调的层</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%8D%E5%8F%98%E7%9A%84%E5%B1%82"><span class="toc-text">不变的层</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-text">2.4 损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-5-%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-text">2.5 优化器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-6-%E6%A0%B8%E5%BF%83%E4%BB%A3%E7%A0%81"><span class="toc-text">2.6 核心代码</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Alignment%EF%BC%88%E5%AF%B9%E9%BD%90%E6%8A%80%E6%9C%AF%EF%BC%89"><span class="toc-text">3. Alignment（对齐技术）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E4%BB%80%E4%B9%88%E6%98%AF%E6%A8%A1%E5%9E%8B%E5%AF%B9%E9%BD%90%EF%BC%88Alignment%EF%BC%89%EF%BC%9F"><span class="toc-text">1. 什么是模型对齐（Alignment）？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%A8%A1%E5%9E%8B%E5%AF%B9%E9%BD%90%E7%9A%84%E6%95%B0%E5%AD%A6%E5%8E%9F%E7%90%86"><span class="toc-text">2. 模型对齐的数学原理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-%E7%AD%96%E7%95%A5%E6%A8%A1%E5%9E%8B"><span class="toc-text">2.1 策略模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-%E6%8F%90%E9%AB%98%E6%AD%A3%E6%A0%B7%E6%9C%AC%E6%A6%82%E7%8E%87%E4%B8%8E%E9%99%8D%E4%BD%8E%E8%B4%9F%E6%A0%B7%E6%9C%AC%E6%A6%82%E7%8E%87%E7%9A%84%E6%9C%BA%E5%88%B6"><span class="toc-text">2.2 提高正样本概率与降低负样本概率的机制</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8EKL%E6%95%A3%E5%BA%A6%E7%9A%84%E4%BD%9C%E7%94%A8"><span class="toc-text">3. 损失函数与KL散度的作用</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-KL%E6%95%A3%E5%BA%A6%E7%9A%84%E4%BD%9C%E7%94%A8"><span class="toc-text">3.1 KL散度的作用</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%85%AC%E5%BC%8F"><span class="toc-text">3.2 损失函数公式</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#DPO%E4%B8%AD%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%EF%BC%9A"><span class="toc-text">DPO中的损失函数：</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#KTO%E4%B8%AD%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%EF%BC%9A"><span class="toc-text">KTO中的损失函数：</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E5%A6%82%E4%BD%95%E4%BC%98%E5%8C%96%E6%A8%A1%E5%9E%8B"><span class="toc-text">4. 如何优化模型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-DPO%EF%BC%88Direct-Preference-Optimization%EF%BC%89"><span class="toc-text">4. DPO（Direct Preference Optimization）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E5%8E%9F%E7%90%86"><span class="toc-text">4.1 原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-text">4.2 损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-text">4.3 优化器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-%E6%A0%B8%E5%BF%83%E4%BB%A3%E7%A0%81"><span class="toc-text">4.4 核心代码</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-KTO%EF%BC%88Kahneman-Tversky-Optimization%EF%BC%89"><span class="toc-text">5. KTO（Kahneman-Tversky Optimization）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-%E5%8E%9F%E7%90%86"><span class="toc-text">5.1 原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-text">5.2 损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-text">5.3 优化器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-4-%E6%A0%B8%E5%BF%83%E4%BB%A3%E7%A0%81"><span class="toc-text">5.4 核心代码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-text">总结</span></a></li></ol></div><div class="column is-9"><header class="my-4"><a href="/tags/LLM"><i class="tag post-item-tag">LLM</i></a></header><h1 class="mt-0 mb-1 is-family-serif" id="postTitle">LoRA, DPO, KTO 与 SFT 技术详解</h1><time class="has-text-grey" datetime="2024-10-23T08:26:29.000Z">2024-10-23</time><article class="mt-2 post-content"><h3 id="LoRA-DPO-KTO-与-SFT-技术详解"><a href="#LoRA-DPO-KTO-与-SFT-技术详解" class="headerlink" title="LoRA, DPO, KTO 与 SFT 技术详解"></a><strong>LoRA, DPO, KTO 与 SFT 技术详解</strong></h3><p>本篇文档将详细介绍几种在大型语言模型（如 LLAMA3）微调和优化中的重要技术，包括 <strong>SFT（Supervised Fine-Tuning）</strong>、<strong>LoRA（Low-Rank Adaptation）</strong>、<strong>Alignment</strong> 技术、<strong>KTO（Kahneman-Tversky Optimization）</strong> 和 <strong>DPO（Direct Preference Optimization）</strong>。文中还将详细阐述每种技术的原理、具体实现方法以及相应的损失函数与优化器选择。</p>
<hr>
<h2 id="1-SFT（Supervised-Fine-Tuning）"><a href="#1-SFT（Supervised-Fine-Tuning）" class="headerlink" title="1. SFT（Supervised Fine-Tuning）"></a>1. <strong>SFT（Supervised Fine-Tuning）</strong></h2><h3 id="1-1-原理"><a href="#1-1-原理" class="headerlink" title="1.1 原理"></a>1.1 <strong>原理</strong></h3><p>SFT 是一种传统的微调方法，通过监督学习对预训练模型进行微调，调整模型的参数使其在特定任务上表现更好。SFT 通常用于针对特定的标注数据进行模型微调，训练的过程类似于常规的监督学习。</p>
<h3 id="1-2-实现方法"><a href="#1-2-实现方法" class="headerlink" title="1.2 实现方法"></a>1.2 <strong>实现方法</strong></h3><ul>
<li><strong>选择预训练模型</strong>：如 GPT、BERT 等语言模型。</li>
<li><strong>准备标注数据集</strong>：数据集包含输入和输出对。</li>
<li><strong>训练模型</strong>：使用标准的交叉熵损失函数对模型进行训练，通过梯度下降优化参数。</li>
</ul>
<h3 id="1-3-核心代码"><a href="#1-3-核心代码" class="headerlink" title="1.3 核心代码"></a>1.3 <strong>核心代码</strong></h3><p>使用 Hugging Face 的 <code>Trainer</code> 接口进行 SFT：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Trainer, TrainingArguments, AutoModelForSeq2SeqLM, AutoTokenizer</span><br><span class="line"></span><br><span class="line">model_name = <span class="string">"gpt2"</span></span><br><span class="line">model = AutoModelForSeq2SeqLM.from_pretrained(model_name)</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_name)</span><br><span class="line"></span><br><span class="line">train_dataset = load_dataset(<span class="string">"my_dataset"</span>, split=<span class="string">"train"</span>)</span><br><span class="line"></span><br><span class="line">training_args = TrainingArguments(</span><br><span class="line">    output_dir=<span class="string">"./results"</span>,</span><br><span class="line">    evaluation_strategy=<span class="string">"epoch"</span>,</span><br><span class="line">    learning_rate=<span class="number">2e-5</span>,</span><br><span class="line">    per_device_train_batch_size=<span class="number">16</span>,</span><br><span class="line">    num_train_epochs=<span class="number">3</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model=model,</span><br><span class="line">    args=training_args,</span><br><span class="line">    train_dataset=train_dataset,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer.train()</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h2 id="2-LoRA（Low-Rank-Adaptation）"><a href="#2-LoRA（Low-Rank-Adaptation）" class="headerlink" title="2. LoRA（Low-Rank Adaptation）"></a>2. <strong>LoRA（Low-Rank Adaptation）</strong></h2><h3 id="2-1-原理"><a href="#2-1-原理" class="headerlink" title="2.1 原理"></a>2.1 <strong>原理</strong></h3><p>LoRA 是一种参数高效的微调技术，通过对大模型中的权重矩阵进行低秩分解，将原始权重矩阵 $W$ 分解为两个低秩矩阵 $B$ 和 $A$，并仅对这些低秩矩阵进行微调。LoRA 的设计目标是减少微调参数的数量，在保留预训练模型权重的同时，通过调整低秩矩阵来优化模型表现。</p>
<h3 id="2-2-实现方法"><a href="#2-2-实现方法" class="headerlink" title="2.2 实现方法"></a>2.2 <strong>实现方法</strong></h3><ul>
<li><strong>权重分解</strong>：对于模型的线性层（如注意力机制中的 <code>q_proj</code> 和 <code>v_proj</code> 层），将权重矩阵分解为两个低秩矩阵 $B$ 和 $A$。</li>
<li><strong>微调特定层</strong>：仅对这些特定的线性层应用 LoRA，而模型中的其他层保持不变。</li>
</ul>
<h3 id="2-3-可微调的层与不变的层"><a href="#2-3-可微调的层与不变的层" class="headerlink" title="2.3 可微调的层与不变的层"></a>2.3 <strong>可微调的层与不变的层</strong></h3><h4 id="可微调的层"><a href="#可微调的层" class="headerlink" title="可微调的层"></a><strong>可微调的层</strong></h4><p>LoRA 通常应用于 Transformer 模型中的线性投影层，尤其是多头注意力机制中的几个关键层：</p>
<ul>
<li><strong>q_proj</strong>（Query 投影层）</li>
<li><strong>k_proj</strong>（Key 投影层）</li>
<li><strong>v_proj</strong>（Value 投影层）</li>
<li><strong>o_proj</strong>（Output 投影层）</li>
<li><strong>ffn_up_proj</strong> 和 <strong>ffn_down_proj</strong>（前馈神经网络的上下投影层）</li>
</ul>
<h4 id="不变的层"><a href="#不变的层" class="headerlink" title="不变的层"></a><strong>不变的层</strong></h4><ul>
<li><strong>Embedding 层</strong>：负责输入和输出的编码，通常不需要微调。</li>
<li><strong>LayerNorm 层</strong>：这些层主要用于归一化，不含大量参数，通常保持不变。</li>
<li><strong>激活函数层</strong>：如 ReLU 或 GELU 等非线性激活函数不涉及参数，不需要进行微调。</li>
</ul>
<h3 id="2-4-损失函数"><a href="#2-4-损失函数" class="headerlink" title="2.4 损失函数"></a>2.4 <strong>损失函数</strong></h3><p>LoRA 的损失函数通常与具体任务相关。在语言生成任务中，LoRA 使用<strong>交叉熵损失</strong>来度量生成文本和目标文本之间的差异：</p>
<p>$$<br>\mathcal{L}<em>{\text{LoRA}} = - \sum</em>{i} y_i \log(\hat{y}_i)<br>$$</p>
<p>其中 $y_i$ 是真实标签，$\hat{y}_i$ 是模型的输出概率。</p>
<h3 id="2-5-优化器"><a href="#2-5-优化器" class="headerlink" title="2.5 优化器"></a>2.5 <strong>优化器</strong></h3><p>LoRA 微调通常使用 <strong>AdamW</strong> 优化器，具体代码如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.AdamW(lora_model.parameters(), lr=<span class="number">5e-5</span>)</span><br></pre></td></tr></tbody></table></figure>

<h3 id="2-6-核心代码"><a href="#2-6-核心代码" class="headerlink" title="2.6 核心代码"></a>2.6 <strong>核心代码</strong></h3><p>使用 <code>peft</code> 库实现 LoRA：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> peft <span class="keyword">import</span> LoraConfig, get_peft_model</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForSeq2SeqLM</span><br><span class="line"></span><br><span class="line">model_name = <span class="string">"gpt2"</span></span><br><span class="line">model = AutoModelForSeq2SeqLM.from_pretrained(model_name)</span><br><span class="line"></span><br><span class="line">lora_config = LoraConfig(</span><br><span class="line">    r=<span class="number">8</span>,                </span><br><span class="line">    lora_alpha=<span class="number">32</span>,      </span><br><span class="line">    target_modules=[<span class="string">"q_proj"</span>, <span class="string">"v_proj"</span>],  </span><br><span class="line">    lora_dropout=<span class="number">0.1</span>,   </span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">lora_model = get_peft_model(model, lora_config)</span><br><span class="line">lora_model.train()</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h2 id="3-Alignment（对齐技术）"><a href="#3-Alignment（对齐技术）" class="headerlink" title="3. Alignment（对齐技术）"></a>3. <strong>Alignment（对齐技术）</strong></h2><p>在引入KL散度之前，我们首先需要明确LLM对齐（Alignment）是如何实现的，以及背后的原理和数学公式。</p>
<h3 id="1-什么是模型对齐（Alignment）？"><a href="#1-什么是模型对齐（Alignment）？" class="headerlink" title="1. 什么是模型对齐（Alignment）？"></a><strong>1. 什么是模型对齐（Alignment）？</strong></h3><p>模型对齐的核心目标是让语言模型的输出符合人类的期望或偏好。通常，模型最初通过大规模监督学习（SFT，Supervised Fine-Tuning）训练，生成具有基础能力的模型。接下来，通过对齐技术，进一步调整模型，使其生成的内容更符合人类偏好或避免产生有害、错误的信息。</p>
<p><strong>对齐的核心机制</strong>：</p>
<ul>
<li><strong>正样本</strong>：符合人类预期的输出（如正确回答）。</li>
<li><strong>负样本</strong>：不符合人类预期的输出（如错误回答）。</li>
</ul>
<p>通过使用成对偏好数据或标签（正确/错误），对模型的输出进行进一步微调，使模型能够生成更多的正样本，同时减少负样本的生成概率。</p>
<hr>
<h3 id="2-模型对齐的数学原理"><a href="#2-模型对齐的数学原理" class="headerlink" title="2. 模型对齐的数学原理"></a><strong>2. 模型对齐的数学原理</strong></h3><p>在对齐过程中，模型会通过<strong>策略模型</strong>（Policy Model）来生成输出，策略模型通常是经过SFT训练的语言模型，用来在给定输入下生成输出。为了优化模型的输出，使其更加符合人类偏好，常常使用以下损失函数和优化方法：</p>
<h4 id="2-1-策略模型"><a href="#2-1-策略模型" class="headerlink" title="2.1 策略模型"></a><strong>2.1 策略模型</strong></h4><p>假设当前模型的策略为 $\pi_\theta$，它表示在给定输入 $x$ 时，模型生成输出 $y$ 的概率：<br>$$<br>\pi_\theta(y|x)<br>$$<br>策略模型的目标是通过调整参数 $\theta$，提高生成正确输出（正样本）的概率，降低生成错误输出（负样本）的概率。</p>
<h4 id="2-2-提高正样本概率与降低负样本概率的机制"><a href="#2-2-提高正样本概率与降低负样本概率的机制" class="headerlink" title="2.2 提高正样本概率与降低负样本概率的机制"></a><strong>2.2 提高正样本概率与降低负样本概率的机制</strong></h4><p>为了实现这个目标，通常使用带有偏好比较或标签的损失函数进行优化：</p>
<ol>
<li><p><strong>正样本的优化</strong>：通过增加正样本的损失权重，使得模型生成正样本的概率更高。</p>
<ul>
<li>正样本的损失函数会引导模型在面对相同问题时，生成更多符合人类期望的答案。</li>
</ul>
</li>
<li><p><strong>负样本的惩罚</strong>：对负样本施加更高的损失权重，模型会学习到减少这些错误输出的概率。</p>
<ul>
<li>负样本的损失函数旨在让模型在生成错误答案时感知到更大的惩罚，从而减少这些输出的生成。</li>
</ul>
</li>
</ol>
<p>在某些方法中，例如DPO和KTO，还会通过计算当前策略模型与参考模型之间的<strong>KL散度</strong>，来防止模型在优化过程中过度偏离原始预训练模型。</p>
<hr>
<h3 id="3-损失函数与KL散度的作用"><a href="#3-损失函数与KL散度的作用" class="headerlink" title="3. 损失函数与KL散度的作用"></a><strong>3. 损失函数与KL散度的作用</strong></h3><p>在模型对齐的过程中，损失函数通常包含两部分：</p>
<ol>
<li><strong>偏好损失</strong>或<strong>标签损失</strong>，用于优化模型生成符合人类期望的输出。</li>
<li><strong>KL散度</strong>，用于约束模型不要偏离参考模型。</li>
</ol>
<h4 id="3-1-KL散度的作用"><a href="#3-1-KL散度的作用" class="headerlink" title="3.1 KL散度的作用"></a><strong>3.1 KL散度的作用</strong></h4><p>KL散度（Kullback-Leibler Divergence）衡量的是两个概率分布之间的差异。在模型对齐中，KL散度用于限制当前模型 \(\pi_\theta\) 和参考模型 \(\pi_{\text{ref}}\) 的分布差异，确保在优化过程中模型的输出不会过度偏离预训练模型。具体公式为：<br>$$<br>\text{KL}(\pi_\theta(y|x) | \pi_{\text{ref}}(y|x)) = \sum_y \pi_\theta(y|x) \log \frac{\pi_\theta(y|x)}{\pi_{\text{ref}}(y|x)}<br>$$</p>
<ul>
<li>如果KL散度较大，表示当前模型生成的分布与参考模型有较大的差异，这可能意味着模型生成了不合理的输出。</li>
<li>通过最小化KL散度，模型能够在保证输出合理性的基础上，进行进一步的优化。</li>
</ul>
<h4 id="3-2-损失函数公式"><a href="#3-2-损失函数公式" class="headerlink" title="3.2 损失函数公式"></a><strong>3.2 损失函数公式</strong></h4><p>根据偏好或标签，模型的损失函数可以表达为以下形式：</p>
<h5 id="DPO中的损失函数："><a href="#DPO中的损失函数：" class="headerlink" title="DPO中的损失函数："></a><strong>DPO中的损失函数</strong>：</h5><p>$$<br>L_{DPO} = -\mathbb{E}_{x, y_w, y_l \sim D} [\log \sigma(\beta (\log(\pi_\theta(y_w|x)) - \log(\pi_\theta(y_l|x))))]<br>$$</p>
<ul>
<li>$y_w$：偏好较高的答案。</li>
<li>$y_l$：偏好较低的答案。</li>
</ul>
<p>DPO中可以引入KL散度作为正则化项：<br>$$<br>L_{DPO} = -\log \sigma(\log(\pi_\theta(y_w|x)) - \log(\pi_\theta(y_l|x)) + \beta \cdot \text{KL}(\pi_\theta | \pi_{\text{ref}}))<br>$$<br>通过控制KL散度，模型的输出不会偏离参考模型太多。</p>
<h5 id="KTO中的损失函数："><a href="#KTO中的损失函数：" class="headerlink" title="KTO中的损失函数："></a><strong>KTO中的损失函数</strong>：</h5><p>KTO的损失函数基于前景理论，并将KL散度作为核心部分，表达为：<br>$$<br>L_{KTO} = \lambda_U \cdot \sigma(\beta \cdot (\text{KL}(\pi_\theta(\text{Answer 2}) | \pi_{\text{ref}}) - r_{\theta}(\text{Answer 2})))<br>$$</p>
<ul>
<li>$r_{\theta}(x, y)$：当前策略对负样本（错误答案）的置信度。</li>
<li>KL散度用于衡量当前模型与参考模型的差异，确保模型在减少负样本生成的同时，不偏离原始参考模型。</li>
</ul>
<p>通过增加负样本的损失（即增加 $\lambda_U$ 的值），模型会降低负样本的置信度，使未来生成类似错误答案的概率变小。</p>
<hr>
<h3 id="4-如何优化模型"><a href="#4-如何优化模型" class="headerlink" title="4. 如何优化模型"></a><strong>4. 如何优化模型</strong></h3><p>通过上面介绍的损失函数，模型的优化通常是通过<strong>梯度下降</strong>（Gradient Descent）来完成的。损失函数的梯度反映了模型输出与期望输出之间的差异，优化目标是最小化损失函数。</p>
<p><strong>梯度更新公式</strong>：<br>$$<br>\theta_{\text{new}} = \theta_{\text{old}} - \eta \nabla_{\theta} L<br>$$<br>其中：</p>
<ul>
<li>$\eta$ 是学习率，决定每次参数更新的步长。</li>
<li>$\nabla_{\theta} L$ 是损失函数对模型参数的梯度，表示当前参数对损失的贡献。</li>
</ul>
<p>通过不断迭代，模型会逐渐提高生成正样本的概率，减少负样本的生成概率，最终实现模型对齐。</p>
<ul>
<li>模型对齐（Alignment）的核心目标是通过偏好或标签数据，优化模型的输出，使其符合人类期望。</li>
<li><strong>策略模型</strong>（$\pi_\theta$）生成输出，KL散度用于控制模型与参考模型的偏离程度，避免模型在优化过程中产生不合理的偏差。</li>
<li><strong>正样本的概率</strong>通过损失函数的优化逐步提升，<strong>负样本的概率</strong>通过增加损失权重和降低置信度来减少。</li>
<li>梯度下降用于更新模型参数，最终实现模型对齐</li>
</ul>
<hr>
<h2 id="4-DPO（Direct-Preference-Optimization）"><a href="#4-DPO（Direct-Preference-Optimization）" class="headerlink" title="4. DPO（Direct Preference Optimization）"></a>4. <strong>DPO（Direct Preference Optimization）</strong></h2><h3 id="4-1-原理"><a href="#4-1-原理" class="headerlink" title="4.1 原理"></a>4.1 <strong>原理</strong></h3><p>DPO 通过直接优化模型输出的偏好函数，使模型的输出更加符合人类偏好。它比较模型的不同输出，并通过偏好函数评估这两个输出哪个更好，从而指导模型参数的优化。</p>
<h3 id="4-2-损失函数"><a href="#4-2-损失函数" class="headerlink" title="4.2 损失函数"></a>4.2 <strong>损失函数</strong></h3><p>DPO 使用偏好损失函数（Preference Loss），用于比较两个输出的优劣：</p>
<p>$$<br>\mathcal{L}_{\text{DPO}} = \log(1 + \exp(-\sigma \cdot (\hat{y}_a - \hat{y}_b) \cdot p))<br>$$</p>
<ul>
<li>$ \hat{y}_a $ 和 $ \hat{y}_b $ 是模型对两个样本的预测值。</li>
<li>$ p $ 是人类偏好（1 表示偏好 $a$，-1 表示偏好 $b$）。</li>
<li>$ \sigma $ 是平滑参数。</li>
</ul>
<h3 id="4-3-优化器"><a href="#4-3-优化器" class="headerlink" title="4.3 优化器"></a>4.3 <strong>优化器</strong></h3><p>DPO 通常使用 <strong>AdamW</strong> 优化器，适用于大规模参数模型的优化，代码如下：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.AdamW(model.parameters(), lr=<span class="number">1e-5</span>)</span><br></pre></td></tr></tbody></table></figure>

<h3 id="4-4-核心代码"><a href="#4-4-核心代码" class="headerlink" title="4.4 核心代码"></a>4.4 <strong>核心代码</strong></h3><p>以下是 DPO 的训练步骤：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preference_loss</span>(<span class="params">output_a, output_b, human_preference</span>):</span><br><span class="line">    preference = human_preference(output_a, output_b)</span><br><span class="line">    loss = torch.log(<span class="number">1</span> + torch.exp(-preference * (output_a - output_b)))</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dpo_training_step</span>(<span class="params">model, data_a, data_b, human_preference</span>):</span><br><span class="line">    output_a = model(data_a)</span><br><span class="line">    output_b = model(data_b)</span><br><span class="line">    </span><br><span class="line">    loss = preference_loss(output_a, output_b, human_preference)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> batch_a, batch_b <span class="keyword">in</span> training_data:</span><br><span class="line">    dpo_training_step(model, batch_a, batch_b, human_preference_function)</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h2 id="5-KTO（Kahneman-Tversky-Optimization）"><a href="#5-KTO（Kahneman-Tversky-Optimization）" class="headerlink" title="5. KTO（Kahneman-Tversky Optimization）"></a>5. <strong>KTO（Kahneman-Tversky Optimization）</strong></h2><h3 id="5-1-原理"><a href="#5-1-原理" class="headerlink" title="5.1 原理"></a>5.1 <strong>原理</strong></h3><p>KTO 基于 Kahneman 和 Tversky 的前景理论（Prospect Theory），通过非对称效用函数衡量模型的增益和损失，旨在优化模型的表现，尤其在风险和收益不对称的场景下。效用函数定义如下：</p>
<p>$$<br>\mathcal{U}(x) =<br>\begin{cases}<br>x^{\alpha}, &amp; x \geq 0 \<br>-\lambda (-x)^{\alpha}, &amp; x &lt; 0<br>\end{cases}<br>$$</p>
<ul>
<li>$x$ 是模型预测与真实值的差异。</li>
<li>$\alpha$ 是非线性系数，通常为 0</li>
</ul>
<p>.88。</p>
<ul>
<li>$\lambda$ 是损失的惩罚权重，通常为 2.25。</li>
</ul>
<h3 id="5-2-损失函数"><a href="#5-2-损失函数" class="headerlink" title="5.2 损失函数"></a>5.2 <strong>损失函数</strong></h3><p>KTO 的损失函数基于前景理论的效用函数，用于惩罚模型的预测误差：</p>
<p>$$<br>\mathcal{L}<em>{\text{KTO}} = -\mathbb{E}[\mathcal{U}(y</em>{\text{pred}} - y_{\text{true}})]<br>$$</p>
<h3 id="5-3-优化器"><a href="#5-3-优化器" class="headerlink" title="5.3 优化器"></a>5.3 <strong>优化器</strong></h3><p>KTO 常使用 <strong>AdamW</strong> 优化器，以确保训练过程的稳定性：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.AdamW(model.parameters(), lr=<span class="number">3e-5</span>)</span><br></pre></td></tr></tbody></table></figure>

<h3 id="5-4-核心代码"><a href="#5-4-核心代码" class="headerlink" title="5.4 核心代码"></a>5.4 <strong>核心代码</strong></h3><p>以下是 KTO 损失函数的计算代码：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">prospect_utility</span>(<span class="params">value, alpha=<span class="number">0.88</span>, lambda_=<span class="number">2.25</span></span>):</span><br><span class="line">    <span class="keyword">if</span> value &gt;= <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> torch.<span class="built_in">pow</span>(value, alpha)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> -lambda_ * torch.<span class="built_in">pow</span>(-value, alpha)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">kto_loss</span>(<span class="params">predictions, targets</span>):</span><br><span class="line">    value = predictions - targets</span><br><span class="line">    utility = prospect_utility(value)</span><br><span class="line">    loss = -torch.mean(utility)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">    predictions = model(data)</span><br><span class="line">    loss = kto_loss(predictions, targets)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h3><table>
<thead>
<tr>
<th>方法</th>
<th>损失函数</th>
<th>优化器</th>
</tr>
</thead>
<tbody><tr>
<td><strong>SFT</strong></td>
<td>交叉熵损失</td>
<td>AdamW，RMSprop，SGD</td>
</tr>
<tr>
<td><strong>LoRA</strong></td>
<td>交叉熵损失</td>
<td>AdamW，RMSprop，SGD</td>
</tr>
<tr>
<td><strong>DPO</strong></td>
<td>偏好损失函数： $\log(1 + \exp(-\sigma (\hat{y}_a - \hat{y}_b)p))$</td>
<td>AdamW</td>
</tr>
<tr>
<td><strong>KTO</strong></td>
<td>前景理论效用函数： $-\mathbb{E}[\mathcal{U}(y_{\text{pred}} - y_{\text{true}})]$</td>
<td>AdamW</td>
</tr>
</tbody></table>
<p>通过本文档的整理，读者能够清晰理解 SFT、LoRA、DPO 和 KTO 等技术的原理、具体实现步骤、损失函数设计和优化器选择，特别是在 LLAMA3 这种大规模预训练模型的微调场景下的实际应用。</p>
</article><section class="jump-container is-flex is-justify-content-space-between my-6"><!-- em is empty placeholder--><a class="button is-default" href="/2024/10/23/NLP%20Insights/LLM%E8%AE%AD%E7%BB%83%E6%9C%AF%E8%AF%AD%E4%BB%8B%E7%BB%8D%EF%BC%9ASFT%E3%80%81LoRA%E3%80%81Alignment%E3%80%81KTO%E5%92%8CDPO.en/" title="LoRA, DPO, KTO 与 SFT 技术详解"><i class="iconfont icon-prev mr-2 has-text-grey"></i><span class="has-text-weight-semibold">上一页: LoRA, DPO, KTO 与 SFT 技术详解</span></a><a class="button is-default" href="/2024/08/13/NLP%20Insights/%E4%BD%BF%E7%94%A8%E5%8E%8B%E7%BC%A9%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA%E8%BF%9B%E8%A1%8C%E6%9C%AC%E5%9C%B0%20LLM%20%E7%9A%84%E5%BF%AB%E9%80%9F%20JSON%20%E8%A7%A3%E7%A0%81.en/" title="使用压缩有限状态机进行本地 LLM 的快速 JSON 解码"><span class="has-text-weight-semibold">下一页: 使用压缩有限状态机进行本地 LLM 的快速 JSON 解码</span><i class="iconfont icon-next ml-2 has-text-grey"></i></a></section><article class="mt-6 comment-container"><script async repo="Haojen/Claudia-theme-blog" src="https://utteranc.es/client.js" issue-term="pathname" theme="preferred-color-scheme"></script></article></div></div></main></main><footer class="is-flex is-flex-direction-column is-align-items-center is-flex-shrink-0 is-family-serif"><section class="sns-container"><a title="twitter" target="_blank" rel="noopener nofollow" href="//twitter.com//"><i class="iconfont icon-twitter"></i></a><!-- Github--><a title="github" target="_blank" rel="noopener nofollow" href="//github.com/haojen"><i class="iconfont icon-github"></i></a><!-- Ins--><a title="instagram" target="_blank" rel="noopener nofollow" href="//www.instagram.com//"><i class="iconfont icon-ins"></i></a><!-- RSS--><!-- 知乎--><!-- 领英--><!-- 脸书--><a title="facebook" target="_blank" rel="noopener nofollow" href="//www.facebook.com//"><i class="iconfont icon-tian7_facebook"></i></a></section><p><span>Copyright ©</span><span> user 2026</span></p><div class="is-flex is-justify-content-center is-flex-wrap-wrap"><p>Powered by Hexo &verbar;&nbsp;</p><p class="is-flex is-justify-content-center"><a title="Hexo theme author" target="_blank" rel="noopener" href="//github.com/haojen">Theme by Haojen&nbsp;</a></p><div style="margin-top: 2px"><a class="github-button" title="github-button" target="_blank" rel="noopener" href="https://github.com/haojen/hexo-theme-Claudia" data-color-scheme="no-preference: light; light: light; dark: dark;" data-show-count="true"></a></div></div><div><span></span></div></footer><script src="/js/lang-switch.js"></script><script async defer src="https://buttons.github.io/buttons.js"></script><script src="/js/jquery-3.6.1.min.js"></script><script src="/js/jquery-fancybox.min.js"></script><script src="/js/img_zoom.js"></script><script src="/js/post.js"></script></body></html>