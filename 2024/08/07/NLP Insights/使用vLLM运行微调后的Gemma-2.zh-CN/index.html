<!DOCTYPE html><html lang="[&quot;zh-CN&quot;,&quot;en&quot;,&quot;default&quot;]" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>使用vLLM运行微调后的Gemma-2 | 黑头呆鱼进化之旅</title><meta name="author" content="Huiyu Chen"><meta name="copyright" content="Huiyu Chen"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="使用vLLM运行微调后的Gemma-2-2b-it的详细步骤在这里分享一下我运行微调后的Gemma-2-2b-it模型并使用vLLM的步骤，希望对其他人有所帮助。本文将详细介绍安装过程、环境配置以及常见问题的解决方法。 安装和验证vLLM首先，确保安装并验证vLLM的版本是0.5.3。  安装vLLM： 1pip install vllm&#x3D;&#x3D;0.5.3  验证安装： 123import vllmp">
<meta property="og:type" content="article">
<meta property="og:title" content="使用vLLM运行微调后的Gemma-2">
<meta property="og:url" content="https://chenhuiyu.github.io/2024/08/07/NLP%20Insights/%E4%BD%BF%E7%94%A8vllm%E8%BF%90%E8%A1%8C%E5%BE%AE%E8%B0%83%E5%90%8E%E7%9A%84gemma-2.zh-CN/index.html">
<meta property="og:site_name" content="黑头呆鱼进化之旅">
<meta property="og:description" content="使用vLLM运行微调后的Gemma-2-2b-it的详细步骤在这里分享一下我运行微调后的Gemma-2-2b-it模型并使用vLLM的步骤，希望对其他人有所帮助。本文将详细介绍安装过程、环境配置以及常见问题的解决方法。 安装和验证vLLM首先，确保安装并验证vLLM的版本是0.5.3。  安装vLLM： 1pip install vllm&#x3D;&#x3D;0.5.3  验证安装： 123import vllmp">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://chenhuiyu.github.io/img/butterfly-icon.png">
<meta property="article:published_time" content="2024-08-07T02:30:00.000Z">
<meta property="article:modified_time" content="2026-02-20T22:19:17.269Z">
<meta property="article:author" content="Huiyu Chen">
<meta property="article:tag" content="vLLM">
<meta property="article:tag" content="Gemma-2">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://chenhuiyu.github.io/img/butterfly-icon.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "使用vLLM运行微调后的Gemma-2",
  "url": "https://chenhuiyu.github.io/2024/08/07/NLP%20Insights/%E4%BD%BF%E7%94%A8vllm%E8%BF%90%E8%A1%8C%E5%BE%AE%E8%B0%83%E5%90%8E%E7%9A%84gemma-2.zh-CN/",
  "image": "https://chenhuiyu.github.io/img/butterfly-icon.png",
  "datePublished": "2024-08-07T02:30:00.000Z",
  "dateModified": "2026-02-20T22:19:17.269Z",
  "author": [
    {
      "@type": "Person",
      "name": "Huiyu Chen",
      "url": "https://chenhuiyu.github.io"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://chenhuiyu.github.io/2024/08/07/NLP%20Insights/%E4%BD%BF%E7%94%A8vllm%E8%BF%90%E8%A1%8C%E5%BE%AE%E8%B0%83%E5%90%8E%E7%9A%84gemma-2.zh-CN/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=5.5.4"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.13.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '使用vLLM运行微调后的Gemma-2',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 6.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="黑头呆鱼进化之旅" type="application/atom+xml">
</head><body><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">黑头呆鱼进化之旅</span></a><a class="nav-page-title" href="/"><span class="site-name">使用vLLM运行微调后的Gemma-2</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"></div></nav><div id="post-info"><h1 class="post-title">使用vLLM运行微调后的Gemma-2</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-08-07T02:30:00.000Z" title="发表于 2024-08-07 10:30:00">2024-08-07</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2026-02-20T22:19:17.269Z" title="更新于 2026-02-21 06:19:17">2026-02-21</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/NLP-Insights/">NLP Insights</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="使用vLLM运行微调后的Gemma-2-2b-it的详细步骤"><a href="#使用vLLM运行微调后的Gemma-2-2b-it的详细步骤" class="headerlink" title="使用vLLM运行微调后的Gemma-2-2b-it的详细步骤"></a>使用vLLM运行微调后的Gemma-2-2b-it的详细步骤</h1><p>在这里分享一下我运行微调后的Gemma-2-2b-it模型并使用vLLM的步骤，希望对其他人有所帮助。本文将详细介绍安装过程、环境配置以及常见问题的解决方法。</p>
<h2 id="安装和验证vLLM"><a href="#安装和验证vLLM" class="headerlink" title="安装和验证vLLM"></a>安装和验证vLLM</h2><p>首先，确保安装并验证vLLM的版本是0.5.3。</p>
<ol>
<li><p>安装vLLM：</p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install vllm==0.5.3</span><br></pre></td></tr></tbody></table></figure>
</li>
<li><p>验证安装：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> vllm</span><br><span class="line"><span class="built_in">print</span>(vllm.__version__)</span><br><span class="line"><span class="comment"># 输出: 0.5.3</span></span><br></pre></td></tr></tbody></table></figure></li>
</ol>
<h2 id="安装Flashinfer"><a href="#安装Flashinfer" class="headerlink" title="安装Flashinfer"></a>安装Flashinfer</h2><p>按照以下步骤安装Flashinfer，并确保您的torch版本和CUDA兼容性。</p>
<ol>
<li><p>检查torch版本和CUDA兼容性：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="built_in">print</span>(torch.__version__)  <span class="comment"># 应输出: 2.3.1+cu121</span></span><br><span class="line"><span class="built_in">print</span>(torch.version.cuda) <span class="comment"># 应输出: 12.1</span></span><br></pre></td></tr></tbody></table></figure>
</li>
<li><p>安装Flashinfer：<br>根据文档，Gemma运行在版本0.08。vLLM需要FlashInfer v0.0.8（请参阅<a target="_blank" rel="noopener" href="https://github.com/vllm-project/vllm/issues/7060">vLLM版本和Flashinfer文档</a>中关于Gemma 2的部分）。</p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!pip install flashinfer==0.0.8 -i https://flashinfer.ai/whl/cu121/torch2.3/</span><br></pre></td></tr></tbody></table></figure></li>
</ol>
<h2 id="更新环境中的VLLM后端变量"><a href="#更新环境中的VLLM后端变量" class="headerlink" title="更新环境中的VLLM后端变量"></a>更新环境中的VLLM后端变量</h2><p>确保设置Flashinfer为vLLM的注意力机制后端：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">"VLLM_ATTENTION_BACKEND"</span>] = <span class="string">"FLASHINFER"</span></span><br></pre></td></tr></tbody></table></figure>

<h2 id="测试vLLM"><a href="#测试vLLM" class="headerlink" title="测试vLLM"></a>测试vLLM</h2><p>以下是使用vLLM生成文本的测试代码：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> vllm <span class="keyword">import</span> LLM, SamplingParams</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">llm = LLM(model=<span class="string">"gemma-2-2b-model"</span>, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">sampling_params = SamplingParams(</span><br><span class="line">    temperature=<span class="number">0.8</span>,</span><br><span class="line">    max_tokens=<span class="number">512</span>,</span><br><span class="line">    top_p=<span class="number">0.95</span>,</span><br><span class="line">    top_k=<span class="number">1</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例测试数据</span></span><br><span class="line">test_data = [{<span class="string">"text"</span>: <span class="string">"输入测试文本1"</span>}, {<span class="string">"text"</span>: <span class="string">"输入测试文本2"</span>}]</span><br><span class="line"></span><br><span class="line">prompts = [</span><br><span class="line">    test_data[random.randint(<span class="number">0</span>, <span class="built_in">len</span>(test_data) - <span class="number">1</span>)][<span class="string">"text"</span>],</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">outputs = llm.generate(</span><br><span class="line">    prompts,</span><br><span class="line">    sampling_params</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预期输出:</span></span><br><span class="line"><span class="comment"># Processed prompts: 100%|██████████| 1/1 [00:01&lt;00:00,  1.24s/it, est. speed input: 991.44 toks/s, output: 87.79 toks/s]</span></span><br></pre></td></tr></tbody></table></figure>

<p>通过上述步骤，您应该能够成功运行微调后的Gemma-2-2b-it模型。</p>
<h2 id="常见错误及解决方法"><a href="#常见错误及解决方法" class="headerlink" title="常见错误及解决方法"></a>常见错误及解决方法</h2><p>在运行过程中，可能会遇到以下常见错误：</p>
<ol>
<li><p><strong>RuntimeError: <code>CHECK_EQ(paged_kv_indptr.size(0), batch_size + 1) failed. 1 vs 257</code></strong></p>
<ul>
<li><strong>原因</strong>：Flashinfer版本错误。</li>
<li><strong>解决方法</strong>：请确保安装了正确版本的Flashinfer。</li>
</ul>
</li>
<li><p><strong>TypeError: <code>'NoneType' object is not callable</code></strong></p>
<ul>
<li><strong>原因</strong>：没有安装Flashinfer。</li>
<li><strong>解决方法</strong>：按照上述步骤安装Flashinfer。</li>
</ul>
</li>
<li><p><strong>ValueError: <code>Please use Flashinfer backend for models with logits_soft_cap (i.e., Gemma-2). Otherwise, the output might be wrong. Set Flashinfer backend by export VLLM_ATTENTION_BACKEND=FLASHINFER.</code></strong></p>
<ul>
<li><strong>原因</strong>：未设置Flashinfer后端。</li>
<li><strong>解决方法</strong>：设置环境变量<code>VLLM_ATTENTION_BACKEND</code>为<code>FLASHINFER</code>。</li>
</ul>
</li>
</ol>
<p>通过上述详细步骤和解决方法，您应该能够成功运行并调试微调后的Gemma-2-2b-it模型。如果您在任何一步遇到问题，请参考相应的文档或在社区中寻求帮助。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://chenhuiyu.github.io">Huiyu Chen</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://chenhuiyu.github.io/2024/08/07/NLP%20Insights/%E4%BD%BF%E7%94%A8vllm%E8%BF%90%E8%A1%8C%E5%BE%AE%E8%B0%83%E5%90%8E%E7%9A%84gemma-2.zh-CN/">https://chenhuiyu.github.io/2024/08/07/NLP%20Insights/%E4%BD%BF%E7%94%A8vllm%E8%BF%90%E8%A1%8C%E5%BE%AE%E8%B0%83%E5%90%8E%E7%9A%84gemma-2.zh-CN/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://chenhuiyu.github.io" target="_blank">黑头呆鱼进化之旅</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/vLLM/">vLLM</a><a class="post-meta__tags" href="/tags/Gemma-2/">Gemma-2</a></div><div class="post-share"><div class="social-share" data-image="/img/butterfly-icon.png" data-sites="facebook,x,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2024/08/13/NLP%20Insights/%E4%BD%BF%E7%94%A8%E5%8E%8B%E7%BC%A9%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA%E8%BF%9B%E8%A1%8C%E6%9C%AC%E5%9C%B0-llm-%E7%9A%84%E5%BF%AB%E9%80%9F-json-%E8%A7%A3%E7%A0%81.en/" title="使用压缩有限状态机进行本地 LLM 的快速 JSON 解码"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">使用压缩有限状态机进行本地 LLM 的快速 JSON 解码</div></div><div class="info-2"><div class="info-item-1">使用压缩有限状态机进行本地 LLM 的快速 JSON 解码作者: Liangsheng Yin, Ying Sheng, Lianmin Zheng日期: 2024 年 2 月 5 日  本文内容基于 LMSYS Org 发布的一篇博客文章，原文链接：LMSYS Org 博客。相关的代码库可以在以下链接找到：SGLang 代码库。 让一个 LLM 始终生成符合特定模式的有效 JSON 或 YAML，对于许多应用来说是一个关键特性。在这篇博客文章中，我们介绍了一种显著加速这种约束解码的优化方法。我们的方法利用了压缩的有限状态机，并且兼容任何正则表达式，因此可以适用于任何 JSON 或 YAML 模式。与现有系统逐步解码一个标记的方式不同，我们的方法分析了正则表达式的有限状态机，压缩了单一的转换路径，并在可能的情况下一次性解码多个标记。与最先进的系统（guidance + llama.cpp，outlines + vLLM）相比，我们的方法可以将延迟减少最多 2 倍，并提高吞吐量最多 2.5 倍。这一优化还使得约束解码比普通解码更快。你可以在 SGLang 上试用它。  图一展示了 ...</div></div></div></a><a class="pagination-related" href="/2024/08/07/NLP%20Insights/%E4%BD%BF%E7%94%A8vllm%E8%BF%90%E8%A1%8C%E5%BE%AE%E8%B0%83%E5%90%8E%E7%9A%84gemma-2.en/" title="使用vLLM运行微调后的Gemma-2"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">使用vLLM运行微调后的Gemma-2</div></div><div class="info-2"><div class="info-item-1">使用vLLM运行微调后的Gemma-2-2b-it的详细步骤在这里分享一下我运行微调后的Gemma-2-2b-it模型并使用vLLM的步骤，希望对其他人有所帮助。本文将详细介绍安装过程、环境配置以及常见问题的解决方法。 安装和验证vLLM首先，确保安装并验证vLLM的版本是0.5.3。  安装vLLM： 1pip install vllm==0.5.3  验证安装： 123import vllmprint(vllm.__version__)# 输出: 0.5.3  安装Flashinfer按照以下步骤安装Flashinfer，并确保您的torch版本和CUDA兼容性。  检查torch版本和CUDA兼容性： 123import torchprint(torch.__version__)  # 应输出: 2.3.1+cu121print(torch.version.cuda) # 应输出: 12.1  安装Flashinfer：根据文档，Gemma运行在版本0.08。vLLM需要FlashInfer v0.0.8（请参阅vLLM版本和Flashinfer文档中关于Gemma 2的部...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2024/08/07/NLP%20Insights/%E4%BD%BF%E7%94%A8vllm%E8%BF%90%E8%A1%8C%E5%BE%AE%E8%B0%83%E5%90%8E%E7%9A%84gemma-2.en/" title="使用vLLM运行微调后的Gemma-2"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-08-07</div><div class="info-item-2">使用vLLM运行微调后的Gemma-2</div></div><div class="info-2"><div class="info-item-1">使用vLLM运行微调后的Gemma-2-2b-it的详细步骤在这里分享一下我运行微调后的Gemma-2-2b-it模型并使用vLLM的步骤，希望对其他人有所帮助。本文将详细介绍安装过程、环境配置以及常见问题的解决方法。 安装和验证vLLM首先，确保安装并验证vLLM的版本是0.5.3。  安装vLLM： 1pip install vllm==0.5.3  验证安装： 123import vllmprint(vllm.__version__)# 输出: 0.5.3  安装Flashinfer按照以下步骤安装Flashinfer，并确保您的torch版本和CUDA兼容性。  检查torch版本和CUDA兼容性： 123import torchprint(torch.__version__)  # 应输出: 2.3.1+cu121print(torch.version.cuda) # 应输出: 12.1  安装Flashinfer：根据文档，Gemma运行在版本0.08。vLLM需要FlashInfer v0.0.8（请参阅vLLM版本和Flashinfer文档中关于Gemma 2的部...</div></div></div></a><a class="pagination-related" href="/2024/08/07/NLP%20Insights/detailed-steps-for-running-fine-tuned-gemma-2-2b-it-with-vllm.en/" title="Detailed Steps for Running Fine-tuned Gemma-2-2b-it with vLLM"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-08-07</div><div class="info-item-2">Detailed Steps for Running Fine-tuned Gemma-2-2b-it with vLLM</div></div><div class="info-2"><div class="info-item-1">In this post, I will share the steps to run the fine-tuned Gemma-2-2b-it model using vLLM. This guide will cover the installation process, environment configuration, and common troubleshooting tips. Installation and Verification of vLLMFirst, ensure that you have installed and verified vLLM version 0.5.3.  Install vLLM: 1!pip install vllm==0.5.3  Verify the installation: 123import vllmprint(vllm.__version__)# Output: 0.5.3  Installing FlashinferFollow these steps to install Flashinfer, ensuri...</div></div></div></a><a class="pagination-related" href="/2024/08/07/NLP%20Insights/detailed-steps-for-running-fine-tuned-gemma-2-2b-it-with-vllm.zh-CN/" title="Detailed Steps for Running Fine-tuned Gemma-2-2b-it with vLLM"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-08-07</div><div class="info-item-2">Detailed Steps for Running Fine-tuned Gemma-2-2b-it with vLLM</div></div><div class="info-2"><div class="info-item-1">In this post, I will share the steps to run the fine-tuned Gemma-2-2b-it model using vLLM. This guide will cover the installation process, environment configuration, and common troubleshooting tips. Installation and Verification of vLLMFirst, ensure that you have installed and verified vLLM version 0.5.3.  Install vLLM: 1!pip install vllm==0.5.3  Verify the installation: 123import vllmprint(vllm.__version__)# Output: 0.5.3  Installing FlashinferFollow these steps to install Flashinfer, ensuri...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Huiyu Chen</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">118</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">46</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8vLLM%E8%BF%90%E8%A1%8C%E5%BE%AE%E8%B0%83%E5%90%8E%E7%9A%84Gemma-2-2b-it%E7%9A%84%E8%AF%A6%E7%BB%86%E6%AD%A5%E9%AA%A4"><span class="toc-number">1.</span> <span class="toc-text">使用vLLM运行微调后的Gemma-2-2b-it的详细步骤</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%89%E8%A3%85%E5%92%8C%E9%AA%8C%E8%AF%81vLLM"><span class="toc-number">1.1.</span> <span class="toc-text">安装和验证vLLM</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%89%E8%A3%85Flashinfer"><span class="toc-number">1.2.</span> <span class="toc-text">安装Flashinfer</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9B%B4%E6%96%B0%E7%8E%AF%E5%A2%83%E4%B8%AD%E7%9A%84VLLM%E5%90%8E%E7%AB%AF%E5%8F%98%E9%87%8F"><span class="toc-number">1.3.</span> <span class="toc-text">更新环境中的VLLM后端变量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B5%8B%E8%AF%95vLLM"><span class="toc-number">1.4.</span> <span class="toc-text">测试vLLM</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95"><span class="toc-number">1.5.</span> <span class="toc-text">常见错误及解决方法</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/21/NLP%20Insights/evaluation-of-generation-based-large-language-models-llms-opportunities-and-challenges-from-generation-to-judgment.en/" title="evaluation-of-generation-based-large-language-models-llms-opportunities-and-challenges-from-generation-to-judgment">evaluation-of-generation-based-large-language-models-llms-opportunities-and-challenges-from-generation-to-judgment</a><time datetime="2026-02-20T16:00:00.000Z" title="发表于 2026-02-21 00:00:00">2026-02-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/21/NLP%20Insights/evaluation-of-generation-based-large-language-models-llms-opportunities-and-challenges-from-generation-to-judgment.zh-CN/" title="evaluation-of-generation-based-large-language-models-llms-opportunities-and-challenges-from-generation-to-judgment">evaluation-of-generation-based-large-language-models-llms-opportunities-and-challenges-from-generation-to-judgment</a><time datetime="2026-02-20T16:00:00.000Z" title="发表于 2026-02-21 00:00:00">2026-02-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/24/NLP%20Insights/secom-redefining-memory-management-in-conversational-ai.zh-CN/" title="SeCom: 重新定义对话AI的记忆管理">SeCom: 重新定义对话AI的记忆管理</a><time datetime="2025-06-24T08:00:00.000Z" title="发表于 2025-06-24 16:00:00">2025-06-24</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/24/NLP%20Insights/secom-redefining-memory-management-in-conversational-ai.en/" title="SeCom: Redefining Memory Management in Conversational AI">SeCom: Redefining Memory Management in Conversational AI</a><time datetime="2025-06-24T08:00:00.000Z" title="发表于 2025-06-24 16:00:00">2025-06-24</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/03/06/NLP%20Insights/decoder-only%E4%B8%8Eencoder-only%E6%A8%A1%E5%9E%8Bpadding%E7%AD%96%E7%95%A5%E7%9A%84%E5%B7%AE%E5%BC%82.en/" title="Decoder-only与Encoder-only模型Padding策略的差异">Decoder-only与Encoder-only模型Padding策略的差异</a><time datetime="2025-03-06T09:43:10.000Z" title="发表于 2025-03-06 17:43:10">2025-03-06</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 - 2026 By Huiyu Chen</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 6.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.5.4</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.4"></script><script src="/js/main.js?v=5.5.4"></script><div class="js-pjax"></div><script src="/js/lang-switch.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>