<!DOCTYPE html><html lang="[&quot;zh-CN&quot;,&quot;en&quot;]" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>FastChat 训练脚本代码逐行解析-Train.py 【FastChat 系列第 1 篇】 | 黑头呆鱼进化之旅</title><meta name="author" content="Huiyu Chen"><meta name="copyright" content="Huiyu Chen"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="FastChat 训练脚本代码逐行解析-Train.py 【FastChat 系列第 1 篇】在本文中，我们将深入探讨 FastChat 的 train.py 脚本，这是一个用于训练和优化大型语言模型的关键组件。FastChat 是一个先进的开源平台，专注于开发、部署和评估基于大型语言模型（LLM）的聊天机器人。该平台不仅提供对顶尖模型如 Vicuna 和 MT-Bench 的支持，还包括一个分布">
<meta property="og:type" content="article">
<meta property="og:title" content="FastChat 训练脚本代码逐行解析-Train.py 【FastChat 系列第 1 篇】">
<meta property="og:url" content="https://chenhuiyu.github.io/2024/02/27/NLP%20Insights/FastChat%20%E8%AE%AD%E7%BB%83%E8%84%9A%E6%9C%AC%E4%BB%A3%E7%A0%81%E9%80%90%E8%A1%8C%E8%A7%A3%E6%9E%90-Train.py%20%E3%80%90FastChat%20%E7%B3%BB%E5%88%97%E7%AC%AC%201%20%E7%AF%87%E3%80%91.en/index.html">
<meta property="og:site_name" content="黑头呆鱼进化之旅">
<meta property="og:description" content="FastChat 训练脚本代码逐行解析-Train.py 【FastChat 系列第 1 篇】在本文中，我们将深入探讨 FastChat 的 train.py 脚本，这是一个用于训练和优化大型语言模型的关键组件。FastChat 是一个先进的开源平台，专注于开发、部署和评估基于大型语言模型（LLM）的聊天机器人。该平台不仅提供对顶尖模型如 Vicuna 和 MT-Bench 的支持，还包括一个分布">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://chenhuiyu.github.io/img/butterfly-icon.png">
<meta property="article:published_time" content="2024-02-26T17:43:18.000Z">
<meta property="article:modified_time" content="2026-02-20T22:13:37.936Z">
<meta property="article:author" content="Huiyu Chen">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="FastChat">
<meta property="article:tag" content="Train">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://chenhuiyu.github.io/img/butterfly-icon.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "FastChat 训练脚本代码逐行解析-Train.py 【FastChat 系列第 1 篇】",
  "url": "https://chenhuiyu.github.io/2024/02/27/NLP%20Insights/FastChat%20%E8%AE%AD%E7%BB%83%E8%84%9A%E6%9C%AC%E4%BB%A3%E7%A0%81%E9%80%90%E8%A1%8C%E8%A7%A3%E6%9E%90-Train.py%20%E3%80%90FastChat%20%E7%B3%BB%E5%88%97%E7%AC%AC%201%20%E7%AF%87%E3%80%91.en/",
  "image": "https://chenhuiyu.github.io/img/butterfly-icon.png",
  "datePublished": "2024-02-26T17:43:18.000Z",
  "dateModified": "2026-02-20T22:13:37.936Z",
  "author": [
    {
      "@type": "Person",
      "name": "Huiyu Chen",
      "url": "https://chenhuiyu.github.io"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://chenhuiyu.github.io/2024/02/27/NLP%20Insights/FastChat%20%E8%AE%AD%E7%BB%83%E8%84%9A%E6%9C%AC%E4%BB%A3%E7%A0%81%E9%80%90%E8%A1%8C%E8%A7%A3%E6%9E%90-Train.py%20%E3%80%90FastChat%20%E7%B3%BB%E5%88%97%E7%AC%AC%201%20%E7%AF%87%E3%80%91.en/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=5.5.4"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.13.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'FastChat 训练脚本代码逐行解析-Train.py 【FastChat 系列第 1 篇】',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 6.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="黑头呆鱼进化之旅" type="application/atom+xml">
</head><body><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">黑头呆鱼进化之旅</span></a><a class="nav-page-title" href="/"><span class="site-name">FastChat 训练脚本代码逐行解析-Train.py 【FastChat 系列第 1 篇】</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  Back to Home</span></span></a></span><div id="menus"></div></nav><div id="post-info"><h1 class="post-title">FastChat 训练脚本代码逐行解析-Train.py 【FastChat 系列第 1 篇】</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2024-02-26T17:43:18.000Z" title="Created 2024-02-27 01:43:18">2024-02-27</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2026-02-20T22:13:37.936Z" title="Updated 2026-02-21 06:13:37">2026-02-21</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/NLP-Insights/">NLP Insights</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="FastChat-训练脚本代码逐行解析-Train-py-【FastChat-系列第-1-篇】"><a href="#FastChat-训练脚本代码逐行解析-Train-py-【FastChat-系列第-1-篇】" class="headerlink" title="FastChat 训练脚本代码逐行解析-Train.py 【FastChat 系列第 1 篇】"></a>FastChat 训练脚本代码逐行解析-Train.py 【FastChat 系列第 1 篇】</h1><p>在本文中，我们将深入探讨 <a target="_blank" rel="noopener" href="https://github.com/lm-sys/FastChat">FastChat</a> 的 <code>train.py</code> 脚本，这是一个用于训练和优化大型语言模型的关键组件。FastChat 是一个先进的开源平台，专注于开发、部署和评估基于大型语言模型（LLM）的聊天机器人。该平台不仅提供对顶尖模型如 Vicuna 和 MT-Bench 的支持，还包括一个分布式的多模型服务系统，配备了 Web UI 和与 OpenAI 兼容的 RESTful API，使用户能够高效地训练和评估他们的模型。</p>
<p>本文的深入分析将聚焦于 <code>train.py</code> 脚本的源代码。这个脚本是基于 transformers 库的自然语言处理模型训练脚本，涵盖了数据预处理、模型训练和保存等关键步骤。我们旨在提供对 <code>train.py</code> 中每个类和函数的详细解释，包括它们的功能和在整个训练过程中的作用。</p>
<h2 id="1-导入模块"><a href="#1-导入模块" class="headerlink" title="1. 导入模块"></a>1. 导入模块</h2><h3 id="1-内置模块"><a href="#1-内置模块" class="headerlink" title="1. 内置模块"></a>1. 内置模块</h3><p>这些是 Python 自带的标准库模块，无需额外安装。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> dataclasses <span class="keyword">import</span> dataclass, field</span><br></pre></td></tr></tbody></table></figure>

<p>导入 Python 的<code>dataclasses</code>模块，用于创建带有默认值的类。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br></pre></td></tr></tbody></table></figure>

<p>导入<code>json</code>模块，用于处理 JSON 格式的数据。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br></pre></td></tr></tbody></table></figure>

<p>导入<code>math</code>模块，用于数学运算。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pathlib</span><br></pre></td></tr></tbody></table></figure>

<p>导入<code>pathlib</code>模块，用于处理文件路径。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Dict</span>, <span class="type">Optional</span>, <span class="type">Sequence</span></span><br></pre></td></tr></tbody></table></figure>

<p>导入<code>typing</code>模块，用于类型注解。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></tbody></table></figure>

<h3 id="2-依赖库"><a href="#2-依赖库" class="headerlink" title="2. 依赖库"></a>2. 依赖库</h3><p>这些是外部安装的依赖库，通常通过包管理器如 pip 安装。<br>导入<code>numpy</code>库，一个常用的科学计算库。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></tbody></table></figure>

<p>导入<code>PyTorch</code>，一个流行的深度学习框架。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br></pre></td></tr></tbody></table></figure>

<p>从<code>torch</code>中导入<code>Dataset</code>，用于创建自定义数据集。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> transformers</span><br></pre></td></tr></tbody></table></figure>

<p>导入<code>transformers</code>库，一个流行的自然语言处理库。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Trainer</span><br></pre></td></tr></tbody></table></figure>

<p>从<code>transformers</code>中导入<code>Trainer</code>，用于训练模型。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers.trainer_pt_utils <span class="keyword">import</span> LabelSmoother</span><br></pre></td></tr></tbody></table></figure>

<p>从<code>transformers</code>中导入<code>LabelSmoother</code>，用于标签平滑。</p>
<h3 id="3-项目特定函数"><a href="#3-项目特定函数" class="headerlink" title="3. 项目特定函数"></a>3. 项目特定函数</h3><p>这些是在 Fast Chat 项目中自定义实现的函数或类。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fastchat.conversation <span class="keyword">import</span> SeparatorStyle</span><br><span class="line"></span><br><span class="line"><span class="comment">## code from FastChat/fastchat/conversation.py</span></span><br><span class="line">    <span class="string">"""Separator styles."""</span></span><br><span class="line"></span><br><span class="line">    ADD_COLON_SINGLE = auto()</span><br><span class="line">    ADD_COLON_TWO = auto()</span><br><span class="line">    ADD_COLON_SPACE_SINGLE = auto()</span><br><span class="line">    NO_COLON_SINGLE = auto()</span><br><span class="line">    NO_COLON_TWO = auto()</span><br><span class="line">    ADD_NEW_LINE_SINGLE = auto()</span><br><span class="line">    LLAMA2 = auto()</span><br><span class="line">    CHATGLM = auto()</span><br><span class="line">    CHATML = auto()</span><br><span class="line">    CHATINTERN = auto()</span><br><span class="line">    DOLLY = auto()</span><br><span class="line">    RWKV = auto()</span><br><span class="line">    PHOENIX = auto()</span><br><span class="line">    ROBIN = auto()</span><br><span class="line">    FALCON_CHAT = auto()</span><br><span class="line">    CHATGLM3 = auto()</span><br><span class="line">    DEEPSEEK_CHAT = auto()</span><br><span class="line">    METAMATH = auto()</span><br><span class="line">    YUAN2 = auto()</span><br></pre></td></tr></tbody></table></figure>

<p>从<code>fastchat</code>包导入<code>SeparatorStyle</code>，用于定义对话分隔符风格。<code>SeparatorStyle</code> 类是一个使用 Python 的 <code>enum</code> 模块创建的枚举类，用于定义一系列的分隔符样式。枚举（Enumeration）是一种编程概念，用于定义一组命名的常数，使代码更加清晰和易于维护。</p>
<p>在 <code>SeparatorStyle</code> 类中，每个成员代表一种特定的分隔符样式。这些样式通常用于文本处理中，特别是在需要区分不同部分或元素的情况下。例如，在处理对话或文本数据时，可能需要不同的方式来区分用户输入和机器回复。</p>
<p>关于 <code>auto()</code> 函数的使用：</p>
<ul>
<li><code>auto()</code> 是 Python <code>enum</code> 模块提供的一个特殊函数。它在枚举类中自动分配一个唯一的值给每个成员。</li>
<li>在不使用 <code>auto()</code> 的情况下，你需要手动为每个枚举成员指定一个唯一的值。使用 <code>auto()</code> 可以简化这个过程，让 Python 自动处理这些值的分配。</li>
<li><code>auto()</code> 分配的值通常是整数，从 1 开始依次递增。</li>
</ul>
<p>具体到 <code>SeparatorStyle</code> 类，<code>auto()</code> 被用来为每种分隔符样式自动分配一个唯一的整数值。例如，<code>ADD_COLON_SINGLE</code>、<code>ADD_COLON_TWO</code> 等将分别被赋予不同的整数值。</p>
<p>每个枚举成员的名称（如 <code>ADD_COLON_SINGLE</code>、<code>NO_COLON_SINGLE</code> 等）通常描述了该分隔符样式的特点。例如，<code>ADD_COLON_SINGLE</code> 可能表示在某个元素后添加一个冒号作为分隔符，而 <code>NO_COLON_SINGLE</code> 则表示不添加冒号。</p>
<p>这种方式使得在代码中引用和处理这些分隔符样式变得更加方便和清晰。例如，可以根据不同的场景或需求选择使用不同的分隔符样式，而无需记住它们对应的具体值。</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fastchat.model.model_adapter <span class="keyword">import</span> get_conversation_template</span><br><span class="line"></span><br><span class="line"><span class="comment">## code from FastChat/fastchat/model/model_adapter.py</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_conversation_template</span>(<span class="params">model_path: <span class="built_in">str</span></span>) -&gt; Conversation:</span><br><span class="line">    <span class="string">"""Get the default conversation template."""</span></span><br><span class="line">    adapter = get_model_adapter(model_path)</span><br><span class="line">    <span class="keyword">return</span> adapter.get_default_conv_template(model_path)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## code from FastChat/fastchat/model/model_adapter.py</span></span><br><span class="line"><span class="meta">@cache</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_model_adapter</span>(<span class="params">model_path: <span class="built_in">str</span></span>) -&gt; BaseModelAdapter:</span><br><span class="line">    <span class="string">"""Get a model adapter for a model_path."""</span></span><br><span class="line">    model_path_basename = os.path.basename(os.path.normpath(model_path))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Try the basename of model_path at first</span></span><br><span class="line">    <span class="keyword">for</span> adapter <span class="keyword">in</span> model_adapters:</span><br><span class="line">        <span class="keyword">if</span> adapter.<span class="keyword">match</span>(model_path_basename) <span class="keyword">and</span> <span class="built_in">type</span>(adapter) != BaseModelAdapter:</span><br><span class="line">            <span class="keyword">return</span> adapter</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Then try the full path</span></span><br><span class="line">    <span class="keyword">for</span> adapter <span class="keyword">in</span> model_adapters:</span><br><span class="line">        <span class="keyword">if</span> adapter.<span class="keyword">match</span>(model_path):</span><br><span class="line">            <span class="keyword">return</span> adapter</span><br><span class="line"></span><br><span class="line">    <span class="keyword">raise</span> ValueError(<span class="string">f"No valid model adapter for <span class="subst">{model_path}</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## code from FastChat/fastchat/model/model_adapter.py</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_default_conv_template</span>(<span class="params">self, model_path: <span class="built_in">str</span></span>) -&gt; Conversation:</span><br><span class="line">        <span class="keyword">return</span> get_conv_template(<span class="string">"one_shot"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## code from FastChat/fastchat/conversation.py</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_conv_template</span>(<span class="params">name: <span class="built_in">str</span></span>) -&gt; Conversation:</span><br><span class="line">    <span class="string">"""Get a conversation template."""</span></span><br><span class="line">    <span class="keyword">return</span> conv_templates[name].copy()</span><br><span class="line"></span><br><span class="line"><span class="comment">## code from FastChat/fastchat/conversation.py</span></span><br><span class="line"><span class="comment"># A global registry for all conversation templates</span></span><br><span class="line">conv_templates: <span class="type">Dict</span>[<span class="built_in">str</span>, Conversation] = {}</span><br></pre></td></tr></tbody></table></figure>

<p>从<code>fastchat</code>包导入<code>get_conversation_template</code>，用于获取对话模板。<br>在这段代码中，调用逻辑主要涉及到获取特定模型的默认对话模板。调用链路如下：</p>
<ol>
<li><p><strong>起始调用 - <code>get_conversation_template(model_path: str)</code></strong></p>
<ul>
<li>这个函数是调用链的起点。它接收一个参数 <code>model_path</code>，用于指定模型的路径。</li>
<li>这个函数的目的是获取给定模型路径的默认对话模板。</li>
</ul>
</li>
<li><p><strong>调用 <code>get_model_adapter(model_path: str)</code></strong></p>
<ul>
<li><code>get_conversation_template</code> 函数首先调用 <code>get_model_adapter</code>，传入模型路径。</li>
<li><code>get_model_adapter</code> 的目的是根据提供的模型路径，找到并返回一个适合该模型的 <code>BaseModelAdapter</code> 对象。</li>
<li>这个函数首先尝试匹配 <code>model_path</code> 的基本名称（basename），如果没有找到匹配项，它会尝试匹配完整的路径。</li>
<li>如果找到合适的适配器，则返回该适配器；如果没有找到，则抛出一个 <code>ValueError</code>。</li>
</ul>
</li>
<li><p><strong>执行 <code>BaseModelAdapter.get_default_conv_template(model_path: str)</code></strong></p>
<ul>
<li>在获取到适当的模型适配器后，<code>get_conversation_template</code> 通过调用该适配器的 <code>get_default_conv_template</code> 方法来获取默认的对话模板。</li>
<li>注意这个方法在 <code>BaseModelAdapter</code> 类中定义，但可能在子类中被重写。</li>
</ul>
</li>
<li><p><strong>调用 <code>get_conv_template(name: str)</code></strong></p>
<ul>
<li>在 <code>get_default_conv_template</code> 方法内部，它调用 <code>get_conv_template</code> 函数，通常传入一个预定义的模板名称，比如 <code>"one_shot"</code>。</li>
<li><code>get_conv_template</code> 的作用是从全局注册的对话模板字典 <code>conv_templates</code> 中获取指定名称的模板。</li>
</ul>
</li>
<li><p><strong>获取并返回 <code>Conversation</code> 对象</strong></p>
<ul>
<li><code>get_conv_template</code> 函数返回 <code>Conversation</code> 类的一个实例，这通常是从 <code>conv_templates</code> 字典中复制得到的。</li>
<li>最终，这个 <code>Conversation</code> 实例被返回到最初调用 <code>get_conversation_template</code> 的地方。</li>
</ul>
</li>
</ol>
<p>总结调用链路：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">get_conversation_template(model_path)</span><br><span class="line">  -&gt; get_model_adapter(model_path)</span><br><span class="line">  -&gt; [BaseModelAdapter].get_default_conv_template(model_path)</span><br><span class="line">    -&gt; get_conv_template(name)</span><br><span class="line">      -&gt; 返回 Conversation 对象</span><br></pre></td></tr></tbody></table></figure>

<p>在这个过程中，代码通过一系列函数调用，根据提供的模型路径，找到相应的模型适配器，并从中获取特定的对话模板。这种设计模式允许灵活地为不同的模型提供不同的对话模板，从而提高了代码的可重用性和可扩展性。</p>
<hr>
<h2 id="2-配置类"><a href="#2-配置类" class="headerlink" title="2. 配置类"></a>2. 配置类</h2><p>这些类是使用 Python 的 <code>dataclass</code> 装饰器定义的，主要用于存储配置和参数。这些类通常不包含复杂的方法或逻辑，而是用于定义和存储数据结构。这些类包括：</p>
<ul>
<li><code>ModelArguments</code>: 存储与模型相关的参数，如模型路径、远程代码信任等。</li>
<li><code>DataArguments</code>: 存储与数据相关的参数，如数据路径、评估数据路径以及是否使用懒加载预处理。</li>
<li><code>TrainingArguments</code>: 存储与训练相关的参数，如缓存目录、优化器类型、模型最大长度等。这个类继承自 <code>transformers.TrainingArguments</code>，增加了一些自定义参数。</li>
</ul>
<p>这些类主要用于简化和组织代码中的参数管理，使得参数的修改和访问更加方便。</p>
<h3 id="1-ModelArguments-类"><a href="#1-ModelArguments-类" class="headerlink" title="1. ModelArguments 类"></a>1. ModelArguments 类</h3><h4 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ModelArguments</span>:</span><br><span class="line">    model_name_or_path: <span class="type">Optional</span>[<span class="built_in">str</span>] = field(default=<span class="string">"facebook/opt-125m"</span>)</span><br><span class="line">    trust_remote_code: <span class="built_in">bool</span> = field(</span><br><span class="line">        default=<span class="literal">False</span>,</span><br><span class="line">        metadata={</span><br><span class="line">            <span class="string">"help"</span>: <span class="string">"Whether or not to allow for custom models defined on the Hub in their own modeling files"</span></span><br><span class="line">        },</span><br><span class="line">    )</span><br><span class="line">    padding_side: <span class="built_in">str</span> = field(</span><br><span class="line">        default=<span class="string">"right"</span>, metadata={<span class="string">"help"</span>: <span class="string">"The padding side in tokenizer"</span>}</span><br><span class="line">    )</span><br></pre></td></tr></tbody></table></figure>

<h4 id="Explanation"><a href="#Explanation" class="headerlink" title="Explanation"></a>Explanation</h4><p><code>ModelArguments</code> 是一个数据类（<code>dataclass</code>），用于存储与模型相关的配置参数。<br><strong>属性：</strong></p>
<ol>
<li><code>model_name_or_path</code>: 指定预训练模型的名称或路径。</li>
<li><code>trust_remote_code</code>: 是否允许使用自定义模型，这些模型在 Hub 上有自己的模型文件。</li>
<li><code>padding_side</code>: 指定在分词器（<code>tokenizer</code>）中使用的填充方式，通常是左填充或右填充。</li>
</ol>
<details>
<summary> `@dataclass`装饰器的介绍，点击展开</summary>
`@dataclass` 是一个装饰器，用于自动化生成特殊方法，如 `__init__()`、`__repr__()`、`__eq__()` 等，从而简化数据类的编写。这个装饰器是 Python 3.7 中引入的一部分，属于 `dataclasses` 模块。

<p>当你在一个类定义前使用 <code>@dataclass</code> 装饰器时，Python 会自动为这个类添加一些由属性定义的特殊方法。这对于创建存储少量数据但不需要复杂方法的类非常有用。</p>
<p>具体来说，使用 <code>@dataclass</code> 时：</p>
<ol>
<li><p><strong>自动生成构造函数（<code>__init__</code> 方法）</strong>：Python 会根据类中定义的字段自动创建一个 <code>__init__</code> 方法，这样你就不需要手动编写这个方法来初始化类的实例了。</p>
</li>
<li><p><strong>自动生成 <code>__repr__</code> 方法</strong>：这使得打印类的实例时能够得到更具可读性的字符串表示，通常包含类名和其中的字段及其值。</p>
</li>
<li><p><strong>自动生成 <code>__eq__</code> 方法</strong>：这使得可以使用 <code>==</code> 操作符来比较两个类的实例，比较的是实例中字段的值。</p>
</li>
<li><p><strong>支持类型注解</strong>：在定义字段时，你可以使用类型注解，这不仅有助于代码清晰性，还可以通过一些工具进行类型检查。</p>
</li>
</ol>
<p>在<code>ModelArguments</code>类的例子中，<code>@dataclass</code>装饰器会为这个类生成上述的方法。这意味着你可以很方便地创建<code>ModelArguments</code>的实例，并在打印或比较这些实例时得到预期的行为。</p>
<p>例如，当你创建一个<code>ModelArguments</code>实例时：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">args = ModelArguments()</span><br></pre></td></tr></tbody></table></figure>

<p>这将调用自动生成的<code>__init__</code>方法，使用默认值”facebook/opt-125m”为<code>model_name_or_path</code>、<code>False</code>为<code>trust_remote_code</code>和”right”为<code>padding_side</code>。</p>
<p>当你打印这个实例：</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(args)</span><br></pre></td></tr></tbody></table></figure>

<p>这将调用自动生成的<code>__repr__</code>方法，显示类实例的详细信息，如<code>ModelArguments(model_name_or_path="facebook/opt-125m", trust_remote_code=False, padding_side="right")</code>。</p>
<p>这样，<code>@dataclass</code>装饰器简化了类的创建过程，使得代码更加简洁和易于维护。</p>
<p>总的来说，<code>@dataclass</code> 装饰器是 Python 提供的一个便捷工具，用于快速创建主要用于存储数据的类。</p>
</details>

<h3 id="2-DataArguments-类"><a href="#2-DataArguments-类" class="headerlink" title="2. DataArguments 类"></a>2. DataArguments 类</h3><h4 id="Code-1"><a href="#Code-1" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DataArguments</span>:</span><br><span class="line">    data_path: <span class="built_in">str</span> = field(</span><br><span class="line">        default=<span class="literal">None</span>, metadata={<span class="string">"help"</span>: <span class="string">"Path to the training data."</span>}</span><br><span class="line">    )</span><br><span class="line">    eval_data_path: <span class="built_in">str</span> = field(</span><br><span class="line">        default=<span class="literal">None</span>, metadata={<span class="string">"help"</span>: <span class="string">"Path to the evaluation data."</span>}</span><br><span class="line">    )</span><br><span class="line">    lazy_preprocess: <span class="built_in">bool</span> = <span class="literal">False</span></span><br></pre></td></tr></tbody></table></figure>

<h4 id="Explanation-1"><a href="#Explanation-1" class="headerlink" title="Explanation"></a>Explanation</h4><p><strong>DataArguments 类</strong></p>
<ul>
<li><code>DataArguments</code> 也是一个数据类，用于存储数据相关的配置参数。</li>
<li>属性：<ul>
<li><code>data_path</code>: 训练数据的路径。</li>
<li><code>eval_data_path</code>: 评估数据的路径。</li>
<li><code>lazy_preprocess</code>: 是否在数据预处理时使用延迟加载，即在需要时才加载和处理数据。</li>
</ul>
</li>
</ul>
<h3 id="3-TrainingArguments-类"><a href="#3-TrainingArguments-类" class="headerlink" title="3. TrainingArguments 类"></a>3. TrainingArguments 类</h3><h4 id="Code-2"><a href="#Code-2" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TrainingArguments</span>(transformers.TrainingArguments):</span><br><span class="line">    cache_dir: <span class="type">Optional</span>[<span class="built_in">str</span>] = field(default=<span class="literal">None</span>)</span><br><span class="line">    optim: <span class="built_in">str</span> = field(default=<span class="string">"adamw_torch"</span>)</span><br><span class="line">    model_max_length: <span class="built_in">int</span> = field(</span><br><span class="line">        default=<span class="number">512</span>,</span><br><span class="line">        metadata={</span><br><span class="line">            <span class="string">"help"</span>: <span class="string">"Maximum sequence length. Sequences will be right padded (and possibly truncated)."</span></span><br><span class="line">        },</span><br><span class="line">    )</span><br></pre></td></tr></tbody></table></figure>

<h4 id="Explanation-2"><a href="#Explanation-2" class="headerlink" title="Explanation"></a>Explanation</h4><p><code>TrainingArguments</code> 类继承自 <code>transformers.TrainingArguments</code>。。</p>
<ol>
<li><p><strong>TrainingArguments 类</strong></p>
<ul>
<li><code>TrainingArguments</code> 是一个数据类，它通过继承 <code>transformers.TrainingArguments</code>，获得了处理训练参数的能力。</li>
<li>在 <code>TrainingArguments</code> 中定义的属性：<ul>
<li><code>cache_dir</code>: 用于指定模型和分词器缓存的目录路径。</li>
<li><code>optim</code>: 定义了要使用的优化器类型，例如 <code>'adamw_torch'</code>。</li>
<li><code>model_max_length</code>: 指定模型能处理的最大序列长度。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>transformers.TrainingArguments 类</strong></p>
<ul>
<li><code>transformers.TrainingArguments</code> 是 <code>transformers</code> 库中的一个类，用于配置模型训练过程中的各种参数。</li>
<li>这个类包含大量的属性，用于控制训练过程，例如：<ul>
<li><code>output_dir</code>: 指定保存模型和训练结果的目录。</li>
<li><code>num_train_epochs</code>: 训练的轮数（epochs）。</li>
<li><code>per_device_train_batch_size</code>: 每个设备上的训练批次大小。</li>
<li><code>save_steps</code>: 保存模型的步数间隔。</li>
<li><code>evaluation_strategy</code>: 评估模型的策略，如在每个 epoch 结束时进行评估。</li>
<li><code>learning_rate</code>: 学习率。</li>
<li><code>warmup_steps</code>: 在学习率调度中用于预热的步数。</li>
</ul>
</li>
<li><code>transformers.TrainingArguments</code> 还包含了许多其他参数，用于微调训练过程，包括日志记录、模型保存策略、学习率调度等。</li>
</ul>
</li>
</ol>
<p>通过继承 <code>transformers.TrainingArguments</code>，<code>TrainingArguments</code> 类不仅继承了所有这些训练参数的配置能力，而且还可以添加一些自定义的训练参数，如本例中的 <code>cache_dir</code>、<code>optim</code> 和 <code>model_max_length</code>。这种做法提高了代码的可复用性和灵活性，使得您可以根据项目的具体需求调整和扩展训练配置。</p>
<h2 id="3-功能型函数-Functional-Utility-Functions"><a href="#3-功能型函数-Functional-Utility-Functions" class="headerlink" title="3.功能型函数 (Functional Utility Functions)"></a>3.功能型函数 (Functional Utility Functions)</h2><h3 id="1-rank0-print-args"><a href="#1-rank0-print-args" class="headerlink" title="1. rank0_print(*args)"></a>1. rank0_print(*args)</h3><h4 id="Code-3"><a href="#Code-3" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">local_rank = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">rank0_print</span>(<span class="params">*args</span>):</span><br><span class="line">    <span class="keyword">if</span> local_rank == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(*args)</span><br></pre></td></tr></tbody></table></figure>

<h4 id="Explanation-3"><a href="#Explanation-3" class="headerlink" title="Explanation"></a>Explanation</h4><p>定义一个全局变量 local_rank，用于分布式训练。<br>定义一个函数 rank0_print，只在 local_rank 为 0 时打印信息，用于分布式训练中的信息输出控制。这样可以避免在多个节点上重复打印相同的信息，使得输出更加清晰和简洁。</p>
<ul>
<li>用于只在分布式训练环境中的主节点（rank 0）上打印信息。</li>
<li>参数：可变数量的参数，用于打印。</li>
</ul>
<h3 id="2-trainer-save-model-safe-trainer-transformers-Trainer"><a href="#2-trainer-save-model-safe-trainer-transformers-Trainer" class="headerlink" title="2. trainer_save_model_safe(trainer: transformers.Trainer)"></a>2. <code>trainer_save_model_safe(trainer: transformers.Trainer)</code></h3><h4 id="Code-4"><a href="#Code-4" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">trainer_save_model_safe</span>(<span class="params">trainer: transformers.Trainer</span>):</span><br><span class="line">    <span class="keyword">from</span> torch.distributed.fsdp <span class="keyword">import</span> FullyShardedDataParallel <span class="keyword">as</span> FSDP</span><br><span class="line">    <span class="keyword">from</span> torch.distributed.fsdp <span class="keyword">import</span> StateDictType, FullStateDictConfig</span><br><span class="line"></span><br><span class="line">    save_policy = FullStateDictConfig(offload_to_cpu=<span class="literal">True</span>, rank0_only=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">with</span> FSDP.state_dict_type(</span><br><span class="line">        trainer.model, StateDictType.FULL_STATE_DICT, save_policy</span><br><span class="line">    ):</span><br><span class="line">        trainer.save_model()</span><br></pre></td></tr></tbody></table></figure>

<p>函数 <code>trainer_save_model_safe(trainer: transformers.Trainer)</code> 旨在安全地保存使用 PyTorch 分布式框架训练的模型。让我们详细了解此函数及其涉及的关键组件。</p>
<h4 id="Explanation-4"><a href="#Explanation-4" class="headerlink" title="Explanation"></a>Explanation</h4><ol>
<li>参数：</li>
</ol>
<ul>
<li><code>trainer</code>: <code>transformers.Trainer</code> 的实例。这个类是 Hugging Face Transformers 库的核心组件之一，用于训练和评估模型。</li>
</ul>
<ol start="2">
<li>功能：</li>
</ol>
<ul>
<li>此函数的主要目的是在分布式训练环境中安全地保存模型。它特别考虑了使用 <code>FullyShardedDataParallel</code> (FSDP) 进行训练时的模型保存策略。</li>
</ul>
<ol start="3">
<li>FSDP</li>
</ol>
<ul>
<li><strong>FullyShardedDataParallel (FSDP)</strong><ul>
<li>这是 PyTorch 分布式训练框架的一个组件。FSDP 通过将模型参数分片到多个 GPU 上来减少每个 GPU 的内存占用，从而实现更大模型的训练。</li>
<li>在此场景中，FSDP 主要用于处理和保存分布式训练中的模型状态。</li>
</ul>
</li>
<li><strong>StateDictType</strong><ul>
<li>这是一个枚举类型，定义了如何保存模型的状态字典（state dict）。在 FSDP 环境中，保存和加载模型状态可能需要特殊的处理。</li>
</ul>
</li>
<li><strong>FullStateDictConfig</strong><ul>
<li>这个类用于配置保存完整状态字典时的参数。它是 FSDP 功能的一部分，用于控制如何保存模型状态。</li>
</ul>
</li>
</ul>
<ol start="4">
<li>函数实现</li>
</ol>
<ul>
<li><strong>设置保存策略</strong><ul>
<li><code>save_policy = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)</code> 创建了一个保存策略。这里指定两个关键参数：<ul>
<li><code>offload_to_cpu</code>: 在保存状态字典之前，将模型参数卸载到 CPU，这有助于减少 GPU 内存的使用。</li>
<li><code>rank0_only</code>: 只在 rank 0（通常是主节点）上保存模型。在分布式训练中，这可以避免每个节点都保存相同的模型副本，节省存储空间。</li>
</ul>
</li>
</ul>
</li>
<li><strong>保存模型</strong><ul>
<li>使用 <code>with FSDP.state_dict_type(trainer.model, StateDictType.FULL_STATE_DICT, save_policy)</code> 上下文管理器设置模型保存的状态字典类型和策略。</li>
<li>在这个上下文内，调用 <code>trainer.save_model()</code> 来保存模型。由于使用了 <code>save_policy</code>，模型将根据上述配置安全地保存。</li>
</ul>
</li>
</ul>
<p>函数 <code>trainer_save_model_safe</code> 封装了一个安全的模型保存逻辑，特别是针对使用 PyTorch 的 FSDP 进行分布式训练的场景。它确保了只在一个节点上保存完整的模型状态，并且在保存之前将模型参数转移到 CPU，从而优化内存使用和存储效率。这对于训练大型模型和管理大规模分布式训练环境至关重要。</p>
<h3 id="3-preprocess-sources-tokenizer-transformers-PreTrainedTokenizer-gt-Dict"><a href="#3-preprocess-sources-tokenizer-transformers-PreTrainedTokenizer-gt-Dict" class="headerlink" title="3.preprocess(sources,tokenizer: transformers.PreTrainedTokenizer) -> Dict"></a>3.<code>preprocess(sources,tokenizer: transformers.PreTrainedTokenizer) -&gt; Dict</code></h3><h4 id="Code-5"><a href="#Code-5" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess</span>(<span class="params"></span></span><br><span class="line"><span class="params">    sources,</span></span><br><span class="line"><span class="params">    tokenizer: transformers.PreTrainedTokenizer,</span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="type">Dict</span>:</span><br><span class="line">    conv = get_conversation_template(<span class="string">"vicuna"</span>)</span><br><span class="line">    roles = {<span class="string">"human"</span>: conv.roles[<span class="number">0</span>], <span class="string">"gpt"</span>: conv.roles[<span class="number">1</span>]}</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Apply prompt templates</span></span><br><span class="line">    conversations = []</span><br><span class="line">    <span class="keyword">for</span> i, source <span class="keyword">in</span> <span class="built_in">enumerate</span>(sources):</span><br><span class="line">        <span class="keyword">if</span> roles[source[<span class="number">0</span>][<span class="string">"from"</span>]] != conv.roles[<span class="number">0</span>]:</span><br><span class="line">            <span class="comment"># Skip the first one if it is not from human</span></span><br><span class="line">            source = source[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">        conv.messages = []</span><br><span class="line">        <span class="keyword">for</span> j, sentence <span class="keyword">in</span> <span class="built_in">enumerate</span>(source):</span><br><span class="line">            role = roles[sentence[<span class="string">"from"</span>]]</span><br><span class="line">            <span class="keyword">assert</span> role == conv.roles[j % <span class="number">2</span>], <span class="string">f"<span class="subst">{i}</span>"</span></span><br><span class="line">            conv.append_message(role, sentence[<span class="string">"value"</span>])</span><br><span class="line">        conversations.append(conv.get_prompt())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Tokenize conversations</span></span><br><span class="line">    input_ids = tokenizer(</span><br><span class="line">        conversations,</span><br><span class="line">        return_tensors=<span class="string">"pt"</span>,</span><br><span class="line">        padding=<span class="string">"max_length"</span>,</span><br><span class="line">        max_length=tokenizer.model_max_length,</span><br><span class="line">        truncation=<span class="literal">True</span>,</span><br><span class="line">    ).input_ids</span><br><span class="line">    targets = input_ids.clone()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> conv.sep_style == SeparatorStyle.ADD_COLON_TWO</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Mask targets. Only compute loss on the assistant outputs.</span></span><br><span class="line">    sep = conv.sep + conv.roles[<span class="number">1</span>] + <span class="string">": "</span></span><br><span class="line">    <span class="keyword">for</span> conversation, target <span class="keyword">in</span> <span class="built_in">zip</span>(conversations, targets):</span><br><span class="line">        total_len = <span class="built_in">int</span>(target.ne(tokenizer.pad_token_id).<span class="built_in">sum</span>())</span><br><span class="line"></span><br><span class="line">        turns = conversation.split(conv.sep2)</span><br><span class="line">        cur_len = <span class="number">1</span></span><br><span class="line">        target[:cur_len] = IGNORE_TOKEN_ID</span><br><span class="line">        <span class="keyword">for</span> i, turn <span class="keyword">in</span> <span class="built_in">enumerate</span>(turns):</span><br><span class="line">            <span class="keyword">if</span> turn == <span class="string">""</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            turn_len = <span class="built_in">len</span>(tokenizer(turn).input_ids)</span><br><span class="line"></span><br><span class="line">            parts = turn.split(sep)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(parts) != <span class="number">2</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            parts[<span class="number">0</span>] += sep</span><br><span class="line">            <span class="comment"># "-2" is hardcoded for the Llama tokenizer to make the offset correct.</span></span><br><span class="line">            instruction_len = <span class="built_in">len</span>(tokenizer(parts[<span class="number">0</span>]).input_ids) - <span class="number">2</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i != <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> tokenizer.legacy:</span><br><span class="line">                <span class="comment"># The legacy and non-legacy modes handle special tokens differently</span></span><br><span class="line">                instruction_len -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Ignore the user instructions</span></span><br><span class="line">            target[cur_len : cur_len + instruction_len] = IGNORE_TOKEN_ID</span><br><span class="line">            cur_len += turn_len</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i != <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> tokenizer.legacy:</span><br><span class="line">                <span class="comment"># The legacy and non-legacy modes handle special tokens differently</span></span><br><span class="line">                cur_len -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        target[cur_len:] = IGNORE_TOKEN_ID</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="literal">False</span>:  <span class="comment"># Inspect and check the correctness of masking</span></span><br><span class="line">            z = target.clone()</span><br><span class="line">            z = torch.where(z == IGNORE_TOKEN_ID, tokenizer.unk_token_id, z)</span><br><span class="line">            rank0_print(tokenizer.decode(z))</span><br><span class="line">            exit()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> cur_len &lt; tokenizer.model_max_length:</span><br><span class="line">            <span class="keyword">if</span> cur_len != total_len:</span><br><span class="line">                target[:] = IGNORE_TOKEN_ID</span><br><span class="line">                rank0_print(</span><br><span class="line">                    <span class="string">f"WARNING: tokenization mismatch: <span class="subst">{cur_len}</span> vs. <span class="subst">{total_len}</span>."</span></span><br><span class="line">                    <span class="string">f" #turn = <span class="subst">{<span class="built_in">len</span>(turns) - <span class="number">1</span>}</span>. (ignored)"</span></span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">dict</span>(</span><br><span class="line">        input_ids=input_ids,</span><br><span class="line">        labels=targets,</span><br><span class="line">        attention_mask=input_ids.ne(tokenizer.pad_token_id),</span><br><span class="line">    )</span><br></pre></td></tr></tbody></table></figure>

<p>函数 <code>preprocess(sources, tokenizer: transformers.PreTrainedTokenizer) -&gt; Dict</code> 用于预处理对话数据，使其适用于机器学习模型的训练。这个函数可以分为几个主要部分进行详细介绍：</p>
<h4 id="1-获取对话模板和角色定义"><a href="#1-获取对话模板和角色定义" class="headerlink" title="1. 获取对话模板和角色定义"></a>1. 获取对话模板和角色定义</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conv = get_conversation_template(<span class="string">"vicuna"</span>)</span><br><span class="line">roles = {<span class="string">"human"</span>: conv.roles[<span class="number">0</span>], <span class="string">"gpt"</span>: conv.roles[<span class="number">1</span>]}</span><br></pre></td></tr></tbody></table></figure>

<ul>
<li><strong>功能</strong>: 初始化对话模板和定义对话参与者的角色。</li>
<li><strong>实现</strong>:<ul>
<li><code>conv = get_conversation_template("vicuna")</code> 获取指定模型（如 “vicuna”）的对话模板。</li>
<li><code>roles</code> 字典将 “human” 和 “gpt” 分别映射到对话模板中定义的角色。</li>
</ul>
</li>
<li><strong>示例</strong>:<ul>
<li>如果对话模板是 “vicuna”，则 <code>roles</code> 可能是 <code>{"human": "user", "gpt": "assistant"}</code>。<ul>
<li><code>conv = get_conversation_template("vicuna")</code> 得到的模板如下：<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Conversation(name=<span class="string">'vicuna_v1.1'</span>, system_template=<span class="string">'{system_message}'</span>, system_message=<span class="string">"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."</span>, roles=(<span class="string">'USER'</span>, <span class="string">'ASSISTANT'</span>), messages=[], offset=<span class="number">0</span>, sep_style=&lt;SeparatorStyle.ADD_COLON_TWO: <span class="number">2</span>&gt;, sep=<span class="string">' '</span>, sep2=<span class="string">'&lt;/s&gt;'</span>, stop_str=<span class="literal">None</span>, stop_token_ids=<span class="literal">None</span>)</span><br></pre></td></tr></tbody></table></figure></li>
<li><code>roles</code> 将 “human” 映射到 “USER”，将 “gpt” 映射到 “ASSISTANT”。<code>{'human': 'USER', 'gpt': 'ASSISTANT'}</code></li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="2-prompt-模板"><a href="#2-prompt-模板" class="headerlink" title="2. prompt 模板"></a>2. prompt 模板</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Apply prompt templates</span></span><br><span class="line">conversations = []</span><br><span class="line"><span class="keyword">for</span> i, source <span class="keyword">in</span> <span class="built_in">enumerate</span>(sources):</span><br><span class="line">    <span class="keyword">if</span> roles[source[<span class="number">0</span>][<span class="string">"from"</span>]] != conv.roles[<span class="number">0</span>]:</span><br><span class="line">        <span class="comment"># Skip the first one if it is not from human</span></span><br><span class="line">        source = source[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">    conv.messages = []</span><br><span class="line">    <span class="keyword">for</span> j, sentence <span class="keyword">in</span> <span class="built_in">enumerate</span>(source):</span><br><span class="line">        role = roles[sentence[<span class="string">"from"</span>]]</span><br><span class="line">        <span class="keyword">assert</span> role == conv.roles[j % <span class="number">2</span>], <span class="string">f"<span class="subst">{i}</span>"</span></span><br><span class="line">        conv.append_message(role, sentence[<span class="string">"value"</span>])</span><br><span class="line">    conversations.append(conv.get_prompt())</span><br></pre></td></tr></tbody></table></figure>

<ul>
<li><p><strong>功能</strong>: 为源数据应用提示模板，构建对话。</p>
</li>
<li><p><strong>实现</strong>:</p>
<ul>
<li>遍历 <code>sources</code>（原始对话数据），将每个对话源转换为模板格式的对话。</li>
<li>如果对话的第一部分不是 “human” 角色发起，则跳过该部分。</li>
<li>为每个句子指定角色，并将其添加到对话模板中。</li>
<li>最终，每个处理后的对话被添加到 <code>conversations</code> 列表中。</li>
</ul>
</li>
<li><p><strong>示例</strong>:</p>
<ul>
<li>假如我们的 source 是 dummy input 中的第一条数据:<br><code>python source = [{'from': 'human', 'value': 'Who are you?'}, {'from': 'gpt', 'value': 'I am Vicuna, a language model trained by researchers from Large Model Systems Organization (LMSYS).'}, {'from': 'human', 'value': 'Have a nice day!'}, {'from': 'gpt', 'value': 'You too!'}]</code></li>
<li><code>conversations</code> 在 Vicuna template 下,我们会使用<code>SeparatorStyle.ADD_COLON_TWO</code>作为分隔符风格，构成的数据可能是 [“A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user’s questions. USER: Who are you? ASSISTANT: I am Vicuna, a language model trained by researchers from Large Model Systems Organization (LMSYS).USER: Have a nice day! ASSISTANT: You too!“]</li>
<li><details>
<summary> get_prompt的实现 </summary>
`get_prompt` 方法的实现根据不同的 `SeparatorStyle` 有着不同的行为。下面是一个表格，详细介绍了各种风格的 `get_prompt` 方法，以及对应的英文示例：

<table>
<thead>
<tr>
<th>分隔符风格 (<code>SeparatorStyle</code>)</th>
<th>描述</th>
<th>示例</th>
</tr>
</thead>
<tbody><tr>
<td><code>ADD_COLON_SINGLE</code></td>
<td>在每个消息后加冒号和分隔符。</td>
<td>USER: Hello there!\nASSISTANT: Hi, how can I help?\n</td>
</tr>
<tr>
<td><code>ADD_COLON_TWO</code></td>
<td>使用两种分隔符交替，通常在不同角色之间切换。</td>
<td>USER: What’s the weather?\nASSISTANT: It’s sunny today.\n\n</td>
</tr>
<tr>
<td><code>ADD_COLON_SPACE_SINGLE</code></td>
<td>消息后加冒号、空格和分隔符。</td>
<td>USER: Can you book a flight?\nASSISTANT: Sure, where to?\n</td>
</tr>
<tr>
<td><code>NO_COLON_SINGLE</code></td>
<td>消息直接跟在角色后，不加冒号，后接分隔符。</td>
<td>USERWhat are you doing?\nASSISTANTI’m here to assist you.\n</td>
</tr>
<tr>
<td><code>NO_COLON_TWO</code></td>
<td>无冒号，使用两种分隔符交替。</td>
<td>USERHow’s the project going?\nASSISTANTIt’s on track.\n\n</td>
</tr>
<tr>
<td><code>ADD_NEW_LINE_SINGLE</code></td>
<td>每条消息前换行，消息后加分隔符。</td>
<td>USER\nHow can I reset my password?\nASSISTANT\nYou can reset it via email.\n</td>
</tr>
<tr>
<td><code>RWKV</code></td>
<td>特殊格式，通常用于特定模型。</td>
<td>USER: What is AI?\n\nASSISTANT: AI stands for Artificial Intelligence.\n\n</td>
</tr>
<tr>
<td><code>LLAMA2</code></td>
<td>特殊标签格式，针对特定模型。</td>
<td>[INST] USER How does blockchain work?\nASSISTANT It is a distributed ledger.\n\n</td>
</tr>
<tr>
<td><code>CHATGLM</code></td>
<td>特定于 <code>CHATGLM</code> 模型的格式。</td>
<td>[Round 1]\nUSER: Tell me a joke.\nASSISTANT: Why did the chicken cross the road?\n</td>
</tr>
<tr>
<td><code>CHATML</code></td>
<td>类似 <code>CHATGLM</code>，但每条消息前后都有换行。</td>
<td>USER\nDo you like music?\n\nASSISTANT\nYes, I enjoy many genres.\n\n</td>
</tr>
<tr>
<td><code>CHATGLM3</code></td>
<td>适用于 <code>CHATGLM3</code> 模型的格式。</td>
<td>USER\nCan you play chess?\nASSISTANTYes, I can play.\n</td>
</tr>
<tr>
<td><code>CHATINTERN</code></td>
<td>适用于 <code>CHATINTERN</code> 模型的格式，使用特殊标记。</td>
<td><s>USER:Where is the nearest ATM?<s>\nASSISTANT:It’s next to the post office.\n</s></s></td>
</tr>
<tr>
<td><code>DOLLY</code></td>
<td>特定于 <code>DOLLY</code> 模型的格式。</td>
<td>USER:\nWhat is quantum computing?\nASSISTANT:\nIt involves computation using quantum-mechanical phenomena.\n\n</td>
</tr>
<tr>
<td><code>PHOENIX</code></td>
<td>适用于 <code>PHOENIX</code> 模型，消息被特殊标记包裹。</td>
<td>USER: <s>How to bake a cake?</s>\nASSISTANT: <s>You need flour, sugar, and eggs.</s>\n</td>
</tr>
<tr>
<td><code>ROBIN</code></td>
<td>类似 <code>ADD_NEW_LINE_SINGLE</code>，但角色后有换行。</td>
<td>USER:\nIs AI dangerous?\nASSISTANT:\nIt depends on how it’s used.\n</td>
</tr>
<tr>
<td><code>FALCON_CHAT</code></td>
<td>类似 <code>ADD_COLON_SINGLE</code>，但可适用于 <code>FALCON</code> 模型。</td>
<td>USER: What is the capital of France?\nASSISTANT: It’s Paris.\n</td>
</tr>
<tr>
<td><code>METAMATH</code></td>
<td>对话中使用特殊前缀和后缀，适用于 <code>METAMATH</code> 模型。</td>
<td>USER:\nWhat is 2+2?\n: It’s 4\n</td>
</tr>
<tr>
<td><code>DEEPSEEK_CHAT</code></td>
<td>适用于 <code>DEEPSEEK</code> 模型的特定格式。</td>
<td>USER: What’s your favorite color?\nASSISTANT: I like blue.\n\n</td>
</tr>
<tr>
<td><code>YUAN2</code></td>
<td>适用于 <code>YUAN2</code> 模型，特殊的分隔符应用。</td>
<td>How are you today?<n>I’m fine, thank you!<n></n></n></td>
</tr>
</tbody></table>
每种风格都有其特定的格式，这在处理与不同模型或任务相关的对话数据时非常重要。通过 <code>get_prompt</code> 方法的不同实现，可以灵活地适应各种需求，使对话生成或处理更加准确和高效。</details></li>
</ul>
</li>
</ul>
<h4 id="3-对话的分词"><a href="#3-对话的分词" class="headerlink" title="3. 对话的分词"></a>3. 对话的分词</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Tokenize conversations</span></span><br><span class="line">input_ids = tokenizer(</span><br><span class="line">    conversations,</span><br><span class="line">    return_tensors=<span class="string">"pt"</span>,</span><br><span class="line">    padding=<span class="string">"max_length"</span>,</span><br><span class="line">    max_length=tokenizer.model_max_length,</span><br><span class="line">    truncation=<span class="literal">True</span>,</span><br><span class="line">).input_ids</span><br><span class="line">targets = input_ids.clone()</span><br></pre></td></tr></tbody></table></figure>

<ul>
<li><p><strong>功能</strong>: 文本对话首先被分词处理，转换成模型能够处理的数值序列。然后，这些序列被克隆以形成初始的训练目标。这样做的目的是为了在训练过程中提供一个基准，指导模型学习生成正确的输出。在后续步骤中，这些目标可能会根据特定的训练目标进行调整。</p>
</li>
<li><p><strong>实现</strong>：</p>
<ul>
<li><code>tokenizer</code> 函数接收文本列表（这里是 <code>conversations</code>），并返回一个包含数值化表示的 <code>input_ids</code>。<ul>
<li><code>return_tensors="pt"</code> 指定返回的数据类型为 PyTorch 张量。</li>
<li><code>padding="max_length"</code> 和 <code>max_length=tokenizer.model_max_length</code> 确保所有输入长度统一，不足的部分使用填充。</li>
<li><code>truncation=True</code> 表示如果输入过长，将其截断到最大长度。</li>
</ul>
</li>
<li>在训练期间，模型需要知道期望的输出以计算损失和进行反向传播。这些期望的输出被称为 “targets”。<code>targets = input_ids.clone()</code> 表示创建 <code>input_ids</code> 的一个副本作为初始的目标。<ul>
<li>之所以需要克隆 <code>input_ids</code>，是因为在许多语言模型训练任务中（特别是像自回归模型这样的生成任务），模型的目标输出往往与输入非常相似，但在某些细节上存在差异。</li>
<li>在后续步骤中，这个 <code>targets</code> 可能会根据特定的训练需求进一步修改或掩码（例如，在对话任务中，可能只对模型生成的回复部分计算损失，而不是整个对话）。</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="4-目标掩码"><a href="#4-目标掩码" class="headerlink" title="4. 目标掩码"></a>4. 目标掩码</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Mask targets. Only compute loss on the assistant outputs.</span></span><br><span class="line">sep = conv.sep + conv.roles[<span class="number">1</span>] + <span class="string">": "</span></span><br><span class="line"><span class="keyword">for</span> conversation, target <span class="keyword">in</span> <span class="built_in">zip</span>(conversations, targets):</span><br><span class="line">    total_len = <span class="built_in">int</span>(target.ne(tokenizer.pad_token_id).<span class="built_in">sum</span>())</span><br><span class="line"></span><br><span class="line">    turns = conversation.split(conv.sep2)</span><br><span class="line">    cur_len = <span class="number">1</span></span><br><span class="line">    target[:cur_len] = IGNORE_TOKEN_ID</span><br><span class="line">    <span class="keyword">for</span> i, turn <span class="keyword">in</span> <span class="built_in">enumerate</span>(turns):</span><br><span class="line">        <span class="keyword">if</span> turn == <span class="string">""</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        turn_len = <span class="built_in">len</span>(tokenizer(turn).input_ids)</span><br><span class="line"></span><br><span class="line">        parts = turn.split(sep)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(parts) != <span class="number">2</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        parts[<span class="number">0</span>] += sep</span><br><span class="line">        <span class="comment"># "-2" is hardcoded for the Llama tokenizer to make the offset correct.</span></span><br><span class="line">        instruction_len = <span class="built_in">len</span>(tokenizer(parts[<span class="number">0</span>]).input_ids) - <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> i != <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> tokenizer.legacy:</span><br><span class="line">            <span class="comment"># The legacy and non-legacy modes handle special tokens differently</span></span><br><span class="line">            instruction_len -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Ignore the user instructions</span></span><br><span class="line">        target[cur_len : cur_len + instruction_len] = IGNORE_TOKEN_ID</span><br><span class="line">        cur_len += turn_len</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> i != <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> tokenizer.legacy:</span><br><span class="line">            <span class="comment"># The legacy and non-legacy modes handle special tokens differently</span></span><br><span class="line">            cur_len -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    target[cur_len:] = IGNORE_TOKEN_ID</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="literal">False</span>:  <span class="comment"># Inspect and check the correctness of masking</span></span><br><span class="line">        z = target.clone()</span><br><span class="line">        z = torch.where(z == IGNORE_TOKEN_ID, tokenizer.unk_token_id, z)</span><br><span class="line">        rank0_print(tokenizer.decode(z))</span><br><span class="line">        exit()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> cur_len &lt; tokenizer.model_max_length:</span><br><span class="line">        <span class="keyword">if</span> cur_len != total_len:</span><br><span class="line">            target[:] = IGNORE_TOKEN_ID</span><br><span class="line">            rank0_print(</span><br><span class="line">                <span class="string">f"WARNING: tokenization mismatch: <span class="subst">{cur_len}</span> vs. <span class="subst">{total_len}</span>."</span></span><br><span class="line">                <span class="string">f" #turn = <span class="subst">{<span class="built_in">len</span>(turns) - <span class="number">1</span>}</span>. (ignored)"</span></span><br><span class="line">            )</span><br></pre></td></tr></tbody></table></figure>

<ul>
<li><p><strong>功能</strong>: 对目标输出进行掩码处理，以便模型只对特定输出计算损失。目标是对生成的 targets（即模型的输出标签）进行掩码处理。这是为了确保在训练过程中只对助手（assistant）的输出计算损失，而不是整个对话。</p>
</li>
<li><p><strong>实现</strong>：</p>
<ul>
<li><code>sep = conv.sep + conv.roles[1] + ": "</code> 定义了用于识别助手回复的分隔符。在这个例子中，<code>sep</code> 可能是 “\n\nAssistant: “。</li>
<li>循环遍历每个处理后的对话 (<code>conversation</code>) 及其对应的目标 (<code>target</code>)。</li>
<li><code>total_len</code> 是当前目标序列中非填充（padding）部分的长度。</li>
<li><code>turns</code> 是将对话根据 <code>conv.sep2</code> 分隔成不同轮次的列表。</li>
</ul>
</li>
</ul>
<ol>
<li>对每个轮次进行处理</li>
</ol>
<ul>
<li>每个轮次（turn）包含用户和助手的消息。</li>
<li>使用 <code>tokenizer(turn)</code> 将每个轮次的文本转换为模型能理解的 ID 序列。</li>
<li>通过 <code>parts = turn.split(sep)</code> 分离用户和助手的消息。</li>
<li><code>instruction_len</code> 是用户消息部分的长度（在某些情况下需要调整，比如 <code>-2</code> 是为了适应特定的分词器）。</li>
</ul>
<ol start="2">
<li>掩码目标</li>
</ol>
<ul>
<li><code>target[cur_len : cur_len + instruction_len] = IGNORE_TOKEN_ID</code> 将用户消息部分的目标 ID 替换为 <code>IGNORE_TOKEN_ID</code>，这意味着在计算损失时会忽略这部分。</li>
<li><code>cur_len</code> 用于跟踪当前处理到的位置。</li>
<li>每处理完一个轮次，更新 <code>cur_len</code>。</li>
</ul>
<ol start="3">
<li>最终处理</li>
</ol>
<ul>
<li><code>target[cur_len:] = IGNORE_TOKEN_ID</code> 确保在最后一个轮次之后的所有内容都被忽略。</li>
<li>如果 <code>cur_len</code> 小于 <code>tokenizer.model_max_length</code>，但不等于 <code>total_len</code>，则表示有不一致性，此时会发出警告，并将整个目标序列设置为 <code>IGNORE_TOKEN_ID</code>。</li>
</ul>
<h4 id="5-返回处理后的数据"><a href="#5-返回处理后的数据" class="headerlink" title="5. 返回处理后的数据"></a>5. 返回处理后的数据</h4><ul>
<li><strong>功能</strong>: 返回预处理后的数据，包括输入 ID、目标标签和注意力掩码。</li>
<li><strong>实现</strong>:<ul>
<li>返回一个字典，包含 <code>input_ids</code>（模型输入）、<code>labels</code>（训练目标）和 <code>attention_mask</code>（指示哪些部分是有效输入的掩码）。</li>
</ul>
</li>
</ul>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>这个 <code>preprocess</code> 函数通过将原始文本数据转换为模型可以理解的格式，为训练准备数据。它涵盖了从文本处理到分词，再到目标掩码的整个预处理流程。这个过程对于任何基于对话的自然语言处理任务至关重要，特别是在需要模型专注于对特定部分的响应时。</p>
<h2 id="4-数据集类"><a href="#4-数据集类" class="headerlink" title="4. 数据集类"></a>4. 数据集类</h2><p>这些类继承自 PyTorch 的 <code>Dataset</code> 类，并且是为特定的数据处理任务定制的。这些类包含具体的方法来处理和准备数据，以便用于模型训练。这些类包括：</p>
<ul>
<li><code>SupervisedDataset</code>: 用于有监督学习的数据集。它处理原始数据，将其转换为适合模型训练的格式。</li>
<li><code>LazySupervisedDataset</code>: 类似于 <code>SupervisedDataset</code>，但使用懒加载方式处理数据。这意味着数据只在需要时才被加载和处理，这对于处理大型数据集特别有用。</li>
</ul>
<p>这些类通常包含 <code>__init__</code>, <code>__len__</code>, 和 <code>__getitem__</code> 方法，分别用于初始化数据集、获取数据集大小和检索特定索引的数据。这样的设计模式使得数据集可以轻松地与 PyTorch 的 DataLoader 配合使用，从而实现高效的数据加载和批处理。</p>
<h3 id="1-SupervisedDataset-类"><a href="#1-SupervisedDataset-类" class="headerlink" title="1. SupervisedDataset 类"></a>1. SupervisedDataset 类</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SupervisedDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="string">"""Dataset for supervised fine-tuning."""</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, raw_data, tokenizer: transformers.PreTrainedTokenizer</span>):</span><br><span class="line">        <span class="built_in">super</span>(SupervisedDataset, self).__init__()</span><br><span class="line"></span><br><span class="line">        rank0_print(<span class="string">"Formatting inputs..."</span>)</span><br><span class="line">        sources = [example[<span class="string">"conversations"</span>] <span class="keyword">for</span> example <span class="keyword">in</span> raw_data]</span><br><span class="line">        data_dict = preprocess(sources, tokenizer)</span><br><span class="line"></span><br><span class="line">        self.input_ids = data_dict[<span class="string">"input_ids"</span>]</span><br><span class="line">        self.labels = data_dict[<span class="string">"labels"</span>]</span><br><span class="line">        self.attention_mask = data_dict[<span class="string">"attention_mask"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.input_ids)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, i</span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, torch.Tensor]:</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">dict</span>(</span><br><span class="line">            input_ids=self.input_ids[i],</span><br><span class="line">            labels=self.labels[i],</span><br><span class="line">            attention_mask=self.attention_mask[i],</span><br><span class="line">        )</span><br></pre></td></tr></tbody></table></figure>

<p><code>SupervisedDataset</code> 类是一个用于有监督学习的数据集类，特别是为了微调（fine-tuning）任务设计。这个类继承自 PyTorch 的 <code>Dataset</code> 类，并重写了其方法以适应特定的数据处理需求。下面是对这个类的详细介绍：<code>SupervisedDataset</code> 类提供了一种结构化和高效的方法来处理和加载用于有监督学习的对话数据。它遵循 PyTorch 数据集（<code>Dataset</code>）的标准结构，使得与 PyTorch 的数据加载器（<code>DataLoader</code>）等其他组件兼容，从而方便在训练循环中使用。通过预处理步骤，该类确保数据以适当的格式提供给模型，以便进行有效的训练。</p>
<ul>
<li><strong>类名</strong>: <code>SupervisedDataset</code></li>
<li><strong>继承</strong>: <code>Dataset</code>（来自 PyTorch）</li>
<li><strong>目的</strong>: 用于有监督的模型微调任务。</li>
</ul>
<h4 id="1-1-init-self-raw-data-tokenizer-transformers-PreTrainedTokenizer"><a href="#1-1-init-self-raw-data-tokenizer-transformers-PreTrainedTokenizer" class="headerlink" title="1.1 __init__(self, raw_data, tokenizer: transformers.PreTrainedTokenizer)"></a>1.1 <code>__init__(self, raw_data, tokenizer: transformers.PreTrainedTokenizer)</code></h4><ul>
<li><strong>调用时机</strong>：创建 <code>SupervisedDataset</code> 类的实例时。这通常发生在准备训练数据集的阶段，当你创建数据加载器（DataLoader）之前。</li>
<li><strong>功能</strong>：初始化数据集实例，处理原始对话数据，并将其转换为模型可以理解的格式。</li>
<li><strong>参数</strong>：<ul>
<li><code>raw_data</code>：包含对话数据的列表或类似结构。</li>
<li><code>tokenizer</code>：一个预训练的分词器实例，用于将文本转换为模型可以处理的格式。</li>
</ul>
</li>
<li><strong>返回值</strong>：无返回值，但此方法会设置 <code>input_ids</code>、<code>labels</code> 和 <code>attention_mask</code> 作为类的内部状态。</li>
<li><strong>实现细节</strong>:<ul>
<li>使用列表推导式从 <code>raw_data</code> 中提取每个样本的对话内容。</li>
<li>调用 <code>preprocess</code> 函数处理这些对话，将其转换为适合模型输入的格式。</li>
<li>从返回的 <code>data_dict</code> 中提取 <code>input_ids</code>（模型输入 ID）、<code>labels</code>（目标标签）和 <code>attention_mask</code>（注意力掩码）。</li>
</ul>
</li>
</ul>
<h4 id="1-2-len-self"><a href="#1-2-len-self" class="headerlink" title="1.2 __len__(self)"></a>1.2 <code>__len__(self)</code></h4><ul>
<li><strong>调用时机</strong>：当需要获取数据集大小时，例如在设置数据加载器时，或者在训练循环中迭代数据集时。</li>
<li><strong>功能</strong>：返回数据集中的样本数量。</li>
<li><strong>返回值</strong>：一个整数，表示数据集中的样本数量。</li>
<li><strong>实现</strong>: 直接返回 <code>input_ids</code> 的长度，即样本的数量。</li>
</ul>
<h4 id="1-3-getitem-self-i"><a href="#1-3-getitem-self-i" class="headerlink" title="1.3 __getitem__(self, i)"></a>1.3 <code>__getitem__(self, i)</code></h4><ul>
<li><strong>调用时机</strong>：在数据加载器请求数据集的特定样本时，这通常发生在训练或评估循环的每个迭代中。</li>
<li><strong>功能</strong>：获取指定索引 <code>i</code> 处的数据样本。</li>
<li><strong>参数</strong>：<ul>
<li><code>i</code>：所请求样本的索引。</li>
</ul>
</li>
<li><strong>返回值</strong>：一个字典，包含索引 <code>i</code> 处样本的 <code>input_ids</code>、<code>labels</code> 和 <code>attention_mask</code>。这些是 PyTorch 张量（<code>torch.Tensor</code>），适用于模型的训练或评估。</li>
</ul>
<p>在有监督学习的场景中，<code>SupervisedDataset</code> 类扮演着数据预处理和封装的角色，确保数据以正确的格式提供给模型。<code>__init__</code> 方法在数据集实例化时调用，负责数据的初始化和预处理。<code>__len__</code> 和 <code>__getitem__</code> 方法则在训练和评估过程中被频繁调用，分别用于获取数据集的大小和提取特定的数据样本。这些方法的设计和实现使得 <code>SupervisedDataset</code> 类可以无缝地与 PyTorch 的其他数据处理和训练工具集成。</p>
<h3 id="2-LazySupervisedDataset-类"><a href="#2-LazySupervisedDataset-类" class="headerlink" title="2. LazySupervisedDataset 类"></a>2. LazySupervisedDataset 类</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LazySupervisedDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="string">"""Dataset for supervised fine-tuning."""</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, raw_data, tokenizer: transformers.PreTrainedTokenizer</span>):</span><br><span class="line">        <span class="built_in">super</span>(LazySupervisedDataset, self).__init__()</span><br><span class="line">        self.tokenizer = tokenizer</span><br><span class="line"></span><br><span class="line">        rank0_print(<span class="string">"Formatting inputs...Skip in lazy mode"</span>)</span><br><span class="line">        self.tokenizer = tokenizer</span><br><span class="line">        self.raw_data = raw_data</span><br><span class="line">        self.cached_data_dict = {}</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.raw_data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, i</span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, torch.Tensor]:</span><br><span class="line">        <span class="keyword">if</span> i <span class="keyword">in</span> self.cached_data_dict:</span><br><span class="line">            <span class="keyword">return</span> self.cached_data_dict[i]</span><br><span class="line"></span><br><span class="line">        ret = preprocess([self.raw_data[i][<span class="string">"conversations"</span>]], self.tokenizer)</span><br><span class="line">        ret = <span class="built_in">dict</span>(</span><br><span class="line">            input_ids=ret[<span class="string">"input_ids"</span>][<span class="number">0</span>],</span><br><span class="line">            labels=ret[<span class="string">"labels"</span>][<span class="number">0</span>],</span><br><span class="line">            attention_mask=ret[<span class="string">"attention_mask"</span>][<span class="number">0</span>],</span><br><span class="line">        )</span><br><span class="line">        self.cached_data_dict[i] = ret</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ret</span><br></pre></td></tr></tbody></table></figure>

<p><code>LazySupervisedDataset</code> 类是另一种数据集实现，用于有监督的模型微调。与 <code>SupervisedDataset</code> 相比，它采用了一种“懒加载”（lazy loading）的策略。以下是对该类的详细解释，以及它与非懒加载版本的比较。</p>
<h4 id="2-1-init-self-raw-data-tokenizer-transformers-PreTrainedTokenizer"><a href="#2-1-init-self-raw-data-tokenizer-transformers-PreTrainedTokenizer" class="headerlink" title="2.1 __init__(self, raw_data, tokenizer: transformers.PreTrainedTokenizer)"></a>2.1 <code>__init__(self, raw_data, tokenizer: transformers.PreTrainedTokenizer)</code></h4><ul>
<li><strong>作用</strong>：初始化 <code>LazySupervisedDataset</code> 实例。</li>
<li><strong>实现细节</strong>：<ul>
<li>将原始数据 (<code>raw_data</code>) 和分词器 (<code>tokenizer</code>) 保存为类的属性。</li>
<li>初始化一个空字典 <code>cached_data_dict</code>，用于缓存已处理的数据。</li>
</ul>
</li>
<li><strong>与 <code>SupervisedDataset</code> 的差异</strong>：<ul>
<li>在 <code>LazySupervisedDataset</code> 中，原始数据不是在初始化时立即处理，而是存储原始形式以便稍后处理。</li>
<li><code>cached_data_dict</code> 用于缓存按需处理的数据，以避免重复处理。</li>
</ul>
</li>
</ul>
<h4 id="2-2-len-self"><a href="#2-2-len-self" class="headerlink" title="2.2 __len__(self)"></a>2.2 <code>__len__(self)</code></h4><ul>
<li><p><strong>作用</strong>：返回数据集中的样本数量。</p>
</li>
<li><p><strong>实现</strong>：直接返回原始数据 (<code>raw_data</code>) 的长度。</p>
</li>
<li><p><strong>与 <code>SupervisedDataset</code> 的差异</strong>：</p>
<ul>
<li><p>在 <code>LazySupervisedDataset</code> 类中，<code>__len__</code> 方法确实返回的是数据集中样本的数量，但是这里的“样本数量”是指原始数据 (<code>raw_data</code>) 中的样本数量，而不是处理后的数据的数量。由于 <code>LazySupervisedDataset</code> 采用懒加载策略，数据在初始化时并未被处理，因此 <code>__len__</code> 方法基于原始数据计算长度是合理的。</p>
</li>
<li><p>这意味着即便数据尚未被转换为模型可用的格式，<code>__len__</code> 方法仍能准确反映数据集中待处理样本的数量。这与 <code>SupervisedDataset</code> 的主要区别在于后者在初始化时就对所有数据进行预处理，因此其 <code>__len__</code> 方法返回的是已处理数据的数量。而在 <code>LazySupervisedDataset</code> 中，数据处理是按需进行的，因此 <code>__len__</code> 返回的是原始数据中的样本数。</p>
</li>
</ul>
</li>
</ul>
<h4 id="getitem-self-i"><a href="#getitem-self-i" class="headerlink" title="__getitem__(self, i)"></a><code>__getitem__(self, i)</code></h4><ul>
<li><strong>作用</strong>：按需获取并处理指定索引 <code>i</code> 处的数据样本。</li>
<li><strong>实现细节</strong>：<ul>
<li>首先检查索引 <code>i</code> 是否在缓存 <code>cached_data_dict</code> 中。</li>
<li>如果是，则直接返回缓存的数据；如果不是，则处理原始数据中索引 <code>i</code> 处的样本，并将处理后的结果添加到缓存中。</li>
<li>返回一个包含 <code>input_ids</code>、<code>labels</code> 和 <code>attention_mask</code> 的字典。</li>
</ul>
</li>
<li><strong>与 <code>SupervisedDataset</code> 的差异</strong>：<ul>
<li><code>LazySupervisedDataset</code> 在 <code>__getitem__</code> 被调用时才处理数据，而 <code>SupervisedDataset</code> 在初始化时就处理所有数据。</li>
<li><code>LazySupervisedDataset</code> 使用缓存来避免重复处理同一样本，而 <code>SupervisedDataset</code> 不需要这种机制，因为所有数据在初始化时就已经被处理。</li>
</ul>
</li>
</ul>
<h4 id="懒加载-vs-非懒加载"><a href="#懒加载-vs-非懒加载" class="headerlink" title="懒加载 vs 非懒加载"></a>懒加载 vs 非懒加载</h4><ul>
<li><strong>懒加载（Lazy Loading）</strong>：<ul>
<li><strong>优点</strong>：减少内存占用，因为只有需要时才处理数据。对于大型数据集非常有用。</li>
<li><strong>缺点</strong>：可能增加训练时的数据加载时间，尤其是当缓存未命中时。</li>
</ul>
</li>
<li><strong>非懒加载（Eager Loading）</strong>：<ul>
<li><strong>优点</strong>：在训练开始前一次性处理所有数据，可以减少训练过程中的延迟。</li>
<li><strong>缺点</strong>：需要更多的初始内存来存储处理后的所有数据，对于非常大的数据集可能不实用。</li>
</ul>
</li>
</ul>
<h3 id="3-make-supervised-data-module-函数"><a href="#3-make-supervised-data-module-函数" class="headerlink" title="3. make_supervised_data_module 函数"></a>3. <code>make_supervised_data_module</code> 函数</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">make_supervised_data_module</span>(<span class="params"></span></span><br><span class="line"><span class="params">    tokenizer: transformers.PreTrainedTokenizer, data_args</span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="type">Dict</span>:</span><br><span class="line">    <span class="string">"""Make dataset and collator for supervised fine-tuning."""</span></span><br><span class="line">    dataset_cls = (</span><br><span class="line">        LazySupervisedDataset <span class="keyword">if</span> data_args.lazy_preprocess <span class="keyword">else</span> SupervisedDataset</span><br><span class="line">    )</span><br><span class="line">    rank0_print(<span class="string">"Loading data..."</span>)</span><br><span class="line"></span><br><span class="line">    train_json = json.load(<span class="built_in">open</span>(data_args.data_path, <span class="string">"r"</span>))</span><br><span class="line">    train_dataset = dataset_cls(train_json, tokenizer=tokenizer)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> data_args.eval_data_path:</span><br><span class="line">        eval_json = json.load(<span class="built_in">open</span>(data_args.eval_data_path, <span class="string">"r"</span>))</span><br><span class="line">        eval_dataset = dataset_cls(eval_json, tokenizer=tokenizer)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        eval_dataset = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">dict</span>(train_dataset=train_dataset, eval_dataset=eval_dataset)</span><br></pre></td></tr></tbody></table></figure>

<p>函数 <code>make_supervised_data_module</code> 的目的是为有监督的模型微调创建数据集和数据整理器（collator）。这个函数根据提供的参数构建适合训练和评估的数据集。下面是对这个函数的超级详细解释：</p>
<h3 id="函数签名"><a href="#函数签名" class="headerlink" title="函数签名"></a>函数签名</h3><ul>
<li><strong>函数名</strong>：<code>make_supervised_data_module</code></li>
<li><strong>参数</strong>：<ul>
<li><code>tokenizer</code>: <code>transformers.PreTrainedTokenizer</code> 的实例，用于文本的分词处理。</li>
<li><code>data_args</code>: 包含数据相关设置的对象，通常包括数据文件路径等信息。</li>
</ul>
</li>
<li><strong>返回值</strong>：一个字典，包含训练和评估数据集。</li>
</ul>
<h3 id="函数实现细节"><a href="#函数实现细节" class="headerlink" title="函数实现细节"></a>函数实现细节</h3><h4 id="1-选择数据集类"><a href="#1-选择数据集类" class="headerlink" title="1. 选择数据集类"></a>1. 选择数据集类</h4><ul>
<li>根据 <code>data_args.lazy_preprocess</code> 的值选择使用 <code>LazySupervisedDataset</code> 还是 <code>SupervisedDataset</code> 类。<ul>
<li>如果 <code>data_args.lazy_preprocess</code> 为 <code>True</code>，则使用 <code>LazySupervisedDataset</code> 实现懒加载。</li>
<li>否则，使用 <code>SupervisedDataset</code> 进行预加载。</li>
</ul>
</li>
<li>这一选择影响数据的加载方式，即数据是一次性全部加载并预处理，还是按需加载和处理。</li>
</ul>
<h4 id="2-加载训练数据"><a href="#2-加载训练数据" class="headerlink" title="2. 加载训练数据"></a>2. 加载训练数据</h4><ul>
<li>使用 <code>json.load(open(data_args.data_path, "r"))</code> 加载训练数据。<ul>
<li>这里假设训练数据以 JSON 格式存储，并且 <code>data_args.data_path</code> 包含了数据文件的路径。</li>
</ul>
</li>
<li>创建训练数据集 <code>train_dataset</code> 实例，传入加载的训练数据和分词器。</li>
</ul>
<h4 id="3-加载评估数据（如果提供）"><a href="#3-加载评估数据（如果提供）" class="headerlink" title="3. 加载评估数据（如果提供）"></a>3. 加载评估数据（如果提供）</h4><ul>
<li>检查是否提供了评估数据路径 <code>data_args.eval_data_path</code>。<ul>
<li>如果提供，同样使用 <code>json.load</code> 加载评估数据。</li>
<li>创建评估数据集 <code>eval_dataset</code> 实例。</li>
</ul>
</li>
<li>如果没有提供评估数据路径，将 <code>eval_dataset</code> 设置为 <code>None</code>。</li>
</ul>
<h4 id="4-返回结果"><a href="#4-返回结果" class="headerlink" title="4. 返回结果"></a>4. 返回结果</h4><ul>
<li>返回一个字典，包含两个键：<code>train_dataset</code> 和 <code>eval_dataset</code>。<ul>
<li><code>train_dataset</code> 对应训练数据集实例。</li>
<li><code>eval_dataset</code> 对应评估数据集实例（如果有的话）。</li>
</ul>
</li>
</ul>
<h2 id="5-训练流程"><a href="#5-训练流程" class="headerlink" title="5. 训练流程"></a>5. 训练流程</h2><p>了解了，我将为 <code>train()</code> 函数中每行代码提供更详细的解释：</p>
<h3 id="解析命令行参数"><a href="#解析命令行参数" class="headerlink" title="解析命令行参数"></a>解析命令行参数</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parser = transformers.HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))</span><br></pre></td></tr></tbody></table></figure>

<ul>
<li>创建一个 <code>HfArgumentParser</code> 实例，这是一个帮助解析命令行参数的工具，特别用于处理 Hugging Face transformers 库中的参数。</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model_args, data_args, training_args = parser.parse_args_into_dataclasses()</span><br></pre></td></tr></tbody></table></figure>

<ul>
<li>解析命令行参数并将它们映射到三个数据类 (<code>ModelArguments</code>, <code>DataArguments</code>, <code>TrainingArguments</code>) 的实例中。</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">local_rank = training_args.local_rank</span><br></pre></td></tr></tbody></table></figure>

<ul>
<li><code>local_rank</code> 用于标识分布式训练中的进程编号。<code>training_args.local_rank</code> 获取这个编号。</li>
</ul>
<h3 id="设置模型配置"><a href="#设置模型配置" class="headerlink" title="设置模型配置"></a>设置模型配置</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">config = transformers.AutoConfig.from_pretrained(</span><br><span class="line">    model_args.model_name_or_path,</span><br><span class="line">    cache_dir=training_args.cache_dir,</span><br><span class="line">    trust_remote_code=model_args.trust_remote_code,</span><br><span class="line">)</span><br></pre></td></tr></tbody></table></figure>

<ul>
<li>从预训练模型的配置创建 <code>AutoConfig</code> 实例。它自动加载与特定模型相关的配置。</li>
<li><code>model_args.model_name_or_path</code>: 指定模型的名称或路径。</li>
<li><code>cache_dir=training_args.cache_dir</code>: 指定缓存目录。</li>
<li><code>trust_remote_code=model_args.trust_remote_code</code>: 指定是否信任从远程下载的代码。</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">orig_ctx_len = <span class="built_in">getattr</span>(config, <span class="string">"max_position_embeddings"</span>, <span class="literal">None</span>)</span><br></pre></td></tr></tbody></table></figure>

<ul>
<li>使用 <code>getattr</code> 函数从配置中获取 <code>max_position_embeddings</code> 属性，该属性指示模型的最大位置嵌入数（即模型能处理的最大序列长度）。如果不存在该属性，则返回 <code>None</code>。<code>getattr</code> 是一个 Python 内置函数，用于获取对象的属性值。如果属性不存在，返回第三个参数指定的默认值（此处为 <code>None</code>）。</li>
<li><code>orig_ctx_len</code> 存储模型配置中的 <code>max_position_embeddings</code> 属性值，即模型可以处理的最大位置嵌入数（通常与最大序列长度相关）。</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> orig_ctx_len <span class="keyword">and</span> training_args.model_max_length &gt; orig_ctx_len:</span><br><span class="line">    scaling_factor = <span class="built_in">float</span>(math.ceil(training_args.model_max_length / orig_ctx_len))</span><br><span class="line">    config.rope_scaling = {<span class="string">"type"</span>: <span class="string">"linear"</span>, <span class="string">"factor"</span>: scaling_factor}</span><br></pre></td></tr></tbody></table></figure>

<ul>
<li>如果提供的模型最大长度 (<code>training_args.model_max_length</code>) 超过了原始模型的最大长度 (<code>orig_ctx_len</code>)，则计算一个缩放因子以进行位置编码的调整。这通常用于处理超出预训练模型原始设计的序列长度。</li>
<li><code>rope_scaling</code> 用于调整相对位置编码。</li>
<li><code>scaling_factor</code> 和 RoPE 缩放<ul>
<li>如果模型的最大长度超过原始配置的最大长度，<code>scaling_factor</code> 被用来计算缩放因子。</li>
<li>这涉及到 Rotary Positional Embedding（RoPE）的概念，即在位置嵌入中使用的技术，可以随序列长度线性缩放。</li>
<li>缩放因子用于调整位置嵌入，使其适应更长的序列。</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">config.use_cache = <span class="literal">False</span></span><br></pre></td></tr></tbody></table></figure>

<ul>
<li>禁用模型在前向传播时缓存中间计算结果的功能，这有助于减少内存消耗。这个设置告诉模型在前向传播时不使用或保存缓存。</li>
</ul>
<h3 id="加载模型和分词器"><a href="#加载模型和分词器" class="headerlink" title="加载模型和分词器"></a>加载模型和分词器</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = transformers.AutoModelForCausalLM.from_pretrained(</span><br><span class="line">    model_args.model_name_or_path,</span><br><span class="line">    config=config,</span><br><span class="line">    cache_dir=training_args.cache_dir,</span><br><span class="line">    trust_remote_code=model_args.trust_remote_code,</span><br><span class="line">)</span><br></pre></td></tr></tbody></table></figure>

<ul>
<li>加载预训练的因果语言模型（Causal Language Model）。这类模型通常用于生成任务。</li>
<li><code>trust_remote_code</code>这个参数用于确定是否信任从远程（如 Hugging Face Hub）加载的自定义模型代码。</li>
<li><code>cache_dir=training_args.cache_dir</code><ul>
<li>指定下载和缓存预训练模型和分词器的目录。</li>
<li>如果指定，模型和分词器将从这个目录加载，如果不存在，将从远程下载并缓存到此目录。</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tokenizer = transformers.AutoTokenizer.from_pretrained(</span><br><span class="line">    model_args.model_name_or_path,</span><br><span class="line">    cache_dir=training_args.cache_dir,</span><br><span class="line">    model_max_length=training_args.model_max_length,</span><br><span class="line">    padding_side=model_args.padding_side,</span><br><span class="line">    use_fast=<span class="literal">False</span>,</span><br><span class="line">    trust_remote_code=model_args.trust_remote_code,</span><br><span class="line">)</span><br></pre></td></tr></tbody></table></figure>

<ul>
<li>加载与模型对应的分词器。</li>
<li><code>use_fast=False</code>: 表示不使用快速分词器，快速分词器通常是基于 Rust 的分词器，提供更高效的分词处理。</li>
<li><code>padding_side=model_args.padding_side</code>: 指定填充（padding）应该发生在序列的哪一侧。</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> tokenizer.pad_token != tokenizer.unk_token:</span><br><span class="line">    tokenizer.pad_token = tokenizer.unk_token</span><br></pre></td></tr></tbody></table></figure>

<ul>
<li>将分词器的填充令牌设置为未知令牌（<code>unk_token</code>），如果它们不一致的话。这是因为某些模型需要在填充位置使用特定的令牌。</li>
</ul>
<h3 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)</span><br></pre></td></tr></tbody></table></figure>

<ul>
<li>调用 <code>make_supervised_data_module</code> 函数，为训练和评估准备数据集。这个函数会根据 <code>data_args</code> 中的设置，选择使用懒加载或预加载的方式处理数据。</li>
</ul>
<h3 id="初始化并启动训练器"><a href="#初始化并启动训练器" class="headerlink" title="初始化并启动训练器"></a>初始化并启动训练器</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">trainer = Trainer(</span><br><span class="line">    model=model, tokenizer=tokenizer, args=training_args, **data_module</span><br><span class="line">)</span><br></pre></td></tr></tbody></table></figure>

<ul>
<li>初始化 <code>Trainer</code> 对象，传入模型、分词器、训练参数以及通过 <code>make_supervised_data_module</code> 函数准备好的数据。</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="built_in">list</span>(pathlib.Path(training_args.output_dir).glob(<span class="string">"checkpoint-*"</span>)):</span><br><span class="line">    trainer.train(resume_from_checkpoint=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    trainer.train()</span><br></pre></td></tr></tbody></table></figure>

<ul>
<li>检查是否存在训练检查点，如果存在，则从检查点恢复训练；如果不存在，开始新的训练过程。</li>
</ul>
<h3 id="保存模型"><a href="#保存模型" class="headerlink" title="保存模型"></a>保存模型</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model.config.use_cache = <span class="literal">True</span></span><br><span class="line">trainer.save_state()</span><br></pre></td></tr></tbody></table></figure>

<ul>
<li>启用模型的缓存并保存训练器的状态。</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> trainer.is_deepspeed_enabled:</span><br><span class="line">    trainer.save_model()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    trainer_save_model_safe(trainer)</span><br></pre></td></tr></tbody></table></figure>

<ul>
<li>检查是否启用了 DeepSpeed。如果启用了，则使用 <code>trainer.save_model()</code> 保存模型。如果没有启用 DeepSpeed，则使用 <code>trainer_save_model_safe</code> 安全地保存模型，特别是在使用分布式训练时。</li>
</ul>
<h3 id="Trainer-类解释"><a href="#Trainer-类解释" class="headerlink" title="Trainer 类解释"></a>Trainer 类解释</h3><p><code>Trainer</code> 是 Hugging Face Transformers 库提供的一个类，用于封装模型的训练逻辑。以下是对 <code>Trainer</code> 类的功能的详细介绍：</p>
<ul>
<li><p><strong>模型训练与评估</strong>：<code>Trainer</code> 类负责设置和执行模型的训练和评估过程。它自动处理数据的批处理、梯度计算、优化器步骤和设备管理等任务。</p>
</li>
<li><p><strong>参数</strong>：在初始化时，<code>Trainer</code> 接受多种参数，包括模型（<code>model</code>）、分词器（<code>tokenizer</code>）、训练参数（如学习率、批大小等，通过 <code>training_args</code> 传入）和数据集。</p>
</li>
<li><p><strong>灵活性和高级功能</strong>：<code>Trainer</code> 支持多种训练设置，如多 GPU 训练、混合精度训练和 TPU 训练。它还支持自定义回调函数，用于在训练过程中执行特定操作。</p>
</li>
<li><p><strong>简化 API</strong>：<code>Trainer</code> 类提供了一个简化的 API，使得用户可以用几行代码配置和运行模型训练。它抽象了许多底层细节，使得用户可以专注于模型的构建和训练策略。</p>
</li>
<li><p><strong>检查点和恢复</strong>：<code>Trainer</code> 支持保存和加载检查点，这意味着训练过程可以在中断后从上次保存的状态恢复。</p>
</li>
</ul>
<p>总体来说，<code>Trainer</code> 类是一个功能强大且灵活的工具，为训练复杂的 Transformer 模型提供了便利和高效性。</p>
<h2 id="6-run-bash"><a href="#6-run-bash" class="headerlink" title="6. run bash"></a>6. run bash</h2><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">torchrun --nproc_per_node=8 --master_port=20001 fastchat/train/train.py \</span><br><span class="line">    --model_name_or_path ~/vicuna-7b-v1.5-16k  \</span><br><span class="line">    --data_path data/dummy_conversation.json \</span><br><span class="line">    --fp16 True \</span><br><span class="line">    --output_dir output_vicuna \</span><br><span class="line">    --num_train_epochs 3 \</span><br><span class="line">    --per_device_train_batch_size 8 \</span><br><span class="line">    --per_device_eval_batch_size 1 \</span><br><span class="line">    --gradient_accumulation_steps 1 \</span><br><span class="line">    --evaluation_strategy <span class="string">"no"</span> \</span><br><span class="line">    --save_strategy <span class="string">"steps"</span> \</span><br><span class="line">    --save_steps 1200 \</span><br><span class="line">    --save_total_limit 10 \</span><br><span class="line">    --learning_rate 2e-5 \</span><br><span class="line">    --weight_decay 0. \</span><br><span class="line">    --warmup_ratio 0.03 \</span><br><span class="line">    --lr_scheduler_type <span class="string">"cosine"</span> \</span><br><span class="line">    --logging_steps 1 \</span><br><span class="line">    --fsdp <span class="string">"full_shard auto_wrap"</span> \</span><br><span class="line">    --fsdp_transformer_layer_cls_to_wrap <span class="string">'LlamaDecoderLayer'</span> \</span><br><span class="line">    --model_max_length 2048 \</span><br><span class="line">    --gradient_checkpointing True \</span><br><span class="line">    --lazy_preprocess True</span><br></pre></td></tr></tbody></table></figure>

<h3 id="torchrun"><a href="#torchrun" class="headerlink" title="torchrun"></a>torchrun</h3><p><code>torchrun</code> 是 PyTorch 提供的一个命令行工具，用于启动分布式训练。它是 <code>torch.distributed.launch</code> 模块的一部分，旨在简化在多个进程上运行 PyTorch 程序的过程。以下是对 <code>torchrun</code> 中使用的参数的详细解释：</p>
<ol>
<li><p><code>--nproc_per_node=8</code></p>
<ul>
<li><code>--nproc_per_node</code> 指定每个节点（在这种情况下通常是一台机器）上要启动的进程数。这里设置为 8，意味着在当前节点上将启动 8 个训练进程。</li>
<li>作用：用于控制每个节点上的并行度。在多 GPU 系统中，这通常等于 GPU 的数量。</li>
</ul>
</li>
<li><p><code>--master_port=20001</code></p>
<ul>
<li><code>--master_port</code> 指定主节点用于通信的端口。这里设置为 20001。</li>
<li>作用：在分布式训练中，不同进程需要通过网络进行通信。这个参数指定了用于进程间通信的端口。</li>
</ul>
</li>
<li><p><code>fastchat/train/train.py</code></p>
<ul>
<li>这不是 <code>torchrun</code> 的参数，而是指定了要执行的 Python 脚本，即训练脚本的路径。</li>
</ul>
</li>
</ol>
<p>在分布式训练中，<code>torchrun</code> 负责在每个进程中正确地设置环境变量，如 <code>LOCAL_RANK</code>（当前进程在其节点上的排名）、<code>WORLD_SIZE</code>（总进程数）和 <code>RANK</code>（全局进程排名）。这些环境变量对于使用 PyTorch 分布式包（如 <code>torch.distributed</code>）进行有效通信至关重要。</p>
<h3 id="使用示例"><a href="#使用示例" class="headerlink" title="使用示例"></a>使用示例</h3><p>假设您有一台拥有 8 个 GPU 的机器，您想在所有 GPU 上并行运行训练。使用 <code>torchrun</code>，您的命令可能如下所示：</p>
<figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torchrun --nproc_per_node=8 --master_port=20001 fastchat/train/train.py --其他参数</span><br></pre></td></tr></tbody></table></figure>

<p>这个命令会在每个 GPU 上启动一个训练进程，每个进程运行 <code>train.py</code> 脚本，并且所有进程能够通过分布式通信有效协作。</p>
<p><code>torchrun</code> 是分布式训练的关键工具，它简化了在多个进程上启动 PyTorch 程序的流程，特别是在多 GPU 环境中。通过自动设置必要的环境变量，<code>torchrun</code> 使得实现和运行分布式训练变得更加容易和可靠。</p>
<h3 id="2-参数"><a href="#2-参数" class="headerlink" title="2. 参数"></a>2. 参数</h3><ol>
<li><p><code>--model_name_or_path</code></p>
<ul>
<li>可以是预训练模型的官方名称（如 “bert-base-uncased”）、自定义训练的模型路径或 Hugging Face Model Hub 上的模型。</li>
<li>作用：指定用于训练的模型。</li>
</ul>
</li>
<li><p><code>--data_path</code></p>
<ul>
<li>路径可以是本地文件系统上的路径。</li>
<li>作用：指定训练使用的数据文件。</li>
</ul>
</li>
<li><p><code>--fp16</code></p>
<ul>
<li>可取值为 True 或 False。</li>
<li>作用：启用或禁用混合精度训练，以提高训练速度和降低显存使用。</li>
</ul>
</li>
<li><p><code>--output_dir</code></p>
<ul>
<li>任何有效的文件路径。</li>
<li>作用：指定输出目录，用于保存训练过程中产生的文件。</li>
</ul>
</li>
<li><p><code>--num_train_epochs</code></p>
<ul>
<li>任何正整数。</li>
<li>作用：指定训练的轮次。</li>
</ul>
</li>
<li><p><code>--per_device_train_batch_size</code> 和 <code>--per_device_eval_batch_size</code></p>
<ul>
<li>任何正整数。</li>
<li>作用：分别指定每个设备上的训练和评估批次大小。</li>
</ul>
</li>
<li><p><code>--gradient_accumulation_steps</code></p>
<ul>
<li>任何正整数。</li>
<li>作用：指定梯度累积的步骤数，用于在有限的显存下增加有效的批次大小。</li>
</ul>
</li>
<li><p><code>--evaluation_strategy</code></p>
<ul>
<li>可取值包括 “no”、”steps”、”epoch”。</li>
<li>作用：指定评估的策略，如每个 epoch 或特定步数后进行评估，或不进行评估。</li>
</ul>
</li>
<li><p><code>--save_strategy</code></p>
<ul>
<li>可取值包括 “no”、”steps”、”epoch”。</li>
<li>作用：指定模型保存的策略。</li>
</ul>
</li>
<li><p><code>--save_steps</code> 和 <code>--save_total_limit</code></p>
<ul>
<li><code>--save_steps</code> 取任何正整数。</li>
<li><code>--save_total_limit</code> 取任何正整数或 None。</li>
<li>作用：分别指定保存模型的步数间隔和最大保存的检查点数量。</li>
</ul>
</li>
<li><p><code>--learning_rate</code></p>
<ul>
<li>任何正浮点数。</li>
<li>作用：指定优化器的学习率。</li>
</ul>
</li>
<li><p><code>--weight_decay</code></p>
<ul>
<li>任何非负浮点数。</li>
<li>作用：指定权重衰减，用于正则化。</li>
</ul>
</li>
<li><p><code>--warmup_ratio</code></p>
<ul>
<li>任何非负浮点数，通常在 0 到 1 之间。</li>
<li>作用：指定预热的比例，即学习率在初始阶段逐渐增加的过程。</li>
</ul>
</li>
<li><p><code>--lr_scheduler_type</code></p>
<ul>
<li>可取值如 “linear”、”cosine”、”cosine_with_restarts”、”polynomial” 等。</li>
<li>作用：指定学习率调度器的类型。</li>
</ul>
</li>
<li><p><code>--logging_steps</code></p>
<ul>
<li>任何正整数。</li>
<li>作用：指定记录日志的步数间隔。</li>
</ul>
</li>
<li><p><code>--fsdp</code></p>
<ul>
<li>可取值如 “full_shard”、”auto_wrap” 等，或它们的组合。</li>
<li>作用：指定使用全分片数据并行（Fully Sharded Data Parallel）的配置。</li>
</ul>
</li>
<li><p><code>--fsdp_transformer_layer_cls_to_wrap</code></p>
<ul>
<li>指定要在 FSDP 中包装的特定层的类名。</li>
<li>作用：针对大型模型的分布式训练进行优化。</li>
</ul>
</li>
<li><p><code>--model_max_length</code></p>
<ul>
<li>任何正整数。</li>
<li>作用：指定模型处理的最大序列长度。</li>
</ul>
</li>
<li><p><code>--gradient_checkpointing</code></p>
<ul>
<li>可取值为 True 或 False。</li>
<li>作用：启用或禁用梯度检查点，以减少显存使用。</li>
</ul>
</li>
<li><p><code>--lazy_preprocess</code></p>
<ul>
<li>可取值为 True 或 False。</li>
<li>作用：启用或禁用懒加载预处理，即按需加载和处理数据。</li>
</ul>
</li>
</ol>
<p>这些参数共同构成了一个复杂的训练配置，允许用户根据特定需求灵活调整模型训练过程。</p>
<h2 id="7-总结"><a href="#7-总结" class="headerlink" title="7. 总结"></a>7. 总结</h2><p>随着本文的结束，我们完成了对 FastChat 平台中 train.py 脚本的深入解析，这只是我们系列技术博客中的第一部分。在这一部分中，我们聚焦于 train.py 脚本的结构和功能，涵盖了从数据预处理到模型训练和保存等关键步骤。通过这次解析，读者不仅能够更好地理解 FastChat 平台的工作原理，还能获得如何有效利用这个工具进行大型语言模型训练的宝贵知识。</p>
<p>随着我们技术博客系列的不断展开，我们将继续深入探索 FastChat 的其他组件和功能。接下来的文章将进一步拓展我们的讨论范围，涉及到更多高级功能和实际应用场景。我们期望这些内容能够为对 AI 和机器学习感兴趣的读者提供更全面、深入的见解。</p>
<p>最后，我们鼓励读者持续关注我们的博客，以获取关于 FastChat 及其在大型语言模型训练领域应用的最新信息和分析。无论您是该领域的专家还是初学者，我们相信这个系列将为您提供价值和启发。敬请期待我们下一篇文章的发布，它将为您揭开 FastChat 更多令人兴奋的面纱。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="https://chenhuiyu.github.io">Huiyu Chen</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="https://chenhuiyu.github.io/2024/02/27/NLP%20Insights/FastChat%20%E8%AE%AD%E7%BB%83%E8%84%9A%E6%9C%AC%E4%BB%A3%E7%A0%81%E9%80%90%E8%A1%8C%E8%A7%A3%E6%9E%90-Train.py%20%E3%80%90FastChat%20%E7%B3%BB%E5%88%97%E7%AC%AC%201%20%E7%AF%87%E3%80%91.en/">https://chenhuiyu.github.io/2024/02/27/NLP%20Insights/FastChat%20%E8%AE%AD%E7%BB%83%E8%84%9A%E6%9C%AC%E4%BB%A3%E7%A0%81%E9%80%90%E8%A1%8C%E8%A7%A3%E6%9E%90-Train.py%20%E3%80%90FastChat%20%E7%B3%BB%E5%88%97%E7%AC%AC%201%20%E7%AF%87%E3%80%91.en/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles on this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless otherwise stated.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/LLM/">LLM</a><a class="post-meta__tags" href="/tags/FastChat/">FastChat</a><a class="post-meta__tags" href="/tags/Train/">Train</a></div><div class="post-share"><div class="social-share" data-image="/img/butterfly-icon.png" data-sites="facebook,x,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2024/02/27/NLP%20Insights/FastChat%20%E8%AE%AD%E7%BB%83%E8%84%9A%E6%9C%AC%E4%BB%A3%E7%A0%81%E9%80%90%E8%A1%8C%E8%A7%A3%E6%9E%90-Train.py%20%E3%80%90FastChat%20%E7%B3%BB%E5%88%97%E7%AC%AC%201%20%E7%AF%87%E3%80%91.zh-CN/" title="FastChat 训练脚本代码逐行解析-Train.py 【FastChat 系列第 1 篇】"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">Previous</div><div class="info-item-2">FastChat 训练脚本代码逐行解析-Train.py 【FastChat 系列第 1 篇】</div></div><div class="info-2"><div class="info-item-1">FastChat 训练脚本代码逐行解析-Train.py 【FastChat 系列第 1 篇】在本文中，我们将深入探讨 FastChat 的 train.py 脚本，这是一个用于训练和优化大型语言模型的关键组件。FastChat 是一个先进的开源平台，专注于开发、部署和评估基于大型语言模型（LLM）的聊天机器人。该平台不仅提供对顶尖模型如 Vicuna 和 MT-Bench 的支持，还包括一个分布式的多模型服务系统，配备了 Web UI 和与 OpenAI 兼容的 RESTful API，使用户能够高效地训练和评估他们的模型。 本文的深入分析将聚焦于 train.py 脚本的源代码。这个脚本是基于 transformers 库的自然语言处理模型训练脚本，涵盖了数据预处理、模型训练和保存等关键步骤。我们旨在提供对 train.py 中每个类和函数的详细解释，包括它们的功能和在整个训练过程中的作用。 1. 导入模块1. 内置模块这些是 Python 自带的标准库模块，无需额外安装。 1from dataclasses import dataclass, field  导入 Pytho...</div></div></div></a><a class="pagination-related" href="/2024/02/27/NLP%20Insights/FastChat%20Training%20Script%20Code%20Analysis%20-%20Train.py%20%E3%80%90FastChat%20Series%20Part%201%E3%80%91.zh-CN/" title="FastChat Training Script Code Analysis - Train.py 【FastChat Series Part 1】"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">Next</div><div class="info-item-2">FastChat Training Script Code Analysis - Train.py 【FastChat Series Part 1】</div></div><div class="info-2"><div class="info-item-1">FastChat Training Script Code Analysis - Train.py 【FastChat Series Part 1】In this article, we delve into the train.py script of FastChat (https://github.com/lm-sys/FastChat) (https://github.com/lm-sys/FastChat/blob/main/fastchat/train/train.py), a key component for training and optimizing large language models (LLMs). FastChat is an advanced open-source platform focused on developing, deploying, and evaluating chatbots based on LLMs. The platform not only supports top-tier models like Vicuna ...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2024/02/27/NLP%20Insights/FastChat%20Training%20Script%20Code%20Analysis%20-%20Train.py%20%E3%80%90FastChat%20Series%20Part%201%E3%80%91.en/" title="FastChat Training Script Code Analysis - Train.py 【FastChat Series Part 1】"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-27</div><div class="info-item-2">FastChat Training Script Code Analysis - Train.py 【FastChat Series Part 1】</div></div><div class="info-2"><div class="info-item-1">FastChat Training Script Code Analysis - Train.py 【FastChat Series Part 1】In this article, we delve into the train.py script of FastChat (https://github.com/lm-sys/FastChat) (https://github.com/lm-sys/FastChat/blob/main/fastchat/train/train.py), a key component for training and optimizing large language models (LLMs). FastChat is an advanced open-source platform focused on developing, deploying, and evaluating chatbots based on LLMs. The platform not only supports top-tier models like Vicuna ...</div></div></div></a><a class="pagination-related" href="/2024/02/27/NLP%20Insights/FastChat%20Training%20Script%20Code%20Analysis%20-%20Train.py%20%E3%80%90FastChat%20Series%20Part%201%E3%80%91.zh-CN/" title="FastChat Training Script Code Analysis - Train.py 【FastChat Series Part 1】"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-27</div><div class="info-item-2">FastChat Training Script Code Analysis - Train.py 【FastChat Series Part 1】</div></div><div class="info-2"><div class="info-item-1">FastChat Training Script Code Analysis - Train.py 【FastChat Series Part 1】In this article, we delve into the train.py script of FastChat (https://github.com/lm-sys/FastChat) (https://github.com/lm-sys/FastChat/blob/main/fastchat/train/train.py), a key component for training and optimizing large language models (LLMs). FastChat is an advanced open-source platform focused on developing, deploying, and evaluating chatbots based on LLMs. The platform not only supports top-tier models like Vicuna ...</div></div></div></a><a class="pagination-related" href="/2024/02/27/NLP%20Insights/FastChat%20%E8%AE%AD%E7%BB%83%E8%84%9A%E6%9C%AC%E4%BB%A3%E7%A0%81%E9%80%90%E8%A1%8C%E8%A7%A3%E6%9E%90-Train.py%20%E3%80%90FastChat%20%E7%B3%BB%E5%88%97%E7%AC%AC%201%20%E7%AF%87%E3%80%91.zh-CN/" title="FastChat 训练脚本代码逐行解析-Train.py 【FastChat 系列第 1 篇】"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-27</div><div class="info-item-2">FastChat 训练脚本代码逐行解析-Train.py 【FastChat 系列第 1 篇】</div></div><div class="info-2"><div class="info-item-1">FastChat 训练脚本代码逐行解析-Train.py 【FastChat 系列第 1 篇】在本文中，我们将深入探讨 FastChat 的 train.py 脚本，这是一个用于训练和优化大型语言模型的关键组件。FastChat 是一个先进的开源平台，专注于开发、部署和评估基于大型语言模型（LLM）的聊天机器人。该平台不仅提供对顶尖模型如 Vicuna 和 MT-Bench 的支持，还包括一个分布式的多模型服务系统，配备了 Web UI 和与 OpenAI 兼容的 RESTful API，使用户能够高效地训练和评估他们的模型。 本文的深入分析将聚焦于 train.py 脚本的源代码。这个脚本是基于 transformers 库的自然语言处理模型训练脚本，涵盖了数据预处理、模型训练和保存等关键步骤。我们旨在提供对 train.py 中每个类和函数的详细解释，包括它们的功能和在整个训练过程中的作用。 1. 导入模块1. 内置模块这些是 Python 自带的标准库模块，无需额外安装。 1from dataclasses import dataclass, field  导入 Pytho...</div></div></div></a><a class="pagination-related" href="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1.en/" title="推理 LLM 的可视化指南：探索推理时计算技术与 DeepSeek-R1"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-02-11</div><div class="info-item-2">推理 LLM 的可视化指南：探索推理时计算技术与 DeepSeek-R1</div></div><div class="info-2"><div class="info-item-1">推理 LLM 的可视化指南：探索推理时计算技术与 DeepSeek-R1原文地址：A Visual Guide to Reasoning LLMs 📅 作者：Maarten Grootendorst 📆 日期：2025 年 2 月 3 日  📌 引言DeepSeek-R1、OpenAI o3-mini 和 Google Gemini 2.0 Flash Thinking 是如何通过“推理”框架将 LLM（大型语言模型, Large Language Models） 扩展到新高度的典型示例。 它们标志着从 扩展训练时计算（train-time compute） 到 扩展推理时计算（test-time compute） 的范式转变。 在本篇文章中，我们提供了 超过 40 张定制可视化图表，带你深入探索：  推理 LLM（Reasoning LLMs） 领域 推理时计算（Test-Time Compute） 机制 DeepSeek-R1 的核心思想  我们将逐步介绍相关概念，帮助你建立对这一新范式的直觉理解。    📖 什么是推理 LLM？与普通 LLM（Large Langu...</div></div></div></a><a class="pagination-related" href="/2025/03/06/NLP%20Insights/Differences%20in%20Padding%20Strategies%20Between%20Decoder-only%20and%20Encoder-only%20Models.en/" title="Differences in Padding Strategies Between Decoder-only and Encoder-only Models"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-06</div><div class="info-item-2">Differences in Padding Strategies Between Decoder-only and Encoder-only Models</div></div><div class="info-2"><div class="info-item-1">📌 What is Padding?In Large Language Models (LLMs), padding is a method used to standardize sequence lengths for batch processing. For example: 12Sentence 1: "I love NLP"Sentence 2: "Padding is useful in LLM training"  Using the &lt;pad&gt; token for alignment: 12"I love NLP &lt;pad&gt; &lt;pad&gt; &lt;pad&gt;""Padding is useful in LLM training"   📌 Padding Positioning: Left vs RightThere are two common padding strategies:  Right padding: 1"I love NLP &lt;pad&gt; &lt;pad&gt;"  Left padding: ...</div></div></div></a><a class="pagination-related" href="/2024/02/19/NLP%20Insights/%E7%90%86%E8%A7%A3%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%ADFine-tuning%E5%92%8CFurther%20Pretraining%E7%9A%84%E5%8C%BA%E5%88%AB.en/" title="理解大型语言模型中Fine-tuning和Further Pretraining的区别"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-19</div><div class="info-item-2">理解大型语言模型中Fine-tuning和Further Pretraining的区别</div></div><div class="info-2"><div class="info-item-1">理解大型语言模型中 Fine-tuning 和 Further Pretraining 的区别在自然语言处理（NLP）领域，大型语言模型，如 GPT 和 BERT 的出现，彻底改变了我们处理文本分类、情感分析和问答等任务的方式。在这些模型的应用中，Fine-tuning（微调）和 Further Pretraining（进一步预训练）是两种关键技术。虽然它们看起来相似，但实际上服务于 NLP 流程中的不同需求和场景。 什么是 Fine-tuning？Fine-tuning 是指在特定任务的数据集上进一步训练（或“微调”）一个预训练好的模型的过程。这种方法在数据集相对较小但标注良好的情况下特别有效。 示例场景：情感分析假设你有一组电影评论数据，每条评论都标记了正面或负面情感。你想创建一个模型来预测评论的情感。 Python 代码示例（使用 PyTorch 和 HuggingFace 的 Transformers）This notebook demonstrates the fine-tuning of a BERT model on the IMDB dataset for sen...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Huiyu Chen</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">125</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">46</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#FastChat-%E8%AE%AD%E7%BB%83%E8%84%9A%E6%9C%AC%E4%BB%A3%E7%A0%81%E9%80%90%E8%A1%8C%E8%A7%A3%E6%9E%90-Train-py-%E3%80%90FastChat-%E7%B3%BB%E5%88%97%E7%AC%AC-1-%E7%AF%87%E3%80%91"><span class="toc-number">1.</span> <span class="toc-text">FastChat 训练脚本代码逐行解析-Train.py 【FastChat 系列第 1 篇】</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E5%AF%BC%E5%85%A5%E6%A8%A1%E5%9D%97"><span class="toc-number">1.1.</span> <span class="toc-text">1. 导入模块</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E5%86%85%E7%BD%AE%E6%A8%A1%E5%9D%97"><span class="toc-number">1.1.1.</span> <span class="toc-text">1. 内置模块</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E4%BE%9D%E8%B5%96%E5%BA%93"><span class="toc-number">1.1.2.</span> <span class="toc-text">2. 依赖库</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E9%A1%B9%E7%9B%AE%E7%89%B9%E5%AE%9A%E5%87%BD%E6%95%B0"><span class="toc-number">1.1.3.</span> <span class="toc-text">3. 项目特定函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E9%85%8D%E7%BD%AE%E7%B1%BB"><span class="toc-number">1.2.</span> <span class="toc-text">2. 配置类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-ModelArguments-%E7%B1%BB"><span class="toc-number">1.2.1.</span> <span class="toc-text">1. ModelArguments 类</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Code"><span class="toc-number">1.2.1.1.</span> <span class="toc-text">Code</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Explanation"><span class="toc-number">1.2.1.2.</span> <span class="toc-text">Explanation</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-DataArguments-%E7%B1%BB"><span class="toc-number">1.2.2.</span> <span class="toc-text">2. DataArguments 类</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Code-1"><span class="toc-number">1.2.2.1.</span> <span class="toc-text">Code</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Explanation-1"><span class="toc-number">1.2.2.2.</span> <span class="toc-text">Explanation</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-TrainingArguments-%E7%B1%BB"><span class="toc-number">1.2.3.</span> <span class="toc-text">3. TrainingArguments 类</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Code-2"><span class="toc-number">1.2.3.1.</span> <span class="toc-text">Code</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Explanation-2"><span class="toc-number">1.2.3.2.</span> <span class="toc-text">Explanation</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E5%8A%9F%E8%83%BD%E5%9E%8B%E5%87%BD%E6%95%B0-Functional-Utility-Functions"><span class="toc-number">1.3.</span> <span class="toc-text">3.功能型函数 (Functional Utility Functions)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-rank0-print-args"><span class="toc-number">1.3.1.</span> <span class="toc-text">1. rank0_print(*args)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Code-3"><span class="toc-number">1.3.1.1.</span> <span class="toc-text">Code</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Explanation-3"><span class="toc-number">1.3.1.2.</span> <span class="toc-text">Explanation</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-trainer-save-model-safe-trainer-transformers-Trainer"><span class="toc-number">1.3.2.</span> <span class="toc-text">2. trainer_save_model_safe(trainer: transformers.Trainer)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Code-4"><span class="toc-number">1.3.2.1.</span> <span class="toc-text">Code</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Explanation-4"><span class="toc-number">1.3.2.2.</span> <span class="toc-text">Explanation</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-preprocess-sources-tokenizer-transformers-PreTrainedTokenizer-gt-Dict"><span class="toc-number">1.3.3.</span> <span class="toc-text">3.preprocess(sources,tokenizer: transformers.PreTrainedTokenizer) -&gt; Dict</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Code-5"><span class="toc-number">1.3.3.1.</span> <span class="toc-text">Code</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E8%8E%B7%E5%8F%96%E5%AF%B9%E8%AF%9D%E6%A8%A1%E6%9D%BF%E5%92%8C%E8%A7%92%E8%89%B2%E5%AE%9A%E4%B9%89"><span class="toc-number">1.3.3.2.</span> <span class="toc-text">1. 获取对话模板和角色定义</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-prompt-%E6%A8%A1%E6%9D%BF"><span class="toc-number">1.3.3.3.</span> <span class="toc-text">2. prompt 模板</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E5%AF%B9%E8%AF%9D%E7%9A%84%E5%88%86%E8%AF%8D"><span class="toc-number">1.3.3.4.</span> <span class="toc-text">3. 对话的分词</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E7%9B%AE%E6%A0%87%E6%8E%A9%E7%A0%81"><span class="toc-number">1.3.3.5.</span> <span class="toc-text">4. 目标掩码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-%E8%BF%94%E5%9B%9E%E5%A4%84%E7%90%86%E5%90%8E%E7%9A%84%E6%95%B0%E6%8D%AE"><span class="toc-number">1.3.3.6.</span> <span class="toc-text">5. 返回处理后的数据</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">1.3.4.</span> <span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E6%95%B0%E6%8D%AE%E9%9B%86%E7%B1%BB"><span class="toc-number">1.4.</span> <span class="toc-text">4. 数据集类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-SupervisedDataset-%E7%B1%BB"><span class="toc-number">1.4.1.</span> <span class="toc-text">1. SupervisedDataset 类</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-init-self-raw-data-tokenizer-transformers-PreTrainedTokenizer"><span class="toc-number">1.4.1.1.</span> <span class="toc-text">1.1 __init__(self, raw_data, tokenizer: transformers.PreTrainedTokenizer)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-len-self"><span class="toc-number">1.4.1.2.</span> <span class="toc-text">1.2 __len__(self)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-getitem-self-i"><span class="toc-number">1.4.1.3.</span> <span class="toc-text">1.3 __getitem__(self, i)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-LazySupervisedDataset-%E7%B1%BB"><span class="toc-number">1.4.2.</span> <span class="toc-text">2. LazySupervisedDataset 类</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-init-self-raw-data-tokenizer-transformers-PreTrainedTokenizer"><span class="toc-number">1.4.2.1.</span> <span class="toc-text">2.1 __init__(self, raw_data, tokenizer: transformers.PreTrainedTokenizer)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-len-self"><span class="toc-number">1.4.2.2.</span> <span class="toc-text">2.2 __len__(self)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#getitem-self-i"><span class="toc-number">1.4.2.3.</span> <span class="toc-text">__getitem__(self, i)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%87%92%E5%8A%A0%E8%BD%BD-vs-%E9%9D%9E%E6%87%92%E5%8A%A0%E8%BD%BD"><span class="toc-number">1.4.2.4.</span> <span class="toc-text">懒加载 vs 非懒加载</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-make-supervised-data-module-%E5%87%BD%E6%95%B0"><span class="toc-number">1.4.3.</span> <span class="toc-text">3. make_supervised_data_module 函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%87%BD%E6%95%B0%E7%AD%BE%E5%90%8D"><span class="toc-number">1.4.4.</span> <span class="toc-text">函数签名</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%87%BD%E6%95%B0%E5%AE%9E%E7%8E%B0%E7%BB%86%E8%8A%82"><span class="toc-number">1.4.5.</span> <span class="toc-text">函数实现细节</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E9%80%89%E6%8B%A9%E6%95%B0%E6%8D%AE%E9%9B%86%E7%B1%BB"><span class="toc-number">1.4.5.1.</span> <span class="toc-text">1. 选择数据集类</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%8A%A0%E8%BD%BD%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE"><span class="toc-number">1.4.5.2.</span> <span class="toc-text">2. 加载训练数据</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E5%8A%A0%E8%BD%BD%E8%AF%84%E4%BC%B0%E6%95%B0%E6%8D%AE%EF%BC%88%E5%A6%82%E6%9E%9C%E6%8F%90%E4%BE%9B%EF%BC%89"><span class="toc-number">1.4.5.3.</span> <span class="toc-text">3. 加载评估数据（如果提供）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-%E8%BF%94%E5%9B%9E%E7%BB%93%E6%9E%9C"><span class="toc-number">1.4.5.4.</span> <span class="toc-text">4. 返回结果</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B"><span class="toc-number">1.5.</span> <span class="toc-text">5. 训练流程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A7%A3%E6%9E%90%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%8F%82%E6%95%B0"><span class="toc-number">1.5.1.</span> <span class="toc-text">解析命令行参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%BE%E7%BD%AE%E6%A8%A1%E5%9E%8B%E9%85%8D%E7%BD%AE"><span class="toc-number">1.5.2.</span> <span class="toc-text">设置模型配置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A0%E8%BD%BD%E6%A8%A1%E5%9E%8B%E5%92%8C%E5%88%86%E8%AF%8D%E5%99%A8"><span class="toc-number">1.5.3.</span> <span class="toc-text">加载模型和分词器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE"><span class="toc-number">1.5.4.</span> <span class="toc-text">加载数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E5%B9%B6%E5%90%AF%E5%8A%A8%E8%AE%AD%E7%BB%83%E5%99%A8"><span class="toc-number">1.5.5.</span> <span class="toc-text">初始化并启动训练器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BF%9D%E5%AD%98%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.5.6.</span> <span class="toc-text">保存模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Trainer-%E7%B1%BB%E8%A7%A3%E9%87%8A"><span class="toc-number">1.5.7.</span> <span class="toc-text">Trainer 类解释</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-run-bash"><span class="toc-number">1.6.</span> <span class="toc-text">6. run bash</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#torchrun"><span class="toc-number">1.6.1.</span> <span class="toc-text">torchrun</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B"><span class="toc-number">1.6.2.</span> <span class="toc-text">使用示例</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E5%8F%82%E6%95%B0"><span class="toc-number">1.6.3.</span> <span class="toc-text">2. 参数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-%E6%80%BB%E7%BB%93"><span class="toc-number">1.7.</span> <span class="toc-text">7. 总结</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/21/NLP%20Insights/Evaluation%20of%20Generation-Based%20Large%20Language%20Models%20(LLMs):%20Opportunities%20and%20Challenges%20from%20Generation%20to%20Judgment.zh-CN/" title="Untitled">Untitled</a><time datetime="2026-02-20T22:15:00.000Z" title="发表于 2026-02-21 06:15:00">2026-02-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/21/NLP%20Insights/Evaluation%20of%20Generation-Based%20Large%20Language%20Models%20(LLMs):%20Opportunities%20and%20Challenges%20from%20Generation%20to%20Judgment.en/" title="Untitled">Untitled</a><time datetime="2026-02-20T22:15:00.000Z" title="发表于 2026-02-21 06:15:00">2026-02-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/24/NLP%20Insights/SeCom%20Redefining%20Memory%20Management%20in%20Conversational%20AI.en/" title="SeCom: Redefining Memory Management in Conversational AI">SeCom: Redefining Memory Management in Conversational AI</a><time datetime="2025-06-24T08:00:00.000Z" title="发表于 2025-06-24 16:00:00">2025-06-24</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/24/NLP%20Insights/SeCom%20Redefining%20Memory%20Management%20in%20Conversational%20AI.zh-CN/" title="SeCom: Redefining Memory Management in Conversational AI">SeCom: Redefining Memory Management in Conversational AI</a><time datetime="2025-06-24T08:00:00.000Z" title="发表于 2025-06-24 16:00:00">2025-06-24</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/24/NLP%20Insights/SeCom:%20%E9%87%8D%E6%96%B0%E5%AE%9A%E4%B9%89%E5%AF%B9%E8%AF%9DAI%E7%9A%84%E8%AE%B0%E5%BF%86%E7%AE%A1%E7%90%86.en/" title="SeCom: 重新定义对话AI的记忆管理">SeCom: 重新定义对话AI的记忆管理</a><time datetime="2025-06-24T08:00:00.000Z" title="发表于 2025-06-24 16:00:00">2025-06-24</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 - 2026 By Huiyu Chen</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 6.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.5.4</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Reading Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light and Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle Between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Settings"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back to Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.4"></script><script src="/js/main.js?v=5.5.4"></script><div class="js-pjax"></div><script src="/js/lang-switch.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>