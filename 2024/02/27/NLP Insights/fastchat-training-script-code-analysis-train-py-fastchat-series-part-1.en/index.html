<!DOCTYPE html><html lang="[&quot;zh-CN&quot;,&quot;en&quot;,&quot;default&quot;]" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>FastChat Training Script Code Analysis - Train.py 【FastChat Series Part 1】 | 黑头呆鱼进化之旅</title><meta name="author" content="Huiyu Chen"><meta name="copyright" content="Huiyu Chen"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="FastChat Training Script Code Analysis - Train.py 【FastChat Series Part 1】In this article, we delve into the train.py script of FastChat (https:&#x2F;&#x2F;github.com&#x2F;lm-sys&#x2F;FastChat) (https:&#x2F;&#x2F;github.com&#x2F;lm-sys">
<meta property="og:type" content="article">
<meta property="og:title" content="FastChat Training Script Code Analysis - Train.py 【FastChat Series Part 1】">
<meta property="og:url" content="https://chenhuiyu.github.io/2024/02/27/NLP%20Insights/fastchat-training-script-code-analysis-train-py-fastchat-series-part-1.en/index.html">
<meta property="og:site_name" content="黑头呆鱼进化之旅">
<meta property="og:description" content="FastChat Training Script Code Analysis - Train.py 【FastChat Series Part 1】In this article, we delve into the train.py script of FastChat (https:&#x2F;&#x2F;github.com&#x2F;lm-sys&#x2F;FastChat) (https:&#x2F;&#x2F;github.com&#x2F;lm-sys">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://chenhuiyu.github.io/img/butterfly-icon.png">
<meta property="article:published_time" content="2024-02-26T17:43:18.000Z">
<meta property="article:modified_time" content="2026-02-20T22:19:17.274Z">
<meta property="article:author" content="Huiyu Chen">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="FastChat">
<meta property="article:tag" content="Train">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://chenhuiyu.github.io/img/butterfly-icon.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "FastChat Training Script Code Analysis - Train.py 【FastChat Series Part 1】",
  "url": "https://chenhuiyu.github.io/2024/02/27/NLP%20Insights/fastchat-training-script-code-analysis-train-py-fastchat-series-part-1.en/",
  "image": "https://chenhuiyu.github.io/img/butterfly-icon.png",
  "datePublished": "2024-02-26T17:43:18.000Z",
  "dateModified": "2026-02-20T22:19:17.274Z",
  "author": [
    {
      "@type": "Person",
      "name": "Huiyu Chen",
      "url": "https://chenhuiyu.github.io"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://chenhuiyu.github.io/2024/02/27/NLP%20Insights/fastchat-training-script-code-analysis-train-py-fastchat-series-part-1.en/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=5.5.4"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.13.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'FastChat Training Script Code Analysis - Train.py 【FastChat Series Part 1】',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 6.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="黑头呆鱼进化之旅" type="application/atom+xml">
</head><body><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">黑头呆鱼进化之旅</span></a><a class="nav-page-title" href="/"><span class="site-name">FastChat Training Script Code Analysis - Train.py 【FastChat Series Part 1】</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  Back to Home</span></span></a></span><div id="menus"></div></nav><div id="post-info"><h1 class="post-title">FastChat Training Script Code Analysis - Train.py 【FastChat Series Part 1】</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2024-02-26T17:43:18.000Z" title="Created 2024-02-27 01:43:18">2024-02-27</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2026-02-20T22:19:17.274Z" title="Updated 2026-02-21 06:19:17">2026-02-21</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/NLP-Insights/">NLP Insights</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="FastChat-Training-Script-Code-Analysis-Train-py-【FastChat-Series-Part-1】"><a href="#FastChat-Training-Script-Code-Analysis-Train-py-【FastChat-Series-Part-1】" class="headerlink" title="FastChat Training Script Code Analysis - Train.py 【FastChat Series Part 1】"></a>FastChat Training Script Code Analysis - Train.py 【FastChat Series Part 1】</h1><p>In this article, we delve into the train.py script of FastChat (<a target="_blank" rel="noopener" href="https://github.com/lm-sys/FastChat">https://github.com/lm-sys/FastChat</a>) (<a target="_blank" rel="noopener" href="https://github.com/lm-sys/FastChat/blob/main/fastchat/train/train.py">https://github.com/lm-sys/FastChat/blob/main/fastchat/train/train.py</a>), a key component for training and optimizing large language models (LLMs). FastChat is an advanced open-source platform focused on developing, deploying, and evaluating chatbots based on LLMs. The platform not only supports top-tier models like Vicuna and MT-Bench but also includes a distributed multi-model service system equipped with a Web UI and RESTful API compatible with OpenAI, enabling efficient training and evaluation of models.</p>
<p>We provide a detailed analysis of the train.py script’s source code. This script is a training script for natural language processing models based on the transformers library, covering critical steps such as data preprocessing, model training, and saving. Our goal is to offer a detailed explanation of each class and function in train.py, including their functionality and role in the overall training process.</p>
<h2 id="1-Importing-Modules"><a href="#1-Importing-Modules" class="headerlink" title="1. Importing Modules"></a>1. Importing Modules</h2><h3 id="1-Built-in-Modules"><a href="#1-Built-in-Modules" class="headerlink" title="1. Built-in Modules"></a>1. Built-in Modules</h3><p>These are standard library modules that come with Python and don’t require additional installation.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> dataclasses <span class="keyword">import</span> dataclass, field</span><br></pre></td></tr></tbody></table></figure>

<p>Imports Python’s <code>dataclasses</code> module for creating classes with default values.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br></pre></td></tr></tbody></table></figure>

<p>Imports the <code>json</code> module for handling JSON format data.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br></pre></td></tr></tbody></table></figure>

<p>Imports the <code>math</code> module for mathematical operations.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pathlib</span><br></pre></td></tr></tbody></table></figure>

<p>Imports the <code>pathlib</code> module for handling file paths.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Dict</span>, <span class="type">Optional</span>, <span class="type">Sequence</span></span><br></pre></td></tr></tbody></table></figure>

<p>Imports the <code>typing</code> module for type annotations.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></tbody></table></figure>

<h3 id="2-Dependency-Libraries"><a href="#2-Dependency-Libraries" class="headerlink" title="2. Dependency Libraries"></a>2. Dependency Libraries</h3><p>These are external libraries typically installed via a package manager like pip.<br>Imports the <code>numpy</code> library, commonly used for scientific computing.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></tbody></table></figure>

<p>Imports <code>PyTorch</code>, a popular deep learning framework.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br></pre></td></tr></tbody></table></figure>

<p>Imports <code>Dataset</code> from <code>torch</code> for creating custom datasets.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> transformers</span><br></pre></td></tr></tbody></table></figure>

<p>Imports the <code>transformers</code> library, a popular natural language processing library.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Trainer</span><br></pre></td></tr></tbody></table></figure>

<p>Imports <code>Trainer</code> from <code>transformers</code> for training models.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers.trainer_pt_utils <span class="keyword">import</span> LabelSmoother</span><br></pre></td></tr></tbody></table></figure>

<p>Imports <code>LabelSmoother</code> from <code>transformers</code> for label smoothing.</p>
<h3 id="3-Project-Specific-Functions"><a href="#3-Project-Specific-Functions" class="headerlink" title="3. Project-Specific Functions"></a>3. Project-Specific Functions</h3><p>These are functions or classes custom-implemented in the Fast Chat project.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fastchat.conversation <span class="keyword">import</span> SeparatorStyle</span><br></pre></td></tr></tbody></table></figure>

<p>Imports <code>SeparatorStyle</code> from the <code>fastchat</code> package for defining conversation separator styles. The <code>SeparatorStyle</code> class is an enumeration class created using Python’s <code>enum</code> module, defining a series of separator styles. Enumerations are a programming concept used to define a named set of constants, making code clearer and more maintainable.</p>
<p>In the <code>SeparatorStyle</code> class, each member represents a specific style of separator. These styles are often used in text processing, especially in scenarios where different sections or elements need to be distinguished. For instance, in handling dialog or textual data, different methods might be needed to differentiate between user input and machine responses.</p>
<p>Regarding the use of the <code>auto()</code> function:</p>
<ul>
<li><code>auto()</code> is a special function provided by Python’s <code>enum</code> module. It automatically assigns a unique value to each member in an enumeration class.</li>
<li>Without using <code>auto()</code>, you would need to manually assign a unique value to each enumeration member. <code>auto()</code> simplifies this process by letting Python handle the assignment of these values automatically.</li>
<li>The values assigned by <code>auto()</code> are usually integers, starting from 1 and increasing sequentially.</li>
</ul>
<p>In the case of the <code>SeparatorStyle</code> class, <code>auto()</code> is used to automatically assign a unique integer value to each type of separator style. For example, <code>ADD_COLON_SINGLE</code>, <code>ADD_COLON_TWO</code>, etc., will be given different integer values.</p>
<p>The names of each enumeration member (such as <code>ADD_COLON_SINGLE</code>, <code>NO_COLON_SINGLE</code>, etc.) typically describe the characteristics of that separator style. For instance, <code>ADD_COLON_SINGLE</code> might represent adding a colon as a separator after a certain element, whereas <code>NO_COLON_SINGLE</code> means no colon is added.</p>
<p>This approach makes referencing and handling these separator styles in the code more convenient and clear. For example, different separator styles can be chosen based on different scenarios or requirements without having to remember their specific values.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fastchat.model.model_adapter <span class="keyword">import</span> get_conversation_template</span><br></pre></td></tr></tbody></table></figure>

<p>Imports <code>get_conversation_template</code> from the <code>fastchat</code> package for obtaining conversation templates. In this code segment, the call logic primarily involves obtaining the default conversation template for a specific model. The call chain is as follows:</p>
<ol>
<li><p><strong>Starting Call - <code>get_conversation_template(model_path: str)</code></strong></p>
<ul>
<li>This function is the starting point of the call chain. It accepts a parameter <code>model_path</code>, specifying the path of the model.</li>
<li>The purpose of this function is to obtain the default conversation template for the given model path.</li>
</ul>
</li>
<li><p><strong>Call <code>get_model_adapter(model_path: str)</code></strong></p>
<ul>
<li>The <code>get_conversation_template</code> function first calls <code>get_model_adapter</code>, passing in the model path.</li>
<li>The purpose of <code>get_model_adapter</code> is to find and return a suitable <code>BaseModelAdapter</code> object for the provided model path.</li>
<li>This function first tries to match the basename of <code>model_path</code>. If no match is found, it tries the full path.</li>
<li>If a suitable adapter is found, it is returned; otherwise, a <code>ValueError</code> is thrown.</li>
</ul>
</li>
<li><p><strong>Execute <code>BaseModelAdapter.get_default_conv_template(model_path: str)</code></strong></p>
<ul>
<li>Once the appropriate model adapter is obtained, <code>get_conversation_template</code> retrieves the default conversation template by calling the <code>get_default_conv_template</code> method of that adapter.</li>
<li>Note that this method is defined in the <code>BaseModelAdapter</code> class but might be overridden in subclasses.</li>
</ul>
</li>
<li><p><strong>Call <code>get_conv_template(name: str)</code></strong></p>
<ul>
<li>Inside the <code>get_default_conv_template</code> method, it calls the <code>get_conv_template</code> function, usually passing a predefined template name like <code>"one_shot"</code>.</li>
<li>The purpose of <code>get_conv_template</code> is to retrieve a specified name’s template from the global registry of conversation templates <code>conv_templates</code>.</li>
</ul>
</li>
<li><p><strong>Obtain and Return a <code>Conversation</code> Object</strong></p>
<ul>
<li>The <code>get_conv_template</code> function returns an instance of the <code>Conversation</code> class, usually copied from the <code>conv_templates</code> dictionary.</li>
<li>Finally, this <code>Conversation</code> instance is returned to the original call site of <code>get_conversation_template</code>.</li>
</ul>
</li>
</ol>
<p>Summarizing the call chain:</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">get_conversation_template(model_path)</span><br><span class="line">  -&gt; get_model_adapter(model_path)</span><br><span class="line">  -&gt; [BaseModelAdapter].get_default_conv_template(model_path)</span><br><span class="line">    -&gt; get_conv_template(name)</span><br><span class="line">      -&gt; Return Conversation Object</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>

<p>In this process, the code navigates through a series of function calls to find a suitable model adapter based on the provided model path and retrieve a specific conversation template from it. This design pattern allows flexibility in providing different conversation templates for different models, enhancing the reusability and extensibility of the code.</p>
<hr>
<h2 id="2-Configuration-Classes"><a href="#2-Configuration-Classes" class="headerlink" title="2. Configuration Classes"></a>2. Configuration Classes</h2><p>These classes are defined using Python’s <code>dataclass</code> decorator and are mainly used for storing configurations and parameters. These classes usually do not contain complex methods or logic but are used to define and store data structures. These classes include:</p>
<ul>
<li><code>ModelArguments</code>: Stores parameters related to the model, like model path, trust in remote code, etc.</li>
<li><code>DataArguments</code>: Stores parameters related to data, like data path, evaluation data path, and whether to use lazy preprocessing.</li>
<li><code>TrainingArguments</code>: Stores parameters related to training, like cache directory, optimizer type, model maximum length, etc. This class extends <code>transformers.TrainingArguments</code> and adds some custom parameters.</li>
</ul>
<p>These classes are mainly used to simplify and organize parameter management in the code, making parameter modification and access more convenient.</p>
<h3 id="1-ModelArguments-Class"><a href="#1-ModelArguments-Class" class="headerlink" title="1. ModelArguments Class"></a>1. ModelArguments Class</h3><h4 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ModelArguments</span>:</span><br><span class="line">    model_name_or_path: <span class="type">Optional</span>[<span class="built_in">str</span>] = field(default=<span class="string">"facebook/opt-125m"</span>)</span><br><span class="line">    trust_remote_code: <span class="built_in">bool</span> = field(</span><br><span class="line">        default=<span class="literal">False</span>,</span><br><span class="line">        metadata={</span><br><span class="line">            <span class="string">"help"</span>: <span class="string">"Whether or not to allow for custom models defined on the Hub in their own modeling files"</span></span><br><span class="line">        },</span><br><span class="line">    )</span><br><span class="line">    padding_side: <span class="built_in">str</span> = field(</span><br><span class="line">        default=<span class="string">"right"</span>, metadata={<span class="string">"help"</span>: <span class="string">"The padding side in tokenizer"</span>}</span><br><span class="line">    )</span><br></pre></td></tr></tbody></table></figure>

<h4 id="Explanation"><a href="#Explanation" class="headerlink" title="Explanation"></a>Explanation</h4><p><code>ModelArguments</code> is a data class (<code>dataclass</code>) used for storing model-related configuration parameters.<br><strong>Attributes:</strong></p>
<ol>
<li><code>model_name_or_path</code>: Specifies the name or path of the pretrained model.</li>
<li><code>trust_remote_code</code>: Whether to allow custom models that have their modeling files defined on the Hub.</li>
<li><code>padding_side</code>: Specifies the padding side in the tokenizer, typically right or left padding.</li>
</ol>
<details>
<summary> Introduction to `@dataclass` decorator, click to expand </summary>
`@dataclass` is a decorator used to automate the generation of special methods like `__init__()`, `__repr__()`, `__eq__()` etc., thus simplifying the writing of data classes. This decorator is part of Python 3.7 and is in the `dataclasses` module.

<p>When you use <code>@dataclass</code> before a class definition, Python automatically adds some special methods based on the fields defined in the class. This is very useful for creating classes that store a small amount of data but do not need complex methods.</p>
<p>Specifically, using <code>@dataclass</code>:</p>
<ol>
<li><p><strong>Automatically generates a constructor (<code>__init__</code> method)</strong>: Python creates an <code>__init__</code> method automatically based on the fields defined in the class, so you don’t need to manually write this method to initialize your class instances.</p>
</li>
<li><p><strong>Automatically generates a <code>__repr__</code> method</strong>: This makes printing the class instances provide a more readable string representation, usually including the class name and its fields and their values.</p>
</li>
<li><p><strong>Automatically generates an <code>__eq__</code> method</strong>: This allows you to use the <code>==</code> operator to compare two instances of the class, comparing the values of the instance fields.</p>
</li>
<li><p><strong>Support for type annotations</strong>: When defining fields, you can use type annotations, which not only help with clarity of code but can also be checked for type correctness using some tools.</p>
</li>
</ol>
<p>In the case of the <code>ModelArguments</code> class, the <code>@dataclass</code> decorator will generate the above-mentioned methods. This means you can easily create an instance of <code>ModelArguments</code>, and when printing or comparing these instances, you will get the expected behavior.</p>
<p>For example, when you create an instance of <code>ModelArguments</code>:</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">args = ModelArguments()</span><br></pre></td></tr></tbody></table></figure>

<p>This will call the automatically generated <code>__init__</code> method, using the default values “facebook/opt-125m” for <code>model_name_or_path</code>, <code>False</code> for <code>trust_remote_code</code>, and “right” for <code>padding_side</code>.</p>
<p>When you print this instance:</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(args)</span><br></pre></td></tr></tbody></table></figure>

<p>This will call the automatically generated <code>__repr__</code> method, showing a detailed view of the class instance, like <code>ModelArguments(model_name_or_path="facebook/opt-125m", trust_remote_code=False, padding_side="right")</code>.</p>
<p>Thus, the <code>@dataclass</code> decorator simplifies the process of creating classes, making the code more concise and maintainable.</p>
<p>Overall, the <code>@dataclass</code> decorator is a convenient tool provided by Python for quickly creating classes mainly used for storing data.</p>
</details>

<h3 id="2-DataArguments-Class"><a href="#2-DataArguments-Class" class="headerlink" title="2. DataArguments Class"></a>2. DataArguments Class</h3><h4 id="Code-1"><a href="#Code-1" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DataArguments</span>:</span><br><span class="line">    data_path: <span class="built_in">str</span> = field(</span><br><span class="line">        default=<span class="literal">None</span>, metadata={<span class="string">"help"</span>: <span class="string">"Path to the training data."</span>}</span><br><span class="line">    )</span><br><span class="line">    eval_data_path: <span class="built_in">str</span> = field(</span><br><span class="line">        default=<span class="literal">None</span>, metadata={<span class="string">"help"</span>: <span class="string">"Path to the evaluation data."</span>}</span><br><span class="line">    )</span><br><span class="line">    lazy_preprocess: <span class="built_in">bool</span> = <span class="literal">False</span></span><br></pre></td></tr></tbody></table></figure>

<h4 id="Explanation-1"><a href="#Explanation-1" class="headerlink" title="Explanation"></a>Explanation</h4><p><strong>DataArguments Class</strong></p>
<ul>
<li><code>DataArguments</code> is also a data class used for storing data-related configuration parameters.</li>
<li>Attributes:<ul>
<li><code>data_path</code>: Path to the training data.</li>
<li><code>eval_data_path</code>: Path to the evaluation data.</li>
<li><code>lazy_preprocess</code>: Whether to use lazy loading for data preprocessing, i.e., load and process data as needed.</li>
</ul>
</li>
</ul>
<h3 id="3-TrainingArguments-Class"><a href="#3-TrainingArguments-Class" class="headerlink" title="3. TrainingArguments Class"></a>3. TrainingArguments Class</h3><h4 id="Code-2"><a href="#Code-2" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TrainingArguments</span>(transformers.TrainingArguments):</span><br><span class="line">    cache_dir: <span class="type">Optional</span>[<span class="built_in">str</span>] = field(default=<span class="literal">None</span>)</span><br><span class="line">    optim: <span class="built_in">str</span> = field(default=<span class="string">"adamw_torch"</span>)</span><br><span class="line">    model_max_length: <span class="built_in">int</span> = field(</span><br><span class="line">        default=<span class="number">512</span>,</span><br><span class="line">        metadata={</span><br><span class="line">            <span class="string">"help"</span>: <span class="string">"Maximum sequence length. Sequences will be right padded (and possibly truncated)."</span></span><br><span class="line">        },</span><br><span class="line">    )</span><br></pre></td></tr></tbody></table></figure>

<h4 id="Explanation-2"><a href="#Explanation-2" class="headerlink" title="Explanation"></a>Explanation</h4><p><code>TrainingArguments</code> class extends <code>transformers.TrainingArguments</code>.</p>
<ol>
<li><p><strong>TrainingArguments Class</strong></p>
<ul>
<li><code>TrainingArguments</code> is a data class that, by extending <code>transformers.TrainingArguments</code>, gains the capability to handle training parameters.</li>
<li>Attributes defined in <code>TrainingArguments</code>:<ul>
<li><code>cache_dir</code>: Specifies the directory path for caching the model and tokenizer.</li>
<li><code>optim</code>: Defines the type of optimizer to use, like <code>'adamw_torch'</code>.</li>
<li><code>model_max_length</code>: Specifies the maximum sequence length the model can handle.</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>transformers.TrainingArguments Class</strong></p>
<ul>
<li><code>transformers.TrainingArguments</code> is a class in the transformers library that is used for configuring various parameters in the model training process.</li>
<li>This class contains a plethora of attributes for controlling the training process, such as:<ul>
<li><code>output_dir</code>: Specifies the directory to save the model and training results.</li>
<li><code>num_train_epochs</code>: Number of training epochs.</li>
<li><code>per_device_train_batch_size</code>: Batch size per device for training.</li>
<li><code>save_steps</code>: Steps interval for saving the model.</li>
<li><code>evaluation_strategy</code>: Strategy for evaluating the model, like at the end of each epoch.</li>
<li><code>learning_rate</code>: Learning rate.</li>
<li><code>warmup_steps</code>: Steps used for warmup in the learning rate schedule.</li>
</ul>
</li>
<li><code>transformers.TrainingArguments</code> also</li>
</ul>
</li>
</ol>
<p> contains many other parameters for fine-tuning the training process, including logging, model saving strategies, learning rate scheduling, and more.</p>
<p>By extending <code>transformers.TrainingArguments</code>, the <code>TrainingArguments</code> class not only inherits all these training parameter configurations but can also add some custom training parameters, like in this case <code>cache_dir</code>, <code>optim</code>, and <code>model_max_length</code>. This approach enhances code reusability and flexibility, allowing you to adjust and extend training configurations as per the specific requirements of your project.</p>
<h2 id="3-Functional-Utility-Functions"><a href="#3-Functional-Utility-Functions" class="headerlink" title="3. Functional Utility Functions"></a>3. Functional Utility Functions</h2><h3 id="1-rank0-print-args"><a href="#1-rank0-print-args" class="headerlink" title="1. rank0_print(*args)"></a>1. rank0_print(*args)</h3><h4 id="Code-3"><a href="#Code-3" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">local_rank = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">rank0_print</span>(<span class="params">*args</span>):</span><br><span class="line">    <span class="keyword">if</span> local_rank == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(*args)</span><br></pre></td></tr></tbody></table></figure>

<h4 id="Explanation-3"><a href="#Explanation-3" class="headerlink" title="Explanation"></a>Explanation</h4><p>Defines a global variable local_rank for distributed training.<br>Defines a function rank0_print to print information only if local_rank is 0, used for controlling output in distributed training. This way, repetitive printing of the same information across multiple nodes is avoided, making the output clearer and more concise.</p>
<ul>
<li>Used to print information only on the main node (rank 0) in a distributed training environment.</li>
<li>Parameters: A variable number of arguments for printing.</li>
</ul>
<h3 id="2-trainer-save-model-safe-trainer-transformers-Trainer"><a href="#2-trainer-save-model-safe-trainer-transformers-Trainer" class="headerlink" title="2. trainer_save_model_safe(trainer: transformers.Trainer)"></a>2. <code>trainer_save_model_safe(trainer: transformers.Trainer)</code></h3><h4 id="Code-4"><a href="#Code-4" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">trainer_save_model_safe</span>(<span class="params">trainer: transformers.Trainer</span>):</span><br><span class="line">    <span class="keyword">from</span> torch.distributed.fsdp <span class="keyword">import</span> FullyShardedDataParallel <span class="keyword">as</span> FSDP</span><br><span class="line">    <span class="keyword">from</span> torch.distributed.fsdp <span class="keyword">import</span> StateDictType, FullStateDictConfig</span><br><span class="line"></span><br><span class="line">    save_policy = FullStateDictConfig(offload_to_cpu=<span class="literal">True</span>, rank0_only=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">with</span> FSDP.state_dict_type(</span><br><span class="line">        trainer.model, StateDictType.FULL_STATE_DICT, save_policy</span><br><span class="line">    ):</span><br><span class="line">        trainer.save_model()</span><br></pre></td></tr></tbody></table></figure>

<p>The function <code>trainer_save_model_safe(trainer: transformers.Trainer)</code> aims to safely save models trained with the PyTorch distributed framework. Let’s delve into the details of this function and its key components.</p>
<h4 id="Explanation-4"><a href="#Explanation-4" class="headerlink" title="Explanation"></a>Explanation</h4><ol>
<li><p>Parameters:</p>
<ul>
<li><code>trainer</code>: An instance of <code>transformers.Trainer</code>. This class is one of the core components of the Hugging Face Transformers library, used for training and evaluating models.</li>
</ul>
</li>
<li><p>Functionality:</p>
<ul>
<li>The main purpose of this function is to safely save models in a distributed training environment. It particularly considers the model saving strategy when using Fully Sharded Data Parallel (FSDP).</li>
</ul>
</li>
<li><p>FSDP</p>
<ul>
<li><strong>FullyShardedDataParallel (FSDP)</strong><ul>
<li>This is a component of PyTorch’s distributed training framework. FSDP helps reduce memory usage on each GPU by sharding model parameters across multiple GPUs, allowing the training of larger models.</li>
<li>In this context, FSDP is primarily used for handling and saving model states in distributed training.</li>
</ul>
</li>
<li><strong>StateDictType</strong><ul>
<li>This is an enumeration type that defines how to save the model’s state dictionary. In FSDP environments, saving and loading model states might require special handling.</li>
</ul>
</li>
<li><strong>FullStateDictConfig</strong><ul>
<li>This class configures parameters for saving the full state dictionary. It’s part of FSDP’s functionality and is used to control how the model state is saved.</li>
</ul>
</li>
</ul>
</li>
<li><p>Function Implementation</p>
<ul>
<li><strong>Setting Save Policy</strong><ul>
<li><code>save_policy = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)</code> creates a save policy. Here, two key parameters are specified:<ul>
<li><code>offload_to_cpu</code>: Offload model parameters to CPU before saving the state dictionary, which helps reduce GPU memory usage.</li>
<li><code>rank0_only</code>: Save the model only on rank 0 (usually the main node). In distributed training, this avoids saving the same model copy on every node, saving storage space.</li>
</ul>
</li>
</ul>
</li>
<li><strong>Saving the Model</strong><ul>
<li>Using the <code>with FSDP.state_dict_type(trainer.model, StateDictType.FULL_STATE_DICT, save_policy)</code> context manager, the type and policy for saving the model’s state dictionary are set.</li>
<li>Within this context, <code>trainer.save_model()</code> is called to save the model. Due to the <code>save_policy</code>, the model is saved securely following the specified configuration.</li>
</ul>
</li>
</ul>
</li>
</ol>
<p>The function <code>trainer_save_model_safe</code> encapsulates a safe model saving logic, particularly for scenarios involving PyTorch’s FSDP in distributed training. It ensures that only a complete model state is saved on one node and offloads model parameters to CPU before saving, optimizing memory usage and storage efficiency. This is crucial for training large models and managing large-scale distributed training environments.</p>
<h3 id="3-preprocess-sources-tokenizer-transformers-PreTrainedTokenizer-gt-Dict"><a href="#3-preprocess-sources-tokenizer-transformers-PreTrainedTokenizer-gt-Dict" class="headerlink" title="3.preprocess(sources,tokenizer: transformers.PreTrainedTokenizer) -> Dict"></a>3.<code>preprocess(sources,tokenizer: transformers.PreTrainedTokenizer) -&gt; Dict</code></h3><h4 id="Code-5"><a href="#Code-5" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess</span>(<span class="params"></span></span><br><span class="line"><span class="params">    sources,</span></span><br><span class="line"><span class="params">    tokenizer: transformers.PreTrainedTokenizer,</span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="type">Dict</span>:</span><br><span class="line">    conv = get_conversation_template(<span class="string">"vicuna"</span>)</span><br><span class="line">    roles = {<span class="string">"human"</span>: conv.roles[<span class="number">0</span>], <span class="string">"gpt"</span>: conv.roles[<span class="number">1</span>]}</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Apply prompt templates</span></span><br><span class="line">    conversations = []</span><br><span class="line">    <span class="keyword">for</span> i, source <span class="keyword">in</span> <span class="built_in">enumerate</span>(sources):</span><br><span class="line">        <span class="keyword">if</span> roles[source[<span class="number">0</span>][<span class="string">"from"</span>]] != conv.roles[<span class="number">0</span>]:</span><br><span class="line">            <span class="comment"># Skip the first one if it is not from human</span></span><br><span class="line">            source = source[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">        conv.messages = []</span><br><span class="line">        <span class="keyword">for</span> j, sentence <span class="keyword">in</span> <span class="built_in">enumerate</span>(source):</span><br><span class="line">            role = roles[sentence[<span class="string">"from"</span>]]</span><br><span class="line">            <span class="keyword">assert</span> role == conv.roles[j % <span class="number">2</span>], <span class="string">f"<span class="subst">{i}</span>"</span></span><br><span class="line">            conv.append_message(role, sentence[<span class="string">"value"</span>])</span><br><span class="line">        conversations.append(conv.get_prompt())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Tokenize conversations</span></span><br><span class="line">    input_ids = tokenizer(</span><br><span class="line">        conversations,</span><br><span class="line">        return_tensors=<span class="string">"pt"</span>,</span><br><span class="line">        padding=<span class="string">"max_length"</span>,</span><br><span class="line">        max_length=tokenizer.model_max_length,</span><br><span class="line">        truncation=<span class="literal">True</span>,</span><br><span class="line">    ).input_ids</span><br><span class="line">    targets = input_ids.clone()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> conv.sep_style == SeparatorStyle.ADD_COLON_TWO</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Mask targets. Only compute loss on the assistant outputs.</span></span><br><span class="line">    sep = conv.sep + conv.roles[<span class="number">1</span>] + <span class="string">": "</span></span><br><span class="line">    <span class="keyword">for</span> conversation, target <span class="keyword">in</span> <span class="built_in">zip</span>(conversations, targets):</span><br><span class="line">        total_len = <span class="built_in">int</span>(target.ne(tokenizer.pad_token_id).<span class="built_in">sum</span>())</span><br><span class="line"></span><br><span class="line">        turns = conversation.split(conv.sep2)</span><br><span class="line">        cur_len = <span class="number">1</span></span><br><span class="line">        target[:cur_len] = IGNORE_TOKEN_ID</span><br><span class="line">        <span class="keyword">for</span> i, turn <span class="keyword">in</span> <span class="built_in">enumerate</span>(turns):</span><br><span class="line">            <span class="keyword">if</span> turn == <span class="string">""</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            turn_len = <span class="built_in">len</span>(tokenizer(turn).input_ids)</span><br><span class="line"></span><br><span class="line">            parts = turn.split(sep)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(parts) != <span class="number">2</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            parts[<span class="number">0</span>] += sep</span><br><span class="line">            <span class="comment"># "-2" is hardcoded for the Llama tokenizer to make the offset correct.</span></span><br><span class="line">            instruction_len = <span class="built_in">len</span>(tokenizer(parts[<span class="number">0</span>]).input_ids) - <span class="number">2</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i != <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> tokenizer.legacy:</span><br><span class="line">                <span class="comment"># The legacy and non-legacy modes handle special tokens differently</span></span><br><span class="line">                instruction_len -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Ignore the user instructions</span></span><br><span class="line">            target[cur_len : cur_len + instruction_len] = IGNORE_TOKEN_ID</span><br><span class="line">            cur_len += turn_len</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i != <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> tokenizer.legacy:</span><br><span class="line">                <span class="comment"># The legacy and non-legacy modes handle special tokens differently</span></span><br><span class="line">                cur_len -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        target[cur_len:] = IGNORE_TOKEN_ID</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="literal">False</span>:  <span class="comment"># Inspect and check the correctness of masking</span></span><br><span class="line">            z = target.clone()</span><br><span class="line">            z = torch.where(z == IGNORE_TOKEN_ID, tokenizer.unk_token_id, z)</span><br><span class="line">            rank0_print(tokenizer.decode(z))</span><br><span class="line">            exit()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> cur_len &lt; tokenizer.model_max_length:</span><br><span class="line">           </span><br><span class="line"></span><br><span class="line"> <span class="keyword">if</span> cur_len != total_len:</span><br><span class="line">                target[:] = IGNORE_TOKEN_ID</span><br><span class="line">                rank0_print(</span><br><span class="line">                    <span class="string">f"WARNING: tokenization mismatch: <span class="subst">{cur_len}</span> vs. <span class="subst">{total_len}</span>."</span></span><br><span class="line">                    <span class="string">f" #turn = <span class="subst">{<span class="built_in">len</span>(turns) - <span class="number">1</span>}</span>. (ignored)"</span></span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">dict</span>(</span><br><span class="line">        input_ids=input_ids,</span><br><span class="line">        labels=targets,</span><br><span class="line">        attention_mask=input_ids.ne(tokenizer.pad_token_id),</span><br><span class="line">    )</span><br></pre></td></tr></tbody></table></figure>

<p>The function <code>preprocess(sources, tokenizer: transformers.PreTrainedTokenizer) -&gt; Dict</code> is intended for preprocessing dialogue data to be suitable for training machine learning models. This function can be broken down into several main parts for a more detailed explanation:</p>
<h4 id="1-Obtaining-Conversation-Templates-and-Role-Definitions"><a href="#1-Obtaining-Conversation-Templates-and-Role-Definitions" class="headerlink" title="1. Obtaining Conversation Templates and Role Definitions"></a>1. Obtaining Conversation Templates and Role Definitions</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conv = get_conversation_template(<span class="string">"vicuna"</span>)</span><br><span class="line">roles = {<span class="string">"human"</span>: conv.roles[<span class="number">0</span>], <span class="string">"gpt"</span>: conv.roles[<span class="number">1</span>]}</span><br></pre></td></tr></tbody></table></figure>

<ul>
<li><strong>Functionality</strong>: Initializes conversation templates and defines the roles of dialogue participants.</li>
<li><strong>Implementation</strong>:<ul>
<li><code>conv = get_conversation_template("vicuna")</code> obtains the conversation template for a specified model (e.g., “vicuna”).</li>
<li>The <code>roles</code> dictionary maps “human” and “gpt” to the roles defined in the conversation template.</li>
</ul>
</li>
<li><strong>Example</strong>:<ul>
<li>If the conversation template is for “vicuna”, then <code>roles</code> might map “human” to “user” and “gpt” to “assistant”. For example, <code>{'human': 'USER', 'gpt': 'ASSISTANT'}</code>.</li>
</ul>
</li>
</ul>
<h4 id="2-Applying-Prompt-Templates"><a href="#2-Applying-Prompt-Templates" class="headerlink" title="2. Applying Prompt Templates"></a>2. Applying Prompt Templates</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Apply prompt templates</span></span><br><span class="line">conversations = []</span><br><span class="line"><span class="keyword">for</span> i, source <span class="keyword">in</span> <span class="built_in">enumerate</span>(sources):</span><br><span class="line">    <span class="keyword">if</span> roles[source[<span class="number">0</span>][<span class="string">"from"</span>]] != conv.roles[<span class="number">0</span>]:</span><br><span class="line">        <span class="comment"># Skip the first one if it is not from human</span></span><br><span class="line">        source = source[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">    conv.messages = []</span><br><span class="line">    <span class="keyword">for</span> j, sentence <span class="keyword">in</span> <span class="built_in">enumerate</span>(source):</span><br><span class="line">        role = roles[sentence[<span class="string">"from"</span>]]</span><br><span class="line">        <span class="keyword">assert</span> role == conv.roles[j % <span class="number">2</span>], <span class="string">f"<span class="subst">{i}</span>"</span></span><br><span class="line">        conv.append_message(role, sentence[<span class="string">"value"</span>])</span><br><span class="line">    conversations.append(conv.get_prompt())</span><br></pre></td></tr></tbody></table></figure>

<ul>
<li><strong>Functionality</strong>: Applies prompt templates to source data to construct dialogues.</li>
<li><strong>Implementation</strong>:<ul>
<li>Iterates through <code>sources</code> (original dialogue data), transforming each dialogue source into a conversation in template format.</li>
<li>If the first part of a dialogue is not initiated by the “human” role, it skips that part.</li>
<li>Assigns a role to each sentence and adds it to the conversation template.</li>
<li>Ultimately, each processed dialogue is added to the <code>conversations</code> list.</li>
</ul>
</li>
<li><strong>Example</strong>:<ul>
<li>Suppose we have a source which is the first item in dummy input: <code>python source = [{'from': 'human', 'value': 'Who are you?'}, {'from': 'gpt', 'value': 'I am Vicuna, a language model trained by researchers from Large Model Systems Organization (LMSYS).'}, {'from': 'human', 'value': 'Have a nice day!'}, {'from': 'gpt', 'value': 'You too!'}]</code></li>
<li><code>conversations</code> under the Vicuna template, using <code>SeparatorStyle.ADD_COLON_TWO</code> as the separator style, might look like [“A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user’s questions. USER: Who are you? ASSISTANT: I am Vicuna, a language model trained by researchers from Large Model Systems Organization (LMSYS).USER: Have a nice day! ASSISTANT: You too!“]</li>
<li><details>
<summary> Implementation of get_prompt </summary>
The `get_prompt` method implementation varies depending on the `SeparatorStyle`. Below is a table detailing the `get_prompt` method for various styles, along with English examples:

<table>
<thead>
<tr>
<th>Separator Style (<code>SeparatorStyle</code>)</th>
<th>Description</th>
<th>Example</th>
</tr>
</thead>
<tbody><tr>
<td><code>ADD_COLON_SINGLE</code></td>
<td>Adds a colon and separator after each message.</td>
<td>USER: Hello there!\nASSISTANT: Hi, how can I help?\n</td>
</tr>
<tr>
<td><code>ADD_COLON_TWO</code></td>
<td>Uses two alternating separators, usually between different roles.</td>
<td>USER: What’s the weather?\nASSISTANT: It’s sunny today.\n\n</td>
</tr>
<tr>
<td><code>ADD_COLON_SPACE_SINGLE</code></td>
<td>Adds a colon, space, and separator after each message.</td>
<td>USER: Can you book a flight?\nASSISTANT: Sure, where to?\n</td>
</tr>
<tr>
<td><code>NO_COLON_SINGLE</code></td>
<td>Messages directly follow roles without a colon, followed by a separator.</td>
<td>USERWhat are you doing?\nASSISTANTI’m here to assist you.\n</td>
</tr>
<tr>
<td><code>NO_COLON_TWO</code></td>
<td>No colons, with two alternating separators.</td>
<td>USERHow’s the project going?\nASSISTANTIt’s on track.\n\n</td>
</tr>
<tr>
<td><code>ADD_NEW_LINE_SINGLE</code></td>
<td>Each message is preceded by a newline, followed by a separator.</td>
<td>USER\nHow can I reset my password?\nASSISTANT\nYou can reset it via email.\n</td>
</tr>
<tr>
<td><code>RWKV</code></td>
<td>Special format, usually for specific models.</td>
<td>USER: What is AI?\n\nASSISTANT: AI stands for Artificial Intelligence.\n\n</td>
</tr>
<tr>
<td><code>LLAMA2</code></td>
<td>Special label format for specific models.</td>
<td>[INST] USER How does blockchain work?\nASSISTANT It is a distributed ledger.\n\n</td>
</tr>
<tr>
<td><code>CHATGLM</code></td>
<td>Specific format for <code>CHATGLM</code> model.</td>
<td>[Round 1]\nUSER: Tell me a joke.\nASSISTANT: Why did the chicken cross the road?\n</td>
</tr>
<tr>
<td><code>CHATML</code></td>
<td>Similar to <code>CHATGLM</code>, but with newlines before and after each message.</td>
<td>USER\nDo you like music?\n\nASSISTANT\nYes, I enjoy many genres.\n\n</td>
</tr>
<tr>
<td><code>CHATGLM3</code></td>
<td>Format for <code>CHATGLM3</code> model.</td>
<td>USER\nCan you play chess?\nASSISTANTYes, I can play.\n</td>
</tr>
<tr>
<td><code>CHATINTERN</code></td>
<td>Format for <code>CHATINTERN</code> model, using special markers.</td>
<td><s>USER:Where is the nearest ATM?<s>\nASSISTANT:It’s next to the post office.\n</s></s></td>
</tr>
<tr>
<td><code>DOLLY</code></td>
<td>Specific format for <code>DOLLY</code> model.</td>
<td>USER:\nWhat is quantum computing?\nASSISTANT:\nIt involves computation using quantum-mechanical phenomena.\n\n</td>
</tr>
<tr>
<td><code>PHOENIX</code></td>
<td>For <code>PHOENIX</code> model, messages are wrapped in special markers.</td>
<td>USER: <s>How to bake a cake?</s>\nASSISTANT: <s>You need flour, sugar, and eggs.</s>\n</td>
</tr>
<tr>
<td><code>ROBIN</code></td>
<td>Similar to <code>ADD_NEW_LINE_SINGLE</code>, but with a newline after roles.</td>
<td>USER:\nIs AI dangerous?\nASSISTANT:\nIt depends on how it’s used.\n</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
</details></li>
</ul>
</li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="https://chenhuiyu.github.io">Huiyu Chen</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="https://chenhuiyu.github.io/2024/02/27/NLP%20Insights/fastchat-training-script-code-analysis-train-py-fastchat-series-part-1.en/">https://chenhuiyu.github.io/2024/02/27/NLP%20Insights/fastchat-training-script-code-analysis-train-py-fastchat-series-part-1.en/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles on this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless otherwise stated.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/LLM/">LLM</a><a class="post-meta__tags" href="/tags/FastChat/">FastChat</a><a class="post-meta__tags" href="/tags/Train/">Train</a></div><div class="post-share"><div class="social-share" data-image="/img/butterfly-icon.png" data-sites="facebook,x,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2024/02/27/NLP%20Insights/fastchat-training-script-code-analysis-train-py-fastchat-series-part-1.zh-CN/" title="FastChat Training Script Code Analysis - Train.py 【FastChat Series Part 1】"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">Previous</div><div class="info-item-2">FastChat Training Script Code Analysis - Train.py 【FastChat Series Part 1】</div></div><div class="info-2"><div class="info-item-1">FastChat Training Script Code Analysis - Train.py 【FastChat Series Part 1】In this article, we delve into the train.py script of FastChat (https://github.com/lm-sys/FastChat) (https://github.com/lm-sys/FastChat/blob/main/fastchat/train/train.py), a key component for training and optimizing large language models (LLMs). FastChat is an advanced open-source platform focused on developing, deploying, and evaluating chatbots based on LLMs. The platform not only supports top-tier models like Vicuna ...</div></div></div></a><a class="pagination-related" href="/2024/02/26/Life%20Reflections/%E8%8B%B1%E8%AF%AD%E5%AD%A6%E4%B9%A0%E6%97%A5%E8%AE%B0-de-facto.zh-CN/" title="英语学习日记：De Facto"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">Next</div><div class="info-item-2">英语学习日记：De Facto</div></div><div class="info-2"><div class="info-item-1">大家好！🌐 今天我们要探讨一个在英语对话和写作中常见的短语：de facto 理解 “De Facto” 含义：’De facto’ 这个短语用来描述一些实际上存在的事物，即使它们没有被官方认可或法律确立。就像是在说“实际上”或“实践中”，而不是“理论上”或“官方上”。  词源：这个短语有着非常有趣的历史。它源自拉丁语，其中 ‘de’ 意为 ‘来自’，’facto’ 意味着 ‘事实’。随着时间的推移，它被英语采纳，并保留了从拉丁语中原始的精髓。   例句 🌟 In many organizations, there is a de facto leader who isn’t officially the boss but is respected and followed by the team. 🌟 While English is the de facto language of international business, it’s not the official language in many countries where it’s widely spok...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2024/02/27/NLP%20Insights/fastchat-%E8%AE%AD%E7%BB%83%E8%84%9A%E6%9C%AC%E4%BB%A3%E7%A0%81%E9%80%90%E8%A1%8C%E8%A7%A3%E6%9E%90-train-py-fastchat-%E7%B3%BB%E5%88%97%E7%AC%AC-1-%E7%AF%87.en/" title="FastChat 训练脚本代码逐行解析-Train.py 【FastChat 系列第 1 篇】"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-27</div><div class="info-item-2">FastChat 训练脚本代码逐行解析-Train.py 【FastChat 系列第 1 篇】</div></div><div class="info-2"><div class="info-item-1">FastChat 训练脚本代码逐行解析-Train.py 【FastChat 系列第 1 篇】在本文中，我们将深入探讨 FastChat 的 train.py 脚本，这是一个用于训练和优化大型语言模型的关键组件。FastChat 是一个先进的开源平台，专注于开发、部署和评估基于大型语言模型（LLM）的聊天机器人。该平台不仅提供对顶尖模型如 Vicuna 和 MT-Bench 的支持，还包括一个分布式的多模型服务系统，配备了 Web UI 和与 OpenAI 兼容的 RESTful API，使用户能够高效地训练和评估他们的模型。 本文的深入分析将聚焦于 train.py 脚本的源代码。这个脚本是基于 transformers 库的自然语言处理模型训练脚本，涵盖了数据预处理、模型训练和保存等关键步骤。我们旨在提供对 train.py 中每个类和函数的详细解释，包括它们的功能和在整个训练过程中的作用。 1. 导入模块1. 内置模块这些是 Python 自带的标准库模块，无需额外安装。 1from dataclasses import dataclass, field  导入 Pytho...</div></div></div></a><a class="pagination-related" href="/2024/02/27/NLP%20Insights/fastchat-%E8%AE%AD%E7%BB%83%E8%84%9A%E6%9C%AC%E4%BB%A3%E7%A0%81%E9%80%90%E8%A1%8C%E8%A7%A3%E6%9E%90-train-py-fastchat-%E7%B3%BB%E5%88%97%E7%AC%AC-1-%E7%AF%87.zh-CN/" title="FastChat 训练脚本代码逐行解析-Train.py 【FastChat 系列第 1 篇】"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-27</div><div class="info-item-2">FastChat 训练脚本代码逐行解析-Train.py 【FastChat 系列第 1 篇】</div></div><div class="info-2"><div class="info-item-1">FastChat 训练脚本代码逐行解析-Train.py 【FastChat 系列第 1 篇】在本文中，我们将深入探讨 FastChat 的 train.py 脚本，这是一个用于训练和优化大型语言模型的关键组件。FastChat 是一个先进的开源平台，专注于开发、部署和评估基于大型语言模型（LLM）的聊天机器人。该平台不仅提供对顶尖模型如 Vicuna 和 MT-Bench 的支持，还包括一个分布式的多模型服务系统，配备了 Web UI 和与 OpenAI 兼容的 RESTful API，使用户能够高效地训练和评估他们的模型。 本文的深入分析将聚焦于 train.py 脚本的源代码。这个脚本是基于 transformers 库的自然语言处理模型训练脚本，涵盖了数据预处理、模型训练和保存等关键步骤。我们旨在提供对 train.py 中每个类和函数的详细解释，包括它们的功能和在整个训练过程中的作用。 1. 导入模块1. 内置模块这些是 Python 自带的标准库模块，无需额外安装。 1from dataclasses import dataclass, field  导入 Pytho...</div></div></div></a><a class="pagination-related" href="/2024/02/27/NLP%20Insights/fastchat-training-script-code-analysis-train-py-fastchat-series-part-1.zh-CN/" title="FastChat Training Script Code Analysis - Train.py 【FastChat Series Part 1】"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-27</div><div class="info-item-2">FastChat Training Script Code Analysis - Train.py 【FastChat Series Part 1】</div></div><div class="info-2"><div class="info-item-1">FastChat Training Script Code Analysis - Train.py 【FastChat Series Part 1】In this article, we delve into the train.py script of FastChat (https://github.com/lm-sys/FastChat) (https://github.com/lm-sys/FastChat/blob/main/fastchat/train/train.py), a key component for training and optimizing large language models (LLMs). FastChat is an advanced open-source platform focused on developing, deploying, and evaluating chatbots based on LLMs. The platform not only supports top-tier models like Vicuna ...</div></div></div></a><a class="pagination-related" href="/2025/02/11/NLP%20Insights/moe%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97-%E6%8F%AD%E7%A7%98-moe-%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2.en/" title="MoE模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-02-11</div><div class="info-item-2">MoE模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色</div></div><div class="info-2"><div class="info-item-1">MoE 模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色原文地址：A Visual Guide to Mixture of Experts (MoE) 📅 作者：Maarten Grootendorst 📆 日期：2024 年 10 月 7 日  探索语言模型：混合专家模型（MoE）可视化指南目录 MoE 模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色 探索语言模型：混合专家模型（MoE）可视化指南 目录 什么是混合专家（MoE）模型？ Experts Dense Layers Sparse Layers What does an Expert Learn? 专家的架构（Architecture of Experts）      当我们查看最新发布的大型语言模型（LLMs，Large Language Models）时，常常会在标题中看到 “MoE”。这个 “MoE” 代表什么？为什么这么多 LLM 都在使用它？ 在这份可视化指南中，我们会通过 50 多个可视化图示，逐步探索这一关键组件：**Mixture of Experts (MoE)**。   图示内...</div></div></div></a><a class="pagination-related" href="/2023/08/02/NLP%20Insights/training-llama-2-model-on-single-gpu-with-int8-quantization-and-lora.zh-CN/" title="Training Llama 2 Model on Single GPU with int8 Quantization and LoRA"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2023-08-02</div><div class="info-item-2">Training Llama 2 Model on Single GPU with int8 Quantization and LoRA</div></div><div class="info-2"><div class="info-item-1">Training Llama 2 Model on Single GPU with int8 Quantization and LoRALlama 2概述Llama 2 是一个包含预训练和微调的生成式文本模型的集合，其规模从 70 亿到 700 亿个参数不等。Llama2模型是由Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert等人在Llama 2: Open Foundation and Fine-Tuned Chat Models中提出的。 该论文的摘要如下： 在这项工作中，我们开发并发布了Llama 2，这是一组从70亿到700亿参数的预训练和微调的大型语言模型（LLMs）。我们的微调LLMs，称为Llama 2-Chat，针对对话用例进行了优化。我们的模型在我们测试的大多数基准上胜过开源聊天模型，并且基于我们对有用性和安全性的人类评估，可能是闭源模型的合适替代品。我们提供了关于微调和改进Llama 2-Chat安全性的方法的详细描述，以便社区能够在我们的工作基础上构建，并有助于LLMs的负责任发展。 在此处查看所有L...</div></div></div></a><a class="pagination-related" href="/2024/02/19/NLP%20Insights/%E7%90%86%E8%A7%A3%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%ADfine-tuning%E5%92%8Cfurther-pretraining%E7%9A%84%E5%8C%BA%E5%88%AB.zh-CN/" title="理解大型语言模型中Fine-tuning和Further Pretraining的区别"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-19</div><div class="info-item-2">理解大型语言模型中Fine-tuning和Further Pretraining的区别</div></div><div class="info-2"><div class="info-item-1">理解大型语言模型中 Fine-tuning 和 Further Pretraining 的区别在自然语言处理（NLP）领域，大型语言模型，如 GPT 和 BERT 的出现，彻底改变了我们处理文本分类、情感分析和问答等任务的方式。在这些模型的应用中，Fine-tuning（微调）和 Further Pretraining（进一步预训练）是两种关键技术。虽然它们看起来相似，但实际上服务于 NLP 流程中的不同需求和场景。 什么是 Fine-tuning？Fine-tuning 是指在特定任务的数据集上进一步训练（或“微调”）一个预训练好的模型的过程。这种方法在数据集相对较小但标注良好的情况下特别有效。 示例场景：情感分析假设你有一组电影评论数据，每条评论都标记了正面或负面情感。你想创建一个模型来预测评论的情感。 Python 代码示例（使用 PyTorch 和 HuggingFace 的 Transformers）This notebook demonstrates the fine-tuning of a BERT model on the IMDB dataset for sen...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Huiyu Chen</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">118</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">46</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#FastChat-Training-Script-Code-Analysis-Train-py-%E3%80%90FastChat-Series-Part-1%E3%80%91"><span class="toc-number">1.</span> <span class="toc-text">FastChat Training Script Code Analysis - Train.py 【FastChat Series Part 1】</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Importing-Modules"><span class="toc-number">1.1.</span> <span class="toc-text">1. Importing Modules</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-Built-in-Modules"><span class="toc-number">1.1.1.</span> <span class="toc-text">1. Built-in Modules</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-Dependency-Libraries"><span class="toc-number">1.1.2.</span> <span class="toc-text">2. Dependency Libraries</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-Project-Specific-Functions"><span class="toc-number">1.1.3.</span> <span class="toc-text">3. Project-Specific Functions</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Configuration-Classes"><span class="toc-number">1.2.</span> <span class="toc-text">2. Configuration Classes</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-ModelArguments-Class"><span class="toc-number">1.2.1.</span> <span class="toc-text">1. ModelArguments Class</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Code"><span class="toc-number">1.2.1.1.</span> <span class="toc-text">Code</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Explanation"><span class="toc-number">1.2.1.2.</span> <span class="toc-text">Explanation</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-DataArguments-Class"><span class="toc-number">1.2.2.</span> <span class="toc-text">2. DataArguments Class</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Code-1"><span class="toc-number">1.2.2.1.</span> <span class="toc-text">Code</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Explanation-1"><span class="toc-number">1.2.2.2.</span> <span class="toc-text">Explanation</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-TrainingArguments-Class"><span class="toc-number">1.2.3.</span> <span class="toc-text">3. TrainingArguments Class</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Code-2"><span class="toc-number">1.2.3.1.</span> <span class="toc-text">Code</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Explanation-2"><span class="toc-number">1.2.3.2.</span> <span class="toc-text">Explanation</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Functional-Utility-Functions"><span class="toc-number">1.3.</span> <span class="toc-text">3. Functional Utility Functions</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-rank0-print-args"><span class="toc-number">1.3.1.</span> <span class="toc-text">1. rank0_print(*args)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Code-3"><span class="toc-number">1.3.1.1.</span> <span class="toc-text">Code</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Explanation-3"><span class="toc-number">1.3.1.2.</span> <span class="toc-text">Explanation</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-trainer-save-model-safe-trainer-transformers-Trainer"><span class="toc-number">1.3.2.</span> <span class="toc-text">2. trainer_save_model_safe(trainer: transformers.Trainer)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Code-4"><span class="toc-number">1.3.2.1.</span> <span class="toc-text">Code</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Explanation-4"><span class="toc-number">1.3.2.2.</span> <span class="toc-text">Explanation</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-preprocess-sources-tokenizer-transformers-PreTrainedTokenizer-gt-Dict"><span class="toc-number">1.3.3.</span> <span class="toc-text">3.preprocess(sources,tokenizer: transformers.PreTrainedTokenizer) -&gt; Dict</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Code-5"><span class="toc-number">1.3.3.1.</span> <span class="toc-text">Code</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-Obtaining-Conversation-Templates-and-Role-Definitions"><span class="toc-number">1.3.3.2.</span> <span class="toc-text">1. Obtaining Conversation Templates and Role Definitions</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-Applying-Prompt-Templates"><span class="toc-number">1.3.3.3.</span> <span class="toc-text">2. Applying Prompt Templates</span></a></li></ol></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/21/NLP%20Insights/evaluation-of-generation-based-large-language-models-llms-opportunities-and-challenges-from-generation-to-judgment.en/" title="evaluation-of-generation-based-large-language-models-llms-opportunities-and-challenges-from-generation-to-judgment">evaluation-of-generation-based-large-language-models-llms-opportunities-and-challenges-from-generation-to-judgment</a><time datetime="2026-02-20T16:00:00.000Z" title="发表于 2026-02-21 00:00:00">2026-02-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/21/NLP%20Insights/evaluation-of-generation-based-large-language-models-llms-opportunities-and-challenges-from-generation-to-judgment.zh-CN/" title="evaluation-of-generation-based-large-language-models-llms-opportunities-and-challenges-from-generation-to-judgment">evaluation-of-generation-based-large-language-models-llms-opportunities-and-challenges-from-generation-to-judgment</a><time datetime="2026-02-20T16:00:00.000Z" title="发表于 2026-02-21 00:00:00">2026-02-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/24/NLP%20Insights/secom-redefining-memory-management-in-conversational-ai.zh-CN/" title="SeCom: 重新定义对话AI的记忆管理">SeCom: 重新定义对话AI的记忆管理</a><time datetime="2025-06-24T08:00:00.000Z" title="发表于 2025-06-24 16:00:00">2025-06-24</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/24/NLP%20Insights/secom-redefining-memory-management-in-conversational-ai.en/" title="SeCom: Redefining Memory Management in Conversational AI">SeCom: Redefining Memory Management in Conversational AI</a><time datetime="2025-06-24T08:00:00.000Z" title="发表于 2025-06-24 16:00:00">2025-06-24</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/03/06/NLP%20Insights/decoder-only%E4%B8%8Eencoder-only%E6%A8%A1%E5%9E%8Bpadding%E7%AD%96%E7%95%A5%E7%9A%84%E5%B7%AE%E5%BC%82.en/" title="Decoder-only与Encoder-only模型Padding策略的差异">Decoder-only与Encoder-only模型Padding策略的差异</a><time datetime="2025-03-06T09:43:10.000Z" title="发表于 2025-03-06 17:43:10">2025-03-06</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 - 2026 By Huiyu Chen</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 6.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.5.4</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Reading Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light and Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle Between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Settings"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back to Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.4"></script><script src="/js/main.js?v=5.5.4"></script><div class="js-pjax"></div><script src="/js/lang-switch.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>