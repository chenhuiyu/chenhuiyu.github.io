<!DOCTYPE html><html lang="[&quot;en&quot;]" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>FastChat Training Script Code Analysis - Train.py 【FastChat Series Part 1】 | 黑头呆鱼进化之旅</title><meta name="author" content="Huiyu Chen"><meta name="copyright" content="Huiyu Chen"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="FastChat Training Script Code Analysis - Train.py 【FastChat Series Part 1】In this article, we delve into the train.py script of FastChat (https:&#x2F;&#x2F;github.com&#x2F;lm-sys&#x2F;FastChat) (https:&#x2F;&#x2F;github.com&#x2F;lm-sys">
<meta property="og:type" content="article">
<meta property="og:title" content="FastChat Training Script Code Analysis - Train.py 【FastChat Series Part 1】">
<meta property="og:url" content="https://chenhuiyu.github.io/2024/02/27/NLP%20Insights/FastChat%20Training%20Script%20Code%20Analysis%20-%20Train.py%20%E3%80%90FastChat%20Series%20Part%201%E3%80%91/index.html">
<meta property="og:site_name" content="黑头呆鱼进化之旅">
<meta property="og:description" content="FastChat Training Script Code Analysis - Train.py 【FastChat Series Part 1】In this article, we delve into the train.py script of FastChat (https:&#x2F;&#x2F;github.com&#x2F;lm-sys&#x2F;FastChat) (https:&#x2F;&#x2F;github.com&#x2F;lm-sys">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://chenhuiyu.github.io/img/avatar.jpeg">
<meta property="article:published_time" content="2024-02-26T17:43:18.000Z">
<meta property="article:modified_time" content="2024-02-26T18:00:42.331Z">
<meta property="article:author" content="Huiyu Chen">
<meta property="article:tag" content="FastChat">
<meta property="article:tag" content="LLM">
<meta property="article:tag" content="Train">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://chenhuiyu.github.io/img/avatar.jpeg"><link rel="shortcut icon" href="/img/favicon.svg"><link rel="canonical" href="https://chenhuiyu.github.io/2024/02/27/NLP%20Insights/FastChat%20Training%20Script%20Code%20Analysis%20-%20Train.py%20%E3%80%90FastChat%20Series%20Part%201%E3%80%91/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//www.google-analytics.com" crossorigin=""/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta name="google-site-verification" content="CySrjlAceN5JQTPeVkDbVQrJgmS-AP_NxBrhTggcHEM"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-E8VVKC5KLZ"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-E8VVKC5KLZ');
</script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"top_n_per_article":1,"unescape":true,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'FastChat Training Script Code Analysis - Train.py 【FastChat Series Part 1】',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-02-27 02:00:42'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="黑头呆鱼进化之旅" type="application/atom+xml">
</head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">Loading...</div></div></div><script>(()=>{
  const $loadingBox = document.getElementById('loading-box')
  const $body = document.body
  const preloader = {
    endLoading: () => {
      $body.style.overflow = ''
      $loadingBox.classList.add('loaded')
    },
    initLoading: () => {
      $body.style.overflow = 'hidden'
      $loadingBox.classList.remove('loaded')
    }
  }

  preloader.initLoading()
  window.addEventListener('load',() => { preloader.endLoading() })

  if (false) {
    document.addEventListener('pjax:send', () => { preloader.initLoading() })
    document.addEventListener('pjax:complete', () => { preloader.endLoading() })
  }
})()</script><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.jpeg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">49</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">39</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">6</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/books/"><i class="fa-fw fas fa-book"></i><span> Books</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/travel/"><i class="fa-fw fas fa-earth"></i><span> Travel</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-heart"></i><span> About</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/sitemap.xml"><i class="fa-fw sitemap fa-sitemap"></i><span> Sitemap</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/background.jpeg')"><nav id="nav"><span id="blog-info"><a href="/" title="黑头呆鱼进化之旅"><span class="site-name">黑头呆鱼进化之旅</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> List</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/books/"><i class="fa-fw fas fa-book"></i><span> Books</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> Movie</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/travel/"><i class="fa-fw fas fa-earth"></i><span> Travel</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-heart"></i><span> About</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/sitemap.xml"><i class="fa-fw sitemap fa-sitemap"></i><span> Sitemap</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">FastChat Training Script Code Analysis - Train.py 【FastChat Series Part 1】</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2024-02-26T17:43:18.000Z" title="Created 2024-02-27 01:43:18">2024-02-27</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2024-02-26T18:00:42.331Z" title="Updated 2024-02-27 02:00:42">2024-02-27</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/NLP-Insights/">NLP Insights</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word count:</span><span class="word-count">3.4k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>21min</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="FastChat Training Script Code Analysis - Train.py 【FastChat Series Part 1】"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="FastChat-Training-Script-Code-Analysis-Train-py-【FastChat-Series-Part-1】"><a href="#FastChat-Training-Script-Code-Analysis-Train-py-【FastChat-Series-Part-1】" class="headerlink" title="FastChat Training Script Code Analysis - Train.py 【FastChat Series Part 1】"></a>FastChat Training Script Code Analysis - Train.py 【FastChat Series Part 1】</h1><p>In this article, we delve into the train.py script of FastChat (<a target="_blank" rel="noopener" href="https://github.com/lm-sys/FastChat">https://github.com/lm-sys/FastChat</a>) (<a target="_blank" rel="noopener" href="https://github.com/lm-sys/FastChat/blob/main/fastchat/train/train.py">https://github.com/lm-sys/FastChat/blob/main/fastchat/train/train.py</a>), a key component for training and optimizing large language models (LLMs). FastChat is an advanced open-source platform focused on developing, deploying, and evaluating chatbots based on LLMs. The platform not only supports top-tier models like Vicuna and MT-Bench but also includes a distributed multi-model service system equipped with a Web UI and RESTful API compatible with OpenAI, enabling efficient training and evaluation of models.</p>
<p>We provide a detailed analysis of the train.py script’s source code. This script is a training script for natural language processing models based on the transformers library, covering critical steps such as data preprocessing, model training, and saving. Our goal is to offer a detailed explanation of each class and function in train.py, including their functionality and role in the overall training process.</p>
<h2 id="1-Importing-Modules"><a href="#1-Importing-Modules" class="headerlink" title="1. Importing Modules"></a>1. Importing Modules</h2><h3 id="1-Built-in-Modules"><a href="#1-Built-in-Modules" class="headerlink" title="1. Built-in Modules"></a>1. Built-in Modules</h3><p>These are standard library modules that come with Python and don’t require additional installation.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> dataclasses <span class="keyword">import</span> dataclass, field</span><br></pre></td></tr></tbody></table></figure>

<p>Imports Python’s <code>dataclasses</code> module for creating classes with default values.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br></pre></td></tr></tbody></table></figure>

<p>Imports the <code>json</code> module for handling JSON format data.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br></pre></td></tr></tbody></table></figure>

<p>Imports the <code>math</code> module for mathematical operations.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pathlib</span><br></pre></td></tr></tbody></table></figure>

<p>Imports the <code>pathlib</code> module for handling file paths.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Dict</span>, <span class="type">Optional</span>, <span class="type">Sequence</span></span><br></pre></td></tr></tbody></table></figure>

<p>Imports the <code>typing</code> module for type annotations.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></tbody></table></figure>

<h3 id="2-Dependency-Libraries"><a href="#2-Dependency-Libraries" class="headerlink" title="2. Dependency Libraries"></a>2. Dependency Libraries</h3><p>These are external libraries typically installed via a package manager like pip.<br>Imports the <code>numpy</code> library, commonly used for scientific computing.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></tbody></table></figure>

<p>Imports <code>PyTorch</code>, a popular deep learning framework.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br></pre></td></tr></tbody></table></figure>

<p>Imports <code>Dataset</code> from <code>torch</code> for creating custom datasets.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> transformers</span><br></pre></td></tr></tbody></table></figure>

<p>Imports the <code>transformers</code> library, a popular natural language processing library.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Trainer</span><br></pre></td></tr></tbody></table></figure>

<p>Imports <code>Trainer</code> from <code>transformers</code> for training models.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers.trainer_pt_utils <span class="keyword">import</span> LabelSmoother</span><br></pre></td></tr></tbody></table></figure>

<p>Imports <code>LabelSmoother</code> from <code>transformers</code> for label smoothing.</p>
<h3 id="3-Project-Specific-Functions"><a href="#3-Project-Specific-Functions" class="headerlink" title="3. Project-Specific Functions"></a>3. Project-Specific Functions</h3><p>These are functions or classes custom-implemented in the Fast Chat project.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fastchat.conversation <span class="keyword">import</span> SeparatorStyle</span><br></pre></td></tr></tbody></table></figure>

<p>Imports <code>SeparatorStyle</code> from the <code>fastchat</code> package for defining conversation separator styles. The <code>SeparatorStyle</code> class is an enumeration class created using Python’s <code>enum</code> module, defining a series of separator styles. Enumerations are a programming concept used to define a named set of constants, making code clearer and more maintainable.</p>
<p>In the <code>SeparatorStyle</code> class, each member represents a specific style of separator. These styles are often used in text processing, especially in scenarios where different sections or elements need to be distinguished. For instance, in handling dialog or textual data, different methods might be needed to differentiate between user input and machine responses.</p>
<p>Regarding the use of the <code>auto()</code> function:</p>
<ul>
<li><code>auto()</code> is a special function provided by Python’s <code>enum</code> module. It automatically assigns a unique value to each member in an enumeration class.</li>
<li>Without using <code>auto()</code>, you would need to manually assign a unique value to each enumeration member. <code>auto()</code> simplifies this process by letting Python handle the assignment of these values automatically.</li>
<li>The values assigned by <code>auto()</code> are usually integers, starting from 1 and increasing sequentially.</li>
</ul>
<p>In the case of the <code>SeparatorStyle</code> class, <code>auto()</code> is used to automatically assign a unique integer value to each type of separator style. For example, <code>ADD_COLON_SINGLE</code>, <code>ADD_COLON_TWO</code>, etc., will be given different integer values.</p>
<p>The names of each enumeration member (such as <code>ADD_COLON_SINGLE</code>, <code>NO_COLON_SINGLE</code>, etc.) typically describe the characteristics of that separator style. For instance, <code>ADD_COLON_SINGLE</code> might represent adding a colon as a separator after a certain element, whereas <code>NO_COLON_SINGLE</code> means no colon is added.</p>
<p>This approach makes referencing and handling these separator styles in the code more convenient and clear. For example, different separator styles can be chosen based on different scenarios or requirements without having to remember their specific values.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fastchat.model.model_adapter <span class="keyword">import</span> get_conversation_template</span><br></pre></td></tr></tbody></table></figure>

<p>Imports <code>get_conversation_template</code> from the <code>fastchat</code> package for obtaining conversation templates. In this code segment, the call logic primarily involves obtaining the default conversation template for a specific model. The call chain is as follows:</p>
<ol>
<li><p><strong>Starting Call - <code>get_conversation_template(model_path: str)</code></strong></p>
<ul>
<li>This function is the starting point of the call chain. It accepts a parameter <code>model_path</code>, specifying the path of the model.</li>
<li>The purpose of this function is to obtain the default conversation template for the given model path.</li>
</ul>
</li>
<li><p><strong>Call <code>get_model_adapter(model_path: str)</code></strong></p>
<ul>
<li>The <code>get_conversation_template</code> function first calls <code>get_model_adapter</code>, passing in the model path.</li>
<li>The purpose of <code>get_model_adapter</code> is to find and return a suitable <code>BaseModelAdapter</code> object for the provided model path.</li>
<li>This function first tries to match the basename of <code>model_path</code>. If no match is found, it tries the full path.</li>
<li>If a suitable adapter is found, it is returned; otherwise, a <code>ValueError</code> is thrown.</li>
</ul>
</li>
<li><p><strong>Execute <code>BaseModelAdapter.get_default_conv_template(model_path: str)</code></strong></p>
<ul>
<li>Once the appropriate model adapter is obtained, <code>get_conversation_template</code> retrieves the default conversation template by calling the <code>get_default_conv_template</code> method of that adapter.</li>
<li>Note that this method is defined in the <code>BaseModelAdapter</code> class but might be overridden in subclasses.</li>
</ul>
</li>
<li><p><strong>Call <code>get_conv_template(name: str)</code></strong></p>
<ul>
<li>Inside the <code>get_default_conv_template</code> method, it calls the <code>get_conv_template</code> function, usually passing a predefined template name like <code>"one_shot"</code>.</li>
<li>The purpose of <code>get_conv_template</code> is to retrieve a specified name’s template from the global registry of conversation templates <code>conv_templates</code>.</li>
</ul>
</li>
<li><p><strong>Obtain and Return a <code>Conversation</code> Object</strong></p>
<ul>
<li>The <code>get_conv_template</code> function returns an instance of the <code>Conversation</code> class, usually copied from the <code>conv_templates</code> dictionary.</li>
<li>Finally, this <code>Conversation</code> instance is returned to the original call site of <code>get_conversation_template</code>.</li>
</ul>
</li>
</ol>
<p>Summarizing the call chain:</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">get_conversation_template(model_path)</span><br><span class="line">  -&gt; get_model_adapter(model_path)</span><br><span class="line">  -&gt; [BaseModelAdapter].get_default_conv_template(model_path)</span><br><span class="line">    -&gt; get_conv_template(name)</span><br><span class="line">      -&gt; Return Conversation Object</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>

<p>In this process, the code navigates through a series of function calls to find a suitable model adapter based on the provided model path and retrieve a specific conversation template from it. This design pattern allows flexibility in providing different conversation templates for different models, enhancing the reusability and extensibility of the code.</p>
<hr>
<h2 id="2-Configuration-Classes"><a href="#2-Configuration-Classes" class="headerlink" title="2. Configuration Classes"></a>2. Configuration Classes</h2><p>These classes are defined using Python’s <code>dataclass</code> decorator and are mainly used for storing configurations and parameters. These classes usually do not contain complex methods or logic but are used to define and store data structures. These classes include:</p>
<ul>
<li><code>ModelArguments</code>: Stores parameters related to the model, like model path, trust in remote code, etc.</li>
<li><code>DataArguments</code>: Stores parameters related to data, like data path, evaluation data path, and whether to use lazy preprocessing.</li>
<li><code>TrainingArguments</code>: Stores parameters related to training, like cache directory, optimizer type, model maximum length, etc. This class extends <code>transformers.TrainingArguments</code> and adds some custom parameters.</li>
</ul>
<p>These classes are mainly used to simplify and organize parameter management in the code, making parameter modification and access more convenient.</p>
<h3 id="1-ModelArguments-Class"><a href="#1-ModelArguments-Class" class="headerlink" title="1. ModelArguments Class"></a>1. ModelArguments Class</h3><h4 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ModelArguments</span>:</span><br><span class="line">    model_name_or_path: <span class="type">Optional</span>[<span class="built_in">str</span>] = field(default=<span class="string">"facebook/opt-125m"</span>)</span><br><span class="line">    trust_remote_code: <span class="built_in">bool</span> = field(</span><br><span class="line">        default=<span class="literal">False</span>,</span><br><span class="line">        metadata={</span><br><span class="line">            <span class="string">"help"</span>: <span class="string">"Whether or not to allow for custom models defined on the Hub in their own modeling files"</span></span><br><span class="line">        },</span><br><span class="line">    )</span><br><span class="line">    padding_side: <span class="built_in">str</span> = field(</span><br><span class="line">        default=<span class="string">"right"</span>, metadata={<span class="string">"help"</span>: <span class="string">"The padding side in tokenizer"</span>}</span><br><span class="line">    )</span><br></pre></td></tr></tbody></table></figure>

<h4 id="Explanation"><a href="#Explanation" class="headerlink" title="Explanation"></a>Explanation</h4><p><code>ModelArguments</code> is a data class (<code>dataclass</code>) used for storing model-related configuration parameters.<br><strong>Attributes:</strong></p>
<ol>
<li><code>model_name_or_path</code>: Specifies the name or path of the pretrained model.</li>
<li><code>trust_remote_code</code>: Whether to allow custom models that have their modeling files defined on the Hub.</li>
<li><code>padding_side</code>: Specifies the padding side in the tokenizer, typically right or left padding.</li>
</ol>
<details>
<summary> Introduction to `@dataclass` decorator, click to expand </summary>
`@dataclass` is a decorator used to automate the generation of special methods like `__init__()`, `__repr__()`, `__eq__()` etc., thus simplifying the writing of data classes. This decorator is part of Python 3.7 and is in the `dataclasses` module.

<p>When you use <code>@dataclass</code> before a class definition, Python automatically adds some special methods based on the fields defined in the class. This is very useful for creating classes that store a small amount of data but do not need complex methods.</p>
<p>Specifically, using <code>@dataclass</code>:</p>
<ol>
<li><p><strong>Automatically generates a constructor (<code>__init__</code> method)</strong>: Python creates an <code>__init__</code> method automatically based on the fields defined in the class, so you don’t need to manually write this method to initialize your class instances.</p>
</li>
<li><p><strong>Automatically generates a <code>__repr__</code> method</strong>: This makes printing the class instances provide a more readable string representation, usually including the class name and its fields and their values.</p>
</li>
<li><p><strong>Automatically generates an <code>__eq__</code> method</strong>: This allows you to use the <code>==</code> operator to compare two instances of the class, comparing the values of the instance fields.</p>
</li>
<li><p><strong>Support for type annotations</strong>: When defining fields, you can use type annotations, which not only help with clarity of code but can also be checked for type correctness using some tools.</p>
</li>
</ol>
<p>In the case of the <code>ModelArguments</code> class, the <code>@dataclass</code> decorator will generate the above-mentioned methods. This means you can easily create an instance of <code>ModelArguments</code>, and when printing or comparing these instances, you will get the expected behavior.</p>
<p>For example, when you create an instance of <code>ModelArguments</code>:</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">args = ModelArguments()</span><br></pre></td></tr></tbody></table></figure>

<p>This will call the automatically generated <code>__init__</code> method, using the default values “facebook/opt-125m” for <code>model_name_or_path</code>, <code>False</code> for <code>trust_remote_code</code>, and “right” for <code>padding_side</code>.</p>
<p>When you print this instance:</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(args)</span><br></pre></td></tr></tbody></table></figure>

<p>This will call the automatically generated <code>__repr__</code> method, showing a detailed view of the class instance, like <code>ModelArguments(model_name_or_path="facebook/opt-125m", trust_remote_code=False, padding_side="right")</code>.</p>
<p>Thus, the <code>@dataclass</code> decorator simplifies the process of creating classes, making the code more concise and maintainable.</p>
<p>Overall, the <code>@dataclass</code> decorator is a convenient tool provided by Python for quickly creating classes mainly used for storing data.</p>
</details>

<h3 id="2-DataArguments-Class"><a href="#2-DataArguments-Class" class="headerlink" title="2. DataArguments Class"></a>2. DataArguments Class</h3><h4 id="Code-1"><a href="#Code-1" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DataArguments</span>:</span><br><span class="line">    data_path: <span class="built_in">str</span> = field(</span><br><span class="line">        default=<span class="literal">None</span>, metadata={<span class="string">"help"</span>: <span class="string">"Path to the training data."</span>}</span><br><span class="line">    )</span><br><span class="line">    eval_data_path: <span class="built_in">str</span> = field(</span><br><span class="line">        default=<span class="literal">None</span>, metadata={<span class="string">"help"</span>: <span class="string">"Path to the evaluation data."</span>}</span><br><span class="line">    )</span><br><span class="line">    lazy_preprocess: <span class="built_in">bool</span> = <span class="literal">False</span></span><br></pre></td></tr></tbody></table></figure>

<h4 id="Explanation-1"><a href="#Explanation-1" class="headerlink" title="Explanation"></a>Explanation</h4><p><strong>DataArguments Class</strong></p>
<ul>
<li><code>DataArguments</code> is also a data class used for storing data-related configuration parameters.</li>
<li>Attributes:<ul>
<li><code>data_path</code>: Path to the training data.</li>
<li><code>eval_data_path</code>: Path to the evaluation data.</li>
<li><code>lazy_preprocess</code>: Whether to use lazy loading for data preprocessing, i.e., load and process data as needed.</li>
</ul>
</li>
</ul>
<h3 id="3-TrainingArguments-Class"><a href="#3-TrainingArguments-Class" class="headerlink" title="3. TrainingArguments Class"></a>3. TrainingArguments Class</h3><h4 id="Code-2"><a href="#Code-2" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TrainingArguments</span>(transformers.TrainingArguments):</span><br><span class="line">    cache_dir: <span class="type">Optional</span>[<span class="built_in">str</span>] = field(default=<span class="literal">None</span>)</span><br><span class="line">    optim: <span class="built_in">str</span> = field(default=<span class="string">"adamw_torch"</span>)</span><br><span class="line">    model_max_length: <span class="built_in">int</span> = field(</span><br><span class="line">        default=<span class="number">512</span>,</span><br><span class="line">        metadata={</span><br><span class="line">            <span class="string">"help"</span>: <span class="string">"Maximum sequence length. Sequences will be right padded (and possibly truncated)."</span></span><br><span class="line">        },</span><br><span class="line">    )</span><br></pre></td></tr></tbody></table></figure>

<h4 id="Explanation-2"><a href="#Explanation-2" class="headerlink" title="Explanation"></a>Explanation</h4><p><code>TrainingArguments</code> class extends <code>transformers.TrainingArguments</code>.</p>
<ol>
<li><p><strong>TrainingArguments Class</strong></p>
<ul>
<li><code>TrainingArguments</code> is a data class that, by extending <code>transformers.TrainingArguments</code>, gains the capability to handle training parameters.</li>
<li>Attributes defined in <code>TrainingArguments</code>:<ul>
<li><code>cache_dir</code>: Specifies the directory path for caching the model and tokenizer.</li>
<li><code>optim</code>: Defines the type of optimizer to use, like <code>'adamw_torch'</code>.</li>
<li><code>model_max_length</code>: Specifies the maximum sequence length the model can handle.</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>transformers.TrainingArguments Class</strong></p>
<ul>
<li><code>transformers.TrainingArguments</code> is a class in the transformers library that is used for configuring various parameters in the model training process.</li>
<li>This class contains a plethora of attributes for controlling the training process, such as:<ul>
<li><code>output_dir</code>: Specifies the directory to save the model and training results.</li>
<li><code>num_train_epochs</code>: Number of training epochs.</li>
<li><code>per_device_train_batch_size</code>: Batch size per device for training.</li>
<li><code>save_steps</code>: Steps interval for saving the model.</li>
<li><code>evaluation_strategy</code>: Strategy for evaluating the model, like at the end of each epoch.</li>
<li><code>learning_rate</code>: Learning rate.</li>
<li><code>warmup_steps</code>: Steps used for warmup in the learning rate schedule.</li>
</ul>
</li>
<li><code>transformers.TrainingArguments</code> also</li>
</ul>
</li>
</ol>
<p> contains many other parameters for fine-tuning the training process, including logging, model saving strategies, learning rate scheduling, and more.</p>
<p>By extending <code>transformers.TrainingArguments</code>, the <code>TrainingArguments</code> class not only inherits all these training parameter configurations but can also add some custom training parameters, like in this case <code>cache_dir</code>, <code>optim</code>, and <code>model_max_length</code>. This approach enhances code reusability and flexibility, allowing you to adjust and extend training configurations as per the specific requirements of your project.</p>
<h2 id="3-Functional-Utility-Functions"><a href="#3-Functional-Utility-Functions" class="headerlink" title="3. Functional Utility Functions"></a>3. Functional Utility Functions</h2><h3 id="1-rank0-print-args"><a href="#1-rank0-print-args" class="headerlink" title="1. rank0_print(*args)"></a>1. rank0_print(*args)</h3><h4 id="Code-3"><a href="#Code-3" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">local_rank = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">rank0_print</span>(<span class="params">*args</span>):</span><br><span class="line">    <span class="keyword">if</span> local_rank == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(*args)</span><br></pre></td></tr></tbody></table></figure>

<h4 id="Explanation-3"><a href="#Explanation-3" class="headerlink" title="Explanation"></a>Explanation</h4><p>Defines a global variable local_rank for distributed training.<br>Defines a function rank0_print to print information only if local_rank is 0, used for controlling output in distributed training. This way, repetitive printing of the same information across multiple nodes is avoided, making the output clearer and more concise.</p>
<ul>
<li>Used to print information only on the main node (rank 0) in a distributed training environment.</li>
<li>Parameters: A variable number of arguments for printing.</li>
</ul>
<h3 id="2-trainer-save-model-safe-trainer-transformers-Trainer"><a href="#2-trainer-save-model-safe-trainer-transformers-Trainer" class="headerlink" title="2. trainer_save_model_safe(trainer: transformers.Trainer)"></a>2. <code>trainer_save_model_safe(trainer: transformers.Trainer)</code></h3><h4 id="Code-4"><a href="#Code-4" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">trainer_save_model_safe</span>(<span class="params">trainer: transformers.Trainer</span>):</span><br><span class="line">    <span class="keyword">from</span> torch.distributed.fsdp <span class="keyword">import</span> FullyShardedDataParallel <span class="keyword">as</span> FSDP</span><br><span class="line">    <span class="keyword">from</span> torch.distributed.fsdp <span class="keyword">import</span> StateDictType, FullStateDictConfig</span><br><span class="line"></span><br><span class="line">    save_policy = FullStateDictConfig(offload_to_cpu=<span class="literal">True</span>, rank0_only=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">with</span> FSDP.state_dict_type(</span><br><span class="line">        trainer.model, StateDictType.FULL_STATE_DICT, save_policy</span><br><span class="line">    ):</span><br><span class="line">        trainer.save_model()</span><br></pre></td></tr></tbody></table></figure>

<p>The function <code>trainer_save_model_safe(trainer: transformers.Trainer)</code> aims to safely save models trained with the PyTorch distributed framework. Let’s delve into the details of this function and its key components.</p>
<h4 id="Explanation-4"><a href="#Explanation-4" class="headerlink" title="Explanation"></a>Explanation</h4><ol>
<li><p>Parameters:</p>
<ul>
<li><code>trainer</code>: An instance of <code>transformers.Trainer</code>. This class is one of the core components of the Hugging Face Transformers library, used for training and evaluating models.</li>
</ul>
</li>
<li><p>Functionality:</p>
<ul>
<li>The main purpose of this function is to safely save models in a distributed training environment. It particularly considers the model saving strategy when using Fully Sharded Data Parallel (FSDP).</li>
</ul>
</li>
<li><p>FSDP</p>
<ul>
<li><strong>FullyShardedDataParallel (FSDP)</strong><ul>
<li>This is a component of PyTorch’s distributed training framework. FSDP helps reduce memory usage on each GPU by sharding model parameters across multiple GPUs, allowing the training of larger models.</li>
<li>In this context, FSDP is primarily used for handling and saving model states in distributed training.</li>
</ul>
</li>
<li><strong>StateDictType</strong><ul>
<li>This is an enumeration type that defines how to save the model’s state dictionary. In FSDP environments, saving and loading model states might require special handling.</li>
</ul>
</li>
<li><strong>FullStateDictConfig</strong><ul>
<li>This class configures parameters for saving the full state dictionary. It’s part of FSDP’s functionality and is used to control how the model state is saved.</li>
</ul>
</li>
</ul>
</li>
<li><p>Function Implementation</p>
<ul>
<li><strong>Setting Save Policy</strong><ul>
<li><code>save_policy = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)</code> creates a save policy. Here, two key parameters are specified:<ul>
<li><code>offload_to_cpu</code>: Offload model parameters to CPU before saving the state dictionary, which helps reduce GPU memory usage.</li>
<li><code>rank0_only</code>: Save the model only on rank 0 (usually the main node). In distributed training, this avoids saving the same model copy on every node, saving storage space.</li>
</ul>
</li>
</ul>
</li>
<li><strong>Saving the Model</strong><ul>
<li>Using the <code>with FSDP.state_dict_type(trainer.model, StateDictType.FULL_STATE_DICT, save_policy)</code> context manager, the type and policy for saving the model’s state dictionary are set.</li>
<li>Within this context, <code>trainer.save_model()</code> is called to save the model. Due to the <code>save_policy</code>, the model is saved securely following the specified configuration.</li>
</ul>
</li>
</ul>
</li>
</ol>
<p>The function <code>trainer_save_model_safe</code> encapsulates a safe model saving logic, particularly for scenarios involving PyTorch’s FSDP in distributed training. It ensures that only a complete model state is saved on one node and offloads model parameters to CPU before saving, optimizing memory usage and storage efficiency. This is crucial for training large models and managing large-scale distributed training environments.</p>
<h3 id="3-preprocess-sources-tokenizer-transformers-PreTrainedTokenizer-gt-Dict"><a href="#3-preprocess-sources-tokenizer-transformers-PreTrainedTokenizer-gt-Dict" class="headerlink" title="3.preprocess(sources,tokenizer: transformers.PreTrainedTokenizer) -> Dict"></a>3.<code>preprocess(sources,tokenizer: transformers.PreTrainedTokenizer) -&gt; Dict</code></h3><h4 id="Code-5"><a href="#Code-5" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess</span>(<span class="params"></span></span><br><span class="line"><span class="params">    sources,</span></span><br><span class="line"><span class="params">    tokenizer: transformers.PreTrainedTokenizer,</span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="type">Dict</span>:</span><br><span class="line">    conv = get_conversation_template(<span class="string">"vicuna"</span>)</span><br><span class="line">    roles = {<span class="string">"human"</span>: conv.roles[<span class="number">0</span>], <span class="string">"gpt"</span>: conv.roles[<span class="number">1</span>]}</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Apply prompt templates</span></span><br><span class="line">    conversations = []</span><br><span class="line">    <span class="keyword">for</span> i, source <span class="keyword">in</span> <span class="built_in">enumerate</span>(sources):</span><br><span class="line">        <span class="keyword">if</span> roles[source[<span class="number">0</span>][<span class="string">"from"</span>]] != conv.roles[<span class="number">0</span>]:</span><br><span class="line">            <span class="comment"># Skip the first one if it is not from human</span></span><br><span class="line">            source = source[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">        conv.messages = []</span><br><span class="line">        <span class="keyword">for</span> j, sentence <span class="keyword">in</span> <span class="built_in">enumerate</span>(source):</span><br><span class="line">            role = roles[sentence[<span class="string">"from"</span>]]</span><br><span class="line">            <span class="keyword">assert</span> role == conv.roles[j % <span class="number">2</span>], <span class="string">f"<span class="subst">{i}</span>"</span></span><br><span class="line">            conv.append_message(role, sentence[<span class="string">"value"</span>])</span><br><span class="line">        conversations.append(conv.get_prompt())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Tokenize conversations</span></span><br><span class="line">    input_ids = tokenizer(</span><br><span class="line">        conversations,</span><br><span class="line">        return_tensors=<span class="string">"pt"</span>,</span><br><span class="line">        padding=<span class="string">"max_length"</span>,</span><br><span class="line">        max_length=tokenizer.model_max_length,</span><br><span class="line">        truncation=<span class="literal">True</span>,</span><br><span class="line">    ).input_ids</span><br><span class="line">    targets = input_ids.clone()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> conv.sep_style == SeparatorStyle.ADD_COLON_TWO</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Mask targets. Only compute loss on the assistant outputs.</span></span><br><span class="line">    sep = conv.sep + conv.roles[<span class="number">1</span>] + <span class="string">": "</span></span><br><span class="line">    <span class="keyword">for</span> conversation, target <span class="keyword">in</span> <span class="built_in">zip</span>(conversations, targets):</span><br><span class="line">        total_len = <span class="built_in">int</span>(target.ne(tokenizer.pad_token_id).<span class="built_in">sum</span>())</span><br><span class="line"></span><br><span class="line">        turns = conversation.split(conv.sep2)</span><br><span class="line">        cur_len = <span class="number">1</span></span><br><span class="line">        target[:cur_len] = IGNORE_TOKEN_ID</span><br><span class="line">        <span class="keyword">for</span> i, turn <span class="keyword">in</span> <span class="built_in">enumerate</span>(turns):</span><br><span class="line">            <span class="keyword">if</span> turn == <span class="string">""</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            turn_len = <span class="built_in">len</span>(tokenizer(turn).input_ids)</span><br><span class="line"></span><br><span class="line">            parts = turn.split(sep)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(parts) != <span class="number">2</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            parts[<span class="number">0</span>] += sep</span><br><span class="line">            <span class="comment"># "-2" is hardcoded for the Llama tokenizer to make the offset correct.</span></span><br><span class="line">            instruction_len = <span class="built_in">len</span>(tokenizer(parts[<span class="number">0</span>]).input_ids) - <span class="number">2</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i != <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> tokenizer.legacy:</span><br><span class="line">                <span class="comment"># The legacy and non-legacy modes handle special tokens differently</span></span><br><span class="line">                instruction_len -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Ignore the user instructions</span></span><br><span class="line">            target[cur_len : cur_len + instruction_len] = IGNORE_TOKEN_ID</span><br><span class="line">            cur_len += turn_len</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i != <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> tokenizer.legacy:</span><br><span class="line">                <span class="comment"># The legacy and non-legacy modes handle special tokens differently</span></span><br><span class="line">                cur_len -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        target[cur_len:] = IGNORE_TOKEN_ID</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="literal">False</span>:  <span class="comment"># Inspect and check the correctness of masking</span></span><br><span class="line">            z = target.clone()</span><br><span class="line">            z = torch.where(z == IGNORE_TOKEN_ID, tokenizer.unk_token_id, z)</span><br><span class="line">            rank0_print(tokenizer.decode(z))</span><br><span class="line">            exit()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> cur_len &lt; tokenizer.model_max_length:</span><br><span class="line">           </span><br><span class="line"></span><br><span class="line"> <span class="keyword">if</span> cur_len != total_len:</span><br><span class="line">                target[:] = IGNORE_TOKEN_ID</span><br><span class="line">                rank0_print(</span><br><span class="line">                    <span class="string">f"WARNING: tokenization mismatch: <span class="subst">{cur_len}</span> vs. <span class="subst">{total_len}</span>."</span></span><br><span class="line">                    <span class="string">f" #turn = <span class="subst">{<span class="built_in">len</span>(turns) - <span class="number">1</span>}</span>. (ignored)"</span></span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">dict</span>(</span><br><span class="line">        input_ids=input_ids,</span><br><span class="line">        labels=targets,</span><br><span class="line">        attention_mask=input_ids.ne(tokenizer.pad_token_id),</span><br><span class="line">    )</span><br></pre></td></tr></tbody></table></figure>

<p>The function <code>preprocess(sources, tokenizer: transformers.PreTrainedTokenizer) -&gt; Dict</code> is intended for preprocessing dialogue data to be suitable for training machine learning models. This function can be broken down into several main parts for a more detailed explanation:</p>
<h4 id="1-Obtaining-Conversation-Templates-and-Role-Definitions"><a href="#1-Obtaining-Conversation-Templates-and-Role-Definitions" class="headerlink" title="1. Obtaining Conversation Templates and Role Definitions"></a>1. Obtaining Conversation Templates and Role Definitions</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conv = get_conversation_template(<span class="string">"vicuna"</span>)</span><br><span class="line">roles = {<span class="string">"human"</span>: conv.roles[<span class="number">0</span>], <span class="string">"gpt"</span>: conv.roles[<span class="number">1</span>]}</span><br></pre></td></tr></tbody></table></figure>

<ul>
<li><strong>Functionality</strong>: Initializes conversation templates and defines the roles of dialogue participants.</li>
<li><strong>Implementation</strong>:<ul>
<li><code>conv = get_conversation_template("vicuna")</code> obtains the conversation template for a specified model (e.g., “vicuna”).</li>
<li>The <code>roles</code> dictionary maps “human” and “gpt” to the roles defined in the conversation template.</li>
</ul>
</li>
<li><strong>Example</strong>:<ul>
<li>If the conversation template is for “vicuna”, then <code>roles</code> might map “human” to “user” and “gpt” to “assistant”. For example, <code>{'human': 'USER', 'gpt': 'ASSISTANT'}</code>.</li>
</ul>
</li>
</ul>
<h4 id="2-Applying-Prompt-Templates"><a href="#2-Applying-Prompt-Templates" class="headerlink" title="2. Applying Prompt Templates"></a>2. Applying Prompt Templates</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Apply prompt templates</span></span><br><span class="line">conversations = []</span><br><span class="line"><span class="keyword">for</span> i, source <span class="keyword">in</span> <span class="built_in">enumerate</span>(sources):</span><br><span class="line">    <span class="keyword">if</span> roles[source[<span class="number">0</span>][<span class="string">"from"</span>]] != conv.roles[<span class="number">0</span>]:</span><br><span class="line">        <span class="comment"># Skip the first one if it is not from human</span></span><br><span class="line">        source = source[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">    conv.messages = []</span><br><span class="line">    <span class="keyword">for</span> j, sentence <span class="keyword">in</span> <span class="built_in">enumerate</span>(source):</span><br><span class="line">        role = roles[sentence[<span class="string">"from"</span>]]</span><br><span class="line">        <span class="keyword">assert</span> role == conv.roles[j % <span class="number">2</span>], <span class="string">f"<span class="subst">{i}</span>"</span></span><br><span class="line">        conv.append_message(role, sentence[<span class="string">"value"</span>])</span><br><span class="line">    conversations.append(conv.get_prompt())</span><br></pre></td></tr></tbody></table></figure>

<ul>
<li><strong>Functionality</strong>: Applies prompt templates to source data to construct dialogues.</li>
<li><strong>Implementation</strong>:<ul>
<li>Iterates through <code>sources</code> (original dialogue data), transforming each dialogue source into a conversation in template format.</li>
<li>If the first part of a dialogue is not initiated by the “human” role, it skips that part.</li>
<li>Assigns a role to each sentence and adds it to the conversation template.</li>
<li>Ultimately, each processed dialogue is added to the <code>conversations</code> list.</li>
</ul>
</li>
<li><strong>Example</strong>:<ul>
<li>Suppose we have a source which is the first item in dummy input: <code>python source = [{'from': 'human', 'value': 'Who are you?'}, {'from': 'gpt', 'value': 'I am Vicuna, a language model trained by researchers from Large Model Systems Organization (LMSYS).'}, {'from': 'human', 'value': 'Have a nice day!'}, {'from': 'gpt', 'value': 'You too!'}]</code></li>
<li><code>conversations</code> under the Vicuna template, using <code>SeparatorStyle.ADD_COLON_TWO</code> as the separator style, might look like [“A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user’s questions. USER: Who are you? ASSISTANT: I am Vicuna, a language model trained by researchers from Large Model Systems Organization (LMSYS).USER: Have a nice day! ASSISTANT: You too!“]</li>
<li><details>
<summary> Implementation of get_prompt </summary>
The `get_prompt` method implementation varies depending on the `SeparatorStyle`. Below is a table detailing the `get_prompt` method for various styles, along with English examples:

<table>
<thead>
<tr>
<th>Separator Style (<code>SeparatorStyle</code>)</th>
<th>Description</th>
<th>Example</th>
</tr>
</thead>
<tbody><tr>
<td><code>ADD_COLON_SINGLE</code></td>
<td>Adds a colon and separator after each message.</td>
<td>USER: Hello there!\nASSISTANT: Hi, how can I help?\n</td>
</tr>
<tr>
<td><code>ADD_COLON_TWO</code></td>
<td>Uses two alternating separators, usually between different roles.</td>
<td>USER: What’s the weather?\nASSISTANT: It’s sunny today.\n\n</td>
</tr>
<tr>
<td><code>ADD_COLON_SPACE_SINGLE</code></td>
<td>Adds a colon, space, and separator after each message.</td>
<td>USER: Can you book a flight?\nASSISTANT: Sure, where to?\n</td>
</tr>
<tr>
<td><code>NO_COLON_SINGLE</code></td>
<td>Messages directly follow roles without a colon, followed by a separator.</td>
<td>USERWhat are you doing?\nASSISTANTI’m here to assist you.\n</td>
</tr>
<tr>
<td><code>NO_COLON_TWO</code></td>
<td>No colons, with two alternating separators.</td>
<td>USERHow’s the project going?\nASSISTANTIt’s on track.\n\n</td>
</tr>
<tr>
<td><code>ADD_NEW_LINE_SINGLE</code></td>
<td>Each message is preceded by a newline, followed by a separator.</td>
<td>USER\nHow can I reset my password?\nASSISTANT\nYou can reset it via email.\n</td>
</tr>
<tr>
<td><code>RWKV</code></td>
<td>Special format, usually for specific models.</td>
<td>USER: What is AI?\n\nASSISTANT: AI stands for Artificial Intelligence.\n\n</td>
</tr>
<tr>
<td><code>LLAMA2</code></td>
<td>Special label format for specific models.</td>
<td>[INST] USER How does blockchain work?\nASSISTANT It is a distributed ledger.\n\n</td>
</tr>
<tr>
<td><code>CHATGLM</code></td>
<td>Specific format for <code>CHATGLM</code> model.</td>
<td>[Round 1]\nUSER: Tell me a joke.\nASSISTANT: Why did the chicken cross the road?\n</td>
</tr>
<tr>
<td><code>CHATML</code></td>
<td>Similar to <code>CHATGLM</code>, but with newlines before and after each message.</td>
<td>USER\nDo you like music?\n\nASSISTANT\nYes, I enjoy many genres.\n\n</td>
</tr>
<tr>
<td><code>CHATGLM3</code></td>
<td>Format for <code>CHATGLM3</code> model.</td>
<td>USER\nCan you play chess?\nASSISTANTYes, I can play.\n</td>
</tr>
<tr>
<td><code>CHATINTERN</code></td>
<td>Format for <code>CHATINTERN</code> model, using special markers.</td>
<td><s>USER:Where is the nearest ATM?<s>\nASSISTANT:It’s next to the post office.\n</s></s></td>
</tr>
<tr>
<td><code>DOLLY</code></td>
<td>Specific format for <code>DOLLY</code> model.</td>
<td>USER:\nWhat is quantum computing?\nASSISTANT:\nIt involves computation using quantum-mechanical phenomena.\n\n</td>
</tr>
<tr>
<td><code>PHOENIX</code></td>
<td>For <code>PHOENIX</code> model, messages are wrapped in special markers.</td>
<td>USER: <s>How to bake a cake?</s>\nASSISTANT: <s>You need flour, sugar, and eggs.</s>\n</td>
</tr>
<tr>
<td><code>ROBIN</code></td>
<td>Similar to <code>ADD_NEW_LINE_SINGLE</code>, but with a newline after roles.</td>
<td>USER:\nIs AI dangerous?\nASSISTANT:\nIt depends on how it’s used.\n</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
</details></li>
</ul>
</li>
</ul>
</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/FastChat/">FastChat</a><a class="post-meta__tags" href="/tags/LLM/">LLM</a><a class="post-meta__tags" href="/tags/Train/">Train</a></div><div class="post_share"><div class="social-share" data-image="/img/avatar.jpeg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2024/02/28/NLP%20Insights/Gorilla:%20Large%20Language%20Model%20Connected%20with%20Massive%20APIs/" title="Gorilla LLM 大语言模型简介"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">Gorilla LLM 大语言模型简介</div></div></a></div><div class="next-post pull-right"><a href="/2024/02/27/NLP%20Insights/FastChat%20%E8%AE%AD%E7%BB%83%E8%84%9A%E6%9C%AC%E4%BB%A3%E7%A0%81%E9%80%90%E8%A1%8C%E8%A7%A3%E6%9E%90-Train.py%20%E3%80%90FastChat%20%E7%B3%BB%E5%88%97%E7%AC%AC%201%20%E7%AF%87%E3%80%91/" title="FastChat 训练脚本代码逐行解析-Train.py 【FastChat 系列第 1 篇】"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">FastChat 训练脚本代码逐行解析-Train.py 【FastChat 系列第 1 篇】</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2024/02/27/NLP%20Insights/FastChat%20%E8%AE%AD%E7%BB%83%E8%84%9A%E6%9C%AC%E4%BB%A3%E7%A0%81%E9%80%90%E8%A1%8C%E8%A7%A3%E6%9E%90-Train.py%20%E3%80%90FastChat%20%E7%B3%BB%E5%88%97%E7%AC%AC%201%20%E7%AF%87%E3%80%91/" title="FastChat 训练脚本代码逐行解析-Train.py 【FastChat 系列第 1 篇】"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-27</div><div class="title">FastChat 训练脚本代码逐行解析-Train.py 【FastChat 系列第 1 篇】</div></div></a></div><div><a href="/2024/02/28/NLP%20Insights/Gorilla:%20Large%20Language%20Model%20Connected%20with%20Massive%20APIs/" title="Gorilla LLM 大语言模型简介"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-28</div><div class="title">Gorilla LLM 大语言模型简介</div></div></a></div><div><a href="/2023/08/02/NLP%20Insights/LLAMA2/" title="Training Llama 2 Model on Single GPU with int8 Quantization and LoRA"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-08-02</div><div class="title">Training Llama 2 Model on Single GPU with int8 Quantization and LoRA</div></div></a></div><div><a href="/2023/07/28/NLP%20Insights/LONGNET/" title="LONGNET - Scaling Transformers to 1,000,000,000 Tokens"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-07-28</div><div class="title">LONGNET - Scaling Transformers to 1,000,000,000 Tokens</div></div></a></div><div><a href="/2023/07/27/NLP%20Insights/Prompt%20Engineering/" title="Prompt Engineering"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-07-27</div><div class="title">Prompt Engineering</div></div></a></div><div><a href="/2024/02/19/NLP%20Insights/Understanding%20the%20Differences%20Between%20Fine-tuning%20and%20Further%20Pretraining%20in%20Large%20Language%20Models/" title="Understanding the Differences Between Fine-tuning and Further Pretraining in Large Language Models"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-19</div><div class="title">Understanding the Differences Between Fine-tuning and Further Pretraining in Large Language Models</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpeg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Huiyu Chen</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">49</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">39</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/chenhuiyu"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/chenhuiyu" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:chenhuiyu1997@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">Welcome to my blog! I'm Huiyu, a data scientist in Singapore, passionate about NLP and AI. Here, I share insights on tech and sprinkle in some travel stories from my adventures.</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#FastChat-Training-Script-Code-Analysis-Train-py-%E3%80%90FastChat-Series-Part-1%E3%80%91"><span class="toc-number">1.</span> <span class="toc-text">FastChat Training Script Code Analysis - Train.py 【FastChat Series Part 1】</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Importing-Modules"><span class="toc-number">1.1.</span> <span class="toc-text">1. Importing Modules</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-Built-in-Modules"><span class="toc-number">1.1.1.</span> <span class="toc-text">1. Built-in Modules</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-Dependency-Libraries"><span class="toc-number">1.1.2.</span> <span class="toc-text">2. Dependency Libraries</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-Project-Specific-Functions"><span class="toc-number">1.1.3.</span> <span class="toc-text">3. Project-Specific Functions</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Configuration-Classes"><span class="toc-number">1.2.</span> <span class="toc-text">2. Configuration Classes</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-ModelArguments-Class"><span class="toc-number">1.2.1.</span> <span class="toc-text">1. ModelArguments Class</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Code"><span class="toc-number">1.2.1.1.</span> <span class="toc-text">Code</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Explanation"><span class="toc-number">1.2.1.2.</span> <span class="toc-text">Explanation</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-DataArguments-Class"><span class="toc-number">1.2.2.</span> <span class="toc-text">2. DataArguments Class</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Code-1"><span class="toc-number">1.2.2.1.</span> <span class="toc-text">Code</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Explanation-1"><span class="toc-number">1.2.2.2.</span> <span class="toc-text">Explanation</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-TrainingArguments-Class"><span class="toc-number">1.2.3.</span> <span class="toc-text">3. TrainingArguments Class</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Code-2"><span class="toc-number">1.2.3.1.</span> <span class="toc-text">Code</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Explanation-2"><span class="toc-number">1.2.3.2.</span> <span class="toc-text">Explanation</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Functional-Utility-Functions"><span class="toc-number">1.3.</span> <span class="toc-text">3. Functional Utility Functions</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-rank0-print-args"><span class="toc-number">1.3.1.</span> <span class="toc-text">1. rank0_print(*args)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Code-3"><span class="toc-number">1.3.1.1.</span> <span class="toc-text">Code</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Explanation-3"><span class="toc-number">1.3.1.2.</span> <span class="toc-text">Explanation</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-trainer-save-model-safe-trainer-transformers-Trainer"><span class="toc-number">1.3.2.</span> <span class="toc-text">2. trainer_save_model_safe(trainer: transformers.Trainer)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Code-4"><span class="toc-number">1.3.2.1.</span> <span class="toc-text">Code</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Explanation-4"><span class="toc-number">1.3.2.2.</span> <span class="toc-text">Explanation</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-preprocess-sources-tokenizer-transformers-PreTrainedTokenizer-gt-Dict"><span class="toc-number">1.3.3.</span> <span class="toc-text">3.preprocess(sources,tokenizer: transformers.PreTrainedTokenizer) -&gt; Dict</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Code-5"><span class="toc-number">1.3.3.1.</span> <span class="toc-text">Code</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-Obtaining-Conversation-Templates-and-Role-Definitions"><span class="toc-number">1.3.3.2.</span> <span class="toc-text">1. Obtaining Conversation Templates and Role Definitions</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-Applying-Prompt-Templates"><span class="toc-number">1.3.3.3.</span> <span class="toc-text">2. Applying Prompt Templates</span></a></li></ol></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/08/13/NLP%20Insights/%E4%BD%BF%E7%94%A8%E5%8E%8B%E7%BC%A9%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA%E8%BF%9B%E8%A1%8C%E6%9C%AC%E5%9C%B0%20LLM%20%E7%9A%84%E5%BF%AB%E9%80%9F%20JSON%20%E8%A7%A3%E7%A0%81/" title="使用压缩有限状态机进行本地 LLM 的快速 JSON 解码">使用压缩有限状态机进行本地 LLM 的快速 JSON 解码</a><time datetime="2024-08-13T08:12:10.000Z" title="Created 2024-08-13 16:12:10">2024-08-13</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/08/07/NLP%20Insights/Running%20Fine-tuned%20Gemma-2-2b-it%20with%20vLLM/" title="Detailed Steps for Running Fine-tuned Gemma-2-2b-it with vLLM">Detailed Steps for Running Fine-tuned Gemma-2-2b-it with vLLM</a><time datetime="2024-08-07T02:30:00.000Z" title="Created 2024-08-07 10:30:00">2024-08-07</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/08/07/NLP%20Insights/%E4%BD%BF%E7%94%A8vLLM%E8%BF%90%E8%A1%8C%E5%BE%AE%E8%B0%83%E5%90%8E%E7%9A%84Gemma-2/" title="使用vLLM运行微调后的Gemma-2">使用vLLM运行微调后的Gemma-2</a><time datetime="2024-08-07T02:30:00.000Z" title="Created 2024-08-07 10:30:00">2024-08-07</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/04/17/NLP%20Insights/%20%E5%9B%B0%E6%83%91%E5%BA%A6(PPL)/" title="如何准确计算固定长度模型的困惑度（PPL）">如何准确计算固定长度模型的困惑度（PPL）</a><time datetime="2024-04-17T04:00:00.000Z" title="Created 2024-04-17 12:00:00">2024-04-17</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2024/03/08/Code%20Chronicles/2834.%20%E6%89%BE%E5%87%BA%E7%BE%8E%E4%B8%BD%E6%95%B0%E7%BB%84%E7%9A%84%E6%9C%80%E5%B0%8F%E5%92%8C/" title="【Python题解】2834. 找出美丽数组的最小和">【Python题解】2834. 找出美丽数组的最小和</a><time datetime="2024-03-08T15:31:44.000Z" title="Created 2024-03-08 23:31:44">2024-03-08</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By Huiyu Chen</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="Switch Between Traditional Chinese And Simplified Chinese">繁</button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="99" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>