<!DOCTYPE html><html class="appearance-auto" lang="[&quot;en&quot;]"><head><meta charset="UTF-8"><title>FastChat Training Script Code Analysis - Train.py 【FastChat Series Part 1】</title><meta name="description" content="May the Force be with you"><meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no, initial-scale=1"><!-- Google Analytics --><!-- End Google Analytics -->
<!-- Baidu Analytics --><!-- End Baidu Analytics --><link rel="icon" href="/images/favicon.ico"><link rel="stylesheet" href="/style/common/bulma.css"><link rel="stylesheet" href="/style/base.css"><link rel="stylesheet" href="/style/common/helper.css"><script src="/js/common.js"></script><link rel="stylesheet" href="/style/post.css"><link rel="stylesheet" href="/style/themes/highlight-theme-light.css"><link rel="stylesheet" href="/style/common/jquery.fancybox.min.css"><script src="/js/highlight.pack.js"></script><meta name="description" content="FastChat Training Script Code Analysis - Train.py 【FastChat Series Part 1】In this article, we delve into the train.py script of FastChat (https://github.com/lm-sys/FastChat) (https://github.com/lm-sys/FastChat/blob/main/fastchat/train/train.py), a key component for training and optimizing large language models (LLMs). FastChat is an advanced open-source pl.."><meta name="generator" content="Hexo 6.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="黑头呆鱼进化之旅" type="application/atom+xml">
</head><body class="is-flex is-flex-direction-column"><header class="header-widget is-flex-shrink-0 is-hidden-mobile"><div class="container is-fullhd is-flex is-justify-content-space-between is-align-items-center is-full-height"><section class="is-hidden-mobile is-flex-shrink-0"><h2><a href="/">user's blog</a></h2></section><h3 class="is-hidden-mobile is-family-serif is-full-height is-flex is-align-items-center is-flex-shrink-0"><div class="is-full-height" id="postTopic"><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">FastChat Training Script Code Analysis - Train.py 【FastChat Ser..</p><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">Click back to the top</p></div></h3><aside class="is-flex-shrink-0"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></aside></div></header><header class="is-flex header-widget is-flex-shrink-0 is-align-items-center is-justify-content-center is-hidden-tablet"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></header><main><main class="container is-max-widescreen content section post-page pt-4 px-4"><div class="columns is-flex-desktop is-justify-content-center is-flex-direction-row-reverse"><div class="column is-3 is-hidden-mobile"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#FastChat-Training-Script-Code-Analysis-Train-py-%E3%80%90FastChat-Series-Part-1%E3%80%91"><span class="toc-text">FastChat Training Script Code Analysis - Train.py 【FastChat Series Part 1】</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Importing-Modules"><span class="toc-text">1. Importing Modules</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-Built-in-Modules"><span class="toc-text">1. Built-in Modules</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-Dependency-Libraries"><span class="toc-text">2. Dependency Libraries</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-Project-Specific-Functions"><span class="toc-text">3. Project-Specific Functions</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Configuration-Classes"><span class="toc-text">2. Configuration Classes</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-ModelArguments-Class"><span class="toc-text">1. ModelArguments Class</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Code"><span class="toc-text">Code</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Explanation"><span class="toc-text">Explanation</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-DataArguments-Class"><span class="toc-text">2. DataArguments Class</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Code-1"><span class="toc-text">Code</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Explanation-1"><span class="toc-text">Explanation</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-TrainingArguments-Class"><span class="toc-text">3. TrainingArguments Class</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Code-2"><span class="toc-text">Code</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Explanation-2"><span class="toc-text">Explanation</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Functional-Utility-Functions"><span class="toc-text">3. Functional Utility Functions</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-rank0-print-args"><span class="toc-text">1. rank0_print(*args)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Code-3"><span class="toc-text">Code</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Explanation-3"><span class="toc-text">Explanation</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-trainer-save-model-safe-trainer-transformers-Trainer"><span class="toc-text">2. trainer_save_model_safe(trainer: transformers.Trainer)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Code-4"><span class="toc-text">Code</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Explanation-4"><span class="toc-text">Explanation</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-preprocess-sources-tokenizer-transformers-PreTrainedTokenizer-gt-Dict"><span class="toc-text">3.preprocess(sources,tokenizer: transformers.PreTrainedTokenizer) -&gt; Dict</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Code-5"><span class="toc-text">Code</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-Obtaining-Conversation-Templates-and-Role-Definitions"><span class="toc-text">1. Obtaining Conversation Templates and Role Definitions</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-Applying-Prompt-Templates"><span class="toc-text">2. Applying Prompt Templates</span></a></li></ol></li></ol></li></ol></li></ol></div><div class="column is-9"><header class="my-4"><a href="/tags/LLM"><i class="tag post-item-tag">LLM</i></a><a href="/tags/FastChat"><i class="tag post-item-tag">FastChat</i></a><a href="/tags/Train"><i class="tag post-item-tag">Train</i></a></header><h1 class="mt-0 mb-1 is-family-serif" id="postTitle">FastChat Training Script Code Analysis - Train.py 【FastChat Series Part 1】</h1><time class="has-text-grey" datetime="2024-02-26T17:43:18.000Z">2024-02-27</time><article class="mt-2 post-content"><h1 id="FastChat-Training-Script-Code-Analysis-Train-py-【FastChat-Series-Part-1】"><a href="#FastChat-Training-Script-Code-Analysis-Train-py-【FastChat-Series-Part-1】" class="headerlink" title="FastChat Training Script Code Analysis - Train.py 【FastChat Series Part 1】"></a>FastChat Training Script Code Analysis - Train.py 【FastChat Series Part 1】</h1><p>In this article, we delve into the train.py script of FastChat (<a target="_blank" rel="noopener" href="https://github.com/lm-sys/FastChat">https://github.com/lm-sys/FastChat</a>) (<a target="_blank" rel="noopener" href="https://github.com/lm-sys/FastChat/blob/main/fastchat/train/train.py">https://github.com/lm-sys/FastChat/blob/main/fastchat/train/train.py</a>), a key component for training and optimizing large language models (LLMs). FastChat is an advanced open-source platform focused on developing, deploying, and evaluating chatbots based on LLMs. The platform not only supports top-tier models like Vicuna and MT-Bench but also includes a distributed multi-model service system equipped with a Web UI and RESTful API compatible with OpenAI, enabling efficient training and evaluation of models.</p>
<p>We provide a detailed analysis of the train.py script’s source code. This script is a training script for natural language processing models based on the transformers library, covering critical steps such as data preprocessing, model training, and saving. Our goal is to offer a detailed explanation of each class and function in train.py, including their functionality and role in the overall training process.</p>
<h2 id="1-Importing-Modules"><a href="#1-Importing-Modules" class="headerlink" title="1. Importing Modules"></a>1. Importing Modules</h2><h3 id="1-Built-in-Modules"><a href="#1-Built-in-Modules" class="headerlink" title="1. Built-in Modules"></a>1. Built-in Modules</h3><p>These are standard library modules that come with Python and don’t require additional installation.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> dataclasses <span class="keyword">import</span> dataclass, field</span><br></pre></td></tr></tbody></table></figure>

<p>Imports Python’s <code>dataclasses</code> module for creating classes with default values.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br></pre></td></tr></tbody></table></figure>

<p>Imports the <code>json</code> module for handling JSON format data.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br></pre></td></tr></tbody></table></figure>

<p>Imports the <code>math</code> module for mathematical operations.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pathlib</span><br></pre></td></tr></tbody></table></figure>

<p>Imports the <code>pathlib</code> module for handling file paths.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Dict</span>, <span class="type">Optional</span>, <span class="type">Sequence</span></span><br></pre></td></tr></tbody></table></figure>

<p>Imports the <code>typing</code> module for type annotations.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></tbody></table></figure>

<h3 id="2-Dependency-Libraries"><a href="#2-Dependency-Libraries" class="headerlink" title="2. Dependency Libraries"></a>2. Dependency Libraries</h3><p>These are external libraries typically installed via a package manager like pip.<br>Imports the <code>numpy</code> library, commonly used for scientific computing.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></tbody></table></figure>

<p>Imports <code>PyTorch</code>, a popular deep learning framework.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br></pre></td></tr></tbody></table></figure>

<p>Imports <code>Dataset</code> from <code>torch</code> for creating custom datasets.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> transformers</span><br></pre></td></tr></tbody></table></figure>

<p>Imports the <code>transformers</code> library, a popular natural language processing library.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Trainer</span><br></pre></td></tr></tbody></table></figure>

<p>Imports <code>Trainer</code> from <code>transformers</code> for training models.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers.trainer_pt_utils <span class="keyword">import</span> LabelSmoother</span><br></pre></td></tr></tbody></table></figure>

<p>Imports <code>LabelSmoother</code> from <code>transformers</code> for label smoothing.</p>
<h3 id="3-Project-Specific-Functions"><a href="#3-Project-Specific-Functions" class="headerlink" title="3. Project-Specific Functions"></a>3. Project-Specific Functions</h3><p>These are functions or classes custom-implemented in the Fast Chat project.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fastchat.conversation <span class="keyword">import</span> SeparatorStyle</span><br></pre></td></tr></tbody></table></figure>

<p>Imports <code>SeparatorStyle</code> from the <code>fastchat</code> package for defining conversation separator styles. The <code>SeparatorStyle</code> class is an enumeration class created using Python’s <code>enum</code> module, defining a series of separator styles. Enumerations are a programming concept used to define a named set of constants, making code clearer and more maintainable.</p>
<p>In the <code>SeparatorStyle</code> class, each member represents a specific style of separator. These styles are often used in text processing, especially in scenarios where different sections or elements need to be distinguished. For instance, in handling dialog or textual data, different methods might be needed to differentiate between user input and machine responses.</p>
<p>Regarding the use of the <code>auto()</code> function:</p>
<ul>
<li><code>auto()</code> is a special function provided by Python’s <code>enum</code> module. It automatically assigns a unique value to each member in an enumeration class.</li>
<li>Without using <code>auto()</code>, you would need to manually assign a unique value to each enumeration member. <code>auto()</code> simplifies this process by letting Python handle the assignment of these values automatically.</li>
<li>The values assigned by <code>auto()</code> are usually integers, starting from 1 and increasing sequentially.</li>
</ul>
<p>In the case of the <code>SeparatorStyle</code> class, <code>auto()</code> is used to automatically assign a unique integer value to each type of separator style. For example, <code>ADD_COLON_SINGLE</code>, <code>ADD_COLON_TWO</code>, etc., will be given different integer values.</p>
<p>The names of each enumeration member (such as <code>ADD_COLON_SINGLE</code>, <code>NO_COLON_SINGLE</code>, etc.) typically describe the characteristics of that separator style. For instance, <code>ADD_COLON_SINGLE</code> might represent adding a colon as a separator after a certain element, whereas <code>NO_COLON_SINGLE</code> means no colon is added.</p>
<p>This approach makes referencing and handling these separator styles in the code more convenient and clear. For example, different separator styles can be chosen based on different scenarios or requirements without having to remember their specific values.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fastchat.model.model_adapter <span class="keyword">import</span> get_conversation_template</span><br></pre></td></tr></tbody></table></figure>

<p>Imports <code>get_conversation_template</code> from the <code>fastchat</code> package for obtaining conversation templates. In this code segment, the call logic primarily involves obtaining the default conversation template for a specific model. The call chain is as follows:</p>
<ol>
<li><p><strong>Starting Call - <code>get_conversation_template(model_path: str)</code></strong></p>
<ul>
<li>This function is the starting point of the call chain. It accepts a parameter <code>model_path</code>, specifying the path of the model.</li>
<li>The purpose of this function is to obtain the default conversation template for the given model path.</li>
</ul>
</li>
<li><p><strong>Call <code>get_model_adapter(model_path: str)</code></strong></p>
<ul>
<li>The <code>get_conversation_template</code> function first calls <code>get_model_adapter</code>, passing in the model path.</li>
<li>The purpose of <code>get_model_adapter</code> is to find and return a suitable <code>BaseModelAdapter</code> object for the provided model path.</li>
<li>This function first tries to match the basename of <code>model_path</code>. If no match is found, it tries the full path.</li>
<li>If a suitable adapter is found, it is returned; otherwise, a <code>ValueError</code> is thrown.</li>
</ul>
</li>
<li><p><strong>Execute <code>BaseModelAdapter.get_default_conv_template(model_path: str)</code></strong></p>
<ul>
<li>Once the appropriate model adapter is obtained, <code>get_conversation_template</code> retrieves the default conversation template by calling the <code>get_default_conv_template</code> method of that adapter.</li>
<li>Note that this method is defined in the <code>BaseModelAdapter</code> class but might be overridden in subclasses.</li>
</ul>
</li>
<li><p><strong>Call <code>get_conv_template(name: str)</code></strong></p>
<ul>
<li>Inside the <code>get_default_conv_template</code> method, it calls the <code>get_conv_template</code> function, usually passing a predefined template name like <code>"one_shot"</code>.</li>
<li>The purpose of <code>get_conv_template</code> is to retrieve a specified name’s template from the global registry of conversation templates <code>conv_templates</code>.</li>
</ul>
</li>
<li><p><strong>Obtain and Return a <code>Conversation</code> Object</strong></p>
<ul>
<li>The <code>get_conv_template</code> function returns an instance of the <code>Conversation</code> class, usually copied from the <code>conv_templates</code> dictionary.</li>
<li>Finally, this <code>Conversation</code> instance is returned to the original call site of <code>get_conversation_template</code>.</li>
</ul>
</li>
</ol>
<p>Summarizing the call chain:</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">get_conversation_template(model_path)</span><br><span class="line">  -&gt; get_model_adapter(model_path)</span><br><span class="line">  -&gt; [BaseModelAdapter].get_default_conv_template(model_path)</span><br><span class="line">    -&gt; get_conv_template(name)</span><br><span class="line">      -&gt; Return Conversation Object</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>

<p>In this process, the code navigates through a series of function calls to find a suitable model adapter based on the provided model path and retrieve a specific conversation template from it. This design pattern allows flexibility in providing different conversation templates for different models, enhancing the reusability and extensibility of the code.</p>
<hr>
<h2 id="2-Configuration-Classes"><a href="#2-Configuration-Classes" class="headerlink" title="2. Configuration Classes"></a>2. Configuration Classes</h2><p>These classes are defined using Python’s <code>dataclass</code> decorator and are mainly used for storing configurations and parameters. These classes usually do not contain complex methods or logic but are used to define and store data structures. These classes include:</p>
<ul>
<li><code>ModelArguments</code>: Stores parameters related to the model, like model path, trust in remote code, etc.</li>
<li><code>DataArguments</code>: Stores parameters related to data, like data path, evaluation data path, and whether to use lazy preprocessing.</li>
<li><code>TrainingArguments</code>: Stores parameters related to training, like cache directory, optimizer type, model maximum length, etc. This class extends <code>transformers.TrainingArguments</code> and adds some custom parameters.</li>
</ul>
<p>These classes are mainly used to simplify and organize parameter management in the code, making parameter modification and access more convenient.</p>
<h3 id="1-ModelArguments-Class"><a href="#1-ModelArguments-Class" class="headerlink" title="1. ModelArguments Class"></a>1. ModelArguments Class</h3><h4 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ModelArguments</span>:</span><br><span class="line">    model_name_or_path: <span class="type">Optional</span>[<span class="built_in">str</span>] = field(default=<span class="string">"facebook/opt-125m"</span>)</span><br><span class="line">    trust_remote_code: <span class="built_in">bool</span> = field(</span><br><span class="line">        default=<span class="literal">False</span>,</span><br><span class="line">        metadata={</span><br><span class="line">            <span class="string">"help"</span>: <span class="string">"Whether or not to allow for custom models defined on the Hub in their own modeling files"</span></span><br><span class="line">        },</span><br><span class="line">    )</span><br><span class="line">    padding_side: <span class="built_in">str</span> = field(</span><br><span class="line">        default=<span class="string">"right"</span>, metadata={<span class="string">"help"</span>: <span class="string">"The padding side in tokenizer"</span>}</span><br><span class="line">    )</span><br></pre></td></tr></tbody></table></figure>

<h4 id="Explanation"><a href="#Explanation" class="headerlink" title="Explanation"></a>Explanation</h4><p><code>ModelArguments</code> is a data class (<code>dataclass</code>) used for storing model-related configuration parameters.<br><strong>Attributes:</strong></p>
<ol>
<li><code>model_name_or_path</code>: Specifies the name or path of the pretrained model.</li>
<li><code>trust_remote_code</code>: Whether to allow custom models that have their modeling files defined on the Hub.</li>
<li><code>padding_side</code>: Specifies the padding side in the tokenizer, typically right or left padding.</li>
</ol>
<details>
<summary> Introduction to `@dataclass` decorator, click to expand </summary>
`@dataclass` is a decorator used to automate the generation of special methods like `__init__()`, `__repr__()`, `__eq__()` etc., thus simplifying the writing of data classes. This decorator is part of Python 3.7 and is in the `dataclasses` module.

<p>When you use <code>@dataclass</code> before a class definition, Python automatically adds some special methods based on the fields defined in the class. This is very useful for creating classes that store a small amount of data but do not need complex methods.</p>
<p>Specifically, using <code>@dataclass</code>:</p>
<ol>
<li><p><strong>Automatically generates a constructor (<code>__init__</code> method)</strong>: Python creates an <code>__init__</code> method automatically based on the fields defined in the class, so you don’t need to manually write this method to initialize your class instances.</p>
</li>
<li><p><strong>Automatically generates a <code>__repr__</code> method</strong>: This makes printing the class instances provide a more readable string representation, usually including the class name and its fields and their values.</p>
</li>
<li><p><strong>Automatically generates an <code>__eq__</code> method</strong>: This allows you to use the <code>==</code> operator to compare two instances of the class, comparing the values of the instance fields.</p>
</li>
<li><p><strong>Support for type annotations</strong>: When defining fields, you can use type annotations, which not only help with clarity of code but can also be checked for type correctness using some tools.</p>
</li>
</ol>
<p>In the case of the <code>ModelArguments</code> class, the <code>@dataclass</code> decorator will generate the above-mentioned methods. This means you can easily create an instance of <code>ModelArguments</code>, and when printing or comparing these instances, you will get the expected behavior.</p>
<p>For example, when you create an instance of <code>ModelArguments</code>:</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">args = ModelArguments()</span><br></pre></td></tr></tbody></table></figure>

<p>This will call the automatically generated <code>__init__</code> method, using the default values “facebook/opt-125m” for <code>model_name_or_path</code>, <code>False</code> for <code>trust_remote_code</code>, and “right” for <code>padding_side</code>.</p>
<p>When you print this instance:</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(args)</span><br></pre></td></tr></tbody></table></figure>

<p>This will call the automatically generated <code>__repr__</code> method, showing a detailed view of the class instance, like <code>ModelArguments(model_name_or_path="facebook/opt-125m", trust_remote_code=False, padding_side="right")</code>.</p>
<p>Thus, the <code>@dataclass</code> decorator simplifies the process of creating classes, making the code more concise and maintainable.</p>
<p>Overall, the <code>@dataclass</code> decorator is a convenient tool provided by Python for quickly creating classes mainly used for storing data.</p>
</details>

<h3 id="2-DataArguments-Class"><a href="#2-DataArguments-Class" class="headerlink" title="2. DataArguments Class"></a>2. DataArguments Class</h3><h4 id="Code-1"><a href="#Code-1" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DataArguments</span>:</span><br><span class="line">    data_path: <span class="built_in">str</span> = field(</span><br><span class="line">        default=<span class="literal">None</span>, metadata={<span class="string">"help"</span>: <span class="string">"Path to the training data."</span>}</span><br><span class="line">    )</span><br><span class="line">    eval_data_path: <span class="built_in">str</span> = field(</span><br><span class="line">        default=<span class="literal">None</span>, metadata={<span class="string">"help"</span>: <span class="string">"Path to the evaluation data."</span>}</span><br><span class="line">    )</span><br><span class="line">    lazy_preprocess: <span class="built_in">bool</span> = <span class="literal">False</span></span><br></pre></td></tr></tbody></table></figure>

<h4 id="Explanation-1"><a href="#Explanation-1" class="headerlink" title="Explanation"></a>Explanation</h4><p><strong>DataArguments Class</strong></p>
<ul>
<li><code>DataArguments</code> is also a data class used for storing data-related configuration parameters.</li>
<li>Attributes:<ul>
<li><code>data_path</code>: Path to the training data.</li>
<li><code>eval_data_path</code>: Path to the evaluation data.</li>
<li><code>lazy_preprocess</code>: Whether to use lazy loading for data preprocessing, i.e., load and process data as needed.</li>
</ul>
</li>
</ul>
<h3 id="3-TrainingArguments-Class"><a href="#3-TrainingArguments-Class" class="headerlink" title="3. TrainingArguments Class"></a>3. TrainingArguments Class</h3><h4 id="Code-2"><a href="#Code-2" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TrainingArguments</span>(transformers.TrainingArguments):</span><br><span class="line">    cache_dir: <span class="type">Optional</span>[<span class="built_in">str</span>] = field(default=<span class="literal">None</span>)</span><br><span class="line">    optim: <span class="built_in">str</span> = field(default=<span class="string">"adamw_torch"</span>)</span><br><span class="line">    model_max_length: <span class="built_in">int</span> = field(</span><br><span class="line">        default=<span class="number">512</span>,</span><br><span class="line">        metadata={</span><br><span class="line">            <span class="string">"help"</span>: <span class="string">"Maximum sequence length. Sequences will be right padded (and possibly truncated)."</span></span><br><span class="line">        },</span><br><span class="line">    )</span><br></pre></td></tr></tbody></table></figure>

<h4 id="Explanation-2"><a href="#Explanation-2" class="headerlink" title="Explanation"></a>Explanation</h4><p><code>TrainingArguments</code> class extends <code>transformers.TrainingArguments</code>.</p>
<ol>
<li><p><strong>TrainingArguments Class</strong></p>
<ul>
<li><code>TrainingArguments</code> is a data class that, by extending <code>transformers.TrainingArguments</code>, gains the capability to handle training parameters.</li>
<li>Attributes defined in <code>TrainingArguments</code>:<ul>
<li><code>cache_dir</code>: Specifies the directory path for caching the model and tokenizer.</li>
<li><code>optim</code>: Defines the type of optimizer to use, like <code>'adamw_torch'</code>.</li>
<li><code>model_max_length</code>: Specifies the maximum sequence length the model can handle.</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>transformers.TrainingArguments Class</strong></p>
<ul>
<li><code>transformers.TrainingArguments</code> is a class in the transformers library that is used for configuring various parameters in the model training process.</li>
<li>This class contains a plethora of attributes for controlling the training process, such as:<ul>
<li><code>output_dir</code>: Specifies the directory to save the model and training results.</li>
<li><code>num_train_epochs</code>: Number of training epochs.</li>
<li><code>per_device_train_batch_size</code>: Batch size per device for training.</li>
<li><code>save_steps</code>: Steps interval for saving the model.</li>
<li><code>evaluation_strategy</code>: Strategy for evaluating the model, like at the end of each epoch.</li>
<li><code>learning_rate</code>: Learning rate.</li>
<li><code>warmup_steps</code>: Steps used for warmup in the learning rate schedule.</li>
</ul>
</li>
<li><code>transformers.TrainingArguments</code> also</li>
</ul>
</li>
</ol>
<p> contains many other parameters for fine-tuning the training process, including logging, model saving strategies, learning rate scheduling, and more.</p>
<p>By extending <code>transformers.TrainingArguments</code>, the <code>TrainingArguments</code> class not only inherits all these training parameter configurations but can also add some custom training parameters, like in this case <code>cache_dir</code>, <code>optim</code>, and <code>model_max_length</code>. This approach enhances code reusability and flexibility, allowing you to adjust and extend training configurations as per the specific requirements of your project.</p>
<h2 id="3-Functional-Utility-Functions"><a href="#3-Functional-Utility-Functions" class="headerlink" title="3. Functional Utility Functions"></a>3. Functional Utility Functions</h2><h3 id="1-rank0-print-args"><a href="#1-rank0-print-args" class="headerlink" title="1. rank0_print(*args)"></a>1. rank0_print(*args)</h3><h4 id="Code-3"><a href="#Code-3" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">local_rank = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">rank0_print</span>(<span class="params">*args</span>):</span><br><span class="line">    <span class="keyword">if</span> local_rank == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(*args)</span><br></pre></td></tr></tbody></table></figure>

<h4 id="Explanation-3"><a href="#Explanation-3" class="headerlink" title="Explanation"></a>Explanation</h4><p>Defines a global variable local_rank for distributed training.<br>Defines a function rank0_print to print information only if local_rank is 0, used for controlling output in distributed training. This way, repetitive printing of the same information across multiple nodes is avoided, making the output clearer and more concise.</p>
<ul>
<li>Used to print information only on the main node (rank 0) in a distributed training environment.</li>
<li>Parameters: A variable number of arguments for printing.</li>
</ul>
<h3 id="2-trainer-save-model-safe-trainer-transformers-Trainer"><a href="#2-trainer-save-model-safe-trainer-transformers-Trainer" class="headerlink" title="2. trainer_save_model_safe(trainer: transformers.Trainer)"></a>2. <code>trainer_save_model_safe(trainer: transformers.Trainer)</code></h3><h4 id="Code-4"><a href="#Code-4" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">trainer_save_model_safe</span>(<span class="params">trainer: transformers.Trainer</span>):</span><br><span class="line">    <span class="keyword">from</span> torch.distributed.fsdp <span class="keyword">import</span> FullyShardedDataParallel <span class="keyword">as</span> FSDP</span><br><span class="line">    <span class="keyword">from</span> torch.distributed.fsdp <span class="keyword">import</span> StateDictType, FullStateDictConfig</span><br><span class="line"></span><br><span class="line">    save_policy = FullStateDictConfig(offload_to_cpu=<span class="literal">True</span>, rank0_only=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">with</span> FSDP.state_dict_type(</span><br><span class="line">        trainer.model, StateDictType.FULL_STATE_DICT, save_policy</span><br><span class="line">    ):</span><br><span class="line">        trainer.save_model()</span><br></pre></td></tr></tbody></table></figure>

<p>The function <code>trainer_save_model_safe(trainer: transformers.Trainer)</code> aims to safely save models trained with the PyTorch distributed framework. Let’s delve into the details of this function and its key components.</p>
<h4 id="Explanation-4"><a href="#Explanation-4" class="headerlink" title="Explanation"></a>Explanation</h4><ol>
<li><p>Parameters:</p>
<ul>
<li><code>trainer</code>: An instance of <code>transformers.Trainer</code>. This class is one of the core components of the Hugging Face Transformers library, used for training and evaluating models.</li>
</ul>
</li>
<li><p>Functionality:</p>
<ul>
<li>The main purpose of this function is to safely save models in a distributed training environment. It particularly considers the model saving strategy when using Fully Sharded Data Parallel (FSDP).</li>
</ul>
</li>
<li><p>FSDP</p>
<ul>
<li><strong>FullyShardedDataParallel (FSDP)</strong><ul>
<li>This is a component of PyTorch’s distributed training framework. FSDP helps reduce memory usage on each GPU by sharding model parameters across multiple GPUs, allowing the training of larger models.</li>
<li>In this context, FSDP is primarily used for handling and saving model states in distributed training.</li>
</ul>
</li>
<li><strong>StateDictType</strong><ul>
<li>This is an enumeration type that defines how to save the model’s state dictionary. In FSDP environments, saving and loading model states might require special handling.</li>
</ul>
</li>
<li><strong>FullStateDictConfig</strong><ul>
<li>This class configures parameters for saving the full state dictionary. It’s part of FSDP’s functionality and is used to control how the model state is saved.</li>
</ul>
</li>
</ul>
</li>
<li><p>Function Implementation</p>
<ul>
<li><strong>Setting Save Policy</strong><ul>
<li><code>save_policy = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)</code> creates a save policy. Here, two key parameters are specified:<ul>
<li><code>offload_to_cpu</code>: Offload model parameters to CPU before saving the state dictionary, which helps reduce GPU memory usage.</li>
<li><code>rank0_only</code>: Save the model only on rank 0 (usually the main node). In distributed training, this avoids saving the same model copy on every node, saving storage space.</li>
</ul>
</li>
</ul>
</li>
<li><strong>Saving the Model</strong><ul>
<li>Using the <code>with FSDP.state_dict_type(trainer.model, StateDictType.FULL_STATE_DICT, save_policy)</code> context manager, the type and policy for saving the model’s state dictionary are set.</li>
<li>Within this context, <code>trainer.save_model()</code> is called to save the model. Due to the <code>save_policy</code>, the model is saved securely following the specified configuration.</li>
</ul>
</li>
</ul>
</li>
</ol>
<p>The function <code>trainer_save_model_safe</code> encapsulates a safe model saving logic, particularly for scenarios involving PyTorch’s FSDP in distributed training. It ensures that only a complete model state is saved on one node and offloads model parameters to CPU before saving, optimizing memory usage and storage efficiency. This is crucial for training large models and managing large-scale distributed training environments.</p>
<h3 id="3-preprocess-sources-tokenizer-transformers-PreTrainedTokenizer-gt-Dict"><a href="#3-preprocess-sources-tokenizer-transformers-PreTrainedTokenizer-gt-Dict" class="headerlink" title="3.preprocess(sources,tokenizer: transformers.PreTrainedTokenizer) -> Dict"></a>3.<code>preprocess(sources,tokenizer: transformers.PreTrainedTokenizer) -&gt; Dict</code></h3><h4 id="Code-5"><a href="#Code-5" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess</span>(<span class="params"></span></span><br><span class="line"><span class="params">    sources,</span></span><br><span class="line"><span class="params">    tokenizer: transformers.PreTrainedTokenizer,</span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="type">Dict</span>:</span><br><span class="line">    conv = get_conversation_template(<span class="string">"vicuna"</span>)</span><br><span class="line">    roles = {<span class="string">"human"</span>: conv.roles[<span class="number">0</span>], <span class="string">"gpt"</span>: conv.roles[<span class="number">1</span>]}</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Apply prompt templates</span></span><br><span class="line">    conversations = []</span><br><span class="line">    <span class="keyword">for</span> i, source <span class="keyword">in</span> <span class="built_in">enumerate</span>(sources):</span><br><span class="line">        <span class="keyword">if</span> roles[source[<span class="number">0</span>][<span class="string">"from"</span>]] != conv.roles[<span class="number">0</span>]:</span><br><span class="line">            <span class="comment"># Skip the first one if it is not from human</span></span><br><span class="line">            source = source[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">        conv.messages = []</span><br><span class="line">        <span class="keyword">for</span> j, sentence <span class="keyword">in</span> <span class="built_in">enumerate</span>(source):</span><br><span class="line">            role = roles[sentence[<span class="string">"from"</span>]]</span><br><span class="line">            <span class="keyword">assert</span> role == conv.roles[j % <span class="number">2</span>], <span class="string">f"<span class="subst">{i}</span>"</span></span><br><span class="line">            conv.append_message(role, sentence[<span class="string">"value"</span>])</span><br><span class="line">        conversations.append(conv.get_prompt())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Tokenize conversations</span></span><br><span class="line">    input_ids = tokenizer(</span><br><span class="line">        conversations,</span><br><span class="line">        return_tensors=<span class="string">"pt"</span>,</span><br><span class="line">        padding=<span class="string">"max_length"</span>,</span><br><span class="line">        max_length=tokenizer.model_max_length,</span><br><span class="line">        truncation=<span class="literal">True</span>,</span><br><span class="line">    ).input_ids</span><br><span class="line">    targets = input_ids.clone()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> conv.sep_style == SeparatorStyle.ADD_COLON_TWO</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Mask targets. Only compute loss on the assistant outputs.</span></span><br><span class="line">    sep = conv.sep + conv.roles[<span class="number">1</span>] + <span class="string">": "</span></span><br><span class="line">    <span class="keyword">for</span> conversation, target <span class="keyword">in</span> <span class="built_in">zip</span>(conversations, targets):</span><br><span class="line">        total_len = <span class="built_in">int</span>(target.ne(tokenizer.pad_token_id).<span class="built_in">sum</span>())</span><br><span class="line"></span><br><span class="line">        turns = conversation.split(conv.sep2)</span><br><span class="line">        cur_len = <span class="number">1</span></span><br><span class="line">        target[:cur_len] = IGNORE_TOKEN_ID</span><br><span class="line">        <span class="keyword">for</span> i, turn <span class="keyword">in</span> <span class="built_in">enumerate</span>(turns):</span><br><span class="line">            <span class="keyword">if</span> turn == <span class="string">""</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            turn_len = <span class="built_in">len</span>(tokenizer(turn).input_ids)</span><br><span class="line"></span><br><span class="line">            parts = turn.split(sep)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(parts) != <span class="number">2</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            parts[<span class="number">0</span>] += sep</span><br><span class="line">            <span class="comment"># "-2" is hardcoded for the Llama tokenizer to make the offset correct.</span></span><br><span class="line">            instruction_len = <span class="built_in">len</span>(tokenizer(parts[<span class="number">0</span>]).input_ids) - <span class="number">2</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i != <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> tokenizer.legacy:</span><br><span class="line">                <span class="comment"># The legacy and non-legacy modes handle special tokens differently</span></span><br><span class="line">                instruction_len -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Ignore the user instructions</span></span><br><span class="line">            target[cur_len : cur_len + instruction_len] = IGNORE_TOKEN_ID</span><br><span class="line">            cur_len += turn_len</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i != <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> tokenizer.legacy:</span><br><span class="line">                <span class="comment"># The legacy and non-legacy modes handle special tokens differently</span></span><br><span class="line">                cur_len -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        target[cur_len:] = IGNORE_TOKEN_ID</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="literal">False</span>:  <span class="comment"># Inspect and check the correctness of masking</span></span><br><span class="line">            z = target.clone()</span><br><span class="line">            z = torch.where(z == IGNORE_TOKEN_ID, tokenizer.unk_token_id, z)</span><br><span class="line">            rank0_print(tokenizer.decode(z))</span><br><span class="line">            exit()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> cur_len &lt; tokenizer.model_max_length:</span><br><span class="line">           </span><br><span class="line"></span><br><span class="line"> <span class="keyword">if</span> cur_len != total_len:</span><br><span class="line">                target[:] = IGNORE_TOKEN_ID</span><br><span class="line">                rank0_print(</span><br><span class="line">                    <span class="string">f"WARNING: tokenization mismatch: <span class="subst">{cur_len}</span> vs. <span class="subst">{total_len}</span>."</span></span><br><span class="line">                    <span class="string">f" #turn = <span class="subst">{<span class="built_in">len</span>(turns) - <span class="number">1</span>}</span>. (ignored)"</span></span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">dict</span>(</span><br><span class="line">        input_ids=input_ids,</span><br><span class="line">        labels=targets,</span><br><span class="line">        attention_mask=input_ids.ne(tokenizer.pad_token_id),</span><br><span class="line">    )</span><br></pre></td></tr></tbody></table></figure>

<p>The function <code>preprocess(sources, tokenizer: transformers.PreTrainedTokenizer) -&gt; Dict</code> is intended for preprocessing dialogue data to be suitable for training machine learning models. This function can be broken down into several main parts for a more detailed explanation:</p>
<h4 id="1-Obtaining-Conversation-Templates-and-Role-Definitions"><a href="#1-Obtaining-Conversation-Templates-and-Role-Definitions" class="headerlink" title="1. Obtaining Conversation Templates and Role Definitions"></a>1. Obtaining Conversation Templates and Role Definitions</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conv = get_conversation_template(<span class="string">"vicuna"</span>)</span><br><span class="line">roles = {<span class="string">"human"</span>: conv.roles[<span class="number">0</span>], <span class="string">"gpt"</span>: conv.roles[<span class="number">1</span>]}</span><br></pre></td></tr></tbody></table></figure>

<ul>
<li><strong>Functionality</strong>: Initializes conversation templates and defines the roles of dialogue participants.</li>
<li><strong>Implementation</strong>:<ul>
<li><code>conv = get_conversation_template("vicuna")</code> obtains the conversation template for a specified model (e.g., “vicuna”).</li>
<li>The <code>roles</code> dictionary maps “human” and “gpt” to the roles defined in the conversation template.</li>
</ul>
</li>
<li><strong>Example</strong>:<ul>
<li>If the conversation template is for “vicuna”, then <code>roles</code> might map “human” to “user” and “gpt” to “assistant”. For example, <code>{'human': 'USER', 'gpt': 'ASSISTANT'}</code>.</li>
</ul>
</li>
</ul>
<h4 id="2-Applying-Prompt-Templates"><a href="#2-Applying-Prompt-Templates" class="headerlink" title="2. Applying Prompt Templates"></a>2. Applying Prompt Templates</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Apply prompt templates</span></span><br><span class="line">conversations = []</span><br><span class="line"><span class="keyword">for</span> i, source <span class="keyword">in</span> <span class="built_in">enumerate</span>(sources):</span><br><span class="line">    <span class="keyword">if</span> roles[source[<span class="number">0</span>][<span class="string">"from"</span>]] != conv.roles[<span class="number">0</span>]:</span><br><span class="line">        <span class="comment"># Skip the first one if it is not from human</span></span><br><span class="line">        source = source[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">    conv.messages = []</span><br><span class="line">    <span class="keyword">for</span> j, sentence <span class="keyword">in</span> <span class="built_in">enumerate</span>(source):</span><br><span class="line">        role = roles[sentence[<span class="string">"from"</span>]]</span><br><span class="line">        <span class="keyword">assert</span> role == conv.roles[j % <span class="number">2</span>], <span class="string">f"<span class="subst">{i}</span>"</span></span><br><span class="line">        conv.append_message(role, sentence[<span class="string">"value"</span>])</span><br><span class="line">    conversations.append(conv.get_prompt())</span><br></pre></td></tr></tbody></table></figure>

<ul>
<li><strong>Functionality</strong>: Applies prompt templates to source data to construct dialogues.</li>
<li><strong>Implementation</strong>:<ul>
<li>Iterates through <code>sources</code> (original dialogue data), transforming each dialogue source into a conversation in template format.</li>
<li>If the first part of a dialogue is not initiated by the “human” role, it skips that part.</li>
<li>Assigns a role to each sentence and adds it to the conversation template.</li>
<li>Ultimately, each processed dialogue is added to the <code>conversations</code> list.</li>
</ul>
</li>
<li><strong>Example</strong>:<ul>
<li>Suppose we have a source which is the first item in dummy input: <code>python source = [{'from': 'human', 'value': 'Who are you?'}, {'from': 'gpt', 'value': 'I am Vicuna, a language model trained by researchers from Large Model Systems Organization (LMSYS).'}, {'from': 'human', 'value': 'Have a nice day!'}, {'from': 'gpt', 'value': 'You too!'}]</code></li>
<li><code>conversations</code> under the Vicuna template, using <code>SeparatorStyle.ADD_COLON_TWO</code> as the separator style, might look like [“A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user’s questions. USER: Who are you? ASSISTANT: I am Vicuna, a language model trained by researchers from Large Model Systems Organization (LMSYS).USER: Have a nice day! ASSISTANT: You too!“]</li>
<li><details>
<summary> Implementation of get_prompt </summary>
The `get_prompt` method implementation varies depending on the `SeparatorStyle`. Below is a table detailing the `get_prompt` method for various styles, along with English examples:

<table>
<thead>
<tr>
<th>Separator Style (<code>SeparatorStyle</code>)</th>
<th>Description</th>
<th>Example</th>
</tr>
</thead>
<tbody><tr>
<td><code>ADD_COLON_SINGLE</code></td>
<td>Adds a colon and separator after each message.</td>
<td>USER: Hello there!\nASSISTANT: Hi, how can I help?\n</td>
</tr>
<tr>
<td><code>ADD_COLON_TWO</code></td>
<td>Uses two alternating separators, usually between different roles.</td>
<td>USER: What’s the weather?\nASSISTANT: It’s sunny today.\n\n</td>
</tr>
<tr>
<td><code>ADD_COLON_SPACE_SINGLE</code></td>
<td>Adds a colon, space, and separator after each message.</td>
<td>USER: Can you book a flight?\nASSISTANT: Sure, where to?\n</td>
</tr>
<tr>
<td><code>NO_COLON_SINGLE</code></td>
<td>Messages directly follow roles without a colon, followed by a separator.</td>
<td>USERWhat are you doing?\nASSISTANTI’m here to assist you.\n</td>
</tr>
<tr>
<td><code>NO_COLON_TWO</code></td>
<td>No colons, with two alternating separators.</td>
<td>USERHow’s the project going?\nASSISTANTIt’s on track.\n\n</td>
</tr>
<tr>
<td><code>ADD_NEW_LINE_SINGLE</code></td>
<td>Each message is preceded by a newline, followed by a separator.</td>
<td>USER\nHow can I reset my password?\nASSISTANT\nYou can reset it via email.\n</td>
</tr>
<tr>
<td><code>RWKV</code></td>
<td>Special format, usually for specific models.</td>
<td>USER: What is AI?\n\nASSISTANT: AI stands for Artificial Intelligence.\n\n</td>
</tr>
<tr>
<td><code>LLAMA2</code></td>
<td>Special label format for specific models.</td>
<td>[INST] USER How does blockchain work?\nASSISTANT It is a distributed ledger.\n\n</td>
</tr>
<tr>
<td><code>CHATGLM</code></td>
<td>Specific format for <code>CHATGLM</code> model.</td>
<td>[Round 1]\nUSER: Tell me a joke.\nASSISTANT: Why did the chicken cross the road?\n</td>
</tr>
<tr>
<td><code>CHATML</code></td>
<td>Similar to <code>CHATGLM</code>, but with newlines before and after each message.</td>
<td>USER\nDo you like music?\n\nASSISTANT\nYes, I enjoy many genres.\n\n</td>
</tr>
<tr>
<td><code>CHATGLM3</code></td>
<td>Format for <code>CHATGLM3</code> model.</td>
<td>USER\nCan you play chess?\nASSISTANTYes, I can play.\n</td>
</tr>
<tr>
<td><code>CHATINTERN</code></td>
<td>Format for <code>CHATINTERN</code> model, using special markers.</td>
<td><s>USER:Where is the nearest ATM?<s>\nASSISTANT:It’s next to the post office.\n</s></s></td>
</tr>
<tr>
<td><code>DOLLY</code></td>
<td>Specific format for <code>DOLLY</code> model.</td>
<td>USER:\nWhat is quantum computing?\nASSISTANT:\nIt involves computation using quantum-mechanical phenomena.\n\n</td>
</tr>
<tr>
<td><code>PHOENIX</code></td>
<td>For <code>PHOENIX</code> model, messages are wrapped in special markers.</td>
<td>USER: <s>How to bake a cake?</s>\nASSISTANT: <s>You need flour, sugar, and eggs.</s>\n</td>
</tr>
<tr>
<td><code>ROBIN</code></td>
<td>Similar to <code>ADD_NEW_LINE_SINGLE</code>, but with a newline after roles.</td>
<td>USER:\nIs AI dangerous?\nASSISTANT:\nIt depends on how it’s used.\n</td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
</details></li>
</ul>
</li>
</ul>
</article><section class="jump-container is-flex is-justify-content-space-between my-6"><!-- em is empty placeholder--><a class="button is-default" href="/2024/02/28/NLP%20Insights/Gorilla:%20Large%20Language%20Model%20Connected%20with%20Massive%20APIs/" title="Gorilla LLM 大语言模型简介"><i class="iconfont icon-prev mr-2 has-text-grey"></i><span class="has-text-weight-semibold">Previous: Gorilla LLM 大语言模型简介</span></a><a class="button is-default" href="/2024/02/27/NLP%20Insights/FastChat%20%E8%AE%AD%E7%BB%83%E8%84%9A%E6%9C%AC%E4%BB%A3%E7%A0%81%E9%80%90%E8%A1%8C%E8%A7%A3%E6%9E%90-Train.py%20%E3%80%90FastChat%20%E7%B3%BB%E5%88%97%E7%AC%AC%201%20%E7%AF%87%E3%80%91/" title="FastChat 训练脚本代码逐行解析-Train.py 【FastChat 系列第 1 篇】"><span class="has-text-weight-semibold">Next: FastChat 训练脚本代码逐行解析-Train.py 【FastChat 系列第 1 篇】</span><i class="iconfont icon-next ml-2 has-text-grey"></i></a></section><article class="mt-6 comment-container"><script async repo="Haojen/Claudia-theme-blog" src="https://utteranc.es/client.js" issue-term="pathname" theme="preferred-color-scheme"></script></article></div></div></main></main><footer class="is-flex is-flex-direction-column is-align-items-center is-flex-shrink-0 is-family-serif"><section class="sns-container"><a title="twitter" target="_blank" rel="noopener nofollow" href="//twitter.com//"><i class="iconfont icon-twitter"></i></a><!-- Github--><a title="github" target="_blank" rel="noopener nofollow" href="//github.com/haojen"><i class="iconfont icon-github"></i></a><!-- Ins--><a title="instagram" target="_blank" rel="noopener nofollow" href="//www.instagram.com//"><i class="iconfont icon-ins"></i></a><!-- RSS--><!-- 知乎--><!-- 领英--><!-- 脸书--><a title="facebook" target="_blank" rel="noopener nofollow" href="//www.facebook.com//"><i class="iconfont icon-tian7_facebook"></i></a></section><p><span>Copyright ©</span><span> user 2026</span></p><div class="is-flex is-justify-content-center is-flex-wrap-wrap"><p>Powered by Hexo &verbar;&nbsp;</p><p class="is-flex is-justify-content-center"><a title="Hexo theme author" target="_blank" rel="noopener" href="//github.com/haojen">Theme by Haojen&nbsp;</a></p><div style="margin-top: 2px"><a class="github-button" title="github-button" target="_blank" rel="noopener" href="https://github.com/haojen/hexo-theme-Claudia" data-color-scheme="no-preference: light; light: light; dark: dark;" data-show-count="true"></a></div></div><div><span></span></div></footer><script async defer src="https://buttons.github.io/buttons.js"></script><script src="/js/jquery-3.6.1.min.js"></script><script src="/js/jquery-fancybox.min.js"></script><script src="/js/img_zoom.js"></script><script src="/js/post.js"></script></body></html>