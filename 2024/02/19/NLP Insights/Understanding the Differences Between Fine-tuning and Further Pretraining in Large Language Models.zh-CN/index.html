<!DOCTYPE html><html lang="[&quot;zh-CN&quot;,&quot;en&quot;]" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Understanding the Differences Between Fine-tuning and Further Pretraining in Large Language Models | é»‘å¤´å‘†é±¼è¿›åŒ–ä¹‹æ—…</title><meta name="author" content="Huiyu Chen"><meta name="copyright" content="Huiyu Chen"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Understanding the Differences Between Fine-tuning and Further Pretraining in Large Language ModelsIn the world of Natural Language Processing (NLP), the advent of large language models like GPT and BE">
<meta property="og:type" content="article">
<meta property="og:title" content="Understanding the Differences Between Fine-tuning and Further Pretraining in Large Language Models">
<meta property="og:url" content="https://chenhuiyu.github.io/2024/02/19/NLP%20Insights/Understanding%20the%20Differences%20Between%20Fine-tuning%20and%20Further%20Pretraining%20in%20Large%20Language%20Models.zh-CN/index.html">
<meta property="og:site_name" content="é»‘å¤´å‘†é±¼è¿›åŒ–ä¹‹æ—…">
<meta property="og:description" content="Understanding the Differences Between Fine-tuning and Further Pretraining in Large Language ModelsIn the world of Natural Language Processing (NLP), the advent of large language models like GPT and BE">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://chenhuiyu.github.io/img/butterfly-icon.png">
<meta property="article:published_time" content="2024-02-19T07:06:29.000Z">
<meta property="article:modified_time" content="2026-02-20T22:13:37.931Z">
<meta property="article:author" content="Huiyu Chen">
<meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://chenhuiyu.github.io/img/butterfly-icon.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Understanding the Differences Between Fine-tuning and Further Pretraining in Large Language Models",
  "url": "https://chenhuiyu.github.io/2024/02/19/NLP%20Insights/Understanding%20the%20Differences%20Between%20Fine-tuning%20and%20Further%20Pretraining%20in%20Large%20Language%20Models.zh-CN/",
  "image": "https://chenhuiyu.github.io/img/butterfly-icon.png",
  "datePublished": "2024-02-19T07:06:29.000Z",
  "dateModified": "2026-02-20T22:13:37.931Z",
  "author": [
    {
      "@type": "Person",
      "name": "Huiyu Chen",
      "url": "https://chenhuiyu.github.io"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://chenhuiyu.github.io/2024/02/19/NLP%20Insights/Understanding%20the%20Differences%20Between%20Fine-tuning%20and%20Further%20Pretraining%20in%20Large%20Language%20Models.zh-CN/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=5.5.4"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: 'å¤åˆ¶æˆåŠŸ',
    error: 'å¤åˆ¶å¤±è´¥',
    noSupport: 'æµè§ˆå™¨ä¸æ”¯æŒ'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'åˆšåˆš',
    min: 'åˆ†é’Ÿå‰',
    hour: 'å°æ—¶å‰',
    day: 'å¤©å‰',
    month: 'ä¸ªæœˆå‰'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.13.0/dist/infinitegrid.min.js',
    buttonText: 'åŠ è½½æ›´å¤š'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Understanding the Differences Between Fine-tuning and Further Pretraining in Large Language Models',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 6.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="é»‘å¤´å‘†é±¼è¿›åŒ–ä¹‹æ—…" type="application/atom+xml">
</head><body><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">é»‘å¤´å‘†é±¼è¿›åŒ–ä¹‹æ—…</span></a><a class="nav-page-title" href="/"><span class="site-name">Understanding the Differences Between Fine-tuning and Further Pretraining in Large Language Models</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  è¿”å›é¦–é¡µ</span></span></a></span><div id="menus"></div></nav><div id="post-info"><h1 class="post-title">Understanding the Differences Between Fine-tuning and Further Pretraining in Large Language Models</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">å‘è¡¨äº</span><time class="post-meta-date-created" datetime="2024-02-19T07:06:29.000Z" title="å‘è¡¨äº 2024-02-19 15:06:29">2024-02-19</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">æ›´æ–°äº</span><time class="post-meta-date-updated" datetime="2026-02-20T22:13:37.931Z" title="æ›´æ–°äº 2026-02-21 06:13:37">2026-02-21</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/NLP-Insights/">NLP Insights</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">æµè§ˆé‡:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="Understanding-the-Differences-Between-Fine-tuning-and-Further-Pretraining-in-Large-Language-Models"><a href="#Understanding-the-Differences-Between-Fine-tuning-and-Further-Pretraining-in-Large-Language-Models" class="headerlink" title="Understanding the Differences Between Fine-tuning and Further Pretraining in Large Language Models"></a>Understanding the Differences Between Fine-tuning and Further Pretraining in Large Language Models</h1><p>In the world of Natural Language Processing (NLP), the advent of large language models like GPT and BERT has revolutionized how we approach tasks such as text classification, sentiment analysis, and question-answering. Two pivotal techniques in leveraging these models are Fine-tuning and Further Pretraining. While they may seem similar at a glance, they cater to different needs and scenarios in the NLP pipeline.</p>
<h2 id="What-is-Fine-tuning"><a href="#What-is-Fine-tuning" class="headerlink" title="What is Fine-tuning?"></a>What is Fine-tuning?</h2><p>Fine-tuning is a process where a pretrained model is further trained (or â€˜fine-tunedâ€™) on a specific task with a dataset corresponding to that task. This approach is particularly effective when the dataset is relatively small but well-labeled.</p>
<h3 id="Example-Scenario-Sentiment-Analysis"><a href="#Example-Scenario-Sentiment-Analysis" class="headerlink" title="Example Scenario: Sentiment Analysis"></a>Example Scenario: Sentiment Analysis</h3><p>Imagine you have a dataset of movie reviews, each labeled as positive or negative. You want to create a model that can predict the sentiment of a review.</p>
<h4 id="Code-Snippet-in-Python-using-PyTorch-and-HuggingFaceâ€™s-Transformers"><a href="#Code-Snippet-in-Python-using-PyTorch-and-HuggingFaceâ€™s-Transformers" class="headerlink" title="Code Snippet in Python (using PyTorch and HuggingFaceâ€™s Transformers)"></a>Code Snippet in Python (using PyTorch and HuggingFaceâ€™s Transformers)</h4><p>This notebook demonstrates the fine-tuning of a BERT model on the IMDB dataset for sentiment analysis. For detailed code implementation, please refer to the following link:<a target="_blank" rel="noopener" href="https://colab.research.google.com/drive/15naxP8pNMoCCBMgMSOv4ETDRGL46YR38?usp=sharing">link</a>.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br></pre></td><td class="code"><pre><span class="line">ls -al ~/.ssh</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> (</span><br><span class="line">    BertTokenizer,</span><br><span class="line">    BertForSequenceClassification,</span><br><span class="line">    Trainer,</span><br><span class="line">    TrainingArguments,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset, DatasetDict</span><br><span class="line"></span><br><span class="line"><span class="string">"""## 1. Load and Prepare IMDB Dataset Samples</span></span><br><span class="line"><span class="string">Select a portion of the data for Fine-tuning.</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Load IMDB dataset</span></span><br><span class="line">dataset = load_dataset(<span class="string">'imdb'</span>, split=<span class="string">'train'</span>)</span><br><span class="line">small_dataset = dataset.shuffle(seed=<span class="number">42</span>).select(<span class="built_in">range</span>(<span class="number">10000</span>))  <span class="comment"># Selecting the first 10,000 samples</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize tokenizer</span></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">"bert-base-uncased"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Encode the dataset</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">examples</span>):</span><br><span class="line">    <span class="keyword">return</span> tokenizer(examples[<span class="string">"text"</span>], padding=<span class="string">"max_length"</span>, truncation=<span class="literal">True</span>, max_length=<span class="number">512</span>)</span><br><span class="line"></span><br><span class="line">encoded_small_dataset = small_dataset.<span class="built_in">map</span>(encode, batched=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertModel</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">visualize_attention</span>(<span class="params">sentence, model, tokenizer</span>):</span><br><span class="line">    <span class="comment"># Set the model to evaluation mode</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Convert the input text into a format understandable by the model</span></span><br><span class="line">    inputs = tokenizer(sentence, return_tensors=<span class="string">"pt"</span>).to(device) <span class="comment"># Making sure inputs are on the same device</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Get attention weights using the model</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        outputs = model(**inputs)</span><br><span class="line">    attentions = outputs.attentions</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Choose the layer and head to visualize</span></span><br><span class="line">    layer = <span class="number">5</span></span><br><span class="line">    head = <span class="number">1</span></span><br><span class="line">    attention = attentions[layer][<span class="number">0</span>, head].cpu().numpy() <span class="comment"># Move attention weights back to CPU for visualization</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Set tokens for visualization</span></span><br><span class="line">    tokens = tokenizer.convert_ids_to_tokens(inputs[<span class="string">"input_ids"</span>][<span class="number">0</span>].cpu()) <span class="comment"># Also make sure tokens are on CPU</span></span><br><span class="line">    <span class="comment"># Plot attention matrix</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">10</span>, <span class="number">10</span>))</span><br><span class="line">    plt.matshow(attention, cmap=<span class="string">'viridis'</span>)</span><br><span class="line">    plt.xticks(<span class="built_in">range</span>(<span class="built_in">len</span>(tokens)), tokens, rotation=<span class="number">90</span>)</span><br><span class="line">    plt.yticks(<span class="built_in">range</span>(<span class="built_in">len</span>(tokens)), tokens)</span><br><span class="line">    plt.colorbar()</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="string">"""## 2. Visualize Attention Weights of a Sample Sentence (Before Fine-tuning)</span></span><br><span class="line"><span class="string">Select a sentence from the dataset and visualize the attention weights of the original BERT model.</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Use the model without Fine-tuning</span></span><br><span class="line">model = BertModel.from_pretrained(<span class="string">"bert-base-uncased"</span>, output_attentions=<span class="literal">True</span>)</span><br><span class="line">sample_sentence = <span class="string">"I love this movie, it's fantastic!"</span></span><br><span class="line">visualize_attention(sample_sentence, model, tokenizer)</span><br><span class="line"></span><br><span class="line"><span class="string">"""## 3. Fine-tuning the BERT Model</span></span><br><span class="line"><span class="string">Perform Fine-tuning on the selected IMDB samples.</span></span><br><span class="line"><span class="string">### 3.1 Prepare Data Loaders</span></span><br><span class="line"><span class="string">To train the model, we need to create PyTorch's DataLoader. This will allow us to efficiently load data during training.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">### 3.2 Set up Fine-tuning Environment</span></span><br><span class="line"><span class="string">Initialize the model, optimizer, and loss function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">### 3.3 Fine-tuning the Model</span></span><br><span class="line"><span class="string">Execute the training loop for Fine-tuning. Running the above code will Fine-tune the BERT model on a small sample of the IMDB dataset. This may take some time depending on your hardware configuration.</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert the dataset to PyTorch Tensor</span></span><br><span class="line">encoded_small_dataset.set_format(<span class="string">'torch'</span>, columns=[<span class="string">'input_ids'</span>, <span class="string">'attention_mask'</span>, <span class="string">'label'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create data loader</span></span><br><span class="line">train_loader = DataLoader(encoded_small_dataset, batch_size=<span class="number">8</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertConfig, BertForSequenceClassification</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load the configuration and set output attention weights</span></span><br><span class="line">config = BertConfig.from_pretrained(<span class="string">"bert-base-uncased"</span>, output_attentions=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize the BERT model for sequence classification</span></span><br><span class="line"><span class="comment"># Load the model with updated configuration</span></span><br><span class="line">model = BertForSequenceClassification.from_pretrained(<span class="string">"bert-base-uncased"</span>, config=config)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Set up the optimizer</span></span><br><span class="line">optimizer = optim.AdamW(model.parameters(), lr=<span class="number">5e-5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use the cross-entropy loss function</span></span><br><span class="line">criterion = torch.nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">device = <span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span></span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the number of epochs for training</span></span><br><span class="line">epochs = <span class="number">8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    model.train()</span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> train_loader:</span><br><span class="line">        <span class="comment"># Move the data to GPU</span></span><br><span class="line">        input_ids = batch[<span class="string">'input_ids'</span>].to(device)</span><br><span class="line">        attention_mask = batch[<span class="string">'attention_mask'</span>].to(device)</span><br><span class="line">        labels = batch[<span class="string">'label'</span>].to(device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Model forward pass</span></span><br><span class="line">        outputs = model(input_ids, attention_mask=attention_mask)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute the loss</span></span><br><span class="line">        loss = criterion(outputs.logits, labels)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Backpropagation and optimization</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        total_loss += loss.item()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"Epoch: <span class="subst">{epoch+<span class="number">1</span>}</span>, Loss: <span class="subst">{total_loss/<span class="built_in">len</span>(train_loader)}</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">"""### 4. Visualize Attention Weights of the Same Sentence (After Fine-tuning)</span></span><br><span class="line"><span class="string">Visualize the attention weights of the same sentence using the model after Fine-tuning. You can reuse the visualize_attention function provided earlier:</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">visualize_attention(sample_sentence, model, tokenizer)</span><br></pre></td></tr></tbody></table></figure>

<p>In this example, the BERT model is fine-tuned on the movie reviews dataset for sentiment analysis.</p>
<h2 id="What-is-Further-Pretraining"><a href="#What-is-Further-Pretraining" class="headerlink" title="What is Further Pretraining?"></a>What is Further Pretraining?</h2><p>Further Pretraining, also known as Domain-adaptive Pretraining, is where a pretrained model is further trained on a new dataset that is more closely related to the specific domain of interest but not necessarily labeled for a specific task.</p>
<h3 id="Example-Scenario-Legal-Document-Analysis"><a href="#Example-Scenario-Legal-Document-Analysis" class="headerlink" title="Example Scenario: Legal Document Analysis"></a>Example Scenario: Legal Document Analysis</h3><p>Suppose youâ€™re working on legal documents and wish to leverage a language model trained on general texts.</p>
<h4 id="Code-Snippet-for-Further-Pretraining"><a href="#Code-Snippet-for-Further-Pretraining" class="headerlink" title="Code Snippet for Further Pretraining"></a>Code Snippet for Further Pretraining</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertModel, BertTokenizer</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load a pre-trained BERT model and tokenizer</span></span><br><span class="line">model = BertModel.from_pretrained(<span class="string">'bert-base-uncased'</span>)</span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">'bert-base-uncased'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Prepare the legal documents dataset</span></span><br><span class="line"><span class="comment"># Assume 'legal_documents' is a list of text from legal documents</span></span><br><span class="line">encoded_input = tokenizer(legal_documents, padding=<span class="literal">True</span>, truncation=<span class="literal">True</span>, max_length=<span class="number">512</span>, return_tensors=<span class="string">'pt'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Further pretrain the model</span></span><br><span class="line"><span class="comment"># This step typically involves masked language modeling or other pretraining objectives</span></span><br><span class="line"><span class="comment"># Here we provide a conceptual example</span></span><br><span class="line">model.train()</span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> encoded_input:</span><br><span class="line">    outputs = model(**batch)</span><br><span class="line">    <span class="comment"># ... Perform further training steps</span></span><br></pre></td></tr></tbody></table></figure>

<p>In this case, the BERT model is further pretrained on a legal document dataset, making it more adept at understanding legal jargon and concepts before being fine-tuned on a specific legal NLP task.</p>
<h2 id="Key-Differences"><a href="#Key-Differences" class="headerlink" title="Key Differences"></a>Key Differences</h2><ul>
<li><strong>Purpose</strong>: Fine-tuning is tailored for a specific task with labeled data, while Further Pretraining is about adapting the model to a specific domain or style of language.</li>
<li><strong>Dataset</strong>: Fine-tuning uses task-specific, labeled datasets. Further Pretraining uses larger, domain-specific datasets, which may not be labeled for a specific task.</li>
<li><strong>Training Objective</strong>: Fine-tuning involves adjusting the model to make specific predictions, while Further Pretraining focuses on general language understanding in a new domain.</li>
</ul>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Both Fine-tuning and Further Pretraining are powerful techniques in NLP. By understanding their differences and applications, we can better leverage large language models to solve diverse and complex tasks in various domains. Whether youâ€™re building a sentiment analysis model for social media posts or adapting a model to understand legal documents, these techniques offer robust solutions in the ever-evolving field of NLP.</p>
<hr>
<p><strong>Note</strong>: The code examples provided are conceptual and require a suitable environment setup, including necessary libraries and datasets, for execution.</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>æ–‡ç« ä½œè€…: </span><span class="post-copyright-info"><a href="https://chenhuiyu.github.io">Huiyu Chen</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>æ–‡ç« é“¾æ¥: </span><span class="post-copyright-info"><a href="https://chenhuiyu.github.io/2024/02/19/NLP%20Insights/Understanding%20the%20Differences%20Between%20Fine-tuning%20and%20Further%20Pretraining%20in%20Large%20Language%20Models.zh-CN/">https://chenhuiyu.github.io/2024/02/19/NLP%20Insights/Understanding%20the%20Differences%20Between%20Fine-tuning%20and%20Further%20Pretraining%20in%20Large%20Language%20Models.zh-CN/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>ç‰ˆæƒå£°æ˜: </span><span class="post-copyright-info">æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ«å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨ <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº <a href="https://chenhuiyu.github.io" target="_blank">é»‘å¤´å‘†é±¼è¿›åŒ–ä¹‹æ—…</a>ï¼</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/LLM/">LLM</a></div><div class="post-share"><div class="social-share" data-image="/img/butterfly-icon.png" data-sites="facebook,x,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2024/02/19/NLP%20Insights/%E7%90%86%E8%A7%A3%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%ADFine-tuning%E5%92%8CFurther%20Pretraining%E7%9A%84%E5%8C%BA%E5%88%AB.en/" title="ç†è§£å¤§å‹è¯­è¨€æ¨¡å‹ä¸­Fine-tuningå’ŒFurther Pretrainingçš„åŒºåˆ«"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">ä¸Šä¸€ç¯‡</div><div class="info-item-2">ç†è§£å¤§å‹è¯­è¨€æ¨¡å‹ä¸­Fine-tuningå’ŒFurther Pretrainingçš„åŒºåˆ«</div></div><div class="info-2"><div class="info-item-1">ç†è§£å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ Fine-tuning å’Œ Further Pretraining çš„åŒºåˆ«åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¦‚ GPT å’Œ BERT çš„å‡ºç°ï¼Œå½»åº•æ”¹å˜äº†æˆ‘ä»¬å¤„ç†æ–‡æœ¬åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æå’Œé—®ç­”ç­‰ä»»åŠ¡çš„æ–¹å¼ã€‚åœ¨è¿™äº›æ¨¡å‹çš„åº”ç”¨ä¸­ï¼ŒFine-tuningï¼ˆå¾®è°ƒï¼‰å’Œ Further Pretrainingï¼ˆè¿›ä¸€æ­¥é¢„è®­ç»ƒï¼‰æ˜¯ä¸¤ç§å…³é”®æŠ€æœ¯ã€‚è™½ç„¶å®ƒä»¬çœ‹èµ·æ¥ç›¸ä¼¼ï¼Œä½†å®é™…ä¸ŠæœåŠ¡äº NLP æµç¨‹ä¸­çš„ä¸åŒéœ€æ±‚å’Œåœºæ™¯ã€‚ ä»€ä¹ˆæ˜¯ Fine-tuningï¼ŸFine-tuning æ˜¯æŒ‡åœ¨ç‰¹å®šä»»åŠ¡çš„æ•°æ®é›†ä¸Šè¿›ä¸€æ­¥è®­ç»ƒï¼ˆæˆ–â€œå¾®è°ƒâ€ï¼‰ä¸€ä¸ªé¢„è®­ç»ƒå¥½çš„æ¨¡å‹çš„è¿‡ç¨‹ã€‚è¿™ç§æ–¹æ³•åœ¨æ•°æ®é›†ç›¸å¯¹è¾ƒå°ä½†æ ‡æ³¨è‰¯å¥½çš„æƒ…å†µä¸‹ç‰¹åˆ«æœ‰æ•ˆã€‚ ç¤ºä¾‹åœºæ™¯ï¼šæƒ…æ„Ÿåˆ†æå‡è®¾ä½ æœ‰ä¸€ç»„ç”µå½±è¯„è®ºæ•°æ®ï¼Œæ¯æ¡è¯„è®ºéƒ½æ ‡è®°äº†æ­£é¢æˆ–è´Ÿé¢æƒ…æ„Ÿã€‚ä½ æƒ³åˆ›å»ºä¸€ä¸ªæ¨¡å‹æ¥é¢„æµ‹è¯„è®ºçš„æƒ…æ„Ÿã€‚ Python ä»£ç ç¤ºä¾‹ï¼ˆä½¿ç”¨ PyTorch å’Œ HuggingFace çš„ Transformersï¼‰This notebook demonstrates the fine-tuning of a BERT model on the IMDB dataset for sen...</div></div></div></a><a class="pagination-related" href="/2024/02/19/NLP%20Insights/Understanding%20the%20Differences%20Between%20Fine-tuning%20and%20Further%20Pretraining%20in%20Large%20Language%20Models.en/" title="Understanding the Differences Between Fine-tuning and Further Pretraining in Large Language Models"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">ä¸‹ä¸€ç¯‡</div><div class="info-item-2">Understanding the Differences Between Fine-tuning and Further Pretraining in Large Language Models</div></div><div class="info-2"><div class="info-item-1">Understanding the Differences Between Fine-tuning and Further Pretraining in Large Language ModelsIn the world of Natural Language Processing (NLP), the advent of large language models like GPT and BERT has revolutionized how we approach tasks such as text classification, sentiment analysis, and question-answering. Two pivotal techniques in leveraging these models are Fine-tuning and Further Pretraining. While they may seem similar at a glance, they cater to different needs and scenarios in t...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>ç›¸å…³æ¨è</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2023/07/28/NLP%20Insights/LONGNET.zh-CN/" title="LONGNET - Scaling Transformers to 1,000,000,000 Tokens"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2023-07-28</div><div class="info-item-2">LONGNET - Scaling Transformers to 1,000,000,000 Tokens</div></div><div class="info-2"><div class="info-item-1">LONGNETï¼šå°†Transformeræ‰©å±•åˆ°10äº¿ä¸ªæ ‡è®°åœ¨æœ¬ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†è¯¦ç»†è®¨è®ºä¸€ä¸ªè¿‘æœŸå‘å¸ƒçš„å…ˆè¿›æ¨¡å‹â€”â€”â€œLongNetâ€ã€‚è¯¥æ¨¡å‹ç”±å¾®è½¯äºšæ´²ç ”ç©¶é™¢ç ”å‘ï¼Œäºå¤§çº¦ä¸¤å‘¨å‰æ­£å¼å…¬å¸ƒã€‚LongNetåŸºäºTransformeræ¨¡å‹æ„å»ºï¼Œå…¶æ ¸å¿ƒç†å¿µåœ¨äºæ‹“å±•Transformerçš„åº”ç”¨è§„æ¨¡ã€‚å€¼å¾—ä¸€æçš„æ˜¯ï¼Œç ”ç©¶å›¢é˜ŸæˆåŠŸåœ°å°†å…¶æ‰©å±•è‡³å¤„ç†10äº¿ä¸ªä»¤ç‰Œçš„è§„æ¨¡ã€‚å¯¹äºç†Ÿæ‚‰è¯­è¨€æ¨¡å‹çš„äººæ¥è¯´ï¼Œä¼šæ˜ç™½åºåˆ—é•¿åº¦å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œå› ä¸ºåºåˆ—é•¿åº¦å†³å®šäº†åœ¨æ‰§è¡Œæ³¨æ„åŠ›æœºåˆ¶æ—¶ï¼Œèƒ½å¤Ÿå…³è”çš„ä»¤ç‰Œæ•°é‡ï¼Œä»è€Œå½±å“æ¨¡å‹å¯ä»¥è·å–çš„ä¸Šä¸‹æ–‡ä¿¡æ¯é•¿åº¦ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¸Œæœ›åƒGPTè¿™æ ·çš„æ¨¡å‹èƒ½æ‹¥æœ‰æ›´é•¿çš„ä¸Šä¸‹æ–‡ï¼Œä½¿å¾—æ¨¡å‹å¯ä»¥å‚è€ƒæ›´ä¹…ä¹‹å‰çš„å•è¯æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªä»¤ç‰Œã€‚è€ŒLongNetå°±æˆåŠŸåœ°å°†è¿™ä¸ªèƒ½åŠ›æ‰©å±•åˆ°äº†10äº¿ä¸ªä»¤ç‰Œã€‚ä»¥ä¸‹å›¾ä¸ºä¾‹ï¼Œå¯ä»¥æ¸…æ™°çœ‹å‡ºï¼ŒGPTçš„åºåˆ—é•¿åº¦ä»…ä¸º512ï¼Œè€ŒPower Transformerçš„åºåˆ—é•¿åº¦å¯æ‰©å±•è‡³12ã€000ã€64ã€262ã€000ã€ç”šè‡³1000ä¸‡ï¼Œç„¶è€ŒLongNetå°†åºåˆ—é•¿åº¦æ‰©å±•è‡³æƒŠäººçš„10äº¿ä¸ªä»¤ç‰Œã€‚è¯•æƒ³ä¸€ä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥å°†æ‰€æœ‰ç»´åŸºç™¾ç§‘çš„æ–‡æœ¬ä¿¡æ¯è¾“å…¥åˆ°æ¨¡å‹ä¸­ï¼Œæ¨¡å‹å¯ä»¥åˆ©ç”¨æ‰€æœ‰è¿™äº›ä»¤ç‰Œè¿›è¡Œæ³¨æ„åŠ›è®¡ç®—ã€‚æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬é¦–å…ˆæ¥äº†è§£ä¸€ä¸‹...</div></div></div></a><a class="pagination-related" href="/2025/03/06/NLP%20Insights/Decoder%E6%A8%A1%E5%9E%8B%E5%92%8CEncoder%E6%A8%A1%E5%9E%8B%E5%9C%A8Padding%E4%B8%AD%E7%9A%84%E4%B8%8D%E5%90%8C.zh-CN/" title="Decoder-onlyä¸Encoder-onlyæ¨¡å‹Paddingç­–ç•¥çš„å·®å¼‚"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-06</div><div class="info-item-2">Decoder-onlyä¸Encoder-onlyæ¨¡å‹Paddingç­–ç•¥çš„å·®å¼‚</div></div><div class="info-2"><div class="info-item-1">ğŸ“Œ Padding çš„å«ä¹‰åœ¨å¤§æ¨¡å‹ (LLM) ä¸­ï¼Œpadding æ˜¯ç”¨äºå°†ä¸åŒé•¿åº¦çš„åºåˆ—è°ƒæ•´ä¸ºåŒä¸€é•¿åº¦çš„æ–¹æ³•ï¼Œä»¥ä¾¿äºæ‰¹é‡ (batch) å¤„ç†ã€‚ ä¾‹å¦‚ï¼š 12å¥å­1: "I love NLP"å¥å­2: "Padding is useful in LLM training"  ä½¿ç”¨ &lt;pad&gt; token è¿›è¡Œå¯¹é½ï¼š 12"I love NLP &lt;pad&gt; &lt;pad&gt; &lt;pad&gt;""Padding is useful in LLM training"   ğŸ“Œ Padding ä½ç½®çš„é€‰æ‹©ï¼šLeft vs RightPadding æœ‰ä¸¤ç§å¸¸è§æ–¹å¼ï¼š  Right paddingï¼ˆå³å¡«å……ï¼‰ï¼š 1"I love NLP &lt;pad&gt; &lt;pad&gt;"  Left paddingï¼ˆå·¦å¡«å……ï¼‰ï¼š 1"&lt;pad&gt; &lt;pad&gt; I love NLP"  é€šå¸¸ï¼š  Decoder-only æ¨¡å‹ï¼ˆå¦‚ GPT, Llamaï¼‰ï¼šé‡‡ç”¨ Left padding Encoder-only æ¨¡å‹ï¼ˆå¦‚ BERTï¼‰ï¼šé‡‡ç”¨...</div></div></div></a><a class="pagination-related" href="/2023/07/27/NLP%20Insights/Prompt%20Engineering.en/" title="Prompt Engineering"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2023-07-27</div><div class="info-item-2">Prompt Engineering</div></div><div class="info-2"><div class="info-item-1">Prompt EngineeringPrompt Engineering, ä¹Ÿè¢«ç§°ä¸ºä¸Šä¸‹æ–‡æç¤ºï¼Œæ˜¯æŒ‡åœ¨ä¸æ›´æ–°æ¨¡å‹æƒé‡çš„æƒ…å†µä¸‹ï¼Œä¸LLMï¼ˆè¯­è¨€æ¨¡å‹ï¼‰è¿›è¡Œäº¤äº’ä»¥å¼•å¯¼å…¶äº§ç”ŸæœŸæœ›è¾“å‡ºçš„æ–¹æ³•ã€‚å®ƒæ˜¯ä¸€é—¨å®è¯ç§‘å­¦ï¼Œæç¤ºå·¥ç¨‹æ–¹æ³•çš„æ•ˆæœåœ¨ä¸åŒæ¨¡å‹ä¹‹é—´å¯èƒ½ä¼šæœ‰å¾ˆå¤§çš„å·®å¼‚ï¼Œå› æ­¤éœ€è¦è¿›è¡Œå¤§é‡çš„å®éªŒå’Œè¯•æ¢ã€‚ æœ¬æ–‡ä»…å…³æ³¨è‡ªå›å½’è¯­è¨€æ¨¡å‹çš„æç¤ºå·¥ç¨‹ï¼Œä¸æ¶‰åŠå¡«ç©ºæµ‹è¯•ã€å›¾åƒç”Ÿæˆæˆ–å¤šæ¨¡æ€æ¨¡å‹ã€‚åœ¨æœ¬è´¨ä¸Šï¼Œæç¤ºå·¥ç¨‹çš„ç›®æ ‡æ˜¯å®ç°æ¨¡å‹çš„å¯¹é½å’Œå¯æ“æ§æ€§ã€‚æ‚¨å¯ä»¥æŸ¥é˜…æˆ‘ä¹‹å‰å…³äºå¯æ§æ–‡æœ¬ç”Ÿæˆçš„å¸–å­ã€‚ åŸºæœ¬æç¤ºæ–¹æ³•zero-shotå­¦ä¹ å’Œfew-shotå­¦ä¹ æ˜¯ä¸¤ç§æœ€åŸºæœ¬çš„æç¤ºæ¨¡å‹æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•ç”±è®¸å¤šLLMè®ºæ–‡é¦–åˆ›ï¼Œå¹¶ä¸”é€šå¸¸ç”¨äºè¯„ä¼°LLMæ€§èƒ½ã€‚ zero-shotå­¦ä¹ zero-shotå­¦ä¹ æ˜¯å°†ä»»åŠ¡æ–‡æœ¬ç›´æ¥è¾“å…¥æ¨¡å‹å¹¶è¦æ±‚è·å¾—ç»“æœã€‚ ï¼ˆæ‰€æœ‰æƒ…æ„Ÿåˆ†æç¤ºä¾‹æ¥è‡ªäºSST-2æ•°æ®é›†ï¼‰ 12Text: i'll bet the video game is a lot more fun than the film.Sentiment: few-shotå­¦ä¹ few-shotå­¦ä¹ é€šè¿‡æä¾›ä¸€ç»„é«˜è´¨é‡çš„ç¤ºä¾‹æ¼”ç¤ºï¼Œæ¯ä¸ªç¤ºä¾‹éƒ½åŒ…å«ç›®æ ‡ä»»åŠ¡çš„è¾“å…¥å’ŒæœŸæœ›è¾“å‡ºã€‚å½“æ¨¡å‹é¦–...</div></div></div></a><a class="pagination-related" href="/2025/03/06/NLP%20Insights/Differences%20in%20Padding%20Strategies%20Between%20Decoder-only%20and%20Encoder-only%20Models.zh-CN/" title="Differences in Padding Strategies Between Decoder-only and Encoder-only Models"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-06</div><div class="info-item-2">Differences in Padding Strategies Between Decoder-only and Encoder-only Models</div></div><div class="info-2"><div class="info-item-1">ğŸ“Œ What is Padding?In Large Language Models (LLMs), padding is a method used to standardize sequence lengths for batch processing. For example: 12Sentence 1: "I love NLP"Sentence 2: "Padding is useful in LLM training"  Using the &lt;pad&gt; token for alignment: 12"I love NLP &lt;pad&gt; &lt;pad&gt; &lt;pad&gt;""Padding is useful in LLM training"   ğŸ“Œ Padding Positioning: Left vs RightThere are two common padding strategies:  Right padding: 1"I love NLP &lt;pad&gt; &lt;pad&gt;"  Left padding: ...</div></div></div></a><a class="pagination-related" href="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1.zh-CN/" title="æ¨ç† LLM çš„å¯è§†åŒ–æŒ‡å—ï¼šæ¢ç´¢æ¨ç†æ—¶è®¡ç®—æŠ€æœ¯ä¸ DeepSeek-R1"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-02-11</div><div class="info-item-2">æ¨ç† LLM çš„å¯è§†åŒ–æŒ‡å—ï¼šæ¢ç´¢æ¨ç†æ—¶è®¡ç®—æŠ€æœ¯ä¸ DeepSeek-R1</div></div><div class="info-2"><div class="info-item-1">æ¨ç† LLM çš„å¯è§†åŒ–æŒ‡å—ï¼šæ¢ç´¢æ¨ç†æ—¶è®¡ç®—æŠ€æœ¯ä¸ DeepSeek-R1åŸæ–‡åœ°å€ï¼šA Visual Guide to Reasoning LLMs ğŸ“… ä½œè€…ï¼šMaarten Grootendorst ğŸ“† æ—¥æœŸï¼š2025 å¹´ 2 æœˆ 3 æ—¥  ğŸ“Œ å¼•è¨€DeepSeek-R1ã€OpenAI o3-mini å’Œ Google Gemini 2.0 Flash Thinking æ˜¯å¦‚ä½•é€šè¿‡â€œæ¨ç†â€æ¡†æ¶å°† LLMï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹, Large Language Modelsï¼‰ æ‰©å±•åˆ°æ–°é«˜åº¦çš„å…¸å‹ç¤ºä¾‹ã€‚ å®ƒä»¬æ ‡å¿—ç€ä» æ‰©å±•è®­ç»ƒæ—¶è®¡ç®—ï¼ˆtrain-time computeï¼‰ åˆ° æ‰©å±•æ¨ç†æ—¶è®¡ç®—ï¼ˆtest-time computeï¼‰ çš„èŒƒå¼è½¬å˜ã€‚ åœ¨æœ¬ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬æä¾›äº† è¶…è¿‡ 40 å¼ å®šåˆ¶å¯è§†åŒ–å›¾è¡¨ï¼Œå¸¦ä½ æ·±å…¥æ¢ç´¢ï¼š  æ¨ç† LLMï¼ˆReasoning LLMsï¼‰ é¢†åŸŸ æ¨ç†æ—¶è®¡ç®—ï¼ˆTest-Time Computeï¼‰ æœºåˆ¶ DeepSeek-R1 çš„æ ¸å¿ƒæ€æƒ³  æˆ‘ä»¬å°†é€æ­¥ä»‹ç»ç›¸å…³æ¦‚å¿µï¼Œå¸®åŠ©ä½ å»ºç«‹å¯¹è¿™ä¸€æ–°èŒƒå¼çš„ç›´è§‰ç†è§£ã€‚    ğŸ“– ä»€ä¹ˆæ˜¯æ¨ç† LLMï¼Ÿä¸æ™®é€š LLMï¼ˆLarge Langu...</div></div></div></a><a class="pagination-related" href="/2024/02/28/NLP%20Insights/Gorilla:%20Large%20Language%20Model%20Connected%20with%20Massive%20APIs.zh-CN/" title="Gorilla LLM å¤§è¯­è¨€æ¨¡å‹ç®€ä»‹"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-28</div><div class="info-item-2">Gorilla LLM å¤§è¯­è¨€æ¨¡å‹ç®€ä»‹</div></div><div class="info-2"><div class="info-item-1">Gorilla LLM å¤§è¯­è¨€æ¨¡å‹ç®€ä»‹ğŸ¦ Gorilla: Large Language Model Connected with Massive APIsLink: https://gorilla.cs.berkeley.edu/blogs/7_open_functions_v2.html  Berkeley åŠŸèƒ½è°ƒç”¨æ’è¡Œæ¦œBerkeley åŠŸèƒ½è°ƒç”¨æ’è¡Œæ¦œ åœ¨çº¿ä½“éªŒæ¨¡å‹ï¼šGorilla OpenFunctions-v2 ç½‘ç»œæ¼”ç¤º é¡¹ç›®è¯¦æƒ…ï¼šGitHub æ¨¡å‹ï¼ˆ7B å‚æ•°ï¼‰åœ¨ HuggingFace ä¸Šçš„é¡µé¢ï¼šgorilla-llm/gorilla-openfunctions-v2  1. ä¼¯å…‹åˆ©å‡½æ•°è°ƒç”¨æ’è¡Œæ¦œè‡ª 2022 å¹´åº•ä»¥æ¥ï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å‡­å€Ÿå…¶æ‰§è¡Œé€šç”¨ä»»åŠ¡çš„å¼ºå¤§èƒ½åŠ›ï¼Œæˆä¸ºä¼—äººå…³æ³¨çš„ç„¦ç‚¹ã€‚ä¸ä»…é™äºèŠå¤©åº”ç”¨ï¼Œå°†è¿™äº›æ¨¡å‹åº”ç”¨äºå¼€å‘å„ç±» AI åº”ç”¨å’Œè½¯ä»¶ï¼ˆå¦‚ Langchain, Llama Index, AutoGPT, Voyagerï¼‰å·²æˆä¸ºä¸€ç§è¶‹åŠ¿ã€‚GPT, Gemini, Llama, Mistral ç­‰æ¨¡å‹é€šè¿‡ä¸å¤–éƒ¨ä¸–ç•Œçš„äº¤äº’ï¼Œå¦‚å‡½æ•°è°ƒç”¨å’Œæ‰§è¡Œï¼Œå±•ç°äº†å…¶å·¨å¤§...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Huiyu Chen</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">æ–‡ç« </div><div class="length-num">125</div></a><a href="/tags/"><div class="headline">æ ‡ç­¾</div><div class="length-num">46</div></a><a href="/categories/"><div class="headline">åˆ†ç±»</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>å…¬å‘Š</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>ç›®å½•</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Understanding-the-Differences-Between-Fine-tuning-and-Further-Pretraining-in-Large-Language-Models"><span class="toc-number">1.</span> <span class="toc-text">Understanding the Differences Between Fine-tuning and Further Pretraining in Large Language Models</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#What-is-Fine-tuning"><span class="toc-number">1.1.</span> <span class="toc-text">What is Fine-tuning?</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Example-Scenario-Sentiment-Analysis"><span class="toc-number">1.1.1.</span> <span class="toc-text">Example Scenario: Sentiment Analysis</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Code-Snippet-in-Python-using-PyTorch-and-HuggingFace%E2%80%99s-Transformers"><span class="toc-number">1.1.1.1.</span> <span class="toc-text">Code Snippet in Python (using PyTorch and HuggingFaceâ€™s Transformers)</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#What-is-Further-Pretraining"><span class="toc-number">1.2.</span> <span class="toc-text">What is Further Pretraining?</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Example-Scenario-Legal-Document-Analysis"><span class="toc-number">1.2.1.</span> <span class="toc-text">Example Scenario: Legal Document Analysis</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Code-Snippet-for-Further-Pretraining"><span class="toc-number">1.2.1.1.</span> <span class="toc-text">Code Snippet for Further Pretraining</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Key-Differences"><span class="toc-number">1.3.</span> <span class="toc-text">Key Differences</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Conclusion"><span class="toc-number">1.4.</span> <span class="toc-text">Conclusion</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>æœ€æ–°æ–‡ç« </span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/21/NLP%20Insights/Evaluation%20of%20Generation-Based%20Large%20Language%20Models%20(LLMs):%20Opportunities%20and%20Challenges%20from%20Generation%20to%20Judgment.zh-CN/" title="Untitled">Untitled</a><time datetime="2026-02-20T22:15:00.000Z" title="å‘è¡¨äº 2026-02-21 06:15:00">2026-02-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/21/NLP%20Insights/Evaluation%20of%20Generation-Based%20Large%20Language%20Models%20(LLMs):%20Opportunities%20and%20Challenges%20from%20Generation%20to%20Judgment.en/" title="Untitled">Untitled</a><time datetime="2026-02-20T22:15:00.000Z" title="å‘è¡¨äº 2026-02-21 06:15:00">2026-02-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/24/NLP%20Insights/SeCom%20Redefining%20Memory%20Management%20in%20Conversational%20AI.en/" title="SeCom: Redefining Memory Management in Conversational AI">SeCom: Redefining Memory Management in Conversational AI</a><time datetime="2025-06-24T08:00:00.000Z" title="å‘è¡¨äº 2025-06-24 16:00:00">2025-06-24</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/24/NLP%20Insights/SeCom%20Redefining%20Memory%20Management%20in%20Conversational%20AI.zh-CN/" title="SeCom: Redefining Memory Management in Conversational AI">SeCom: Redefining Memory Management in Conversational AI</a><time datetime="2025-06-24T08:00:00.000Z" title="å‘è¡¨äº 2025-06-24 16:00:00">2025-06-24</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/24/NLP%20Insights/SeCom:%20%E9%87%8D%E6%96%B0%E5%AE%9A%E4%B9%89%E5%AF%B9%E8%AF%9DAI%E7%9A%84%E8%AE%B0%E5%BF%86%E7%AE%A1%E7%90%86.en/" title="SeCom: é‡æ–°å®šä¹‰å¯¹è¯AIçš„è®°å¿†ç®¡ç†">SeCom: é‡æ–°å®šä¹‰å¯¹è¯AIçš„è®°å¿†ç®¡ç†</a><time datetime="2025-06-24T08:00:00.000Z" title="å‘è¡¨äº 2025-06-24 16:00:00">2025-06-24</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 - 2026 By Huiyu Chen</span><span class="framework-info"><span>æ¡†æ¶ </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 6.3.0</a><span class="footer-separator">|</span><span>ä¸»é¢˜ </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.5.4</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="é˜…è¯»æ¨¡å¼"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="æ—¥é—´å’Œå¤œé—´æ¨¡å¼åˆ‡æ¢"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="å•æ å’ŒåŒæ åˆ‡æ¢"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="è®¾ç½®"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="ç›®å½•"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="å›åˆ°é¡¶éƒ¨"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.4"></script><script src="/js/main.js?v=5.5.4"></script><div class="js-pjax"></div><script src="/js/lang-switch.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>