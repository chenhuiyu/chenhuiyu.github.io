<!DOCTYPE html><html lang="[&quot;en&quot;]" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>ç†è§£å¤§å‹è¯­è¨€æ¨¡å‹ä¸­Fine-tuningå’ŒFurther Pretrainingçš„åŒºåˆ« | é»‘å¤´å‘†é±¼è¿›åŒ–ä¹‹æ—…</title><meta name="author" content="Huiyu Chen"><meta name="copyright" content="Huiyu Chen"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="ç†è§£å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ Fine-tuning å’Œ Further Pretraining çš„åŒºåˆ«åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¦‚ GPT å’Œ BERT çš„å‡ºç°ï¼Œå½»åº•æ”¹å˜äº†æˆ‘ä»¬å¤„ç†æ–‡æœ¬åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æå’Œé—®ç­”ç­‰ä»»åŠ¡çš„æ–¹å¼ã€‚åœ¨è¿™äº›æ¨¡å‹çš„åº”ç”¨ä¸­ï¼ŒFine-tuningï¼ˆå¾®è°ƒï¼‰å’Œ Further Pretrainingï¼ˆè¿›ä¸€æ­¥é¢„è®­ç»ƒï¼‰æ˜¯ä¸¤ç§å…³é”®æŠ€æœ¯ã€‚è™½ç„¶å®ƒä»¬çœ‹èµ·æ¥ç›¸ä¼¼ï¼Œä½†å®é™…ä¸ŠæœåŠ¡äº NLP æµç¨‹">
<meta property="og:type" content="article">
<meta property="og:title" content="ç†è§£å¤§å‹è¯­è¨€æ¨¡å‹ä¸­Fine-tuningå’ŒFurther Pretrainingçš„åŒºåˆ«">
<meta property="og:url" content="https://chenhuiyu.github.io/2024/02/19/NLP%20Insights/%E7%90%86%E8%A7%A3%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%ADFine-tuning%E5%92%8CFurther%20Pretraining%E7%9A%84%E5%8C%BA%E5%88%AB/index.html">
<meta property="og:site_name" content="é»‘å¤´å‘†é±¼è¿›åŒ–ä¹‹æ—…">
<meta property="og:description" content="ç†è§£å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ Fine-tuning å’Œ Further Pretraining çš„åŒºåˆ«åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¦‚ GPT å’Œ BERT çš„å‡ºç°ï¼Œå½»åº•æ”¹å˜äº†æˆ‘ä»¬å¤„ç†æ–‡æœ¬åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æå’Œé—®ç­”ç­‰ä»»åŠ¡çš„æ–¹å¼ã€‚åœ¨è¿™äº›æ¨¡å‹çš„åº”ç”¨ä¸­ï¼ŒFine-tuningï¼ˆå¾®è°ƒï¼‰å’Œ Further Pretrainingï¼ˆè¿›ä¸€æ­¥é¢„è®­ç»ƒï¼‰æ˜¯ä¸¤ç§å…³é”®æŠ€æœ¯ã€‚è™½ç„¶å®ƒä»¬çœ‹èµ·æ¥ç›¸ä¼¼ï¼Œä½†å®é™…ä¸ŠæœåŠ¡äº NLP æµç¨‹">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://chenhuiyu.github.io/img/butterfly-icon.png">
<meta property="article:published_time" content="2024-02-19T07:10:29.000Z">
<meta property="article:modified_time" content="2026-02-20T21:56:22.876Z">
<meta property="article:author" content="Huiyu Chen">
<meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://chenhuiyu.github.io/img/butterfly-icon.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "ç†è§£å¤§å‹è¯­è¨€æ¨¡å‹ä¸­Fine-tuningå’ŒFurther Pretrainingçš„åŒºåˆ«",
  "url": "https://chenhuiyu.github.io/2024/02/19/NLP%20Insights/%E7%90%86%E8%A7%A3%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%ADFine-tuning%E5%92%8CFurther%20Pretraining%E7%9A%84%E5%8C%BA%E5%88%AB/",
  "image": "https://chenhuiyu.github.io/img/butterfly-icon.png",
  "datePublished": "2024-02-19T07:10:29.000Z",
  "dateModified": "2026-02-20T21:56:22.876Z",
  "author": [
    {
      "@type": "Person",
      "name": "Huiyu Chen",
      "url": "https://chenhuiyu.github.io"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://chenhuiyu.github.io/2024/02/19/NLP%20Insights/%E7%90%86%E8%A7%A3%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%ADFine-tuning%E5%92%8CFurther%20Pretraining%E7%9A%84%E5%8C%BA%E5%88%AB/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=5.5.4"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: 'Copy Successful',
    error: 'Copy Failed',
    noSupport: 'Browser Not Supported'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'Just now',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.13.0/dist/infinitegrid.min.js',
    buttonText: 'Load More'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'ç†è§£å¤§å‹è¯­è¨€æ¨¡å‹ä¸­Fine-tuningå’ŒFurther Pretrainingçš„åŒºåˆ«',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 6.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="é»‘å¤´å‘†é±¼è¿›åŒ–ä¹‹æ—…" type="application/atom+xml">
</head><body><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">é»‘å¤´å‘†é±¼è¿›åŒ–ä¹‹æ—…</span></a><a class="nav-page-title" href="/"><span class="site-name">ç†è§£å¤§å‹è¯­è¨€æ¨¡å‹ä¸­Fine-tuningå’ŒFurther Pretrainingçš„åŒºåˆ«</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  Back to Home</span></span></a></span><div id="menus"></div></nav><div id="post-info"><h1 class="post-title">ç†è§£å¤§å‹è¯­è¨€æ¨¡å‹ä¸­Fine-tuningå’ŒFurther Pretrainingçš„åŒºåˆ«</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2024-02-19T07:10:29.000Z" title="Created 2024-02-19 15:10:29">2024-02-19</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2026-02-20T21:56:22.876Z" title="Updated 2026-02-21 05:56:22">2026-02-21</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/NLP-Insights/">NLP Insights</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="ç†è§£å¤§å‹è¯­è¨€æ¨¡å‹ä¸­-Fine-tuning-å’Œ-Further-Pretraining-çš„åŒºåˆ«"><a href="#ç†è§£å¤§å‹è¯­è¨€æ¨¡å‹ä¸­-Fine-tuning-å’Œ-Further-Pretraining-çš„åŒºåˆ«" class="headerlink" title="ç†è§£å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ Fine-tuning å’Œ Further Pretraining çš„åŒºåˆ«"></a>ç†è§£å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ Fine-tuning å’Œ Further Pretraining çš„åŒºåˆ«</h1><p>åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰é¢†åŸŸï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ï¼Œå¦‚ GPT å’Œ BERT çš„å‡ºç°ï¼Œå½»åº•æ”¹å˜äº†æˆ‘ä»¬å¤„ç†æ–‡æœ¬åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æå’Œé—®ç­”ç­‰ä»»åŠ¡çš„æ–¹å¼ã€‚åœ¨è¿™äº›æ¨¡å‹çš„åº”ç”¨ä¸­ï¼ŒFine-tuningï¼ˆå¾®è°ƒï¼‰å’Œ Further Pretrainingï¼ˆè¿›ä¸€æ­¥é¢„è®­ç»ƒï¼‰æ˜¯ä¸¤ç§å…³é”®æŠ€æœ¯ã€‚è™½ç„¶å®ƒä»¬çœ‹èµ·æ¥ç›¸ä¼¼ï¼Œä½†å®é™…ä¸ŠæœåŠ¡äº NLP æµç¨‹ä¸­çš„ä¸åŒéœ€æ±‚å’Œåœºæ™¯ã€‚</p>
<h2 id="ä»€ä¹ˆæ˜¯-Fine-tuningï¼Ÿ"><a href="#ä»€ä¹ˆæ˜¯-Fine-tuningï¼Ÿ" class="headerlink" title="ä»€ä¹ˆæ˜¯ Fine-tuningï¼Ÿ"></a>ä»€ä¹ˆæ˜¯ Fine-tuningï¼Ÿ</h2><p>Fine-tuning æ˜¯æŒ‡åœ¨ç‰¹å®šä»»åŠ¡çš„æ•°æ®é›†ä¸Šè¿›ä¸€æ­¥è®­ç»ƒï¼ˆæˆ–â€œå¾®è°ƒâ€ï¼‰ä¸€ä¸ªé¢„è®­ç»ƒå¥½çš„æ¨¡å‹çš„è¿‡ç¨‹ã€‚è¿™ç§æ–¹æ³•åœ¨æ•°æ®é›†ç›¸å¯¹è¾ƒå°ä½†æ ‡æ³¨è‰¯å¥½çš„æƒ…å†µä¸‹ç‰¹åˆ«æœ‰æ•ˆã€‚</p>
<h3 id="ç¤ºä¾‹åœºæ™¯ï¼šæƒ…æ„Ÿåˆ†æ"><a href="#ç¤ºä¾‹åœºæ™¯ï¼šæƒ…æ„Ÿåˆ†æ" class="headerlink" title="ç¤ºä¾‹åœºæ™¯ï¼šæƒ…æ„Ÿåˆ†æ"></a>ç¤ºä¾‹åœºæ™¯ï¼šæƒ…æ„Ÿåˆ†æ</h3><p>å‡è®¾ä½ æœ‰ä¸€ç»„ç”µå½±è¯„è®ºæ•°æ®ï¼Œæ¯æ¡è¯„è®ºéƒ½æ ‡è®°äº†æ­£é¢æˆ–è´Ÿé¢æƒ…æ„Ÿã€‚ä½ æƒ³åˆ›å»ºä¸€ä¸ªæ¨¡å‹æ¥é¢„æµ‹è¯„è®ºçš„æƒ…æ„Ÿã€‚</p>
<h4 id="Python-ä»£ç ç¤ºä¾‹ï¼ˆä½¿ç”¨-PyTorch-å’Œ-HuggingFace-çš„-Transformersï¼‰"><a href="#Python-ä»£ç ç¤ºä¾‹ï¼ˆä½¿ç”¨-PyTorch-å’Œ-HuggingFace-çš„-Transformersï¼‰" class="headerlink" title="Python ä»£ç ç¤ºä¾‹ï¼ˆä½¿ç”¨ PyTorch å’Œ HuggingFace çš„ Transformersï¼‰"></a>Python ä»£ç ç¤ºä¾‹ï¼ˆä½¿ç”¨ PyTorch å’Œ HuggingFace çš„ Transformersï¼‰</h4><p>This notebook demonstrates the fine-tuning of a BERT model on the IMDB dataset for sentiment analysis. For detailed code implementation, please refer to the following link:<a target="_blank" rel="noopener" href="https://colab.research.google.com/drive/15naxP8pNMoCCBMgMSOv4ETDRGL46YR38?usp=sharing">link</a>.</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> (</span><br><span class="line">    BertTokenizer,</span><br><span class="line">    BertForSequenceClassification,</span><br><span class="line">    Trainer,</span><br><span class="line">    TrainingArguments,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset, DatasetDict</span><br><span class="line"></span><br><span class="line"><span class="string">"""## 1.åŠ è½½å’Œå‡†å¤‡IMDBæ•°æ®é›†æ ·æœ¬</span></span><br><span class="line"><span class="string">é€‰å–ä¸€éƒ¨åˆ†æ•°æ®ç”¨äºFine-tuningã€‚</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># åŠ è½½IMDBæ•°æ®é›†</span></span><br><span class="line">dataset = load_dataset(<span class="string">'imdb'</span>, split=<span class="string">'train'</span>)</span><br><span class="line">small_dataset = dataset.shuffle(seed=<span class="number">42</span>).select(<span class="built_in">range</span>(<span class="number">10000</span>))  <span class="comment"># é€‰å–å‰10000ä¸ªæ ·æœ¬</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># åˆå§‹åŒ–tokenizer</span></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">"bert-base-uncased"</span>)</span><br><span class="line"></span><br><span class="line">device = <span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ç¼–ç æ•°æ®é›†</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">examples</span>):</span><br><span class="line">    <span class="keyword">return</span> tokenizer(examples[<span class="string">"text"</span>], padding=<span class="string">"max_length"</span>, truncation=<span class="literal">True</span>, max_length=<span class="number">512</span>)</span><br><span class="line"></span><br><span class="line">encoded_small_dataset = small_dataset.<span class="built_in">map</span>(encode, batched=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertModel</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">visualize_attention</span>(<span class="params">sentence, model, tokenizer</span>):</span><br><span class="line">    model.to(device)</span><br><span class="line">    <span class="comment"># å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># å°†è¾“å…¥æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥ç†è§£çš„å½¢å¼</span></span><br><span class="line">    inputs = tokenizer(sentence, return_tensors=<span class="string">"pt"</span>).to(device) <span class="comment"># ç¡®ä¿è¾“å…¥ä¹Ÿåœ¨ç›¸åŒè®¾å¤‡</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># ä½¿ç”¨æ¨¡å‹è·å–æ³¨æ„åŠ›æƒé‡</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        outputs = model(**inputs)</span><br><span class="line">    attentions = outputs.attentions</span><br><span class="line"></span><br><span class="line">    <span class="comment"># é€‰æ‹©è¦å¯è§†åŒ–çš„å±‚å’Œå¤´</span></span><br><span class="line">    layer = <span class="number">5</span></span><br><span class="line">    head = <span class="number">1</span></span><br><span class="line">    attention = attentions[layer][<span class="number">0</span>, head].cpu().numpy() <span class="comment"># å°†æ³¨æ„åŠ›æƒé‡ç§»å›CPUè¿›è¡Œå¯è§†åŒ–</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># è®¾ç½®å¯è§†åŒ–çš„tokens</span></span><br><span class="line">    tokens = tokenizer.convert_ids_to_tokens(inputs[<span class="string">"input_ids"</span>][<span class="number">0</span>].cpu()) <span class="comment"># åŒæ ·ç¡®ä¿tokensåœ¨CPUä¸Š</span></span><br><span class="line">    <span class="comment"># ç»˜åˆ¶æ³¨æ„åŠ›çŸ©é˜µ</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">10</span>, <span class="number">10</span>))</span><br><span class="line">    plt.matshow(attention, cmap=<span class="string">'viridis'</span>)</span><br><span class="line">    plt.xticks(<span class="built_in">range</span>(<span class="built_in">len</span>(tokens)), tokens, rotation=<span class="number">90</span>)</span><br><span class="line">    plt.yticks(<span class="built_in">range</span>(<span class="built_in">len</span>(tokens)), tokens)</span><br><span class="line">    plt.colorbar()</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="string">"""## 2. å¯è§†åŒ–ä¸€ä¸ªæ ·æœ¬å¥å­çš„æ³¨æ„åŠ›æƒé‡ï¼ˆæœªç»Fine-tuningï¼‰</span></span><br><span class="line"><span class="string">é€‰æ‹©æ•°æ®é›†ä¸­çš„ä¸€ä¸ªå¥å­å¹¶å±•ç¤ºå…¶åŸå§‹BERTæ¨¡å‹çš„æ³¨æ„åŠ›æƒé‡ã€‚</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ä½¿ç”¨æœªç»Fine-tuningçš„æ¨¡å‹</span></span><br><span class="line">model = BertModel.from_pretrained(<span class="string">"bert-base-uncased"</span>, output_attentions=<span class="literal">True</span>)</span><br><span class="line">sample_sentence = <span class="string">"I love this movie, it's fantastic!"</span></span><br><span class="line">visualize_attention(sample_sentence, model, tokenizer)</span><br><span class="line"></span><br><span class="line"><span class="string">"""## 3. Fine-tuning BERTæ¨¡å‹</span></span><br><span class="line"><span class="string">åœ¨é€‰å–çš„IMDBæ ·æœ¬ä¸Šè¿›è¡ŒFine-tuningã€‚</span></span><br><span class="line"><span class="string">### 3.1 å‡†å¤‡æ•°æ®åŠ è½½å™¨</span></span><br><span class="line"><span class="string">ä¸ºäº†è®­ç»ƒæ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦åˆ›å»ºPyTorchçš„DataLoaderã€‚è¿™å°†ä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æœ‰æ•ˆåœ°åŠ è½½æ•°æ®ã€‚</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">### 3.2 è®¾ç½®Fine-tuningç¯å¢ƒ</span></span><br><span class="line"><span class="string">åˆå§‹åŒ–æ¨¡å‹ã€ä¼˜åŒ–å™¨ä»¥åŠæŸå¤±å‡½æ•°ã€‚</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">### 3.3 Fine-tuningæ¨¡å‹</span></span><br><span class="line"><span class="string">æ‰§è¡ŒFine-tuningçš„è®­ç»ƒå¾ªç¯ã€‚æ‰§è¡Œä»¥ä¸Šä»£ç å°†åœ¨IMDBæ•°æ®é›†çš„å°æ ·æœ¬ä¸Šå¯¹BERTæ¨¡å‹è¿›è¡ŒFine-tuningã€‚è¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼Œå…·ä½“å–å†³äºæ‚¨çš„ç¡¬ä»¶é…ç½®ã€‚</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line"><span class="comment"># å°†æ•°æ®é›†è½¬æ¢ä¸ºPyTorch Tensor</span></span><br><span class="line">encoded_small_dataset.set_format(<span class="string">'torch'</span>, columns=[<span class="string">'input_ids'</span>, <span class="string">'attention_mask'</span>, <span class="string">'label'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># åˆ›å»ºæ•°æ®åŠ è½½å™¨</span></span><br><span class="line">train_loader = DataLoader(encoded_small_dataset, batch_size=<span class="number">8</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertConfig, BertForSequenceClassification</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># åŠ è½½é…ç½®å¹¶è®¾ç½®è¾“å‡ºæ³¨æ„åŠ›æƒé‡</span></span><br><span class="line">config = BertConfig.from_pretrained(<span class="string">"bert-base-uncased"</span>, output_attentions=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># åˆå§‹åŒ–ç”¨äºåºåˆ—åˆ†ç±»çš„BERTæ¨¡å‹</span></span><br><span class="line"><span class="comment"># ä½¿ç”¨æ›´æ–°åçš„é…ç½®åŠ è½½æ¨¡å‹</span></span><br><span class="line">model = BertForSequenceClassification.from_pretrained(<span class="string">"bert-base-uncased"</span>, config=config)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># è®¾ç½®ä¼˜åŒ–å™¨</span></span><br><span class="line">optimizer = optim.AdamW(model.parameters(), lr=<span class="number">5e-5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°</span></span><br><span class="line">criterion = torch.nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># è®¾ç½®è®­ç»ƒçš„è½®æ¬¡</span></span><br><span class="line">epochs = <span class="number">8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    model.train()</span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> train_loader:</span><br><span class="line">        <span class="comment"># å°†æ•°æ®ç§»è‡³GPU</span></span><br><span class="line">        input_ids = batch[<span class="string">'input_ids'</span>].to(device)</span><br><span class="line">        attention_mask = batch[<span class="string">'attention_mask'</span>].to(device)</span><br><span class="line">        labels = batch[<span class="string">'label'</span>].to(device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># æ¨¡å‹å‰å‘ä¼ æ’­</span></span><br><span class="line">        outputs = model(input_ids, attention_mask=attention_mask)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># è®¡ç®—æŸå¤±</span></span><br><span class="line">        loss = criterion(outputs.logits, labels)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># åå‘ä¼ æ’­å’Œä¼˜åŒ–</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        total_loss += loss.item()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"Epoch: <span class="subst">{epoch+<span class="number">1</span>}</span>, Loss: <span class="subst">{total_loss/<span class="built_in">len</span>(train_loader)}</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">"""### 4. å¯è§†åŒ–åŒä¸€å¥å­çš„æ³¨æ„åŠ›æƒé‡ï¼ˆç»è¿‡Fine-tuningï¼‰</span></span><br><span class="line"><span class="string">ä½¿ç”¨Fine-tuningåçš„æ¨¡å‹å†æ¬¡å¯è§†åŒ–åŒä¸€å¥å­çš„æ³¨æ„åŠ›æƒé‡ã€‚æ‚¨å¯ä»¥é‡ç”¨ä¹‹å‰æä¾›çš„visualize_attentionå‡½æ•°ï¼š</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">visualize_attention(sample_sentence, model, tokenizer)</span><br></pre></td></tr></tbody></table></figure>

<p>åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼ŒBERT æ¨¡å‹åœ¨ç”µå½±è¯„è®ºæ•°æ®é›†ä¸Šè¿›è¡Œäº† fine-tuningï¼Œç”¨äºæƒ…æ„Ÿåˆ†æã€‚</p>
<h2 id="ä»€ä¹ˆæ˜¯-Further-Pretrainingï¼Ÿ"><a href="#ä»€ä¹ˆæ˜¯-Further-Pretrainingï¼Ÿ" class="headerlink" title="ä»€ä¹ˆæ˜¯ Further Pretrainingï¼Ÿ"></a>ä»€ä¹ˆæ˜¯ Further Pretrainingï¼Ÿ</h2><p>Further Pretrainingï¼ˆä¹Ÿç§°ä¸º Domain-adaptive Pretrainingï¼Œé¢†åŸŸé€‚åº”æ€§é¢„è®­ç»ƒï¼‰æ˜¯åœ¨ä¸€ä¸ªæ–°çš„æ•°æ®é›†ä¸Šç»§ç»­è®­ç»ƒä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹çš„è¿‡ç¨‹ï¼Œè¿™ä¸ªæ–°çš„æ•°æ®é›†ä¸ç‰¹å®šçš„é¢†åŸŸæ›´ç›¸å…³ï¼Œä½†ä¸ä¸€å®šä¸ºç‰¹å®šä»»åŠ¡æ ‡æ³¨ã€‚</p>
<h3 id="ç¤ºä¾‹åœºæ™¯ï¼šæ³•å¾‹æ–‡æ¡£åˆ†æ"><a href="#ç¤ºä¾‹åœºæ™¯ï¼šæ³•å¾‹æ–‡æ¡£åˆ†æ" class="headerlink" title="ç¤ºä¾‹åœºæ™¯ï¼šæ³•å¾‹æ–‡æ¡£åˆ†æ"></a>ç¤ºä¾‹åœºæ™¯ï¼šæ³•å¾‹æ–‡æ¡£åˆ†æ</h3><p>å‡è®¾ä½ æ­£åœ¨å¤„ç†æ³•å¾‹æ–‡æ¡£ï¼Œå¹¶å¸Œæœ›åˆ©ç”¨ä¸€ä¸ªåœ¨é€šç”¨æ–‡æœ¬ä¸Šè®­ç»ƒçš„è¯­è¨€æ¨¡å‹ã€‚</p>
<h4 id="Further-Pretraining-çš„ä»£ç ç¤ºä¾‹"><a href="#Further-Pretraining-çš„ä»£ç ç¤ºä¾‹" class="headerlink" title="Further Pretraining çš„ä»£ç ç¤ºä¾‹"></a>Further Pretraining çš„ä»£ç ç¤ºä¾‹</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertModel, BertTokenizer</span><br><span class="line"></span><br><span class="line"><span class="comment"># åŠ è½½é¢„è®­ç»ƒçš„BERTæ¨¡å‹å’Œåˆ†è¯å™¨</span></span><br><span class="line">model = BertModel.from_pretrained(<span class="string">'bert-base-uncased'</span>)</span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">'bert-base-uncased'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># å‡†å¤‡æ³•å¾‹æ–‡æ¡£æ•°æ®é›†</span></span><br><span class="line"><span class="comment"># å‡è®¾'legal_documents'æ˜¯æ³•å¾‹æ–‡æ¡£çš„æ–‡æœ¬åˆ—è¡¨</span></span><br><span class="line">encoded_input = tokenizer(legal_documents, padding=<span class="literal">True</span>, truncation=<span class="literal">True</span>, max_length=<span class="number">512</span>, return_tensors=<span class="string">'pt'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ç»§ç»­é¢„è®­ç»ƒæ¨¡å‹</span></span><br><span class="line"><span class="comment"># è¿™ä¸€æ­¥é€šå¸¸åŒ…æ‹¬æ©ç è¯­è¨€å»ºæ¨¡æˆ–å…¶ä»–é¢„è®­ç»ƒç›®æ ‡</span></span><br><span class="line"><span class="comment"># è¿™é‡Œæä¾›ä¸€ä¸ªæ¦‚å¿µæ€§ç¤ºä¾‹</span></span><br><span class="line">model.train()</span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> encoded_input:</span><br><span class="line">    outputs = model(**batch)</span><br><span class="line">    <span class="comment"># ... æ‰§è¡Œè¿›ä¸€æ­¥è®­ç»ƒæ­¥éª¤</span></span><br></pre></td></tr></tbody></table></figure>

<p>åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼ŒBERT æ¨¡å‹åœ¨æ³•å¾‹æ–‡æ¡£æ•°æ®é›†ä¸Šè¿›è¡Œäº†è¿›ä¸€æ­¥çš„é¢„è®­ç»ƒï¼Œä½¿å…¶åœ¨è¿›è¡Œç‰¹å®šæ³•å¾‹ NLP ä»»åŠ¡çš„ fine-tuning ä¹‹å‰ï¼Œæ›´æ“…é•¿ç†è§£æ³•å¾‹æœ¯è¯­å’Œæ¦‚å¿µã€‚</p>
<h2 id="å…³é”®åŒºåˆ«"><a href="#å…³é”®åŒºåˆ«" class="headerlink" title="å…³é”®åŒºåˆ«"></a>å…³é”®åŒºåˆ«</h2><ul>
<li><strong>ç›®çš„</strong>ï¼šFine-tuning é’ˆå¯¹å…·æœ‰æ ‡ç­¾æ•°æ®çš„ç‰¹å®šä»»åŠ¡è¿›è¡Œæ¨¡å‹è°ƒæ•´ï¼Œè€Œ Further Pretraining åˆ™æ˜¯ä½¿æ¨¡å‹æ›´å¥½åœ°é€‚åº”ç‰¹å®šé¢†åŸŸæˆ–è¯­è¨€é£æ ¼ã€‚</li>
<li><strong>æ•°æ®é›†</strong>ï¼šFine-tuning ä½¿ç”¨ç‰¹å®šä»»åŠ¡çš„æ ‡æ³¨æ•°æ®é›†ã€‚Further Pretraining ä½¿ç”¨æ›´å¤§çš„ã€ç‰¹å®šé¢†åŸŸçš„æ•°æ®é›†ï¼Œè¿™äº›æ•°æ®é›†å¯èƒ½ä¸æ˜¯ä¸ºç‰¹å®šä»»åŠ¡æ ‡æ³¨çš„ã€‚</li>
<li><strong>è®­ç»ƒç›®æ ‡</strong>ï¼šFine-tuning æ¶‰åŠè°ƒæ•´æ¨¡å‹è¿›è¡Œç‰¹å®šé¢„æµ‹ï¼Œè€Œ Further Pretraining ä¾§é‡äºåœ¨æ–°é¢†åŸŸä¸­çš„é€šç”¨è¯­è¨€ç†è§£</li>
</ul>
<p>ã€‚</p>
<h2 id="ç»“è®º"><a href="#ç»“è®º" class="headerlink" title="ç»“è®º"></a>ç»“è®º</h2><p>Fine-tuning å’Œ Further Pretraining éƒ½æ˜¯ NLP é¢†åŸŸçš„å¼ºå¤§æŠ€æœ¯ã€‚é€šè¿‡ç†è§£å®ƒä»¬çš„åŒºåˆ«å’Œåº”ç”¨ï¼Œæˆ‘ä»¬å¯ä»¥æ›´å¥½åœ°åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹æ¥è§£å†³å„ç§é¢†åŸŸä¸­çš„å¤šæ ·åŒ–å’Œå¤æ‚ä»»åŠ¡ã€‚æ— è®ºä½ æ˜¯åœ¨æ„å»ºç¤¾äº¤åª’ä½“å¸–å­çš„æƒ…æ„Ÿåˆ†ææ¨¡å‹ï¼Œè¿˜æ˜¯è°ƒæ•´æ¨¡å‹ä»¥ç†è§£æ³•å¾‹æ–‡æ¡£ï¼Œè¿™äº›æŠ€æœ¯éƒ½ä¸º NLP é¢†åŸŸçš„ä¸æ–­å‘å±•æä¾›äº†ç¨³å¥çš„è§£å†³æ–¹æ¡ˆã€‚</p>
<hr>
<p><strong>æ³¨æ„</strong>ï¼šæä¾›çš„ä»£ç ç¤ºä¾‹æ˜¯æ¦‚å¿µæ€§çš„ï¼Œéœ€è¦é€‚å½“çš„ç¯å¢ƒè®¾ç½®ï¼ŒåŒ…æ‹¬å¿…è¦çš„åº“å’Œæ•°æ®é›†ï¼Œæ‰èƒ½æ‰§è¡Œã€‚</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="https://chenhuiyu.github.io">Huiyu Chen</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="https://chenhuiyu.github.io/2024/02/19/NLP%20Insights/%E7%90%86%E8%A7%A3%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%ADFine-tuning%E5%92%8CFurther%20Pretraining%E7%9A%84%E5%8C%BA%E5%88%AB/">https://chenhuiyu.github.io/2024/02/19/NLP%20Insights/%E7%90%86%E8%A7%A3%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%ADFine-tuning%E5%92%8CFurther%20Pretraining%E7%9A%84%E5%8C%BA%E5%88%AB/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles on this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless otherwise stated.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/LLM/">LLM</a></div><div class="post-share"><div class="social-share" data-image="/img/butterfly-icon.png" data-sites="facebook,x,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2024/02/19/Debugging%20Diaries/%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E5%A4%9A%E4%B8%AA%20Git%20%E6%9C%8D%E5%8A%A1%E7%9A%84%20SSH%20%E5%AF%86%E9%92%A5%E9%97%AE%E9%A2%98/" title="å¦‚ä½•è§£å†³å¤šä¸ª Git æœåŠ¡çš„ SSH å¯†é’¥é—®é¢˜"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">Previous</div><div class="info-item-2">å¦‚ä½•è§£å†³å¤šä¸ª Git æœåŠ¡çš„ SSH å¯†é’¥é—®é¢˜</div></div><div class="info-2"><div class="info-item-1">å¦‚ä½•è§£å†³å¤šä¸ª Git æœåŠ¡çš„ SSH å¯†é’¥é—®é¢˜ ğŸ—ï¸åœ¨ä½¿ç”¨ Git å’Œä¸åŒçš„ Git æœåŠ¡ï¼ˆå¦‚ GitHub å’Œ GitLabï¼‰æ—¶ï¼Œå¯èƒ½ä¼šé‡åˆ° SSH å¯†é’¥çš„é—®é¢˜ã€‚æœ¬æ–‡å°†æŒ‡å¯¼ä½ å¦‚ä½•è®¾ç½®å’Œé…ç½® SSH å¯†é’¥ï¼Œä»¥ä¾¿å¯ä»¥åŒæ—¶ä¸å¤šä¸ªæœåŠ¡é¡ºåˆ©å·¥ä½œã€‚ 1. ç”Ÿæˆ SSH å¯†é’¥ ğŸ”‘é¦–å…ˆï¼Œä¸ºæ¯ä¸ª Git æœåŠ¡ç”Ÿæˆä¸€ä¸ªç‹¬ç«‹çš„ SSH å¯†é’¥ã€‚ 1ssh-keygen -t rsa -b 4096 -C "your_email@example.com"  åœ¨ç”Ÿæˆå¯†é’¥æ—¶ï¼Œå°†æ¯ä¸ªå¯†é’¥ä¿å­˜ä¸ºä¸åŒçš„æ–‡ä»¶åï¼Œä¾‹å¦‚ id_rsa_github å’Œ id_rsa_gitlabã€‚ 2. å°† SSH å¯†é’¥æ·»åŠ åˆ° Git æœåŠ¡ ğŸŒç™»å½•åˆ°ä½ çš„ GitHub å’Œ GitLab è´¦æˆ·ï¼Œç„¶åå°†ç”Ÿæˆçš„å…¬é’¥ï¼ˆ.pub æ–‡ä»¶ï¼‰æ·»åŠ åˆ°å„è‡ªè´¦æˆ·çš„ SSH å¯†é’¥éƒ¨åˆ†ã€‚ 3. é…ç½® SSH âš™ï¸åˆ›å»ºæˆ–ç¼–è¾‘ ~/.ssh/config æ–‡ä»¶ï¼Œä¸ºæ¯ä¸ªæœåŠ¡é…ç½®ä¸åŒçš„ SSH å¯†é’¥ã€‚ 1234567891011# GitHubHost github.com  HostName github.com  User git  IdentityFile...</div></div></div></a><a class="pagination-related" href="/2024/02/19/NLP%20Insights/Understanding%20the%20Differences%20Between%20Fine-tuning%20and%20Further%20Pretraining%20in%20Large%20Language%20Models/" title="Understanding the Differences Between Fine-tuning and Further Pretraining in Large Language Models"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">Next</div><div class="info-item-2">Understanding the Differences Between Fine-tuning and Further Pretraining in Large Language Models</div></div><div class="info-2"><div class="info-item-1">Understanding the Differences Between Fine-tuning and Further Pretraining in Large Language ModelsIn the world of Natural Language Processing (NLP), the advent of large language models like GPT and BERT has revolutionized how we approach tasks such as text classification, sentiment analysis, and question-answering. Two pivotal techniques in leveraging these models are Fine-tuning and Further Pretraining. While they may seem similar at a glance, they cater to different needs and scenarios in t...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/" title="æ¨ç† LLM çš„å¯è§†åŒ–æŒ‡å—ï¼šæ¢ç´¢æ¨ç†æ—¶è®¡ç®—æŠ€æœ¯ä¸ DeepSeek-R1"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-02-11</div><div class="info-item-2">æ¨ç† LLM çš„å¯è§†åŒ–æŒ‡å—ï¼šæ¢ç´¢æ¨ç†æ—¶è®¡ç®—æŠ€æœ¯ä¸ DeepSeek-R1</div></div><div class="info-2"><div class="info-item-1">æ¨ç† LLM çš„å¯è§†åŒ–æŒ‡å—ï¼šæ¢ç´¢æ¨ç†æ—¶è®¡ç®—æŠ€æœ¯ä¸ DeepSeek-R1åŸæ–‡åœ°å€ï¼šA Visual Guide to Reasoning LLMs ğŸ“… ä½œè€…ï¼šMaarten Grootendorst ğŸ“† æ—¥æœŸï¼š2025 å¹´ 2 æœˆ 3 æ—¥  ğŸ“Œ å¼•è¨€DeepSeek-R1ã€OpenAI o3-mini å’Œ Google Gemini 2.0 Flash Thinking æ˜¯å¦‚ä½•é€šè¿‡â€œæ¨ç†â€æ¡†æ¶å°† LLMï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹, Large Language Modelsï¼‰ æ‰©å±•åˆ°æ–°é«˜åº¦çš„å…¸å‹ç¤ºä¾‹ã€‚ å®ƒä»¬æ ‡å¿—ç€ä» æ‰©å±•è®­ç»ƒæ—¶è®¡ç®—ï¼ˆtrain-time computeï¼‰ åˆ° æ‰©å±•æ¨ç†æ—¶è®¡ç®—ï¼ˆtest-time computeï¼‰ çš„èŒƒå¼è½¬å˜ã€‚ åœ¨æœ¬ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬æä¾›äº† è¶…è¿‡ 40 å¼ å®šåˆ¶å¯è§†åŒ–å›¾è¡¨ï¼Œå¸¦ä½ æ·±å…¥æ¢ç´¢ï¼š  æ¨ç† LLMï¼ˆReasoning LLMsï¼‰ é¢†åŸŸ æ¨ç†æ—¶è®¡ç®—ï¼ˆTest-Time Computeï¼‰ æœºåˆ¶ DeepSeek-R1 çš„æ ¸å¿ƒæ€æƒ³  æˆ‘ä»¬å°†é€æ­¥ä»‹ç»ç›¸å…³æ¦‚å¿µï¼Œå¸®åŠ©ä½ å»ºç«‹å¯¹è¿™ä¸€æ–°èŒƒå¼çš„ç›´è§‰ç†è§£ã€‚    ğŸ“– ä»€ä¹ˆæ˜¯æ¨ç† LLMï¼Ÿä¸æ™®é€š LLMï¼ˆLarge Langu...</div></div></div></a><a class="pagination-related" href="/2024/02/19/NLP%20Insights/Understanding%20the%20Differences%20Between%20Fine-tuning%20and%20Further%20Pretraining%20in%20Large%20Language%20Models/" title="Understanding the Differences Between Fine-tuning and Further Pretraining in Large Language Models"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-19</div><div class="info-item-2">Understanding the Differences Between Fine-tuning and Further Pretraining in Large Language Models</div></div><div class="info-2"><div class="info-item-1">Understanding the Differences Between Fine-tuning and Further Pretraining in Large Language ModelsIn the world of Natural Language Processing (NLP), the advent of large language models like GPT and BERT has revolutionized how we approach tasks such as text classification, sentiment analysis, and question-answering. Two pivotal techniques in leveraging these models are Fine-tuning and Further Pretraining. While they may seem similar at a glance, they cater to different needs and scenarios in t...</div></div></div></a><a class="pagination-related" href="/2024/02/28/NLP%20Insights/Gorilla:%20Large%20Language%20Model%20Connected%20with%20Massive%20APIs/" title="Gorilla LLM å¤§è¯­è¨€æ¨¡å‹ç®€ä»‹"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-28</div><div class="info-item-2">Gorilla LLM å¤§è¯­è¨€æ¨¡å‹ç®€ä»‹</div></div><div class="info-2"><div class="info-item-1">Gorilla LLM å¤§è¯­è¨€æ¨¡å‹ç®€ä»‹ğŸ¦ Gorilla: Large Language Model Connected with Massive APIsLink: https://gorilla.cs.berkeley.edu/blogs/7_open_functions_v2.html  Berkeley åŠŸèƒ½è°ƒç”¨æ’è¡Œæ¦œBerkeley åŠŸèƒ½è°ƒç”¨æ’è¡Œæ¦œ åœ¨çº¿ä½“éªŒæ¨¡å‹ï¼šGorilla OpenFunctions-v2 ç½‘ç»œæ¼”ç¤º é¡¹ç›®è¯¦æƒ…ï¼šGitHub æ¨¡å‹ï¼ˆ7B å‚æ•°ï¼‰åœ¨ HuggingFace ä¸Šçš„é¡µé¢ï¼šgorilla-llm/gorilla-openfunctions-v2  1. ä¼¯å…‹åˆ©å‡½æ•°è°ƒç”¨æ’è¡Œæ¦œè‡ª 2022 å¹´åº•ä»¥æ¥ï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å‡­å€Ÿå…¶æ‰§è¡Œé€šç”¨ä»»åŠ¡çš„å¼ºå¤§èƒ½åŠ›ï¼Œæˆä¸ºä¼—äººå…³æ³¨çš„ç„¦ç‚¹ã€‚ä¸ä»…é™äºèŠå¤©åº”ç”¨ï¼Œå°†è¿™äº›æ¨¡å‹åº”ç”¨äºå¼€å‘å„ç±» AI åº”ç”¨å’Œè½¯ä»¶ï¼ˆå¦‚ Langchain, Llama Index, AutoGPT, Voyagerï¼‰å·²æˆä¸ºä¸€ç§è¶‹åŠ¿ã€‚GPT, Gemini, Llama, Mistral ç­‰æ¨¡å‹é€šè¿‡ä¸å¤–éƒ¨ä¸–ç•Œçš„äº¤äº’ï¼Œå¦‚å‡½æ•°è°ƒç”¨å’Œæ‰§è¡Œï¼Œå±•ç°äº†å…¶å·¨å¤§...</div></div></div></a><a class="pagination-related" href="/2023/07/28/NLP%20Insights/LONGNET/" title="LONGNET - Scaling Transformers to 1,000,000,000 Tokens"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2023-07-28</div><div class="info-item-2">LONGNET - Scaling Transformers to 1,000,000,000 Tokens</div></div><div class="info-2"><div class="info-item-1">LONGNETï¼šå°†Transformeræ‰©å±•åˆ°10äº¿ä¸ªæ ‡è®°åœ¨æœ¬ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬å°†è¯¦ç»†è®¨è®ºä¸€ä¸ªè¿‘æœŸå‘å¸ƒçš„å…ˆè¿›æ¨¡å‹â€”â€”â€œLongNetâ€ã€‚è¯¥æ¨¡å‹ç”±å¾®è½¯äºšæ´²ç ”ç©¶é™¢ç ”å‘ï¼Œäºå¤§çº¦ä¸¤å‘¨å‰æ­£å¼å…¬å¸ƒã€‚LongNetåŸºäºTransformeræ¨¡å‹æ„å»ºï¼Œå…¶æ ¸å¿ƒç†å¿µåœ¨äºæ‹“å±•Transformerçš„åº”ç”¨è§„æ¨¡ã€‚å€¼å¾—ä¸€æçš„æ˜¯ï¼Œç ”ç©¶å›¢é˜ŸæˆåŠŸåœ°å°†å…¶æ‰©å±•è‡³å¤„ç†10äº¿ä¸ªä»¤ç‰Œçš„è§„æ¨¡ã€‚å¯¹äºç†Ÿæ‚‰è¯­è¨€æ¨¡å‹çš„äººæ¥è¯´ï¼Œä¼šæ˜ç™½åºåˆ—é•¿åº¦å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“ï¼Œå› ä¸ºåºåˆ—é•¿åº¦å†³å®šäº†åœ¨æ‰§è¡Œæ³¨æ„åŠ›æœºåˆ¶æ—¶ï¼Œèƒ½å¤Ÿå…³è”çš„ä»¤ç‰Œæ•°é‡ï¼Œä»è€Œå½±å“æ¨¡å‹å¯ä»¥è·å–çš„ä¸Šä¸‹æ–‡ä¿¡æ¯é•¿åº¦ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¸Œæœ›åƒGPTè¿™æ ·çš„æ¨¡å‹èƒ½æ‹¥æœ‰æ›´é•¿çš„ä¸Šä¸‹æ–‡ï¼Œä½¿å¾—æ¨¡å‹å¯ä»¥å‚è€ƒæ›´ä¹…ä¹‹å‰çš„å•è¯æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªä»¤ç‰Œã€‚è€ŒLongNetå°±æˆåŠŸåœ°å°†è¿™ä¸ªèƒ½åŠ›æ‰©å±•åˆ°äº†10äº¿ä¸ªä»¤ç‰Œã€‚ä»¥ä¸‹å›¾ä¸ºä¾‹ï¼Œå¯ä»¥æ¸…æ™°çœ‹å‡ºï¼ŒGPTçš„åºåˆ—é•¿åº¦ä»…ä¸º512ï¼Œè€ŒPower Transformerçš„åºåˆ—é•¿åº¦å¯æ‰©å±•è‡³12ã€000ã€64ã€262ã€000ã€ç”šè‡³1000ä¸‡ï¼Œç„¶è€ŒLongNetå°†åºåˆ—é•¿åº¦æ‰©å±•è‡³æƒŠäººçš„10äº¿ä¸ªä»¤ç‰Œã€‚è¯•æƒ³ä¸€ä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥å°†æ‰€æœ‰ç»´åŸºç™¾ç§‘çš„æ–‡æœ¬ä¿¡æ¯è¾“å…¥åˆ°æ¨¡å‹ä¸­ï¼Œæ¨¡å‹å¯ä»¥åˆ©ç”¨æ‰€æœ‰è¿™äº›ä»¤ç‰Œè¿›è¡Œæ³¨æ„åŠ›è®¡ç®—ã€‚æ¥ä¸‹æ¥ï¼Œè®©æˆ‘ä»¬é¦–å…ˆæ¥äº†è§£ä¸€ä¸‹...</div></div></div></a><a class="pagination-related" href="/2025/03/06/NLP%20Insights/Decoder%E6%A8%A1%E5%9E%8B%E5%92%8CEncoder%E6%A8%A1%E5%9E%8B%E5%9C%A8Padding%E4%B8%AD%E7%9A%84%E4%B8%8D%E5%90%8C/" title="Decoder-onlyä¸Encoder-onlyæ¨¡å‹Paddingç­–ç•¥çš„å·®å¼‚"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-06</div><div class="info-item-2">Decoder-onlyä¸Encoder-onlyæ¨¡å‹Paddingç­–ç•¥çš„å·®å¼‚</div></div><div class="info-2"><div class="info-item-1">ğŸ“Œ Padding çš„å«ä¹‰åœ¨å¤§æ¨¡å‹ (LLM) ä¸­ï¼Œpadding æ˜¯ç”¨äºå°†ä¸åŒé•¿åº¦çš„åºåˆ—è°ƒæ•´ä¸ºåŒä¸€é•¿åº¦çš„æ–¹æ³•ï¼Œä»¥ä¾¿äºæ‰¹é‡ (batch) å¤„ç†ã€‚ ä¾‹å¦‚ï¼š 12å¥å­1: "I love NLP"å¥å­2: "Padding is useful in LLM training"  ä½¿ç”¨ &lt;pad&gt; token è¿›è¡Œå¯¹é½ï¼š 12"I love NLP &lt;pad&gt; &lt;pad&gt; &lt;pad&gt;""Padding is useful in LLM training"   ğŸ“Œ Padding ä½ç½®çš„é€‰æ‹©ï¼šLeft vs RightPadding æœ‰ä¸¤ç§å¸¸è§æ–¹å¼ï¼š  Right paddingï¼ˆå³å¡«å……ï¼‰ï¼š 1"I love NLP &lt;pad&gt; &lt;pad&gt;"  Left paddingï¼ˆå·¦å¡«å……ï¼‰ï¼š 1"&lt;pad&gt; &lt;pad&gt; I love NLP"  é€šå¸¸ï¼š  Decoder-only æ¨¡å‹ï¼ˆå¦‚ GPT, Llamaï¼‰ï¼šé‡‡ç”¨ Left padding Encoder-only æ¨¡å‹ï¼ˆå¦‚ BERTï¼‰ï¼šé‡‡ç”¨...</div></div></div></a><a class="pagination-related" href="/2024/10/23/NLP%20Insights/LLM%E8%AE%AD%E7%BB%83%E6%9C%AF%E8%AF%AD%E4%BB%8B%E7%BB%8D%EF%BC%9ASFT%E3%80%81LoRA%E3%80%81Alignment%E3%80%81KTO%E5%92%8CDPO/" title="LoRA, DPO, KTO ä¸ SFT æŠ€æœ¯è¯¦è§£"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-10-23</div><div class="info-item-2">LoRA, DPO, KTO ä¸ SFT æŠ€æœ¯è¯¦è§£</div></div><div class="info-2"><div class="info-item-1">LoRA, DPO, KTO ä¸ SFT æŠ€æœ¯è¯¦è§£æœ¬ç¯‡æ–‡æ¡£å°†è¯¦ç»†ä»‹ç»å‡ ç§åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ LLAMA3ï¼‰å¾®è°ƒå’Œä¼˜åŒ–ä¸­çš„é‡è¦æŠ€æœ¯ï¼ŒåŒ…æ‹¬ SFTï¼ˆSupervised Fine-Tuningï¼‰ã€LoRAï¼ˆLow-Rank Adaptationï¼‰ã€Alignment æŠ€æœ¯ã€KTOï¼ˆKahneman-Tversky Optimizationï¼‰ å’Œ DPOï¼ˆDirect Preference Optimizationï¼‰ã€‚æ–‡ä¸­è¿˜å°†è¯¦ç»†é˜è¿°æ¯ç§æŠ€æœ¯çš„åŸç†ã€å…·ä½“å®ç°æ–¹æ³•ä»¥åŠç›¸åº”çš„æŸå¤±å‡½æ•°ä¸ä¼˜åŒ–å™¨é€‰æ‹©ã€‚  1. SFTï¼ˆSupervised Fine-Tuningï¼‰1.1 åŸç†SFT æ˜¯ä¸€ç§ä¼ ç»Ÿçš„å¾®è°ƒæ–¹æ³•ï¼Œé€šè¿‡ç›‘ç£å­¦ä¹ å¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œè°ƒæ•´æ¨¡å‹çš„å‚æ•°ä½¿å…¶åœ¨ç‰¹å®šä»»åŠ¡ä¸Šè¡¨ç°æ›´å¥½ã€‚SFT é€šå¸¸ç”¨äºé’ˆå¯¹ç‰¹å®šçš„æ ‡æ³¨æ•°æ®è¿›è¡Œæ¨¡å‹å¾®è°ƒï¼Œè®­ç»ƒçš„è¿‡ç¨‹ç±»ä¼¼äºå¸¸è§„çš„ç›‘ç£å­¦ä¹ ã€‚ 1.2 å®ç°æ–¹æ³• é€‰æ‹©é¢„è®­ç»ƒæ¨¡å‹ï¼šå¦‚ GPTã€BERT ç­‰è¯­è¨€æ¨¡å‹ã€‚ å‡†å¤‡æ ‡æ³¨æ•°æ®é›†ï¼šæ•°æ®é›†åŒ…å«è¾“å…¥å’Œè¾“å‡ºå¯¹ã€‚ è®­ç»ƒæ¨¡å‹ï¼šä½¿ç”¨æ ‡å‡†çš„äº¤å‰ç†µæŸå¤±å‡½æ•°å¯¹æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œé€šè¿‡æ¢¯åº¦ä¸‹é™ä¼˜åŒ–å‚æ•°ã€‚  1.3 æ ¸å¿ƒä»£ç ä½¿ç”¨ Hugging Face çš„ ...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Huiyu Chen</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">64</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">46</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%90%86%E8%A7%A3%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD-Fine-tuning-%E5%92%8C-Further-Pretraining-%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-number">1.</span> <span class="toc-text">ç†è§£å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ Fine-tuning å’Œ Further Pretraining çš„åŒºåˆ«</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF-Fine-tuning%EF%BC%9F"><span class="toc-number">1.1.</span> <span class="toc-text">ä»€ä¹ˆæ˜¯ Fine-tuningï¼Ÿ</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B%E5%9C%BA%E6%99%AF%EF%BC%9A%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90"><span class="toc-number">1.1.1.</span> <span class="toc-text">ç¤ºä¾‹åœºæ™¯ï¼šæƒ…æ„Ÿåˆ†æ</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Python-%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B%EF%BC%88%E4%BD%BF%E7%94%A8-PyTorch-%E5%92%8C-HuggingFace-%E7%9A%84-Transformers%EF%BC%89"><span class="toc-number">1.1.1.1.</span> <span class="toc-text">Python ä»£ç ç¤ºä¾‹ï¼ˆä½¿ç”¨ PyTorch å’Œ HuggingFace çš„ Transformersï¼‰</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF-Further-Pretraining%EF%BC%9F"><span class="toc-number">1.2.</span> <span class="toc-text">ä»€ä¹ˆæ˜¯ Further Pretrainingï¼Ÿ</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A4%BA%E4%BE%8B%E5%9C%BA%E6%99%AF%EF%BC%9A%E6%B3%95%E5%BE%8B%E6%96%87%E6%A1%A3%E5%88%86%E6%9E%90"><span class="toc-number">1.2.1.</span> <span class="toc-text">ç¤ºä¾‹åœºæ™¯ï¼šæ³•å¾‹æ–‡æ¡£åˆ†æ</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Further-Pretraining-%E7%9A%84%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B"><span class="toc-number">1.2.1.1.</span> <span class="toc-text">Further Pretraining çš„ä»£ç ç¤ºä¾‹</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B3%E9%94%AE%E5%8C%BA%E5%88%AB"><span class="toc-number">1.3.</span> <span class="toc-text">å…³é”®åŒºåˆ«</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA"><span class="toc-number">1.4.</span> <span class="toc-text">ç»“è®º</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Posts</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/21/NLP%20Insights/Evaluation%20of%20Generation-Based%20Large%20Language%20Models%20(LLMs):%20Opportunities%20and%20Challenges%20from%20Generation%20to%20Judgment/" title="Untitled">Untitled</a><time datetime="2026-02-20T21:56:22.871Z" title="Created 2026-02-21 05:56:22">2026-02-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/24/NLP%20Insights/SeCom%20Redefining%20Memory%20Management%20in%20Conversational%20AI/" title="SeCom: Redefining Memory Management in Conversational AI">SeCom: Redefining Memory Management in Conversational AI</a><time datetime="2025-06-24T08:00:00.000Z" title="Created 2025-06-24 16:00:00">2025-06-24</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/24/NLP%20Insights/SeCom:%20%E9%87%8D%E6%96%B0%E5%AE%9A%E4%B9%89%E5%AF%B9%E8%AF%9DAI%E7%9A%84%E8%AE%B0%E5%BF%86%E7%AE%A1%E7%90%86/" title="SeCom: é‡æ–°å®šä¹‰å¯¹è¯AIçš„è®°å¿†ç®¡ç†">SeCom: é‡æ–°å®šä¹‰å¯¹è¯AIçš„è®°å¿†ç®¡ç†</a><time datetime="2025-06-24T08:00:00.000Z" title="Created 2025-06-24 16:00:00">2025-06-24</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/03/06/NLP%20Insights/Decoder%E6%A8%A1%E5%9E%8B%E5%92%8CEncoder%E6%A8%A1%E5%9E%8B%E5%9C%A8Padding%E4%B8%AD%E7%9A%84%E4%B8%8D%E5%90%8C/" title="Decoder-onlyä¸Encoder-onlyæ¨¡å‹Paddingç­–ç•¥çš„å·®å¼‚">Decoder-onlyä¸Encoder-onlyæ¨¡å‹Paddingç­–ç•¥çš„å·®å¼‚</a><time datetime="2025-03-06T09:43:10.000Z" title="Created 2025-03-06 17:43:10">2025-03-06</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/03/06/NLP%20Insights/Differences%20in%20Padding%20Strategies%20Between%20Decoder-only%20and%20Encoder-only%20Models/" title="Differences in Padding Strategies Between Decoder-only and Encoder-only Models">Differences in Padding Strategies Between Decoder-only and Encoder-only Models</a><time datetime="2025-03-06T09:43:10.000Z" title="Created 2025-03-06 17:43:10">2025-03-06</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 - 2026 By Huiyu Chen</span><span class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 6.3.0</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.5.4</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Reading Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light and Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle Between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Settings"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back to Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.4"></script><script src="/js/main.js?v=5.5.4"></script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>