<!DOCTYPE html><html class="appearance-auto" lang="zh-CN"><head><meta charset="UTF-8"><title>如何准确计算固定长度模型的困惑度（PPL）</title><meta name="description" content="May the Force be with you"><meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no, initial-scale=1"><!-- Google Analytics --><!-- End Google Analytics -->
<!-- Baidu Analytics --><!-- End Baidu Analytics --><link rel="icon" href="/images/favicon.ico"><link rel="stylesheet" href="/style/common/bulma.css"><link rel="stylesheet" href="/style/base.css"><link rel="stylesheet" href="/style/common/helper.css"><script src="/js/common.js"></script><link rel="stylesheet" href="/style/post.css"><link rel="stylesheet" href="/style/themes/highlight-theme-light.css"><link rel="stylesheet" href="/style/common/jquery.fancybox.min.css"><script src="/js/highlight.pack.js"></script><meta name="description" content="如何计算固定长度模型的困惑度（PPL）困惑度（PPL）是评估语言模型最常用的指标之一。在深入探讨之前，我们应该注意这个指标特别适用于传统语言模型（有时被称为自回归或因果语言模型），而对于像 BERT 这样的 masked language models 则没有明确定义（见模型总结）。
困惑度被定义为序列的指数化平均负对数似然。如果我们有一个标记化序列 $X = (x_0, x_1, \dots, x_t)$，那么 $X$ 的困惑度为，
$$\text{PPL}(X) = \exp \left{ -\frac{1}{t}\sum*{i=1}^t \log p\theta (xi|x*{&amp;lt;i}) \right}$$
其中 $\log p\theta (x_i|x{&amp;lt;i})$ 是第 i 个标记的对数似.."><meta name="generator" content="Hexo 6.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="黑头呆鱼进化之旅" type="application/atom+xml">
</head><body class="is-flex is-flex-direction-column"><header class="header-widget is-flex-shrink-0 is-hidden-mobile"><div class="container is-fullhd is-flex is-justify-content-space-between is-align-items-center is-full-height"><section class="is-hidden-mobile is-flex-shrink-0"><h2><a href="/">user's blog</a></h2></section><h3 class="is-hidden-mobile is-family-serif is-full-height is-flex is-align-items-center is-flex-shrink-0"><div class="is-full-height" id="postTopic"><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">如何准确计算固定长度模型的困惑度（PPL）</p><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">点击返回顶部</p></div></h3><aside class="is-flex-shrink-0"><h3 class="is-inline-block"><a href="/">首页</a></h3><h3 class="is-inline-block"><a href="/about">关于</a></h3><h3 class="is-inline-block"><a href="/archives">归档</a></h3></aside></div></header><header class="is-flex header-widget is-flex-shrink-0 is-align-items-center is-justify-content-center is-hidden-tablet"><h3 class="is-inline-block"><a href="/">首页</a></h3><h3 class="is-inline-block"><a href="/about">关于</a></h3><h3 class="is-inline-block"><a href="/archives">归档</a></h3></header><main><main class="container is-max-widescreen content section post-page pt-4 px-4"><div class="columns is-flex-desktop is-justify-content-center is-flex-direction-row-reverse"><div class="column is-3 is-hidden-mobile"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E8%AE%A1%E7%AE%97%E5%9B%BA%E5%AE%9A%E9%95%BF%E5%BA%A6%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%9B%B0%E6%83%91%E5%BA%A6%EF%BC%88PPL%EF%BC%89"><span class="toc-text">如何计算固定长度模型的困惑度（PPL）</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Calculating-PPL-with-fixed-length-models"><span class="toc-text">Calculating PPL with fixed-length models</span></a></li></ol></li></ol></div><div class="column is-9"><header class="my-4"><a href="/tags/Language%20Modeling"><i class="tag post-item-tag">Language Modeling</i></a><a href="/tags/Perplexity"><i class="tag post-item-tag">Perplexity</i></a></header><h1 class="mt-0 mb-1 is-family-serif" id="postTitle">如何准确计算固定长度模型的困惑度（PPL）</h1><time class="has-text-grey" datetime="2024-04-17T04:00:00.000Z">2024-04-17</time><article class="mt-2 post-content"><h1 id="如何计算固定长度模型的困惑度（PPL）"><a href="#如何计算固定长度模型的困惑度（PPL）" class="headerlink" title="如何计算固定长度模型的困惑度（PPL）"></a>如何计算固定长度模型的困惑度（PPL）</h1><p>困惑度（PPL）是评估语言模型最常用的指标之一。在深入探讨之前，我们应该注意这个指标特别适用于传统语言模型（有时被称为自回归或因果语言模型），而对于像 BERT 这样的 masked language models 则没有明确定义（见<a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/main/en/model_summary">模型总结</a>）。</p>
<p>困惑度被定义为序列的指数化平均负对数似然。如果我们有一个标记化序列 $X = (x_0, x_1, \dots, x_t)$，那么 $X$ 的困惑度为，</p>
<p>$$<br>\text{PPL}(X) = \exp \left{ -\frac{1}{t}\sum*{i=1}^t \log p<em>\theta (x</em>i|x*{&lt;i}) \right}<br>$$</p>
<p>其中 $\log p<em>\theta (x_i|x</em>{&lt;i})$ 是第 i 个标记的对数似然，条件是根据我们的模型前面的标记 $x_{&lt;i}$。直观上，它可以被认为是评估模型在语料库中指定标记集合上预测均匀性的能力。重要的是，这意味着标记化程序直接影响模型的困惑度，这在比较不同模型时应始终考虑。</p>
<p>这也相当于数据和模型预测之间的交叉熵的指数化。想要了解更多关于困惑度及其与每字符位数（BPC）和数据压缩的关系的直觉，可以查看这篇在 The Gradient 上的<a target="_blank" rel="noopener" href="https://thegradient.pub/understanding-evaluation-metrics-for-language-models/">精彩博客文章</a>。</p>
<h2 id="Calculating-PPL-with-fixed-length-models"><a href="#Calculating-PPL-with-fixed-length-models" class="headerlink" title="Calculating PPL with fixed-length models"></a>Calculating PPL with fixed-length models</h2><p>如果我们不受模型上下文大小的限制，我们会通过自回归地分解序列并在每一步都基于整个前序子序列来条件化，从而评估模型的困惑度，如下图所示。</p>
<img width="600" alt="Full decomposition of a sequence with unlimited context length" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/ppl_full.gif">

<p>然而，在处理近似模型时，我们通常受到模型可以处理的标记数量的限制。例如，<a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/main/en/model_doc/gpt2">GPT-2</a>的最大版本有固定的 1024 个标记长度，所以当 $t$ 大于 1024 时，我们无法直接计算 $p<em>\theta(x_t|x</em>{&lt;t})$。</p>
<p>相反，序列通常被分解成等于模型最大输入大小的子序列。如果模型的最大输入大小是 $k$，那么我们通过只条件化前 $k-1$ 个标记（而不是整个上下文）来近似计算一个标记 $x_t$ 的似然。在评估模型序列的困惑度时，一种诱人但次优的方法是将序列分解成不相交的块，并独立地累加每个段的分解对数似然。</p>
<img width="600" alt="Suboptimal PPL not taking advantage of full available context" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/ppl_chunked.gif">

<p>这种计算很快，因为每个段的困惑度可以在一次前向传递中计算出来，但这是一个较差的完全分解困惑度的近似，并且通常会产生更高（更差）的 PPL，因为模型在大多数预测步骤中的上下文较少。</p>
<p>相反，应该使用滑动窗口策略来评估固定长度模型的 PPL。这涉及到重复滑动上下文窗口，使模型在做出每个预测时拥有更多的上下文。</p>
<img width="600" alt="Sliding window PPL taking advantage of all available context" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/ppl_sliding.gif">

<ol>
<li><p><strong>无限上下文分解：</strong> 如果没有对模型输入长度的限制，我们可以在每一步都使用整个前序子序列来预测下一个标记。这样可以最准确地评估模型的性能，因为每次预测都考虑了所有先前的信息。</p>
</li>
<li><p><strong>固定长度限制：</strong> 实际中，大多数模型如 GPT-2 有固定的输入长度限制（例如 1024 个标记）。当序列长度超过这个限制时，不能直接计算每个标记的条件概率，因为不能将整个序列作为条件。</p>
</li>
<li><p><strong>分块近似：</strong> 一种处理长序列的方法是将序列分解成多个与模型最大输入长度相等的子序列。每个子序列单独评估，但这种方法可能会因为没有使用完整的上下文而导致更高的困惑度。</p>
</li>
<li><p><strong>滑动窗口策略：</strong> 为了更好地利用可用的上下文，可以使用滑动窗口策略。这种方法通过不断移动上下文窗口来尝试在每次预测时为模型提供更多的上下文信息，从而更接近于使用完整上下文的理想情况。</p>
</li>
<li><p><strong>跨步滑动窗口：</strong> 一个实际的折中方法是使用跨步滑动窗口，这样可以在保证一定效率的同时，为每次模型预测提供足够的上下文，从而改善困惑度的计算和模型预测的准确性。</p>
</li>
</ol>
<p>这些方法都是为了解决因模型输入长度限制而不能直接评估整个序列的问题，试图通过不同的技术使评估更加准确，同时考虑到计算资源的有效使用。</p>
</article><section class="jump-container is-flex is-justify-content-space-between my-6"><!-- em is empty placeholder--><a class="button is-default" href="/2024/08/07/NLP%20Insights/%E4%BD%BF%E7%94%A8vLLM%E8%BF%90%E8%A1%8C%E5%BE%AE%E8%B0%83%E5%90%8E%E7%9A%84Gemma-2.zh-CN/" title="使用vLLM运行微调后的Gemma-2"><i class="iconfont icon-prev mr-2 has-text-grey"></i><span class="has-text-weight-semibold">上一页: 使用vLLM运行微调后的Gemma-2</span></a><a class="button is-default" href="/2024/04/17/NLP%20Insights/%20%E5%9B%B0%E6%83%91%E5%BA%A6(PPL).en/" title="如何准确计算固定长度模型的困惑度（PPL）"><span class="has-text-weight-semibold">下一页: 如何准确计算固定长度模型的困惑度（PPL）</span><i class="iconfont icon-next ml-2 has-text-grey"></i></a></section><article class="mt-6 comment-container"><script async repo="Haojen/Claudia-theme-blog" src="https://utteranc.es/client.js" issue-term="pathname" theme="preferred-color-scheme"></script></article></div></div></main></main><footer class="is-flex is-flex-direction-column is-align-items-center is-flex-shrink-0 is-family-serif"><section class="sns-container"><a title="twitter" target="_blank" rel="noopener nofollow" href="//twitter.com//"><i class="iconfont icon-twitter"></i></a><!-- Github--><a title="github" target="_blank" rel="noopener nofollow" href="//github.com/haojen"><i class="iconfont icon-github"></i></a><!-- Ins--><a title="instagram" target="_blank" rel="noopener nofollow" href="//www.instagram.com//"><i class="iconfont icon-ins"></i></a><!-- RSS--><!-- 知乎--><!-- 领英--><!-- 脸书--><a title="facebook" target="_blank" rel="noopener nofollow" href="//www.facebook.com//"><i class="iconfont icon-tian7_facebook"></i></a></section><p><span>Copyright ©</span><span> user 2026</span></p><div class="is-flex is-justify-content-center is-flex-wrap-wrap"><p>Powered by Hexo &verbar;&nbsp;</p><p class="is-flex is-justify-content-center"><a title="Hexo theme author" target="_blank" rel="noopener" href="//github.com/haojen">Theme by Haojen&nbsp;</a></p><div style="margin-top: 2px"><a class="github-button" title="github-button" target="_blank" rel="noopener" href="https://github.com/haojen/hexo-theme-Claudia" data-color-scheme="no-preference: light; light: light; dark: dark;" data-show-count="true"></a></div></div><div><span></span></div></footer><script src="/js/lang-switch.js"></script><script async defer src="https://buttons.github.io/buttons.js"></script><script src="/js/jquery-3.6.1.min.js"></script><script src="/js/jquery-fancybox.min.js"></script><script src="/js/img_zoom.js"></script><script src="/js/post.js"></script></body></html>