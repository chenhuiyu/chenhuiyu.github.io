<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title></title>
      <link href="/2025/10/28/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%88%87%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E5%B0%8E%E8%AB%962025%20%C2%B7%20%E7%AC%AC2%E8%AC%9B%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%BE%8C%E7%9A%84%E9%97%9C%E9%8D%B5%E6%8A%80%E8%A1%93/"/>
      <url>/2025/10/28/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E8%88%87%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92%E5%B0%8E%E8%AB%962025%20%C2%B7%20%E7%AC%AC2%E8%AC%9B%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%BE%8C%E7%9A%84%E9%97%9C%E9%8D%B5%E6%8A%80%E8%A1%93/</url>
      
        <content type="html"><![CDATA[<h2 id="生成式人工智能与机器学习导论2025-·-第2讲：上下文工程（Context-Engineering）——AI-Agent-背后的关键技术"><a href="#生成式人工智能与机器学习导论2025-·-第2讲：上下文工程（Context-Engineering）——AI-Agent-背后的关键技术" class="headerlink" title="生成式人工智能与机器学习导论2025 · 第2讲：上下文工程（Context Engineering）——AI Agent 背后的关键技术"></a>生成式人工智能与机器学习导论2025 · 第2讲：上下文工程（Context Engineering）——AI Agent 背后的关键技术</h2><p>在生成式人工智能（Generative AI）与大型语言模型（Large Language Model, LLM）不断演进的浪潮中，<strong>上下文工程（Context Engineering）</strong>逐渐被公认为推动 AI Agent 真正具备智能行为的关键基础。它不仅是 Prompt Engineering 的延伸，更是让模型能够整合世界知识、记忆、个人偏好与任务约束的系统性方法论。</p><p>本文深入整理台大李宏毅教授于《生成式人工智能与机器学习导论2025》第2讲中的核心观点，并扩充理论背景与实务应用，从概念源起、技术结构到未来发展方向，全方位阐述 Context Engineering 的核心价值与挑战。我们将从语言模型的本质出发，探讨为什么在 AI Agent 时代，Context Engineering 成为了一项关键的技术。</p><hr><p>语言模型可抽象为函数 f(x)，其中 x 即为 Prompt/Context，输出由输入与模型先验共同决定。对于输出不理想的问题，工程上只有两条路：要么训练（改 f），要么设计（改 x）。在商用闭源模型难以触碰权重的现实下，优化输入成为最具性价比的控制手段。因而 Context Engineering 的核心即是”在推理时刻构造对的 x”。</p><p>“改 f”与”改 x”的权衡并非二选一：前者需要数据、算力与风险控制；后者强调语义约束、先验引导与即时知识注入。对多数应用而言，先用 Context 把可控空间压到足够小，再评估是否值得进入训练与微调阶段，是更务实的路径。把训练预算花在”最痛点”的能力缺口上，其他由 Context 承担。</p><p>高质量 Prompt 不只是任务描述，还应包含：目标与验收标准、边界与禁止项、语气与读者画像、长度与格式约定，以及 1–3 个覆盖边界情形的示例。示例优于口头定义，能直接对齐隐含规则。把这些元素结构化放入 Context，可显著降低模型歧义与风格漂移。</p><p>补充前提能有效消除歧义，例如”载具是什么意思？”若缺少情境，模型可能在”交通工具/电子载具”间摇摆；一旦加入”超商结账场景”，答案会自然收敛到电子发票载具。语义前提是约束空间的重要手段，越具体的情境越能减少错误自由度。</p><p>场景信息不仅改变术语解释，还会改变对事实的判断分布。曼谷运河中的”鳄鱼”在加入地理与文化线索后被识别为”水巨蜥”，并给出”吉祥/偏财运”的在地含义。Context 让模型”知道自己身在何处”，从而输出贴合环境的答案。</p><p>示例对齐格式远胜抽象描述。”火星文”若无示例，模型可能做字形替换；给出”注音替换”的正例后，模型即可稳定迁移。工程实践中，应优先提供”正反 + 边界”的小样本示例，以最小 token 预算获得最大行为约束。</p><h3 id="二、Prompt-Engineering-与-Context-Engineering-的关系与演进"><a href="#二、Prompt-Engineering-与-Context-Engineering-的关系与演进" class="headerlink" title="二、Prompt Engineering 与 Context Engineering 的关系与演进"></a>二、Prompt Engineering 与 Context Engineering 的关系与演进</h3><h4 id="2-1-神奇咒语时代的终结"><a href="#2-1-神奇咒语时代的终结" class="headerlink" title="2.1 神奇咒语时代的终结"></a>2.1 神奇咒语时代的终结</h4><p>在 2023 年以前，Prompt Engineering 几乎是所有开发者的”魔法技术”：</p><p><strong>早期神奇咒语的效果：</strong></p><ul><li>加入”Let’s think step by step”能显著提升推理能力</li><li>使用”You are a helpful assistant”可稳定生成风格</li><li>甚至调整分隔符号或换行格式，都会改变模型行为</li><li>给模型”小费”（”如果你答对就给你小费”）能提高正确率</li><li>告诉模型”深呼吸再回答”也能改善表现</li></ul><p><strong>神奇咒语失效的原因：</strong><br>随着 GPT-4 及 Gemini 系列的能力提升，模型逐渐具备了内在的逻辑推理与自我校正能力。经验研究显示：”魔法咒语”的效果随模型能力增长而递减，提示的结构与语义关联性取代了符号与格式的魔术效应。</p><table><thead><tr><th>时期</th><th>模型</th><th>无咒语准确率</th><th>有咒语准确率</th><th>效果提升</th></tr></thead><tbody><tr><td>2023 年 6 月</td><td>GPT-3.5</td><td>72%</td><td>88%</td><td>+16%</td></tr><tr><td>2024 年 2 月</td><td>GPT-4</td><td>85%</td><td>89%</td><td>+4%</td></tr></tbody></table><p>由此可见，<strong>当模型”足够聪明”后，真正重要的已非提示字词，而是上下文整体的信息设计。</strong> Context Engineering 应运而生，它不再着眼于如何”说服”模型，而是如何”构建语义环境”让模型自然地做出正确推理。</p><h4 id="2-2-Context-Engineering-的本质"><a href="#2-2-Context-Engineering-的本质" class="headerlink" title="2.2 Context Engineering 的本质"></a>2.2 Context Engineering 的本质</h4><p>Context Engineering 和 Prompt Engineering 在本质上没有根本性的不同，都是通过优化语言模型的输入来获得更好的输出。但它们的关注重点不同：</p><p><strong>Prompt Engineering 关注：</strong></p><ul><li>特定的格式要求（JSON 格式、分隔符等）</li><li>神奇咒语和技巧</li><li>单次交互的优化</li></ul><p><strong>Context Engineering 关注：</strong></p><ul><li>整体上下文的组织和管理</li><li>多轮对话的连贯性</li><li>动态信息整合</li><li>长期记忆管理</li><li>工具和外部知识的整合</li></ul><h4 id="2-3-从-Prompt-到-Context-的演进"><a href="#2-3-从-Prompt-到-Context-的演进" class="headerlink" title="2.3 从 Prompt 到 Context 的演进"></a>2.3 从 Prompt 到 Context 的演进</h4><p>这一变化标志着工程焦点从”单句提示设计”转向”整体上下文组织”。Context Engineering 的核心目标是：</p><ol><li><strong>避免塞爆 context</strong>：只放需要的东西进入 context，清理掉不需要的内容</li><li><strong>动态管理</strong>：根据任务进展实时调整上下文内容</li><li><strong>层次化组织</strong>：将不同类型的信息分层管理</li><li><strong>自动化处理</strong>：让语言模型自己来管理自己的输入</li></ol><hr><p>神奇咒语的效应随模型能力增强而递减：早期”step-by-step””深呼吸””给小费”等提示曾显著提升正确率，如今更关键的是上下文结构化与任务约束。工程实践中，应将提示技巧沉淀为稳定的上下文模版，而非依赖临场口头”魔法”。</p><p>有趣但不可靠的”激励偏好”实验（如”世界和平””母亲为你骄傲”）说明模型对提示语境存在非理性响应，但这类现象在强模型上影响大幅收敛。生产中应避免寄望这类”彩蛋”，将资源投入数据与上下文治理。</p><p>实验对比（2023 vs 2024）显示：无咒语的基线能力显著提升，咒语带来的增益从两位数降至个位数。意味着”如何组织信息”比”如何措辞”更重要。将知识、记忆、工具与约束编排为可复用的上下文模板，成为新一代工程重心。</p><p>arXiv 成为前沿工作的首发阵地（如”2206”表示 2022 年 6 月），方便快速公开与社区复核。研发团队应建立”arXiv→内部评审→复现实验→工程化落地”的快速通道，缩短从论文到产品的转化链路。</p><h3 id="三、Context-Engineering-的核心概念"><a href="#三、Context-Engineering-的核心概念" class="headerlink" title="三、Context Engineering 的核心概念"></a>三、Context Engineering 的核心概念</h3><h4 id="3-1-Context-Engineering-的定义"><a href="#3-1-Context-Engineering-的定义" class="headerlink" title="3.1 Context Engineering 的定义"></a>3.1 Context Engineering 的定义</h4><p>Context Engineering 的核心理念是：</p><blockquote><p><strong>以系统化方式构建、维护与优化语言模型的上下文，使模型在运行阶段能即时获取任务所需的全部信息，并表现出一致、可靠与可控的智能行为。</strong></p></blockquote><p>不同于传统的 Prompt Engineering，Context Engineering 更强调<strong>结构化（structured）、动态化（dynamic）、层次化（hierarchical）</strong>的上下文管理。它被视为”模型外部的认知架构”，为模型临时提供一个模拟的”世界模型（world model）”与”任务记忆（task memory）”。</p><h4 id="3-2-完整-Context-的七大组成要素"><a href="#3-2-完整-Context-的七大组成要素" class="headerlink" title="3.2 完整 Context 的七大组成要素"></a>3.2 完整 Context 的七大组成要素</h4><p>一个完整的 Context 应该包含以下七个核心要素：</p><p><strong>1. 用户 Prompt（User Prompt）</strong></p><ul><li>用户对语言模型下的具体指令</li><li>包含任务描述和详细指引</li><li>例如：写请假信时，除了说明要请假，还要提供语气、长度、格式等具体要求</li></ul><p><strong>2. 系统提示（System Prompt）</strong></p><ul><li>开发语言模型平台的人设定的基础信息</li><li>定义模型的身份、行为准则和限制</li><li>例如：Claude 3 Opus 的 System Prompt 超过 2500 字，包含身份描述、使用说明、禁止事项等</li></ul><p><strong>3. 对话历史记录（Conversation History）</strong></p><ul><li>相当于语言模型的短期记忆</li><li>在多轮对话中保持语义连续性</li><li>让模型能够记住之前讨论的内容</li></ul><p><strong>4. 长期记忆（Long-term Memory）</strong></p><ul><li>跨越多次互动的记忆系统</li><li>记录用户偏好、历史决策、个人化设定等</li><li>例如：ChatGPT 的长期记忆功能，可以记住用户的写作习惯和偏好</li></ul><p><strong>5. 外部知识检索（External Knowledge Retrieval）</strong></p><ul><li>通过 RAG（Retrieval-Augmented Generation）技术</li><li>整合外部文件、数据库、网络搜索等最新信息</li><li>解决模型知识有限和过时的问题</li></ul><p><strong>6. 工具使用说明（Tool Usage Instructions）</strong></p><ul><li>告诉模型如何使用各种工具</li><li>包括工具的功能描述、使用方法和示例</li><li>让模型能够调用外部工具完成任务</li></ul><p><strong>7. 模型自身产生的思考过程（Model’s Own Reasoning）</strong></p><ul><li>模型在回答前的内部思考过程</li><li>包括推理步骤、验证过程、多角度分析等</li><li>例如：GPT-4o 的深度思考能力</li></ul><h4 id="3-3-Context-的运作原理"><a href="#3-3-Context-的运作原理" class="headerlink" title="3.3 Context 的运作原理"></a>3.3 Context 的运作原理</h4><p>Context 不只是静态文本，而是一个动态信息流：</p><ol><li><strong>即时组装</strong>：在每次请求中，系统会根据当前任务即时组装上下文</li><li><strong>信息融合</strong>：将历史对话、知识检索结果、工具说明等整合</li><li><strong>优先级管理</strong>：根据任务重要性安排不同信息的优先级</li><li><strong>动态调整</strong>：根据任务进展实时调整上下文内容</li></ol><p>这样的”上下文拼接”过程类似于临时构建一个任务专属的脑内工作区域（working memory buffer），模型在此基础上进行概率推理与决策生成。</p><h4 id="3-4-Context-Engineering-的技术挑战"><a href="#3-4-Context-Engineering-的技术挑战" class="headerlink" title="3.4 Context Engineering 的技术挑战"></a>3.4 Context Engineering 的技术挑战</h4><p>Context Engineering 涉及多个尚未完全解决的挑战：</p><p><strong>1. Token 预算与压缩（Context Budgeting）</strong></p><ul><li>如何在有限上下文长度下选取最关键信息</li><li>避免”塞爆 context”的问题</li><li>实现信息的有效压缩和摘要</li></ul><p><strong>2. 信息优先级排序（Context Ranking）</strong></p><ul><li>哪些知识应放在模型的注意力焦点？</li><li>如何根据任务动态调整信息重要性</li><li>实现智能的信息筛选机制</li></ul><p><strong>3. 多来源整合（Multimodal Context Fusion）</strong></p><ul><li>如何融合文本、图像、代码、知识图谱等异质信息</li><li>处理不同格式和结构的数据</li><li>保持信息的一致性和连贯性</li></ul><p><strong>4. 记忆一致性（Memory Alignment）</strong></p><ul><li>如何在长期互动中保持角色与知识的连贯性</li><li>避免记忆冲突和矛盾</li><li>实现记忆的智能更新和遗忘</li></ul><p>随着 Context Length 延展（如 Gemini 1.5 的百万 token 规模）与动态上下文选择算法的出现，Context Engineering 正快速演变为一个结合 NLP、IR、知识工程与人机互动的新兴学科。</p><hr><h3 id="四、In-Context-Learning：模型”假学习”的奇迹"><a href="#四、In-Context-Learning：模型”假学习”的奇迹" class="headerlink" title="四、In-Context Learning：模型”假学习”的奇迹"></a>四、In-Context Learning：模型”假学习”的奇迹</h3><h4 id="4-1-In-Context-Learning-的核心机制"><a href="#4-1-In-Context-Learning-的核心机制" class="headerlink" title="4.1 In-Context Learning 的核心机制"></a>4.1 In-Context Learning 的核心机制</h4><p>In-Context Learning（ICL）揭示了一个令人惊讶的现象：大型语言模型在不更新权重的情况下，仅凭上下文提示就能完成类似”学习”的行为。从认知角度看，ICL 代表了一种”条件式适应（conditional adaptation）”机制——模型在观察范例后，于其潜在表征空间中临时建立对应的推理模式。</p><p><strong>ICL 的工作原理：</strong></p><ol><li>模型接收包含示例的上下文</li><li>通过注意力机制识别示例中的模式</li><li>在推理时应用这些模式到新任务</li><li>整个过程不改变模型参数</li></ol><h4 id="4-2-经典案例：Gemini-1-5-的卡拉蒙语翻译"><a href="#4-2-经典案例：Gemini-1-5-的卡拉蒙语翻译" class="headerlink" title="4.2 经典案例：Gemini 1.5 的卡拉蒙语翻译"></a>4.2 经典案例：Gemini 1.5 的卡拉蒙语翻译</h4><p>最著名的 ICL 案例来自 Gemini 1.5 的技术报告。研究人员让 Gemini 翻译一种极少人使用的语言——卡拉蒙语（Kalamang）：</p><p><strong>实验设置：</strong></p><ul><li>卡拉蒙语全球只有数千人使用</li><li>网络上几乎找不到相关学习资料</li><li>模型原本完全不知道这种语言</li></ul><p><strong>实验过程：</strong></p><ol><li>直接要求翻译：模型完全无法处理</li><li>提供卡拉蒙语教科书和字典：模型突然”学会”了翻译</li><li>翻译质量达到人类水平（4-5.5分，满分6分）</li></ol><p><strong>关键发现：</strong></p><ul><li>真正有用的是教科书中的例句，而非语法说明</li><li>模型通过例句学会了语言模式</li><li>移除教科书后，模型又”忘记”了这种语言</li></ul><h4 id="4-3-ICL-的理论意义"><a href="#4-3-ICL-的理论意义" class="headerlink" title="4.3 ICL 的理论意义"></a>4.3 ICL 的理论意义</h4><p>这一现象的理论意涵深远：</p><p><strong>1. 挑战传统学习观</strong></p><ul><li>传统观点：学习必须依赖权重更新</li><li>ICL 显示：模型可以在推理阶段”模拟学习”</li><li>参数不变，但行为发生改变</li></ul><p><strong>2. 内隐的元学习</strong></p><ul><li>模型的权重早已学会”如何从上下文中学习”</li><li>这是一种内隐的元学习（implicit meta-learning）</li><li>模型具备了”学会学习”的能力</li></ul><p><strong>3. 即时知识注入的可能性</strong></p><ul><li>未来 AI 可能实现”即时知识注入”</li><li>不需要重新训练就能获得新能力</li><li>Context Engineering 正是这一过程的设计科学</li></ul><h4 id="4-4-ICL-在-Context-Engineering-中的应用"><a href="#4-4-ICL-在-Context-Engineering-中的应用" class="headerlink" title="4.4 ICL 在 Context Engineering 中的应用"></a>4.4 ICL 在 Context Engineering 中的应用</h4><p>ICL 为 Context Engineering 提供了重要启示：</p><p><strong>1. 示例的重要性</strong></p><ul><li>提供正确的示例比详细的说明更有效</li><li>示例应该覆盖任务的典型情况</li><li>示例的质量直接影响模型表现</li></ul><p><strong>2. 上下文设计原则</strong></p><ul><li>将关键信息放在上下文的合适位置</li><li>利用模型的注意力机制引导学习</li><li>通过示例传递隐含的规则和约束</li></ul><p><strong>3. 动态适应能力</strong></p><ul><li>模型可以根据上下文动态调整行为</li><li>不需要重新训练就能适应新任务</li><li>这为 AI Agent 的灵活性提供了基础</li></ul><hr><p>ICL（In-Context Learning）指模型在不改变权重的情况下，仅凭上下文示例临时”对齐”到新任务。它依靠注意力在潜在表征空间中识别与复用模式，属于推理阶段的条件式适应，而非训练阶段的参数更新。</p><p>ICL 的基本流程：给出高质量示例→模型在上下文中归纳出映射规则→对新样本进行迁移推断→全程不涉及参数更新。示例的覆盖面与放置位置会显著影响归纳质量。</p><p>Kalamang（卡拉蒙语）案例：极低资源、网络几乎无资料，模型初始”不会”。直接要求翻译失败，体现纯模型先验的边界与知识缺口。</p><p>将教科书与字典注入上下文后，模型”会了”翻译，显示 ICL 可在推理时完成能力注入。关键在于上下文内容与结构，而非参数微调。</p><p>人工评分显示，给足上下文后，翻译质量可逼近人类（4–5.5/6）。这并非真正”学会”，而是以上下文为外部工作记忆完成模式复用。</p><p>后续分析发现：语法说明远不如”成对例句”有用。示例是最强的结构化监督，应优先投入 token 预算于”覆盖代表性边界”的对照样例。</p><p>“学习”需加引号：移除教科书后能力随即消散，说明模型权重并未改变。ICL 的收益完全依赖上下文在当前回合中的存在与质量。</p><p>隐式元学习：大模型权重中早已内化”如何从上下文学习”的程序性能力。工程上应把握这一先验，侧重”示例与约束的编排”。</p><p>ICL 设计原则：少而精的高质量示例；覆盖正反与边界情形；将关键信息放在更易被注意的位置（如开头/结尾）；配合明确的任务与验收标准。</p><p>在 Context Engineering 中，ICL 是即时知识注入与行为对齐的主力手段，可在无训练预算下快速获得”可用的正确性”。其上限由示例质量与上下文治理决定。</p><h3 id="五、System-Prompt：人格化模型的隐藏架构"><a href="#五、System-Prompt：人格化模型的隐藏架构" class="headerlink" title="五、System Prompt：人格化模型的隐藏架构"></a>五、System Prompt：人格化模型的隐藏架构</h3><h4 id="5-1-System-Prompt-的核心作用"><a href="#5-1-System-Prompt-的核心作用" class="headerlink" title="5.1 System Prompt 的核心作用"></a>5.1 System Prompt 的核心作用</h4><p>System Prompt 是每个语言模型的”人格蓝图”，是决定其性格、行为准则与思维风格的核心结构。它不仅仅是一段系统指令，而是一套<strong>语义层级极高的行为规范框架</strong>，定义模型在每次对话中该如何”存在”。</p><p><strong>System Prompt 的关键特征：</strong></p><ul><li>在每次对话开始时就被注入</li><li>用户无法直接看到或修改</li><li>为模型提供基础的身份和行为准则</li><li>影响模型的整体人格和回应风格</li></ul><h4 id="5-2-Claude-3-Opus-的-System-Prompt-分析"><a href="#5-2-Claude-3-Opus-的-System-Prompt-分析" class="headerlink" title="5.2 Claude 3 Opus 的 System Prompt 分析"></a>5.2 Claude 3 Opus 的 System Prompt 分析</h4><p>以 Claude 3 Opus 为例，其 System Prompt 超过 2500 字，包含以下核心内容：</p><p><strong>1. 身份描述</strong></p><ul><li>明确告诉模型它是 Claude</li><li>由 Anthropic 公司开发</li><li>提供当前日期信息</li></ul><p><strong>2. 使用说明和限制</strong></p><ul><li>如何与用户互动</li><li>禁止事项（如不提供危险信息）</li><li>错误处理策略</li></ul><p><strong>3. 回应风格设定</strong></p><ul><li>避免使用”好问题”等惯用开头</li><li>不要过于自信或极端化表达</li><li>保持友善和专业的语调</li></ul><p><strong>4. 知识边界</strong></p><ul><li>明确知识截止日期（2025年1月）</li><li>如何承认未知信息</li><li>如何处理超出知识范围的问题</li></ul><p><strong>5. 伦理和行为准则</strong></p><ul><li>不要声称自己是人类</li><li>不要声称有意识</li><li>如何平衡帮助与诚实</li></ul><h4 id="5-3-System-Prompt-的工程价值"><a href="#5-3-System-Prompt-的工程价值" class="headerlink" title="5.3 System Prompt 的工程价值"></a>5.3 System Prompt 的工程价值</h4><p>从工程角度观之，System Prompt 是 Context Engineering 的基石之一：</p><p><strong>1. 人格一致性</strong></p><ul><li>确保模型在不同对话中保持一致的个性</li><li>建立用户对模型的信任和预期</li><li>实现品牌差异化</li></ul><p><strong>2. 行为控制</strong></p><ul><li>通过语义约束控制模型行为</li><li>防止有害或不当的回应</li><li>确保符合企业价值观</li></ul><p><strong>3. 用户体验优化</strong></p><ul><li>提供一致的用户体验</li><li>减少用户需要重复说明的情况</li><li>提高交互效率</li></ul><h4 id="5-4-System-Prompt-的研究发现"><a href="#5-4-System-Prompt-的研究发现" class="headerlink" title="5.4 System Prompt 的研究发现"></a>5.4 System Prompt 的研究发现</h4><p>越来越多的实验证实 System Prompt 的内容不仅影响输出风格，还能实质影响模型的思维链深度与推理策略：</p><p><strong>1. 推理能力影响</strong></p><ul><li>强调”多角度思考”时，Chain-of-Thought 任务表现更好</li><li>加入”逻辑辩证”指令，推理一致性显著提升</li><li>影响模型的思维深度和广度</li></ul><p><strong>2. 情感理解能力</strong></p><ul><li>加入”情绪共感”指令，对情绪线索的捕捉率提高</li><li>影响模型的情感回应质量</li><li>改善人机交互体验</li></ul><p><strong>3. 行为先验作用</strong></p><ul><li>System Prompt 实际上是行为先验的外显化</li><li>为模型提供恒定的语义势场</li><li>在多变上下文中维持人格稳定性</li></ul><h4 id="5-5-未来发展趋势"><a href="#5-5-未来发展趋势" class="headerlink" title="5.5 未来发展趋势"></a>5.5 未来发展趋势</h4><p>System Prompt 的设计趋势将朝向模组化与动态适应：</p><p><strong>1. 模组化设计</strong></p><ul><li>不同功能模块使用不同的 System Prompt</li><li>根据任务类型动态调整人格特征</li><li>支持多角色切换</li></ul><p><strong>2. 动态适应</strong></p><ul><li>根据用户反馈调整 System Prompt</li><li>使用强化学习优化人格特征</li><li>实现个性化的 AI 助手</li></ul><p><strong>3. 多代理协作</strong></p><ul><li>不同 Agent 使用差异化的 System Prompt</li><li>模拟团队协作中的角色分工</li><li>实现”人格层叠式上下文设计”</li></ul><hr><p>System Prompt 是”人格蓝图”：在每次对话开始前注入，决定身份、风格、边界与伦理。它像行为先验场，稳定影响后续每一步生成与决策的倾向。</p><p>典型要素：模型自我身份、日期时间、可参考资源；这些内容解释了”模型为何知道自己是谁、今天几号”。这并非模型”推理”所得，而是被写入的现实锚点。</p><p>使用说明与限制：指导与用户互动的方式与范围，为高风险领域（化学、武器等）设定拒答边界，避免危害性输出与法律风险。</p><p>交互规范：当用户不满意时引导使用反馈机制；在不确定时给出澄清或请求更多信息；保持礼貌、简洁与聚焦用户目标。</p><p>禁止事项：不提供危险配方与手段；不帮助规避法律与平台规则；不制造仇恨与歧视。通过 System Prompt 前置明确，有助于一致合规。</p><p>回应风格：避免”好问题”等口头禅；限制过度自信与夸大语气；在不确定时承认未知，比错误自信更重要。</p><p>知识边界：声明知识截止日期，超出范围时”说明不知道/请求检索”。这能显著降低幻觉与年代错配。</p><p>身份定位：不得自称人类或具备意识；这既是伦理要求，也是避免用户误解的重要规范，维护人机关系的透明度。</p><p>错误处理：被纠正时不要立刻服从，应先自检再给出更新结论；可减少”人类错误提示导致的被动跟随”。</p><p>企业价值：System Prompt 往往超过数千字，是人格与风格的”软权重”，决定产品的品牌体验与信任边界，是 Context Engineering 的基石。</p><h3 id="六、记忆体系：从短期上下文到长期人格"><a href="#六、记忆体系：从短期上下文到长期人格" class="headerlink" title="六、记忆体系：从短期上下文到长期人格"></a>六、记忆体系：从短期上下文到长期人格</h3><h4 id="6-1-短期记忆：对话中的工作记忆"><a href="#6-1-短期记忆：对话中的工作记忆" class="headerlink" title="6.1 短期记忆：对话中的工作记忆"></a>6.1 短期记忆：对话中的工作记忆</h4><p>短期记忆（Short-term Context）对应于单一会话中的历史内容，例如使用者的上一轮提问、上下文推理的连续性以及模型生成时的局部语境连接。</p><p><strong>短期记忆的特征：</strong></p><ul><li>依赖当前上下文长度（context window）维持</li><li>属于瞬时性、可挥发的信息缓存</li><li>负责维持短时间内的语义焦点与思考链条</li><li>使模型能进行多轮对话而不丧失语义连续性</li></ul><p><strong>短期记忆的作用机制：</strong></p><ol><li><strong>语义连续性</strong>：保持对话的逻辑连贯性</li><li><strong>上下文理解</strong>：基于前面的对话理解当前问题</li><li><strong>指代消解</strong>：理解代词和省略的指代关系</li><li><strong>话题追踪</strong>：跟踪对话主题的变化</li></ol><h4 id="6-2-长期记忆：跨越时间的个性化"><a href="#6-2-长期记忆：跨越时间的个性化" class="headerlink" title="6.2 长期记忆：跨越时间的个性化"></a>6.2 长期记忆：跨越时间的个性化</h4><p>长期记忆（Long-term Memory）则跨越多次互动，能记录使用者的偏好、语言风格、专案进度、历史决策脉络与个人化设定等信息。</p><p><strong>长期记忆的存储方式：</strong></p><ul><li>不再依赖单次 prompt</li><li>通过数据库或向量储存（vector storage）长期保存</li><li>在需要时被召回（retrieval）</li><li>支持跨会话的信息共享</li></ul><p><strong>长期记忆的内容类型：</strong></p><ol><li><strong>用户偏好</strong>：写作风格、回答长度、专业领域等</li><li><strong>历史决策</strong>：过去的选择和判断逻辑</li><li><strong>项目进度</strong>：正在进行的任务和完成情况</li><li><strong>知识连接</strong>：跨项目的知识关联</li></ol><h4 id="6-3-ChatGPT-的长期记忆功能"><a href="#6-3-ChatGPT-的长期记忆功能" class="headerlink" title="6.3 ChatGPT 的长期记忆功能"></a>6.3 ChatGPT 的长期记忆功能</h4><p>自 2024 年起，ChatGPT 引入了长期记忆功能，这是一个重要的技术突破：</p><p><strong>记忆功能的特点：</strong></p><ul><li>用户可以明确告诉模型要记住什么</li><li>模型会将这些信息存储到长期记忆中</li><li>在后续对话中自动调用相关记忆</li><li>支持记忆的查看、编辑和删除</li></ul><p><strong>记忆的两种类型：</strong></p><ol><li><strong>显式记忆</strong>：用户明确要求记住的信息</li><li><strong>隐式记忆</strong>：模型自动学习的行为模式</li></ol><p><strong>实际应用示例：</strong></p><ul><li>记住用户的写作偏好：”我喜欢简洁的回答”</li><li>记住项目背景：”我正在开发一个电商网站”</li><li>记住个人信息：”我住在北京，从事软件开发”</li></ul><h4 id="6-4-记忆系统的技术挑战"><a href="#6-4-记忆系统的技术挑战" class="headerlink" title="6.4 记忆系统的技术挑战"></a>6.4 记忆系统的技术挑战</h4><p>长期记忆系统面临多个技术挑战：</p><p><strong>1. 记忆检索机制（Memory Retrieval Policies）</strong></p><ul><li>如何根据当前任务动态决定召回哪些过往信息</li><li>避免信息过载和无关信息干扰</li><li>实现智能的信息筛选和排序</li></ul><p><strong>2. 记忆权重与衰减（Memory Weighting &amp; Forgetting）</strong></p><ul><li>如何模拟人类记忆的遗忘机制</li><li>根据重要性分配记忆权重</li><li>实现记忆的自动清理和更新</li></ul><p><strong>3. 隐私与安全（Privacy-aware Memory）</strong></p><ul><li>在记忆储存过程中保护用户敏感信息</li><li>实现记忆的加密和访问控制</li><li>支持记忆的匿名化和脱敏</li></ul><p><strong>4. 情境持续性（Context Continuity）</strong></p><ul><li>确保长期记忆与短期上下文无缝衔接</li><li>避免语义冲突或人格漂移</li><li>维持模型行为的一致性</li></ul><h4 id="6-5-记忆系统的未来发展方向"><a href="#6-5-记忆系统的未来发展方向" class="headerlink" title="6.5 记忆系统的未来发展方向"></a>6.5 记忆系统的未来发展方向</h4><p>记忆体系的演进使 LLM 不仅能”理解当下”，还能”理解历史”并”预测未来”：</p><p><strong>1. 认知连续性</strong></p><ul><li>从一次性回应系统进化为具备认知连续性的智能体</li><li>实现真正的个性化交互体验</li><li>支持长期的任务规划和执行</li></ul><p><strong>2. 自我一致性</strong></p><ul><li>在长期交互中维持人格和行为的一致性</li><li>避免记忆冲突和矛盾</li><li>实现智能的记忆整合和更新</li></ul><p><strong>3. 时间维度设计</strong></p><ul><li>为 Context Engineering 开启真正具时间维度的设计时代</li><li>支持基于历史的学习和适应</li><li>实现更智能的上下文管理</li></ul><p>这一变革标志着 Context Engineering 已从<strong>静态设计（static context design）</strong>迈向<strong>记忆驱动（memory-driven）</strong>阶段——模型的行为不再仅取决于输入 prompt，而是建立在可持续更新的语义记忆层上。</p><hr><p>对话历史是短期记忆：维持语义连续、指代消解与话题追踪。它像”工作记忆缓冲区”，容量有限，但对多轮对话至关重要。</p><p>长期记忆跨会话保存：记录用户偏好、项目上下文与历史决策，使模型在多次互动间保持个性化与连贯性。</p><p>开启与管理记忆：允许显式”请记住”与隐式习惯学习；工程上需提供查看、编辑、删除接口，确保可控与可监督。</p><p>“我是怎样的人？”是展示长期记忆利用的典型问法：基于过往交互的摘要性理解生成个性化描述，注意只应输出正当且经同意的内容。</p><p>记忆与 RAG 的分工：记忆保存”我与你的历史”，RAG 注入”世界的新事实”。两者共同构建”个体化且最新”的上下文。</p><p>外置记忆：将细节落盘，通过”记忆 RAG”在必要时召回；既节省上下文预算，又保留追溯能力。</p><p>三分打分：近因（Recency）/重要性（Importance）/相关性（Relevance）共同决定召回优先级，类似人类记忆的注意力分配。</p><p>负例注入需谨慎：盲目强调”过去错”的记忆，可能诱导错误迁移。若要使用，应连同更正过程与裁决理由一并注入。</p><p>隐私与安全：记忆应进行最小化存储、加密与访问控制；敏感信息默认不持久化，必要时脱敏与匿名化处理。</p><p>上下文预算：为指令、历史、知识、工具、思考笔记分别设配额；超过阈值触发递归摘要，避免”塞爆 context”。</p><h3 id="七、RAG-与动态知识融合"><a href="#七、RAG-与动态知识融合" class="headerlink" title="七、RAG 与动态知识融合"></a>七、RAG 与动态知识融合</h3><h4 id="7-1-RAG-的核心机制"><a href="#7-1-RAG-的核心机制" class="headerlink" title="7.1 RAG 的核心机制"></a>7.1 RAG 的核心机制</h4><p>RAG（Retrieval-Augmented Generation）是 Context Engineering 的知识增强分支。它通过检索外部资料源（如文件、API、资料库），将即时资讯注入上下文中，使模型能跨越训练时知识的时效限制。</p><p><strong>RAG 的工作流程：</strong></p><ol><li><strong>查询理解</strong>：将用户问题转换为搜索查询</li><li><strong>信息检索</strong>：从外部资料源检索相关信息</li><li><strong>上下文构建</strong>：将检索结果整合到上下文中</li><li><strong>生成回答</strong>：基于增强的上下文生成回答</li></ol><h4 id="7-2-RAG-的优势与挑战"><a href="#7-2-RAG-的优势与挑战" class="headerlink" title="7.2 RAG 的优势与挑战"></a>7.2 RAG 的优势与挑战</h4><p><strong>RAG 的优势：</strong></p><ul><li>解决模型知识有限的问题</li><li>提供最新、最准确的信息</li><li>支持特定领域的专业知识</li><li>减少幻觉和错误信息</li></ul><p><strong>RAG 的挑战：</strong></p><ul><li>信息融合的精确性：语言模型仍可能误读、误配或夸张地再叙述检索结果</li><li>检索质量依赖：检索到的信息质量直接影响最终回答</li><li>上下文长度限制：检索到的信息可能超出上下文长度限制</li><li>引用管理：如何准确引用和验证信息来源</li></ul><h4 id="7-3-RAG-的实际应用案例"><a href="#7-3-RAG-的实际应用案例" class="headerlink" title="7.3 RAG 的实际应用案例"></a>7.3 RAG 的实际应用案例</h4><p><strong>Google AI Overview 的教训：</strong><br>Google 的 AI Overview 功能是一个典型的 RAG 应用，但也暴露了一些问题：</p><ul><li>用户问：”我的起司黏不在披萨上，怎么办？”</li><li>AI 回答：”用 1/8 的无毒胶水把起司黏在披萨上”</li><li>这个错误答案来自 Reddit 上的玩笑帖子</li><li>说明 RAG 系统需要更好的信息筛选和验证机制</li></ul><p><strong>ChatGPT 的搜索功能：</strong></p><ul><li>用户可以开启网络搜索功能</li><li>模型会先搜索相关信息，再基于搜索结果回答</li><li>显著提高了回答的准确性和时效性</li></ul><h4 id="7-4-RAG-的技术发展方向"><a href="#7-4-RAG-的技术发展方向" class="headerlink" title="7.4 RAG 的技术发展方向"></a>7.4 RAG 的技术发展方向</h4><p><strong>1. 智能检索优化</strong></p><ul><li>改进查询重写和扩展技术</li><li>实现多轮检索和迭代优化</li><li>支持多模态信息检索</li></ul><p><strong>2. 信息融合改进</strong></p><ul><li>提高检索信息的整合质量</li><li>实现更好的信息去重和排序</li><li>支持多来源信息的冲突解决</li></ul><p><strong>3. 引用管理</strong></p><ul><li>实现准确的信息来源追踪</li><li>支持引用验证和更新</li><li>提供透明的信息溯源</li></ul><hr><h3 id="八、工具使用与行动能力"><a href="#八、工具使用与行动能力" class="headerlink" title="八、工具使用与行动能力"></a>八、工具使用与行动能力</h3><h4 id="8-1-工具使用的核心机制"><a href="#8-1-工具使用的核心机制" class="headerlink" title="8.1 工具使用的核心机制"></a>8.1 工具使用的核心机制</h4><p>Context Engineering 不仅关乎理解，也关乎行动。现代 AI Agent 已能根据上下文呼叫工具（Tool Invocation）执行具体任务，如查询资料、发送邮件、操作电脑。</p><p><strong>工具使用的工作流程：</strong></p><ol><li><strong>工具定义</strong>：在上下文中提供工具的使用说明</li><li><strong>工具选择</strong>：模型根据任务选择合适的工具</li><li><strong>工具调用</strong>：生成工具调用的指令</li><li><strong>结果整合</strong>：将工具执行结果整合到上下文中</li></ol><h4 id="8-2-工具使用的实际案例"><a href="#8-2-工具使用的实际案例" class="headerlink" title="8.2 工具使用的实际案例"></a>8.2 工具使用的实际案例</h4><p><strong>数学计算工具：</strong></p><ul><li>模型可以调用计算器进行复杂运算</li><li>避免数学计算错误</li><li>提高计算精度和效率</li></ul><p><strong>邮件发送工具：</strong></p><ul><li>模型可以发送邮件</li><li>支持邮件模板和个性化</li><li>实现自动化沟通</li></ul><p><strong>日历管理工具：</strong></p><ul><li>模型可以查看和修改日历</li><li>支持会议安排和提醒</li><li>实现智能日程管理</li></ul><h4 id="8-3-工具使用的技术挑战"><a href="#8-3-工具使用的技术挑战" class="headerlink" title="8.3 工具使用的技术挑战"></a>8.3 工具使用的技术挑战</h4><p><strong>1. 工具指令生成</strong></p><ul><li>如何生成正确的工具调用指令</li><li>处理工具参数和返回值</li><li>支持复杂的工具调用链</li></ul><p><strong>2. 工具结果处理</strong></p><ul><li>如何正确解析工具执行结果</li><li>处理工具执行错误和异常</li><li>实现工具结果的智能整合</li></ul><p><strong>3. 工具选择优化</strong></p><ul><li>如何从多个工具中选择最合适的</li><li>支持工具的组合使用</li><li>实现工具使用的学习优化</li></ul><hr><p>RAG 流程：查询理解→检索→上下文拼接→生成回答。它是弥补模型知识时效与覆盖的主路径，但质量取决于检索与融合。</p><p>披萨”无毒胶水”事件提醒：若直接采信低可信来源，模型会把玩笑当事实。RAG 必须对来源进行质量控制与可信度建模。</p><p>即便结合搜索，语言模型仍可能在叙述层面产生属性错误。应将”引用展示与冲突消解”作为默认能力，而非可选项。</p><p>重排（Reranking）：用轻量模型对候选段落相关性重排，减少噪声注入。句子级粒度的选择能显著提升拼接有效性。</p><p>引用与溯源：保留来源段落与链接，便于用户核验与自纠；在高风险领域强制显示关键证据。</p><p>“倒 U 曲线”：注入越多不一定越好，超过模型吸收能力后正确率下降。应以小批次、可控预算迭代注入。</p><p>“Lost in the Middle”：把关键信息放在上下文开头/结尾更易被命中；长文应做分段摘要，关键句前置。</p><p>去重与结构化：标题化要点、编号列点、图表化指标可提升注意力命中率与事实对齐的稳定性。</p><p>多来源交叉验证：对同一事实最少两路来源背书；对冲突信息进行显式合并与不确定性提示。</p><p>产品化守则：默认开启证据显示；为敏感域增加判别器与人工复核流程；为失败路径提供可追踪日志。</p><h3 id="九、电脑操作：AI-的具身化起点"><a href="#九、电脑操作：AI-的具身化起点" class="headerlink" title="九、电脑操作：AI 的具身化起点"></a>九、电脑操作：AI 的具身化起点</h3><h4 id="9-1-电脑操作的技术突破"><a href="#9-1-电脑操作的技术突破" class="headerlink" title="9.1 电脑操作的技术突破"></a>9.1 电脑操作的技术突破</h4><p>最新的 ChatGPT 和 Claude 能在虚拟环境中以滑鼠与键盘操作电脑，这象征语言模型首次跨入”具身智能（Embodied Intelligence）”领域。</p><p><strong>电脑操作的核心能力：</strong></p><ul><li>理解屏幕画面内容</li><li>生成鼠标和键盘操作指令</li><li>执行复杂的电脑任务</li><li>适应不同的用户界面</li></ul><h4 id="9-2-电脑操作的实际应用"><a href="#9-2-电脑操作的实际应用" class="headerlink" title="9.2 电脑操作的实际应用"></a>9.2 电脑操作的实际应用</h4><p><strong>订票系统操作：</strong></p><ul><li>模型可以自动订高铁票</li><li>处理复杂的订票流程</li><li>适应不同的网站界面</li></ul><p><strong>文件管理：</strong></p><ul><li>模型可以管理文件和文件夹</li><li>执行文件操作任务</li><li>支持批量处理</li></ul><p><strong>软件开发辅助：</strong></p><ul><li>模型可以编写和修改代码</li><li>执行测试和调试</li><li>管理开发环境</li></ul><h4 id="9-3-电脑操作的技术原理"><a href="#9-3-电脑操作的技术原理" class="headerlink" title="9.3 电脑操作的技术原理"></a>9.3 电脑操作的技术原理</h4><p><strong>Context 在电脑操作中的作用：</strong></p><ul><li>描述任务目标和当前状态</li><li>解析屏幕画面信息</li><li>生成操作指令</li><li>验证操作结果</li></ul><p><strong>操作指令的生成：</strong></p><ul><li>鼠标移动：move_mouse(x, y)</li><li>鼠标点击：click_mouse(button)</li><li>键盘输入：type_text(text)</li><li>等待操作：wait(time)</li></ul><h4 id="9-4-电脑操作的发展前景"><a href="#9-4-电脑操作的发展前景" class="headerlink" title="9.4 电脑操作的发展前景"></a>9.4 电脑操作的发展前景</h4><p>电脑操作代表了 AI 具身化的重要起点：</p><p><strong>1. 通用性</strong></p><ul><li>可以操作任何电脑应用</li><li>不限于特定领域或任务</li><li>支持复杂的工作流程</li></ul><p><strong>2. 学习能力</strong></p><ul><li>可以从操作中学习</li><li>适应新的界面和流程</li><li>提高操作效率</li></ul><p><strong>3. 协作能力</strong></p><ul><li>可以与人类协作完成任务</li><li>支持远程操作和协助</li><li>实现人机协同工作</li></ul><hr><h3 id="十、AI-Agent-时代的-Context-Engineering"><a href="#十、AI-Agent-时代的-Context-Engineering" class="headerlink" title="十、AI Agent 时代的 Context Engineering"></a>十、AI Agent 时代的 Context Engineering</h3><h4 id="10-1-AI-Agent-的核心特征"><a href="#10-1-AI-Agent-的核心特征" class="headerlink" title="10.1 AI Agent 的核心特征"></a>10.1 AI Agent 的核心特征</h4><p>AI Agent 是 Context Engineering 的重要应用场景，具有以下核心特征：</p><p><strong>1. 自主决策</strong></p><ul><li>能够自主决定解决问题的步骤</li><li>根据环境变化调整策略</li><li>实现灵活的任务执行</li></ul><p><strong>2. 长期运行</strong></p><ul><li>支持长时间的任务执行</li><li>维持状态和记忆</li><li>处理复杂的多步骤任务</li></ul><p><strong>3. 工具使用</strong></p><ul><li>能够调用各种外部工具</li><li>整合多种能力</li><li>实现复杂的任务目标</li></ul><h4 id="10-2-AI-Agent-的-Context-管理挑战"><a href="#10-2-AI-Agent-的-Context-管理挑战" class="headerlink" title="10.2 AI Agent 的 Context 管理挑战"></a>10.2 AI Agent 的 Context 管理挑战</h4><p><strong>1. 上下文长度限制</strong></p><ul><li>长时间运行会积累大量上下文</li><li>需要智能的上下文压缩和摘要</li><li>避免重要信息丢失</li></ul><p><strong>2. 信息优先级管理</strong></p><ul><li>如何确定哪些信息最重要</li><li>实现动态的信息筛选</li><li>保持任务焦点</li></ul><p><strong>3. 状态一致性</strong></p><ul><li>在长期运行中维持状态一致性</li><li>避免信息冲突和矛盾</li><li>实现智能的状态更新</li></ul><h4 id="10-3-Context-Engineering-的三大策略"><a href="#10-3-Context-Engineering-的三大策略" class="headerlink" title="10.3 Context Engineering 的三大策略"></a>10.3 Context Engineering 的三大策略</h4><p><strong>1. 选择（Selection）</strong></p><ul><li>只选择相关信息进入上下文</li><li>实现智能的信息筛选</li><li>避免信息过载</li></ul><p><strong>2. 压缩（Compression）</strong></p><ul><li>对历史信息进行压缩和摘要</li><li>保留关键信息</li><li>减少上下文长度</li></ul><p><strong>3. 多代理（Multi-Agent）</strong></p><ul><li>使用多个 Agent 分工合作</li><li>每个 Agent 管理部分上下文</li><li>实现更高效的上下文管理</li></ul><hr><p>Computer Use 两种形态：远端虚拟桌面（安全、不触本机）与本地控制（更强也更危险）。二者皆以”屏幕理解+输入控制”为核心。</p><p>风险与权限：默认最小权限、敏感动作二次确认、操作日志与回放是必要的工程底线；必要时启用只读模式与时间盒限制。</p><p>Agent 模式演示：根据高层目标自动分解步骤、定位 UI、填入参数、处理异常弹窗；失败后能自我修正重试。</p><p>SOP vs Agentic：固定流程适合可预测任务；Agentic 工作流适合开放任务，能在 Observation→Action→Observation 循环中自适应调整计划。</p><p>LLM-Agent 与传统 Agent 差异：输出空间是自然语言、近乎无限；可被人类语言直接指导与纠偏；与 AlphaGo 的”离散动作集合”截然不同。</p><p>CLI 实操：让 Agent 真实读写文件、生成项目脚手架、运行与修改代码；需要更严格的沙箱与回滚策略。</p><p>批准与人机协作：对关键步骤请求用户确认；人类可随时插手、修改参数或中止流程，形成可控的半自动协作。</p><p>评测维度：目标达成率、失败可恢复性、对齐与安全事件率、成本与时延；分场景基准才有意义。</p><p>从”会用工具”到”会用电脑”，将通用软件生态纳入能力版图；它是走向具身智能的重要台阶。</p><p>产品建议：以代理-执行者分层封装复杂度；把不稳定长链交互下放给执行者，主代理只保策略与里程碑，降低上下文压力。</p><h3 id="十一、结语：Context-Engineering-——-AI-Agent-时代的灵魂工程"><a href="#十一、结语：Context-Engineering-——-AI-Agent-时代的灵魂工程" class="headerlink" title="十一、结语：Context Engineering —— AI Agent 时代的灵魂工程"></a>十一、结语：Context Engineering —— AI Agent 时代的灵魂工程</h3><h4 id="11-1-Context-Engineering-的核心价值"><a href="#11-1-Context-Engineering-的核心价值" class="headerlink" title="11.1 Context Engineering 的核心价值"></a>11.1 Context Engineering 的核心价值</h4><p>综观全局，Context Engineering 不仅是一种提示设计技巧，更是 AI 智能体的”外部认知建筑学”。它将使用者指令、记忆、检索、工具与伦理规范统合为一体，构筑出模型的即时世界。</p><p><strong>Context Engineering 的三大价值：</strong></p><p><strong>1. 认知扩展</strong></p><ul><li>突破模型固有的知识限制</li><li>整合外部知识和工具</li><li>实现更智能的决策</li></ul><p><strong>2. 个性化体验</strong></p><ul><li>基于用户历史和偏好</li><li>提供个性化的服务</li><li>建立长期的关系</li></ul><p><strong>3. 任务执行能力</strong></p><ul><li>支持复杂的多步骤任务</li><li>实现工具调用和操作</li><li>提供端到端的解决方案</li></ul><h4 id="11-2-未来发展方向"><a href="#11-2-未来发展方向" class="headerlink" title="11.2 未来发展方向"></a>11.2 未来发展方向</h4><p><strong>1. 技术演进</strong></p><ul><li>更智能的上下文管理</li><li>更高效的信息压缩</li><li>更强大的工具集成</li></ul><p><strong>2. 应用拓展</strong></p><ul><li>更广泛的应用场景</li><li>更深入的人机协作</li><li>更智能的自动化</li></ul><p><strong>3. 研究前沿</strong></p><ul><li>多模态上下文融合</li><li>动态上下文适应</li><li>认知架构设计</li></ul><h4 id="11-3-对-AI-工程师的要求"><a href="#11-3-对-AI-工程师的要求" class="headerlink" title="11.3 对 AI 工程师的要求"></a>11.3 对 AI 工程师的要求</h4><p>未来的 AI 工程师，不仅要懂模型架构与训练，更要精通上下文设计与语境管理：</p><p><strong>1. 技术能力</strong></p><ul><li>掌握 Context Engineering 的核心技术</li><li>理解不同上下文组件的相互作用</li><li>能够设计高效的上下文管理策略</li></ul><p><strong>2. 系统思维</strong></p><ul><li>从整体角度思考 AI 系统设计</li><li>平衡不同组件之间的关系</li><li>实现系统的最优配置</li></ul><p><strong>3. 创新意识</strong></p><ul><li>探索新的上下文管理模式</li><li>开发创新的应用场景</li><li>推动技术的前沿发展</li></ul><blockquote><p><strong>AI 的智慧，终将不仅来自权重，而来自上下文的设计。</strong></p></blockquote><p>Context Engineering 正在成为 AI 时代最重要的工程技能之一。它不仅是技术问题，更是艺术问题——如何设计出既智能又可控、既强大又安全的 AI 系统，这需要工程师们具备深厚的技术功底和丰富的创造力。在 AI Agent 时代，掌握 Context Engineering 就是掌握了 AI 智能体的灵魂。</p><h3 id="附录：实战案例与进阶方法（补充）"><a href="#附录：实战案例与进阶方法（补充）" class="headerlink" title="附录：实战案例与进阶方法（补充）"></a>附录：实战案例与进阶方法（补充）</h3><p>以下内容按课堂口语讲解进行系统化整理，并补充工程落地要点，便于直接用于产品与研究实践。相关原始讲解见课程视频：<a href="%60https://www.youtube.com/watch?v=lVdajtNpaGI%60">生成式人工智慧與機器學習導論2025 · 第2講：上下文工程（Context Engineering）——AI Agent 背後的關鍵技術</a>.</p><h4 id="A-神奇咒语的兴衰与-arXiv-生态"><a href="#A-神奇咒语的兴衰与-arXiv-生态" class="headerlink" title="A. 神奇咒语的兴衰与 arXiv 生态"></a>A. 神奇咒语的兴衰与 arXiv 生态</h4><ul><li><strong>早期”神奇咒语”有效的原因</strong>：模型较弱、输入输出关系不稳定，诸如”Let’s think step by step””请先深呼吸””答对给小费””这题对我非常重要”等提示，曾能显著提升正确率。</li><li><strong>趣味实验</strong>：在 GPT-3 年代，甚至在 prompt 中加一串”位置”字样都能异常拉长输出；让模型在写 200 字故事时，”世界和平”比”母亲为你骄傲”更能让它贴合长度目标（因模型没有”母亲”概念）。</li><li><strong>如今为什么逐渐失效</strong>：模型内在推理与自我校正增强后，靠”措辞魔法”获取提升的空间越来越小，取而代之的是上下文组织、检索与工具协同。</li><li><strong>arXiv 说明</strong>：AI 领域节奏快，研究常先在 arXiv 公开（如 2206 即 2022 年 6 月），便于社区快速验证与复现。</li></ul><h4 id="B-语境构成的实例化要点"><a href="#B-语境构成的实例化要点" class="headerlink" title="B. 语境构成的实例化要点"></a>B. 语境构成的实例化要点</h4><ul><li><strong>明确前提能显著减少歧义</strong>：<ul><li>“载具是什么意思？”若未给出情境，模型可能解释为交通工具或电子载具；若补充”在超商结账时店员问’要用载具吗’”，模型即可聚焦到电子发票载具。</li></ul></li><li><strong>场景信息改变答案分布</strong>：<ul><li>曼谷运河照片中的”鳄鱼”在加入地理前提后被识别为”水巨蜥”，并给出文化含义（吉祥、偏财运）。</li></ul></li><li><strong>示例胜于口头定义</strong>：<ul><li>“火星文”若无示例，模型可能做字形替换；给出”要去冒险的人来找我 → 要ㄑ冒险 ㄉ人来找我”后，模型迅速对齐到注音替换规则。</li></ul></li></ul><h4 id="C-RAG-的价值与风险并存"><a href="#C-RAG-的价值与风险并存" class="headerlink" title="C. RAG 的价值与风险并存"></a>C. RAG 的价值与风险并存</h4><ul><li><strong>价值</strong>：在知识时效与覆盖受限情况下，通过检索把”最新、相关”的事实注入上下文，显著提升答案可靠性。</li><li><strong>典型风险</strong>：<ul><li>Google AI Overview 曾因引用玩笑贴文，建议在披萨上加”1/8 无毒胶水”粘奶酪；</li><li>即便是最新模型结合搜索，文本叙述仍可能出现属性错误或夸张叙述。</li></ul></li><li><strong>工程对策</strong>：<ul><li>保留引用并尽量呈现来源段落；</li><li>采用 rerank/判别器过滤噪声；</li><li>对高风险领域（医疗、金融）启用”多来源交叉验证 + 置信阈值”。</li></ul></li></ul><h4 id="D-工具调用的落地方法（从”文字”到”动作”）"><a href="#D-工具调用的落地方法（从”文字”到”动作”）" class="headerlink" title="D. 工具调用的落地方法（从”文字”到”动作”）"></a>D. 工具调用的落地方法（从”文字”到”动作”）</h4><p>模型只能生成文字，无法直接”执行”。因此需要在外层实现”解析—执行—回填—继续生成”的闭环：</p><ol><li>在上下文中提供工具说明与示例（自然语言足矣），并约定调用标记（如〈tool〉…〈/tool〉、〈result〉…〈/result〉）。</li><li>让模型按约定输出工具调用指令（纯文本）。</li><li>在宿主程序中解析指令并实际执行（例如在教学环境用 <code>eval()</code>，生产环境请使用白名单与参数校验的安全执行器）。</li><li>将工具输出以约定格式回填到上下文中。</li><li>重复上述过程直到模型不再发出调用请求，再输出用户可见的最终答复。</li></ol><p>要点：</p><ul><li>不要相信模型”自述已调用工具”的叙事，那只是接龙；必须以宿主程序的执行结果为准。</li><li>将中间的调用细节对用户”可见/不可见”作为产品策略开关，默认对普通用户隐藏、对调试与审计开放。</li></ul><h4 id="E-Computer-Use：强能力与高风险并存"><a href="#E-Computer-Use：强能力与高风险并存" class="headerlink" title="E. Computer Use：强能力与高风险并存"></a>E. Computer Use：强能力与高风险并存</h4><ul><li><strong>两种形态</strong>：<ul><li>远端”虚拟桌面”（如部分 ChatGPT Agent 模式）：不直接触碰本机，安全性较高；</li><li>本地控制（如 Gemini CLI）：可直接创建/修改文件、关程序，能力强但更危险。</li></ul></li><li><strong>实践建议</strong>：<ul><li>默认最小权限，操作需逐步确认；</li><li>文件系统白名单与沙箱隔离；</li><li>关键动作（删除、外联）强制二次确认与审计日志；</li><li>为 UI 元素提供稳定锚点（坐标/语义选择器），降低定位误差。</li></ul></li></ul><h4 id="F-Agentic-Workflow-与-AI-Agent-的本质"><a href="#F-Agentic-Workflow-与-AI-Agent-的本质" class="headerlink" title="F. Agentic Workflow 与 AI Agent 的本质"></a>F. Agentic Workflow 与 AI Agent 的本质</h4><ul><li><strong>工作流（有 SOP）</strong>：将复杂任务拆解为多步骤（识别攻击/评分/验证等），每步可设立输入输出契约。</li><li><strong>AI Agent（自定策略）</strong>：根据 observation 循环规划 action（检索、调用工具、写程序、与人沟通等），直至目标达成。</li><li><strong>与传统 Agent 的差异</strong>：输出是自然语言，表达与控制空间近乎无限；可被人类语言直接指导与反馈。</li><li><strong>提示策略</strong>：与其”挤牙膏式追加要求”，更建议一次性给出完整目标、约束与验收标准（近期研究显示分步追加会降低稳定性与总体能力）。</li></ul><h4 id="G-长上下文的性能陷阱与对策"><a href="#G-长上下文的性能陷阱与对策" class="headerlink" title="G. 长上下文的性能陷阱与对策"></a>G. 长上下文的性能陷阱与对策</h4><ul><li>**”Lost in the Middle”**：答案位于长上下文中段时最易被忽略；尽量将关键信息靠近开头或结尾。</li><li>**”Context Rot”**：即便未触达上限，长度上升也会导致复制/对齐能力快速下滑。</li><li>**检索注入的”倒 U 曲线”**：过多检索文本会让正确率先升后降。</li><li><strong>工程对策</strong>：<ul><li>预算化分配 token（指令、历史、知识、工具、思考笔记分别控额）；</li><li>关键信息置顶/置底；</li><li>动态裁剪与去重；</li><li>对长文采用”分段摘要 + 局部细节回溯”策略；</li><li>重要信息多路冗余（标题、要点、编号）以提升注意力命中率。</li></ul></li></ul><h4 id="H-三大招数：选择、压缩、Multi-Agent（工程视角）"><a href="#H-三大招数：选择、压缩、Multi-Agent（工程视角）" class="headerlink" title="H. 三大招数：选择、压缩、Multi-Agent（工程视角）"></a>H. 三大招数：选择、压缩、Multi-Agent（工程视角）</h4><ul><li><strong>选择（Selection）</strong>：<ul><li>RAG 前置：用 LLM 将用户任务改写成多路查询（Query Expansion），再检索；</li><li>Reranking：使用轻量模型对候选段落进行重排，只保留最相关片段；</li><li>句子级选择：用极小模型（&lt;300M）逐句判定相关性，显著降低噪声注入；</li><li>工具版 RAG：将”工具说明”看作文档，仅检索当下相关工具注入上下文；</li><li>记忆版 RAG：长期记忆外置，按”近因/重要性/相关性”打分召回（可参考 Stanford “小镇”工作流）。</li></ul></li><li><strong>压缩（Compression）</strong>：<ul><li>递归摘要：按窗口占用或交互轮次触发压缩，保留关键信息，细节落盘；</li><li>摘要内置”指针”：在摘要中写入”详情见 path/to/file.txt 第 X 段”，必要时再回读原文；</li><li>区分”永久笔记”（显式记忆）与”易逝记忆”（系统自管随时间衰减）。</li></ul></li><li><strong>Multi-Agent</strong>：<ul><li>以”总召—执行者”结构来分摊上下文：总召保策略与里程碑，执行者承接具体长交互（如订餐厅/订旅馆）；</li><li>大规模文献综述：每篇论文由独立 Agent 阅读并生成结构化摘要，最后由汇总 Agent 进行融合与写作；</li><li>单体 vs 多体权衡：任务简单时单体往往更强；任务复杂且交互链路长时，多体凭借上下文分割与职能并行具有优势。</li></ul></li></ul><h4 id="I-经验与反例：如何用好”记忆”"><a href="#I-经验与反例：如何用好”记忆”" class="headerlink" title="I. 经验与反例：如何用好”记忆”"></a>I. 经验与反例：如何用好”记忆”</h4><ul><li>在”自我改进基准”中，仅注入”过去答对的例子”更稳；盲目注入”错误案例”反而可能伤害整体表现。</li><li>实践建议：<ul><li>错误记忆要”连同判定与更正过程”一起注入，且与当前任务强相关；</li><li>对”负例记忆”设置使用门槛（置信/一致性检查），避免诱导错误迁移。</li></ul></li></ul><h4 id="J-一份可操作的-Context-Engineering-清单"><a href="#J-一份可操作的-Context-Engineering-清单" class="headerlink" title="J. 一份可操作的 Context Engineering 清单"></a>J. 一份可操作的 Context Engineering 清单</h4><ol><li>明确”目标—约束—验收标准”，一次性写清楚。</li><li>关键信息置顶/置底，并编号列点。</li><li>提供 1–3 个覆盖典型边界的高质量示例。</li><li>对检索结果进行去重、重排与句子级筛选。</li><li>工具少而精；按需检索工具说明再注入。</li><li>长对话每 N 轮执行一次递归摘要，细节落盘并在摘要中留”回看指针”。</li><li>采用”记忆 RAG”：近因/重要性/相关性三分尺度召回。</li><li>将”思考过程”与”对用户可见输出”分离，必要时只给摘要。</li><li>关键动作（写档、删除、转账）二次确认并记录审计日志。</li><li>对高风险答案启用”多来源交叉验证 + 引用展示”。</li><li>将 Agent 任务切分为”总召—执行者”，以分摊上下文压力。</li><li>固化成功的上下文模版（含占位符与预算），形成可复用 SOP。</li></ol><p>—— 以上补充旨在把课堂中的案例、工程套路与安全要点系统化，帮助在真实产品与研究中”以最小上下文预算获得最大可靠性”。</p><p>工具使用是从”理解”走向”行动”的关键：通过在上下文定义工具、示例与调用协议，让模型生成可解析的调用指令。</p><p>宿主程序负责把”文字”变”动作”：解析模型输出→执行真实工具→将返回结果回填上下文→继续生成，直到不再调用。</p><p>数学工具示例：用外部计算器替代心算，显著提升数值稳定性。工程上应默认将可形式化的子任务交由工具完成。</p><p>温度查询示例：通过参数化接口获取事实值，模型负责组织语言与用户交互。异常值应伴随一致性检查与反常提示。</p><p>不要相信”我已调用工具”的自述——那只是接龙。以宿主层执行与结果为准，才能消除幻觉干扰。</p><p>安全执行：禁止直接 eval 非受控字符串；使用白名单、Schema 校验与最小权限沙箱；记录调用日志便于审计与回放。</p><p>对用户可见性：把中间调用细节默认隐藏，给专家模式或调试模式开放；为错误路径提供可读的诊断线索。</p><p>调用链组合：复杂任务往往需要多次工具调用与结果聚合；要保证幂等性与错误恢复机制，避免半程失败卡死。</p><p>协议演化：随着能力增长，工具协议可从”弱约束自然语”演进到”强约束 JSON/Schema”，提升可解析性与健壮性。</p><p>评估指标：覆盖率（调用该调用的都调用）、正确率（参数与结果正确）、延迟与成本；对高风险路径启用强制人工复核。</p><p>Context Window 演进：从 3 万到 10 万、100 万甚至千万级 token。更长的输入是 Agent 长时运行的必要不充分条件。</p><p>可输入≠可理解：如同翻完哈利波特并不等于”记住全部情节”。长上下文下注意力稀释与对齐退化是工程常态。</p><p>RAG 注入的”倒 U”：起初有益，过量反而伤害。说明需要预算化的注入、重排与去重，避免噪声淹没信号。</p><p>“Lost in the Middle”：中段最易丢失。将关键信息前置/后置、标题化与编号化，有助于稳定命中。</p><p>“Context Rot”：复制/对齐能力随长度迅速下降，且远未触及模型理论上限时已出现。强调”少即是多”的治理原则。</p><p>三大策略：选择（检索/重排/句子级选择/工具与记忆版 RAG）、压缩（递归摘要+指针回溯）、多代理（总召-执行者分层）。</p><p>实操清单：一次性明确目标与约束；关键要点置顶/置底；高质量示例；小批次注入并交叉验证；对长链交互采用多代理分摊上下文。</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>在不确定中锚定自我</title>
      <link href="/2025/06/25/Life%20Reflections/%E5%9C%A8%E4%B8%8D%E7%A1%AE%E5%AE%9A%E4%B8%AD%E9%94%9A%E5%AE%9A%E8%87%AA%E6%88%91/"/>
      <url>/2025/06/25/Life%20Reflections/%E5%9C%A8%E4%B8%8D%E7%A1%AE%E5%AE%9A%E4%B8%AD%E9%94%9A%E5%AE%9A%E8%87%AA%E6%88%91/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> Life Reflections </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 坡岛生活指北 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SeCom: Redefining Memory Management in Conversational AI</title>
      <link href="/2025/06/24/NLP%20Insights/SeCom%20Redefining%20Memory%20Management%20in%20Conversational%20AI/"/>
      <url>/2025/06/24/NLP%20Insights/SeCom%20Redefining%20Memory%20Management%20in%20Conversational%20AI/</url>
      
        <content type="html"><![CDATA[<h1 id="SeCom-Redefining-Memory-Management-in-Conversational-AI"><a href="#SeCom-Redefining-Memory-Management-in-Conversational-AI" class="headerlink" title="SeCom: Redefining Memory Management in Conversational AI"></a>SeCom: Redefining Memory Management in Conversational AI</h1><h2 id="Foreword"><a href="#Foreword" class="headerlink" title="Foreword"></a>Foreword</h2><p>I’ve recently been diving into memory management for dialog-based AI, especially how to construct and retrieve memories in long-term conversations. During my exploration I came across an eye-opening ICLR 2025 paper—**”SeCom: On Memory Construction and Retrieval for Personalized Conversational Agents”**—a collaboration between Microsoft and Tsinghua University.</p><p>SeCom solves a core problem: <strong>How can an agent effectively manage and retrieve historical information in prolonged conversations?</strong> In this post I’ll unpack the method’s key ideas and technical innovations, hoping to spark inspiration for researchers working in this arena.</p><h2 id="1-Why-Should-We-Care-About-Dialog-Memory-Management"><a href="#1-Why-Should-We-Care-About-Dialog-Memory-Management" class="headerlink" title="1. Why Should We Care About Dialog Memory Management?"></a>1. Why Should We Care About Dialog Memory Management?</h2><h3 id="1-1-Real-World-Challenges-in-Long-Conversations"><a href="#1-1-Real-World-Challenges-in-Long-Conversations" class="headerlink" title="1.1 Real-World Challenges in Long Conversations"></a>1.1 Real-World Challenges in Long Conversations</h3><p>Anyone who chats with LLMs regularly has probably experienced this: once a conversation grows long, the agent seems to “forget” earlier context or respond incoherently. That’s the memory problem in action.</p><p>Even with long-context models, super-long dialogs increase compute cost and often degrade quality. Key challenges include:</p><ul><li><strong>Context length limits</strong>: Token budgets remain finite.</li><li><strong>Information relevance</strong>: History contains plenty of facts irrelevant to the current query.</li><li><strong>Semantic coherence</strong>: Related information may be scattered across non-contiguous turns.</li><li><strong>Personalization</strong>: The agent must remember user preferences and interaction patterns.</li></ul><h3 id="1-2-A-Quick-Landscape-of-Existing-Approaches"><a href="#1-2-A-Quick-Landscape-of-Existing-Approaches" class="headerlink" title="1.2 A Quick Landscape of Existing Approaches"></a>1.2 A Quick Landscape of Existing Approaches</h3><p>The community’s strategies roughly split into three camps:</p><ol><li><strong>“Give Me Everything” (full history)</strong><ul><li>Complete information, zero recall loss.</li><li>But like moving an entire library just to find one book—computational overkill.</li></ul></li><li><strong>“Bullet-Point Digest” (summaries)</strong><ul><li>Compact and efficient.</li><li>Risk of omitting crucial details during abstraction.</li></ul></li><li><strong>“Precision Strike” (retrieval-based)</strong><ul><li>Fetch only what you need, exactly when you need it.</li><li>Success hinges on choosing the right retrieval granularity—precisely the issue SeCom addresses.</li></ul></li></ol><h4 id="1-2-3-Retrieval-Augmented-Generation-RAG-in-Dialog"><a href="#1-2-3-Retrieval-Augmented-Generation-RAG-in-Dialog" class="headerlink" title="1.2.3 Retrieval-Augmented Generation (RAG) in Dialog"></a>1.2.3 Retrieval-Augmented Generation (RAG) in Dialog</h4><p>RAG faces dialog-specific hurdles:</p><ul><li><strong>Chunking strategy</strong>: How to segment a dialog into retrievable units.</li><li><strong>Relevance estimation</strong>: Harder than in static docs due to dialog dynamics.</li><li><strong>Temporal dependency</strong>: Order matters; turns refer to earlier context.</li></ul><h3 id="1-3-The-Granularity-Dilemma"><a href="#1-3-The-Granularity-Dilemma" class="headerlink" title="1.3 The Granularity Dilemma"></a>1.3 The Granularity Dilemma</h3><p>We often index memories at the turn-level or at the whole-conversation level. Both extremes break down:</p><ul><li><strong>Turn-level</strong> → fragments context, loses dependencies, retrieval recall suffers.</li><li><strong>Conversation-level</strong> → topic mixture, lots of noise, retrieval becomes coarse.</li><li><strong>Summaries</strong> → irreversible information loss.</li></ul><p>SeCom’s insight: dialog naturally contains <strong>paragraph-level thematic boundaries</strong>. Segmenting at this “just-right” granularity preserves coherence without exploding memory size.</p><h2 id="2-Inside-SeCom"><a href="#2-Inside-SeCom" class="headerlink" title="2. Inside SeCom"></a>2. Inside SeCom</h2><h3 id="2-1-Two-Key-Insights"><a href="#2-1-Two-Key-Insights" class="headerlink" title="2.1 Two Key Insights"></a>2.1 Two Key Insights</h3><ol><li><strong>Paragraph-like Topic Shifts</strong> exist in dialog just as in essays.</li><li><strong>Natural Language Is Redundant</strong>—filler words, confirmations, small talk, etc. Removing them boosts retrieval precision.</li></ol><p>Hence <strong>SeCom = Segmentation + Compression</strong>.</p><h3 id="2-2-System-Pipeline"><a href="#2-2-System-Pipeline" class="headerlink" title="2.2 System Pipeline"></a>2.2 System Pipeline</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">History → [Segmenter] → Paragraph-level units → [Compressor] → Denoised memories → [Retriever] → Relevant context → [Generator] → Final reply</span><br></pre></td></tr></tbody></table></figure><p>Technically:</p><ol><li>Segmenter $f_{\mathcal I}$ splits the dialog.</li><li>Compressor $f_{comp}$ denoises each segment.</li><li>Retriever $f_R$ ranks memories for the current user utterance $u^*$.</li><li>LLM $f_{LLM}$ produces the answer based on top-N memories.</li></ol><h3 id="2-3-How-to-Segment-Without-Labels"><a href="#2-3-How-to-Segment-Without-Labels" class="headerlink" title="2.3 How to Segment Without Labels"></a>2.3 How to Segment Without Labels</h3><p>SeCom leverages GPT-4 in a <strong>zero-shot</strong> fashion: craft a prompt asking the model to mark topic boundaries and output span indices. No training data required.</p><p>When limited gold data are available, a <strong>reflection-based</strong> loop iteratively refines the guidelines using WindowDiff scores and GPT-4 reasoning.</p><p>An <strong>incremental segmenter</strong> decides on-the-fly whether a new turn merges into the previous segment or starts a fresh one.</p><h3 id="2-4-Denoising-via-LLMLingua-2"><a href="#2-4-Denoising-via-LLMLingua-2" class="headerlink" title="2.4 Denoising via LLMLingua-2"></a>2.4 Denoising via LLMLingua-2</h3><p>LLMLingua-2 scores token importance and keeps the top $(1-r)$ fraction (e.g., 25 %) accordingly. Empirically, retaining just 25 % tokens preserves <strong>&gt;95 %</strong> key information, lifts retrieval GPT4Score by <strong>+9.46</strong>, and yields 4 × speed-up.</p><h3 id="2-5-Hybrid-Retrieval"><a href="#2-5-Hybrid-Retrieval" class="headerlink" title="2.5 Hybrid Retrieval"></a>2.5 Hybrid Retrieval</h3><p>BM25 (sparse) and MPNet (dense) scores are linearly combined:</p><p>$$\text{score}_{hybrid}=\alpha,\text{BM25}+(1-\alpha),\text{MPNet}, \quad \alpha=0.6$$</p><h2 id="3-Final-Thoughts"><a href="#3-Final-Thoughts" class="headerlink" title="3. Final Thoughts"></a>3. Final Thoughts</h2><h3 id="3-1-What-SeCom-Teaches-Us"><a href="#3-1-What-SeCom-Teaches-Us" class="headerlink" title="3.1 What SeCom Teaches Us"></a>3.1 What SeCom Teaches Us</h3><ul><li><strong>Simplicity Wins</strong>: Segment + Compress, nothing fancy, yet highly effective.</li><li><strong>Understand the Problem First</strong>: The authors nailed the granularity pain-point before designing a solution.</li></ul><p>Future directions:</p><ul><li><strong>Personalized segmentation</strong> tuned to each user’s dialog style.</li><li><strong>Real-time adaptation</strong> of compression and segmentation based on quality metrics.</li></ul><hr><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><strong>Paper</strong>: <a href="https://www.arxiv.org/abs/2502.05589">SeCom: On Memory Construction and Retrieval for Personalized Conversational Agents (ICLR 2025)</a></li><li><strong>Project Page</strong>: <a href="https://llmlingua.com/secom.html">https://llmlingua.com/secom.html</a></li><li><strong>Code</strong>: SeCom-main</li><li><strong>Datasets</strong>: LOCOMO, Long-MT-Bench+, DialSeg711, TIAGE, SuperDialSeg</li></ul><p><em>This post is based on Microsoft &amp; Tsinghua University’s ICLR 2025 paper. Please refer to the original publication and open-source repo for implementation details.</em> </p>]]></content>
      
      
      <categories>
          
          <category> NLP Insights </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Conversational AI </tag>
            
            <tag> Memory Management </tag>
            
            <tag> SeCom </tag>
            
            <tag> RAG </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SeCom: 重新定义对话AI的记忆管理</title>
      <link href="/2025/06/24/NLP%20Insights/SeCom:%20%E9%87%8D%E6%96%B0%E5%AE%9A%E4%B9%89%E5%AF%B9%E8%AF%9DAI%E7%9A%84%E8%AE%B0%E5%BF%86%E7%AE%A1%E7%90%86/"/>
      <url>/2025/06/24/NLP%20Insights/SeCom:%20%E9%87%8D%E6%96%B0%E5%AE%9A%E4%B9%89%E5%AF%B9%E8%AF%9DAI%E7%9A%84%E8%AE%B0%E5%BF%86%E7%AE%A1%E7%90%86/</url>
      
        <content type="html"><![CDATA[<h1 id="SeCom-重新定义对话AI的记忆管理"><a href="#SeCom-重新定义对话AI的记忆管理" class="headerlink" title="SeCom: 重新定义对话AI的记忆管理"></a>SeCom: 重新定义对话AI的记忆管理</h1><h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>最近笔者一直在研究对话AI中的内存管理问题，特别是长期对话场景下的记忆构建与检索技术。发现了一篇令人眼前一亮的ICLR 2025论文——<strong>《SeCom: On Memory Construction and Retrieval for Personalized Conversational Agents》</strong>，由Microsoft和清华大学的研究团队联合发表。</p><p>这篇论文提出的SeCom方法巧妙地解决了一个核心问题：<strong>如何在长期对话中有效管理和检索历史信息</strong>？今天想和大家分享一下这个方法的技术细节和创新点，希望能为从事相关研究的朋友们提供一些启发。</p><h2 id="1-为什么我们需要关注对话内存管理？"><a href="#1-为什么我们需要关注对话内存管理？" class="headerlink" title="1. 为什么我们需要关注对话内存管理？"></a>1. 为什么我们需要关注对话内存管理？</h2><h3 id="1-1-长期对话的现实挑战"><a href="#1-1-长期对话的现实挑战" class="headerlink" title="1.1 长期对话的现实挑战"></a>1.1 长期对话的现实挑战</h3><p>在与LLMs的日常交互中，相信大家都遇到过这样的困扰：当对话变得很长时，AI似乎”忘记”了之前讨论的内容，或者给出的回答与前面的上下文不够连贯。这背后反映的正是长期对话中的内存管理挑战。</p><p>随着大语言模型技术的成熟，基于LLM的对话代理已经深入到我们生活的方方面面。但是，当我们希望与AI进行真正的长期、个性化交互时——比如跨越数天、数周的项目讨论，现有的技术就显得力不从心了。</p><p>长期对话面临的主要技术挑战包括：</p><ul><li><strong>上下文长度限制</strong>：即使是支持长上下文的模型，在处理超长对话时也面临计算成本和性能下降的问题</li><li><strong>信息相关性</strong>：历史对话中可能包含大量与当前查询无关的信息</li><li><strong>语义连贯性</strong>：相关信息可能分散在多个不连续的对话轮次中</li><li><strong>个性化记忆</strong>：需要记住用户的偏好、习惯和历史交互模式</li></ul><h3 id="1-2-笔者对Memory管理领域的观察"><a href="#1-2-笔者对Memory管理领域的观察" class="headerlink" title="1.2 笔者对Memory管理领域的观察"></a>1.2 笔者对Memory管理领域的观察</h3><p>在深入研究这个领域的过程中，笔者发现对话内存管理其实是一个相当复杂的系统工程。它的核心目标听起来很简单：从历史对话中提取、存储和检索相关信息，以支持当前对话的生成。但实际实现起来，需要解决三个关键问题：</p><ol><li><strong>内存构建（Memory Construction）</strong>：如何将自然语言对话转换为结构化的内存单元？</li><li><strong>内存检索（Memory Retrieval）</strong>：面对海量历史信息，如何快速准确地找到相关内容？</li><li><strong>响应生成（Response Generation）</strong>：如何基于检索到的记忆生成连贯、个性化的回复？</li></ol><p>听起来是不是很像人类的记忆机制？确实如此，这也是为什么这个问题如此有趣和具有挑战性。</p><h4 id="1-2-2-现有方法的”三国演义”"><a href="#1-2-2-现有方法的”三国演义”" class="headerlink" title="1.2.2 现有方法的”三国演义”"></a>1.2.2 现有方法的”三国演义”</h4><p>在研究过程中，笔者发现现有的方法大致可以分为三大流派，每个都有自己的”哲学”：</p><p><strong>“全盘托出”派（基于完整历史）</strong>：</p><ul><li><strong>核心思想</strong>：既然不知道什么重要，那就全部给你！</li><li><strong>优势</strong>：信息完整，绝不遗漏</li><li><strong>问题</strong>：就像把整个图书馆搬给你找一本书，效率可想而知</li></ul><p><strong>“提纲挈领”派（基于摘要）</strong>：</p><ul><li><strong>核心思想</strong>：重要的信息浓缩成摘要就够了</li><li><strong>优势</strong>：信息压缩，计算高效</li><li><strong>问题</strong>：摘要过程中重要细节可能”意外失踪”</li></ul><p><strong>“精准打击”派（基于检索）</strong>：</p><ul><li><strong>代表方法</strong>：轮次级检索、会话级检索</li><li><strong>核心思想</strong>：需要什么就检索什么，按需取用</li><li><strong>优势</strong>：计算效率高，定位精确</li><li><strong>问题</strong>：关键在于如何确定检索的”粒度”——这正是SeCom要解决的核心问题！</li></ul><h4 id="1-2-3-检索增强生成（RAG）在对话中的应用"><a href="#1-2-3-检索增强生成（RAG）在对话中的应用" class="headerlink" title="1.2.3 检索增强生成（RAG）在对话中的应用"></a>1.2.3 检索增强生成（RAG）在对话中的应用</h4><p>检索增强生成技术在对话系统中的应用日益广泛，主要包括：</p><ul><li>**Dense Passage Retrieval (DPR)**：使用预训练的密集检索模型</li><li><strong>BM25</strong>：基于词频统计的稀疏检索方法</li><li><strong>Hybrid Retrieval</strong>：结合密集检索和稀疏检索的优势</li></ul><p>然而，现有RAG方法在对话场景中面临独特挑战：</p><ul><li><strong>分块策略（Chunking Strategy）</strong>：如何将对话分割为检索单元</li><li><strong>相关性判断</strong>：对话的相关性判断比文档检索更复杂</li><li><strong>时序依赖</strong>：对话具有强时序性，前后文关系重要</li></ul><h3 id="1-3-内存粒度问题的深层分析"><a href="#1-3-内存粒度问题的深层分析" class="headerlink" title="1.3 内存粒度问题的深层分析"></a>1.3 内存粒度问题的深层分析</h3><h4 id="1-3-1-轮次级内存的局限性"><a href="#1-3-1-轮次级内存的局限性" class="headerlink" title="1.3.1 轮次级内存的局限性"></a>1.3.1 轮次级内存的局限性</h4><p>轮次级内存将每个用户-代理交互（turn）作为独立的内存单元：</p><p><strong>数学表示</strong>：<br>设对话历史 $\mathcal{H} = {\mathbf{c}<em>i}</em>{i=1}^C$，其中每个会话 $\mathbf{c}<em>i = {\mathbf{t}<em>j}</em>{j=1}^{T_i}$<br>轮次级内存：$|\mathcal{M}| = \sum</em>{i=1}^C T_i$，每个 $\mathbf{m} \in \mathcal{M}$ 对应一个轮次 $\mathbf{t}$</p><p><strong>主要问题</strong>：</p><ul><li><strong>信息碎片化</strong>：相关信息分散在多个轮次中，单个轮次可能缺乏完整语义</li><li><strong>上下文缺失</strong>：轮次间的依赖关系丢失</li><li><strong>检索精度低</strong>：查询词汇可能不直接出现在相关轮次中</li></ul><p><strong>具体示例</strong>：<br>用户在第3轮询问”什么是机器学习”，第5轮询问”监督学习的例子”，第8轮询问”如何选择算法”。当用户在第10轮询问”之前提到的分类算法性能如何评估”时，轮次级检索可能无法找到完整的上下文。</p><h4 id="1-3-2-会话级内存的局限性"><a href="#1-3-2-会话级内存的局限性" class="headerlink" title="1.3.2 会话级内存的局限性"></a>1.3.2 会话级内存的局限性</h4><p>会话级内存将整个对话会话作为内存单元：</p><p><strong>数学表示</strong>：<br>会话级内存：$|\mathcal{M}| = C$，每个 $\mathbf{m} \in \mathcal{M}$ 对应一个会话 $\mathbf{c}$</p><p><strong>主要问题</strong>：</p><ul><li><strong>主题混杂</strong>：单个会话可能包含多个不相关主题</li><li><strong>噪声干扰</strong>：大量无关信息影响检索和生成质量</li><li><strong>检索粗糙</strong>：无法精确定位到具体相关内容</li></ul><p><strong>具体示例</strong>：<br>一个会话中用户讨论了机器学习、烹饪食谱、旅行计划和电影推荐。当查询机器学习相关问题时，检索到的会话包含大量无关的烹饪和旅行信息。</p><h4 id="1-3-3-摘要化方法的信息损失"><a href="#1-3-3-摘要化方法的信息损失" class="headerlink" title="1.3.3 摘要化方法的信息损失"></a>1.3.3 摘要化方法的信息损失</h4><p>摘要化方法通过压缩对话内容来减少信息量：</p><p><strong>主要问题</strong>：</p><ul><li><strong>细节丢失</strong>：摘要过程中重要细节可能被省略</li><li><strong>主观性</strong>：摘要质量依赖于模型的理解能力</li><li><strong>不可逆性</strong>：一旦信息被摘要，原始细节无法恢复</li></ul><h2 id="2-SeCom的设计"><a href="#2-SeCom的设计" class="headerlink" title="2. SeCom的设计"></a>2. SeCom的设计</h2><h3 id="2-1-核心发现"><a href="#2-1-核心发现" class="headerlink" title="2.1 核心发现"></a>2.1 核心发现</h3><p>SeCom（<strong>Se</strong>gmentation + <strong>Com</strong>pression）的两个核心发现：</p><p><strong>洞察一：对话天然具有”段落”结构</strong><br>就像我们写文章会分段一样，人类的对话其实也有天然的主题边界。比如在一次长对话中，我们可能先讨论工作项目，然后转到周末计划，再聊到最近看的电影。每个主题就是一个天然的”段落”。</p><p>传统方法要么把每句话当作独立单元（太碎片化），要么把整个对话当作一个整体（太粗糙），而SeCom找到了中间的最佳平衡点——<strong>段落级的语义单元</strong>。</p><p><strong>洞察二：自然语言充满”废话”</strong><br>这听起来有点刻薄，但确实如此。我们日常对话中充满了”嗯”、”那个”、”你知道的”这样的冗余表达，还有大量的重复、确认、客套话。这些在人际交流中很重要，但对机器检索来说就是噪声。</p><p>SeCom通过智能压缩，保留关键信息的同时去除这些”噪声”，让检索更加精准。</p><h3 id="2-2-系统设计"><a href="#2-2-系统设计" class="headerlink" title="2.2 系统设计"></a>2.2 系统设计</h3><p>SeCom的整体架构设计非常优雅，就像一条高效的流水线：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">历史对话 → [分段器] → 段落级内存单元 → [压缩器] → 去噪内存单元 → [检索器] → 相关上下文 → [生成器] → 最终回复</span><br></pre></td></tr></tbody></table></figure><p>用更直观的话来解释这个流程：</p><ol><li><strong>分段器</strong>：将杂乱的对话历史按主题”切块”</li><li><strong>压缩器</strong>：将每个”块”中的废话去掉，保留精华</li><li><strong>检索器</strong>：根据当前问题找到最相关的”块”</li><li><strong>生成器</strong>：基于相关信息生成回答</li></ol><p><strong>技术表示</strong>（没什么用，写给喜欢数学的朋友）：<br>设 $f_{\mathcal{I}}$ 为分段器，$f_{Comp}$ 为压缩器，$f_R$ 为检索器，$f_{LLM}$ 为生成器</p><p>完整流程：</p><ol><li>${\mathbf{s}<em>k}</em>{k=1}^K \leftarrow f_{\mathcal{I}}(\mathcal{H})$ （对话分段）</li><li>${\mathbf{m}<em>k}</em>{k=1}^K \leftarrow f_{Comp}({\mathbf{s}<em>k}</em>{k=1}^K)$ （压缩去噪）</li><li>${\mathbf{m}<em>n}</em>{n=1}^N \leftarrow f_R(u^*, {\mathbf{m}<em>k}</em>{k=1}^K, N)$ （内存检索）</li><li>$r^* = f_{LLM}(u^*, {\mathbf{m}<em>n}</em>{n=1}^N)$ （响应生成）</li></ol><h3 id="2-3-分段算法：教AI学会”断句”"><a href="#2-3-分段算法：教AI学会”断句”" class="headerlink" title="2.3 分段算法：教AI学会”断句”"></a>2.3 分段算法：教AI学会”断句”</h3><h4 id="2-3-1-零样本分段"><a href="#2-3-1-零样本分段" class="headerlink" title="2.3.1 零样本分段"></a>2.3.1 零样本分段</h4><p>如何让AI自动识别对话中的主题边界？传统方法需要大量标注数据训练专门的分段模型，而SeCom采用了一个非常聪明的”零样本”方法。</p><p><strong>核心思路</strong>：<br>既然GPT-4这样的大模型已经具备了强大的文本理解能力，为什么不直接让它来判断对话的主题边界呢？就像让一个文学老师来给文章分段一样。</p><p><strong>输入预处理</strong>：<br>将对话会话增强为结构化格式：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Turn j: </span><br><span class="line">[user]: u_j</span><br><span class="line">[agent]: r_j</span><br></pre></td></tr></tbody></table></figure><p><strong>分段提示设计</strong>：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">分析以下对话，识别主题边界，将对话分割为语义连贯的段落。</span><br><span class="line">每个段落应该：</span><br><span class="line">1. 围绕单一主题或相关主题</span><br><span class="line">2. 包含完整的交互序列</span><br><span class="line">3. 具有明确的开始和结束边界</span><br><span class="line"></span><br><span class="line">对话内容：</span><br><span class="line">[对话内容]</span><br><span class="line"></span><br><span class="line">请输出每个段落的起始和结束轮次编号。</span><br></pre></td></tr></tbody></table></figure><p><strong>优势</strong>：</p><ul><li>无需训练数据，适用于开放域对话</li><li>利用LLM的强大理解能力</li><li>可处理复杂的主题转换模式</li></ul><h4 id="2-3-2-基于反思的分段优化"><a href="#2-3-2-基于反思的分段优化" class="headerlink" title="2.3.2 基于反思的分段优化"></a>2.3.2 基于反思的分段优化</h4><p>当有少量标注数据时，采用反思机制优化分段效果：</p><p><strong>算法步骤</strong>：</p><ol><li><strong>初始分段</strong>：使用零样本方法对批量数据进行分段</li><li><strong>错误识别</strong>：基于WindowDiff指标选择top-K个分段错误最大的样本</li><li><strong>反思学习</strong>：让LLM分析分段错误，更新分段指导原则</li><li><strong>迭代优化</strong>：重复上述过程直到收敛</li></ol><p><strong>反思提示设计</strong>：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">分析以下分段错误，并更新分段指导原则：</span><br><span class="line"></span><br><span class="line">错误案例：</span><br><span class="line">[分段结果] vs [标准答案]</span><br><span class="line"></span><br><span class="line">请分析错误原因并提供改进的分段指导原则。</span><br></pre></td></tr></tbody></table></figure><p><strong>数学表示</strong>：<br>设 $\boldsymbol{G}<em>m$ 为第m轮的分段指导原则，更新公式为：<br>$$\boldsymbol{G}</em>{m+1} = \boldsymbol{G}_m - \eta \nabla \mathcal{L}(\boldsymbol{G}_m)$$</p><p>其中 $\nabla \mathcal{L}(\boldsymbol{G}_m)$ 为LLM隐式估计的分段损失梯度。</p><h4 id="2-3-3-增量分段算法"><a href="#2-3-3-增量分段算法" class="headerlink" title="2.3.3 增量分段算法"></a>2.3.3 增量分段算法</h4><p>对于新增的对话轮次，设计增量分段算法：</p><p><strong>算法流程</strong>：</p><ol><li>输入新轮次 $\mathbf{t}<em>{new}$ 和前一段落 $\mathbf{s}</em>{prev}$</li><li>判断是否应该合并：$binary = f_{judge}(\mathbf{t}<em>{new}, \mathbf{s}</em>{prev})$</li><li>如果合并：$\mathbf{s}<em>{prev} \leftarrow \mathbf{s}</em>{prev} \cup {\mathbf{t}_{new}}$</li><li>否则：创建新段落 $\mathbf{s}<em>{new} = {\mathbf{t}</em>{new}}$</li></ol><p><strong>判断提示</strong>：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">判断新的用户-机器人轮次是否应该与前一段落合并：</span><br><span class="line"></span><br><span class="line">新轮次：[新轮次内容]</span><br><span class="line">前一段落：[前一段落内容]</span><br><span class="line"></span><br><span class="line">如果属于同一主题，回答"Yes"，否则回答"No"。</span><br></pre></td></tr></tbody></table></figure><h3 id="2-4-压缩式内存去噪"><a href="#2-4-压缩式内存去噪" class="headerlink" title="2.4 压缩式内存去噪"></a>2.4 压缩式内存去噪</h3><h4 id="2-4-1-自然语言冗余性分析"><a href="#2-4-1-自然语言冗余性分析" class="headerlink" title="2.4.1 自然语言冗余性分析"></a>2.4.1 自然语言冗余性分析</h4><p><strong>理论基础</strong>：<br>根据Shannon信息论，自然语言具有高度冗余性，冗余率约为50-75%。这种冗余在人类交流中有助于错误纠正和理解，但在机器检索中构成噪声。</p><p><strong>冗余类型</strong>：</p><ol><li><strong>词汇冗余</strong>：同义词、重复表达</li><li><strong>语法冗余</strong>：冗余的语法结构</li><li><strong>语义冗余</strong>：重复的语义信息</li><li><strong>对话冗余</strong>：客套话、确认性回复</li></ol><h4 id="2-4-2-LLMLingua-2压缩原理"><a href="#2-4-2-LLMLingua-2压缩原理" class="headerlink" title="2.4.2 LLMLingua-2压缩原理"></a>2.4.2 LLMLingua-2压缩原理</h4><p><strong>算法核心</strong>：<br>LLMLingua-2基于token重要性评分进行压缩：</p><ol><li><p><strong>重要性评分</strong>：<br>$$s_i = f_{score}(x_i | x_{&lt;i}, x_{&gt;i})$$<br>其中 $x_i$ 为第i个token，$x_{&lt;i}$ 和 $x_{&gt;i}$ 为上下文</p></li><li><p><strong>动态压缩</strong>：<br>根据目标压缩率 $r$，保留top $(1-r) \times N$ 个重要token</p></li><li><p><strong>语义保持</strong>：<br>通过双向上下文建模确保关键语义信息不丢失</p></li></ol><p><strong>压缩效果分析</strong>：<br>实验表明，75%压缩率下：</p><ul><li>关键信息保留率 &gt; 95%</li><li>检索相关性提升 9.46分（GPT4Score）</li><li>计算效率提升 4倍</li></ul><h4 id="2-4-3-压缩对检索性能的影响"><a href="#2-4-3-压缩对检索性能的影响" class="headerlink" title="2.4.3 压缩对检索性能的影响"></a>2.4.3 压缩对检索性能的影响</h4><p><strong>相似性变化分析</strong>：<br>设 $sim(q, s)$ 为查询q与段落s的相似性</p><p>压缩前：$sim_{before}(q, s_{relevant})$，$sim_{before}(q, s_{irrelevant})$<br>压缩后：$sim_{after}(q, s’<em>{relevant})$，$sim</em>{after}(q, s’_{irrelevant})$</p><p>实验结果显示：</p><ul><li>$sim_{after}(q, s’<em>{relevant}) &gt; sim</em>{before}(q, s_{relevant})$ （相关段落相似性提升）</li><li>$sim_{after}(q, s’<em>{irrelevant}) &lt; sim</em>{before}(q, s_{irrelevant})$ （无关段落相似性降低）</li></ul><h3 id="2-5-多模态检索系统"><a href="#2-5-多模态检索系统" class="headerlink" title="2.5 多模态检索系统"></a>2.5 多模态检索系统</h3><h4 id="2-5-1-检索器选择与配置"><a href="#2-5-1-检索器选择与配置" class="headerlink" title="2.5.1 检索器选择与配置"></a>2.5.1 检索器选择与配置</h4><p><strong>BM25检索器</strong>：<br>$$BM25(q, d) = \sum_{i=1}^{|q|} IDF(q_i) \cdot \frac{tf(q_i, d) \cdot (k_1 + 1)}{tf(q_i, d) + k_1 \cdot (1 - b + b \cdot \frac{|d|}{avgdl})}$$</p><p>参数设置：$k_1 = 1.2$，$b = 0.75$</p><p><strong>MPNet检索器</strong>：<br>基于MPNet模型的密集检索：<br>$$score = \cos(\mathbf{e}_q, \mathbf{e}_d)$$<br>其中 $\mathbf{e}_q$ 和 $\mathbf{e}_d$ 分别为查询和文档的向量表示</p><h4 id="2-5-2-混合检索策略"><a href="#2-5-2-混合检索策略" class="headerlink" title="2.5.2 混合检索策略"></a>2.5.2 混合检索策略</h4><p>结合稀疏检索和密集检索的优势：<br>$$score_{hybrid} = \alpha \cdot score_{BM25} + (1-\alpha) \cdot score_{MPNet}$$</p><p>通过实验确定最优权重 $\alpha = 0.6$</p><h2 id="3-写在最后：一些思考"><a href="#3-写在最后：一些思考" class="headerlink" title="3. 写在最后：一些思考"></a>3. 写在最后：一些思考</h2><h3 id="3-1-SeCom给我们的启发"><a href="#3-1-SeCom给我们的启发" class="headerlink" title="3.1 SeCom给我们的启发"></a>3.1 SeCom给我们的启发</h3><p>研读这篇论文后，笔者有几点深刻的感悟：</p><p><strong>简单往往是最有效的</strong>：SeCom的核心思想其实很简单——分段+压缩，但正是这种简单的组合解决了复杂的问题。这提醒我们，在面对技术挑战时，有时候最朴素的想法反而是最有效的。</p><p><strong>理解问题比解决问题更重要</strong>：作者团队深入分析了内存粒度问题的本质，发现了段落级内存的最优性。这种对问题本质的深刻理解是技术创新的基础。</p><p>笔者认为未来可能的发展方向包括：</p><ul><li><strong>个性化分段策略</strong>：不同用户的对话模式不同，能否学习个性化的分段方式？</li><li><strong>实时优化机制</strong>：能否根据对话质量动态调整压缩率和分段策略？</li></ul><hr><h2 id="参考资源"><a href="#参考资源" class="headerlink" title="参考资源"></a>参考资源</h2><ul><li><strong>论文链接</strong>：<a href="https://www.arxiv.org/abs/2502.05589">SeCom: On Memory Construction and Retrieval for Personalized Conversational Agents (ICLR 2025)</a></li><li><strong>项目主页</strong>：<a href="https://llmlingua.com/secom.html">https://llmlingua.com/secom.html</a></li><li><strong>代码仓库</strong>：SeCom-main项目</li><li><strong>数据集</strong>：LOCOMO、Long-MT-Bench+、DialSeg711、TIAGE、SuperDialSeg</li></ul><p><em>本文基于Microsoft和清华大学联合研究团队在ICLR 2025发表的论文撰写，详细技术实现请参考原始论文和开源代码。</em> </p>]]></content>
      
      
      <categories>
          
          <category> NLP Insights </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Conversational AI </tag>
            
            <tag> Memory Management </tag>
            
            <tag> SeCom </tag>
            
            <tag> RAG </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Decoder-only与Encoder-only模型Padding策略的差异</title>
      <link href="/2025/03/06/NLP%20Insights/Decoder%E6%A8%A1%E5%9E%8B%E5%92%8CEncoder%E6%A8%A1%E5%9E%8B%E5%9C%A8Padding%E4%B8%AD%E7%9A%84%E4%B8%8D%E5%90%8C/"/>
      <url>/2025/03/06/NLP%20Insights/Decoder%E6%A8%A1%E5%9E%8B%E5%92%8CEncoder%E6%A8%A1%E5%9E%8B%E5%9C%A8Padding%E4%B8%AD%E7%9A%84%E4%B8%8D%E5%90%8C/</url>
      
        <content type="html"><![CDATA[<h2 id="📌-Padding-的含义"><a href="#📌-Padding-的含义" class="headerlink" title="📌 Padding 的含义"></a>📌 <strong>Padding 的含义</strong></h2><p>在大模型 (<strong>LLM</strong>) 中，<strong>padding</strong> 是用于将不同长度的序列调整为同一长度的方法，以便于批量 (<strong>batch</strong>) 处理。</p><p>例如：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">句子1: "I love NLP"</span><br><span class="line">句子2: "Padding is useful in LLM training"</span><br></pre></td></tr></tbody></table></figure><p>使用 <code>&lt;pad&gt;</code> token 进行对齐：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">"I love NLP &lt;pad&gt; &lt;pad&gt; &lt;pad&gt;"</span><br><span class="line">"Padding is useful in LLM training"</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="📌-Padding-位置的选择：Left-vs-Right"><a href="#📌-Padding-位置的选择：Left-vs-Right" class="headerlink" title="📌 Padding 位置的选择：Left vs Right"></a>📌 <strong>Padding 位置的选择：Left vs Right</strong></h2><p>Padding 有两种常见方式：</p><ul><li><p><strong>Right padding</strong>（右填充）：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">"I love NLP &lt;pad&gt; &lt;pad&gt;"</span><br></pre></td></tr></tbody></table></figure></li><li><p><strong>Left padding</strong>（左填充）：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">"&lt;pad&gt; &lt;pad&gt; I love NLP"</span><br></pre></td></tr></tbody></table></figure></li></ul><p>通常：</p><ul><li><strong>Decoder-only 模型</strong>（如 GPT, Llama）：采用 <strong>Left padding</strong></li><li><strong>Encoder-only 模型</strong>（如 BERT）：采用 <strong>Right padding</strong></li></ul><p>具体而言，Transformer 模型通常分为三类结构：</p><table><thead><tr><th>模型类型</th><th>代表模型</th><th>特征</th><th>常见用途</th></tr></thead><tbody><tr><td><strong>Encoder-only</strong></td><td><strong>BERT</strong>、RoBERTa、ALBERT、ELECTRA</td><td>双向注意力（Bidirectional Attention）</td><td>自然语言理解（NLU），如文本分类、序列标注</td></tr><tr><td><strong>Decoder-only</strong></td><td>GPT、GPT-2、GPT-3、GPT-4、LLaMA、Mistral</td><td>单向自回归注意力（Causal Attention）</td><td>文本生成、聊天、写作</td></tr><tr><td><strong>Encoder-Decoder</strong></td><td>Transformer原始论文中的模型、T5、BART、mT5、PEGASUS</td><td>Encoder为双向注意力，Decoder为单向自回归注意力</td><td>机器翻译、摘要生成、对话</td></tr></tbody></table><hr><h2 id="📌-为什么-Encoder-only-模型（如BERT）采用-Right-padding？"><a href="#📌-为什么-Encoder-only-模型（如BERT）采用-Right-padding？" class="headerlink" title="📌 为什么 Encoder-only 模型（如BERT）采用 Right padding？"></a>📌 为什么 Encoder-only 模型（如BERT）采用 Right padding？</h2><ul><li><strong>Encoder-only 模型</strong>（如 BERT）的核心目标是获得<strong>每个 token 的嵌入表示</strong>（Embedding representation）。</li><li>此类模型为<strong>双向注意力（Bidirectional Attention）</strong>，每个 token 可同时关注上下文，因此<strong>位置的轻微变化不会对结果造成严重干扰</strong>。</li><li>此外，encoder-only 模型中通常有特殊 token（如 <code>[CLS]</code>），位置相对稳定，用于句子分类或表示，因此采用 <strong>right padding</strong> 更自然，也更合理。</li></ul><p>示例说明：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[CLS] Hello I love NLP [SEP] &lt;pad&gt; &lt;pad&gt;</span><br></pre></td></tr></tbody></table></figure><ul><li>右填充后，<code>[CLS]</code> 和 <code>[SEP]</code> token 位置稳定，且便于模型专注于前面的有效信息。</li></ul><hr><h2 id="📌-为什么-Decoder-only-LLM-采用-Left-padding？"><a href="#📌-为什么-Decoder-only-LLM-采用-Left-padding？" class="headerlink" title="📌 为什么 Decoder-only LLM 采用 Left padding？"></a>📌 为什么 Decoder-only LLM 采用 Left padding？</h2><p>以 GPT 为代表的 <strong>Decoder-only 模型</strong> 是自回归（<strong>Autoregressive</strong>）模型，每个词的生成仅依赖于当前及之前的词，未来词不可见。因此：</p><ul><li><strong>位置编码的稳定性</strong>：<br>左填充确保真实 token 的相对位置稳定，模型生成新 token 时位置编码始终稳定于序列末尾。<ul><li>当采用<strong>绝对位置编码</strong>（Absolute Positional Encoding）时，每个 token（包括 <code>&lt;pad&gt;</code>）都有对应的位置编号。</li><li>对于左填充的 padding tokens，虽然它们占据了位置编号（如 1、2），但模型通过<strong>掩码机制</strong>忽略其对注意力和输出结果的影响。<br>示例：</li></ul></li></ul><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">位置编码: [ 1      2      3      4      5      6 ]</span><br><span class="line">Token:   [ &lt;pad&gt;, &lt;pad&gt;, Hello,  I,   love,  NLP ]</span><br><span class="line">掩码:     [  0,     0,     1,     1,     1,    1 ]</span><br></pre></td></tr></tbody></table></figure><ul><li>模型只关注掩码为 1 的有效 token，而忽略掩码为 0 的 padding tokens。</li><li><strong>注意力掩码（Attention Mask）</strong>：<br>左侧的 <code>&lt;pad&gt;</code> 会被<strong>注意力掩码（attention mask）忽略</strong>，从而避免 padding token 干扰有效 token 的位置编码和注意力计算。</li></ul><p>示例说明：</p><table><thead><tr><th>Token</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th></tr></thead><tbody><tr><td>Left</td><td><code>&lt;pad&gt;</code></td><td><code>&lt;pad&gt;</code></td><td>Hello</td><td>I</td><td>love</td><td>NLP</td></tr><tr><td>Right</td><td>Hello</td><td>I</td><td>love</td><td>NLP</td><td><code>&lt;pad&gt;</code></td><td><code>&lt;pad&gt;</code></td></tr></tbody></table><ul><li><strong>Left padding</strong> 下，最后有效 token 始终在同一位置（6）。</li><li><strong>Right padding</strong> 下，token 的位置随序列长度变化，影响位置编码的稳定性。</li></ul><hr><h2 id="📌-Padding-在训练与推理阶段的差异"><a href="#📌-Padding-在训练与推理阶段的差异" class="headerlink" title="📌 Padding 在训练与推理阶段的差异"></a>📌 <strong>Padding 在训练与推理阶段的差异</strong></h2><table><thead><tr><th>阶段 (Phase)</th><th>Padding 策略</th><th>原因</th></tr></thead><tbody><tr><td><strong>训练 (Training)</strong></td><td>批量处理时，Decoder-only 常用左填充；Encoder-only 模型则常用右填充</td><td>批量处理，加快计算效率</td></tr><tr><td><strong>推理 (Inference)</strong></td><td>通常单条序列，无需 padding；若需要批量推理，仍采用左填充</td><td>稳定位置编码</td></tr></tbody></table><hr><h2 id="📌-总结与关键要点（TL-DR）"><a href="#📌-总结与关键要点（TL-DR）" class="headerlink" title="📌 总结与关键要点（TL;DR）"></a>📌 <strong>总结与关键要点（TL;DR）</strong></h2><ul><li><strong>Padding</strong> 用于序列长度标准化。</li><li><strong>Decoder-only LLMs (GPT, Llama)</strong> 通常采用<strong>左填充（Left padding）</strong>，目的是<strong>稳定位置编码并避免未来信息泄漏</strong>；左侧 padding 会被掩码忽略，不干扰模型预测。</li><li><strong>Encoder-only 模型（如BERT系列）</strong>通常采用<strong>右填充（Right padding）</strong>，因为模型为双向注意力，且特殊token（如<code>[CLS]</code>）位置需要保持稳定。</li><li>位置编码中虽然 padding token 占位，但会被<strong>注意力掩码</strong>有效屏蔽，不影响模型的最终输出。</li></ul><hr>]]></content>
      
      
      <categories>
          
          <category> NLP Insights </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Differences in Padding Strategies Between Decoder-only and Encoder-only Models</title>
      <link href="/2025/03/06/NLP%20Insights/Differences%20in%20Padding%20Strategies%20Between%20Decoder-only%20and%20Encoder-only%20Models/"/>
      <url>/2025/03/06/NLP%20Insights/Differences%20in%20Padding%20Strategies%20Between%20Decoder-only%20and%20Encoder-only%20Models/</url>
      
        <content type="html"><![CDATA[<h2 id="📌-What-is-Padding"><a href="#📌-What-is-Padding" class="headerlink" title="📌 What is Padding?"></a>📌 <strong>What is Padding?</strong></h2><p>In <strong>Large Language Models (LLMs)</strong>, <strong>padding</strong> is a method used to standardize sequence lengths for <strong>batch processing</strong>.</p><p>For example:</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Sentence 1: "I love NLP"</span><br><span class="line">Sentence 2: "Padding is useful in LLM training"</span><br></pre></td></tr></tbody></table></figure><p>Using the <code>&lt;pad&gt;</code> token for alignment:</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">"I love NLP &lt;pad&gt; &lt;pad&gt; &lt;pad&gt;"</span><br><span class="line">"Padding is useful in LLM training"</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="📌-Padding-Positioning-Left-vs-Right"><a href="#📌-Padding-Positioning-Left-vs-Right" class="headerlink" title="📌 Padding Positioning: Left vs Right"></a>📌 <strong>Padding Positioning: Left vs Right</strong></h2><p>There are two common padding strategies:</p><ul><li><p><strong>Right padding</strong>:</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">"I love NLP &lt;pad&gt; &lt;pad&gt;"</span><br></pre></td></tr></tbody></table></figure></li><li><p><strong>Left padding</strong>:</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">"&lt;pad&gt; &lt;pad&gt; I love NLP"</span><br></pre></td></tr></tbody></table></figure></li></ul><p>Typically:</p><ul><li><strong>Decoder-only models</strong> (e.g., GPT, Llama): Use <strong>Left padding</strong>.</li><li><strong>Encoder-only models</strong> (e.g., BERT): Use <strong>Right padding</strong>.</li></ul><p>Transformers can be categorized into three main architectures:</p><table><thead><tr><th>Model Type</th><th>Representative Models</th><th>Characteristics</th><th>Common Applications</th></tr></thead><tbody><tr><td><strong>Encoder-only</strong></td><td><strong>BERT</strong>, RoBERTa, ALBERT, ELECTRA</td><td><strong>Bidirectional attention</strong></td><td>NLP tasks like text classification, named entity recognition</td></tr><tr><td><strong>Decoder-only</strong></td><td>GPT, GPT-2, GPT-3, GPT-4, LLaMA, Mistral</td><td><strong>Causal attention (Autoregressive)</strong></td><td>Text generation, chatbots, writing assistance</td></tr><tr><td><strong>Encoder-Decoder</strong></td><td>Transformer (original), T5, BART, mT5, PEGASUS</td><td><strong>Encoder: bidirectional, Decoder: autoregressive</strong></td><td>Machine translation, summarization, dialogue systems</td></tr></tbody></table><hr><h2 id="📌-Why-Do-Encoder-only-Models-e-g-BERT-Use-Right-Padding"><a href="#📌-Why-Do-Encoder-only-Models-e-g-BERT-Use-Right-Padding" class="headerlink" title="📌 Why Do Encoder-only Models (e.g., BERT) Use Right Padding?"></a>📌 <strong>Why Do Encoder-only Models (e.g., BERT) Use Right Padding?</strong></h2><ul><li><strong>Encoder-only models</strong> (like BERT) aim to obtain <strong>representations for each token</strong>.</li><li>These models use <strong>bidirectional attention</strong>, meaning each token attends to <strong>both past and future tokens</strong>.</li><li><strong>Slight shifts in position do not significantly impact model performance</strong>.</li><li>Special tokens (e.g., <code>[CLS]</code>) in BERT maintain a <strong>fixed position</strong> for tasks like classification, making <strong>right padding more natural</strong>.</li></ul><p>Example:</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[CLS] Hello I love NLP [SEP] &lt;pad&gt; &lt;pad&gt;</span><br></pre></td></tr></tbody></table></figure><ul><li>Right padding keeps <code>[CLS]</code> and <code>[SEP]</code> in stable positions, allowing the model to focus on meaningful tokens.</li></ul><hr><h2 id="📌-Why-Do-Decoder-only-LLMs-Use-Left-Padding"><a href="#📌-Why-Do-Decoder-only-LLMs-Use-Left-Padding" class="headerlink" title="📌 Why Do Decoder-only LLMs Use Left Padding?"></a>📌 <strong>Why Do Decoder-only LLMs Use Left Padding?</strong></h2><p><strong>Decoder-only models</strong> (like GPT) are <strong>autoregressive</strong>, meaning each token is generated based only on <strong>previous tokens</strong>, and future tokens are <strong>masked</strong>.</p><ul><li><strong>Positional Encoding Stability</strong>:<br>Left padding ensures that meaningful tokens have a <strong>consistent relative position</strong>, preventing <strong>position encoding misalignment</strong>.<ul><li>When using <strong>absolute positional encoding</strong>, every token (including <code>&lt;pad&gt;</code>) gets a unique position index.</li><li>Padding tokens at the beginning <strong>do not affect the model’s attention mechanism</strong> due to <strong>masking</strong>.</li></ul></li></ul><p>Example:</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Position Index: [ 1      2      3      4      5      6 ]</span><br><span class="line">Token:         [ &lt;pad&gt;, &lt;pad&gt;, Hello,  I,   love,  NLP ]</span><br><span class="line">Mask:          [  0,     0,     1,     1,     1,    1 ]</span><br></pre></td></tr></tbody></table></figure><ul><li><p>The model <strong>only attends to tokens where the mask is 1</strong>, ignoring padding tokens.</p></li><li><p><strong>Attention Masking</strong>:<br>Left padding ensures that <code>&lt;pad&gt;</code> tokens <strong>do not interfere with token position encoding</strong>.</p></li></ul><p>Illustration:</p><table><thead><tr><th>Token</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th></tr></thead><tbody><tr><td>Left</td><td><code>&lt;pad&gt;</code></td><td><code>&lt;pad&gt;</code></td><td>Hello</td><td>I</td><td>love</td><td>NLP</td></tr><tr><td>Right</td><td>Hello</td><td>I</td><td>love</td><td>NLP</td><td><code>&lt;pad&gt;</code></td><td><code>&lt;pad&gt;</code></td></tr></tbody></table><ul><li><strong>With Left padding</strong>, the last valid token <strong>always remains in the same position</strong>.</li><li><strong>With Right padding</strong>, token positions shift, affecting positional encoding stability.</li></ul><hr><h2 id="📌-Padding-Differences-in-Training-vs-Inference"><a href="#📌-Padding-Differences-in-Training-vs-Inference" class="headerlink" title="📌 Padding Differences in Training vs Inference"></a>📌 <strong>Padding Differences in Training vs Inference</strong></h2><table><thead><tr><th>Phase</th><th>Padding Strategy</th><th>Reason</th></tr></thead><tbody><tr><td><strong>Training</strong></td><td>Left padding for decoder-only; Right padding for encoder-only</td><td>Optimized for batch processing efficiency</td></tr><tr><td><strong>Inference</strong></td><td>Typically, no padding for single sequences; Left padding for batched inference</td><td>Ensures stable positional encoding</td></tr></tbody></table><hr><h2 id="📌-Summary-Key-Takeaways-TL-DR"><a href="#📌-Summary-Key-Takeaways-TL-DR" class="headerlink" title="📌 Summary &amp; Key Takeaways (TL;DR)"></a>📌 <strong>Summary &amp; Key Takeaways (TL;DR)</strong></h2><ul><li><strong>Padding</strong> standardizes sequence lengths for batch processing.</li><li><strong>Decoder-only models (GPT, Llama)</strong> use <strong>Left padding</strong> to <strong>stabilize positional encoding and prevent future token leakage</strong>. Left padding tokens are masked out.</li><li><strong>Encoder-only models (BERT, RoBERTa)</strong> use <strong>Right padding</strong> since they employ <strong>bidirectional attention</strong> and rely on stable special token positions (e.g., <code>[CLS]</code>).</li><li>Although padding tokens occupy positions in <strong>positional encoding</strong>, <strong>attention masks</strong> effectively filter them out, ensuring they do not affect model predictions.</li></ul><hr>]]></content>
      
      
      <categories>
          
          <category> NLP Insights </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MoE模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色</title>
      <link href="/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2/"/>
      <url>/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2/</url>
      
        <content type="html"><![CDATA[<h1 id="MoE-模型的的可视化指南：揭秘-MoE-在大型语言模型中的角色"><a href="#MoE-模型的的可视化指南：揭秘-MoE-在大型语言模型中的角色" class="headerlink" title="MoE 模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色"></a>MoE 模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色</h1><p><strong>原文地址</strong>：<a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts">A Visual Guide to Mixture of Experts (MoE)</a></p><p>📅 作者：Maarten Grootendorst</p><p>📆 日期：2024 年 10 月 7 日</p><hr><h1 id="探索语言模型：混合专家模型（MoE）可视化指南"><a href="#探索语言模型：混合专家模型（MoE）可视化指南" class="headerlink" title="探索语言模型：混合专家模型（MoE）可视化指南"></a>探索语言模型：混合专家模型（MoE）可视化指南</h1><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li><a href="#moe-%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%E6%8F%AD%E7%A7%98-moe-%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2">MoE 模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色</a></li><li><a href="#%E6%8E%A2%E7%B4%A2%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%B7%B7%E5%90%88%E4%B8%93%E5%AE%B6%E6%A8%A1%E5%9E%8Bmoe%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97">探索语言模型：混合专家模型（MoE）可视化指南</a><ul><li><a href="#%E7%9B%AE%E5%BD%95">目录</a></li><li><a href="#%E4%BB%80%E4%B9%88%E6%98%AF%E6%B7%B7%E5%90%88%E4%B8%93%E5%AE%B6moe%E6%A8%A1%E5%9E%8B">什么是混合专家（MoE）模型？</a></li><li><a href="#experts">Experts</a><ul><li><a href="#dense-layers">Dense Layers</a></li><li><a href="#sparse-layers">Sparse Layers</a></li><li><a href="#what-does-an-expert-learn">What does an Expert Learn?</a></li><li><a href="#%E4%B8%93%E5%AE%B6%E7%9A%84%E6%9E%B6%E6%9E%84architecture-of-experts">专家的架构（Architecture of Experts）</a></li></ul></li></ul></li></ul><p>当我们查看最新发布的大型语言模型（<strong>LLMs</strong>，Large Language Models）时，常常会在标题中看到 “<strong>MoE</strong>”。这个 “MoE” 代表什么？为什么这么多 LLM 都在使用它？</p><p>在这份可视化指南中，我们会通过 50 多个可视化图示，逐步探索这一关键组件：**Mixture of Experts (MoE)**。</p><img src="/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2/20250224_145859.png" class=""><p><strong>图示内容</strong>：在这张图中，可以看到一个典型 <strong>MoE</strong> 结构的两个主要组成部分：<strong>Experts</strong>（专家）和 <strong>Router</strong>（路由器或门控网络）。图中显示了一个 <strong>Router</strong>，以及下方并列的多个 <strong>Experts</strong>，表明在 <strong>LLM</strong> 架构中，MoE 会将输入根据需要路由到合适的专家。<br><strong>图 1 详细说明</strong>：</p><ol><li><strong>Router</strong>：决定将输入（例如 token）发送给哪一个或哪几个专家。</li><li><strong>Experts</strong>：若干个不同的子模型（通常是 <strong>FFNN</strong> 结构），每个专家可能在不同方面具有专长。</li><li><strong>工作流程</strong>：输入先通过 <strong>Router</strong>，再被分配到不同的专家进行处理，最后汇总结果。</li></ol><h2 id="什么是混合专家（MoE）模型？"><a href="#什么是混合专家（MoE）模型？" class="headerlink" title="什么是混合专家（MoE）模型？"></a>什么是混合专家（MoE）模型？</h2><p><strong>Mixture of Experts (MoE)</strong> 是一种技术，它使用许多不同的子模型（或“<strong>experts</strong>”）来提升大型语言模型的质量。</p><p>在 MoE 中，有两个主要组件：</p><ol><li><strong>Experts</strong><ul><li>每个 <strong>FFNN</strong> 层都不再是一个单独的网络，而是有一组“专家”可供选择。</li><li>这些“专家”通常也是 <strong>FFNN</strong>（Feedforward Neural Network）结构。</li></ul></li><li><strong>Router</strong> 或 <strong>gate network</strong><ul><li>负责决定哪些 <strong>tokens</strong> 被发送到哪些专家。</li></ul></li></ol><p>在一个带有 MoE 的 <strong>LLM</strong> 的每一层，我们都能看到（在某种程度上）有所专门化的专家：</p><img src="/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2/20250224_150409.png" class=""><p><strong>图示内容</strong>：展示了在 <strong>LLM</strong> 的每一层都可以拥有多个 <strong>Experts</strong>。它强调了这些专家在不同的上下文中能够处理不同的输入 token。<br><strong>图2详细说明</strong>：  </p><ol><li><strong>层结构</strong>：图中用不同的层级（Layer 1、Layer 2、Layer 3……）表示多层模型。  </li><li><strong>Experts</strong>：在每一层，都有若干个专家（Expert 1、Expert 2、Expert 3、Expert 4），这些专家并行存在。  </li><li><strong>目标</strong>：强调专家在特定上下文或特定输入时更具备“专业性”，从而被选中来处理该输入。</li></ol><p>尽管 MoE 并不会在特定领域（如心理学或生物学）上专门训练专家，但它们仍可能在词法或句法级别上形成一定的偏向：</p><img src="/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2/20250224_153715.png" class=""><ul><li><strong>MoE 专家可能学习到不同的语言特征</strong><ul><li><strong>Expert 1</strong> 处理<strong>标点符号</strong>（Punctuation）：如 <code>, . : &amp; - ?</code> 等。</li><li><strong>Expert 2</strong> 处理<strong>动词</strong>（Verbs）：如 <code>said, read, miss</code> 等。</li><li><strong>Expert 3</strong> 处理<strong>连接词</strong>（Conjunctions）：如 <code>the, and, if, not</code> 等。</li><li><strong>Expert 4</strong> 处理<strong>视觉描述词</strong>（Visual Descriptions）：如 <code>dark, outer, yellow</code> 等。</li></ul></li></ul><p>更具体地说，他们的专长是在特定上下文中处理特定的标记（tokens）。</p><hr><p><strong>Router (gate network)</strong> 选择最适合给定输入的专家或专家组合：</p><img src="/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2/20250224_153924.png" class=""><p><strong>图示内容</strong>：展示了 <strong>Router</strong> 如何在每一层根据输入选择合适的专家。图中高亮了被选中的专家，以及输入 token 的流动过程。<br><strong>图3详细说明</strong>：  </p><ol><li><strong>输入</strong>：图顶部的 Input 代表模型接收到的 token 或向量表示。  </li><li><strong>Router</strong>：位于网络结构中，起到决策作用。  </li><li><strong>专家选择</strong>：被选中的专家会接收输入，其余专家则不被激活。  </li><li><strong>输出</strong>：来自被激活专家的结果被汇总或继续流向下游层。</li></ol><p>需要注意的是，每个专家并不是整个 LLM，而是 <strong>LLM</strong> 架构中的一个子模型部分。</p><hr><h2 id="Experts"><a href="#Experts" class="headerlink" title="Experts"></a>Experts</h2><p>为了理解专家（<strong>Experts</strong>）是什么以及它们如何工作，我们先来看看 MoE 希望替代的东西：<strong>dense layers</strong>。</p><h3 id="Dense-Layers"><a href="#Dense-Layers" class="headerlink" title="Dense Layers"></a>Dense Layers</h3><p>所有的 <strong>Mixture of Experts (MoE)</strong> 都基于 LLM 中一个相对基础的功能：**Feedforward Neural Network (FFNN)**。</p><p>回忆一下，一个标准的 <strong>decoder-only Transformer</strong> 架构中，<strong>FFNN</strong> 通常是在 <strong>layer normalization</strong> 之后应用的：</p><img src="/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2/20250224_154729.png" class=""><p><strong>图示内容</strong>：展示了一个典型的 <strong>decoder</strong> 结构，每个 <strong>decoder block</strong> 包含 <strong>Masked Self-Attention</strong> 和 <strong>FFNN</strong>（中间会有 <strong>Layer Norm</strong>）。  </p><ol><li><strong>Position Embedding</strong>：在输入 token 之前或同时加入位置编码信息。  </li><li><strong>Decoder Block</strong>：包含 <strong>Masked Self-Attention</strong>、<strong>Layer Norm</strong> 和 <strong>FFNN</strong>。  </li><li><strong>FFNN</strong>：在图中用紫色方块表示，是该层对输入进一步变换以捕捉更复杂关系的关键组件。</li></ol><p><strong>FFNN</strong> 可以利用注意力机制产生的上下文信息，对其进行进一步的转换，以捕捉数据中更复杂的关系。</p><p>不过，为了学习这些复杂关系，<strong>FFNN</strong> 的规模会随之增长，通常会在输入上进行扩张（例如，中间层维度会变大）：</p><img src="/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2/20250224_154923.png" class=""><p><strong>图示内容</strong>：展示了一个 <strong>FFNN</strong> 的结构，输入先被映射到更高维度，然后再被映射回输出维度。  </p><ol><li><strong>输入维度</strong>：图中显示有 512 个输入单元。  </li><li><strong>隐藏层</strong>：通常会有 4 倍或更多的扩张（图中示例为 4 倍扩张到 2048 维）。  </li><li><strong>输出维度</strong>：再映射回 512 维的输出。</li></ol><h3 id="Sparse-Layers"><a href="#Sparse-Layers" class="headerlink" title="Sparse Layers"></a>Sparse Layers</h3><p>在传统的 Transformer 中，<strong>FFNN</strong> 称为 <strong>dense model</strong>，因为它的所有参数（权重和偏置）都会被激活。也就是说，模型的全部参数都参与计算输出。</p><p>如果我们仔细观察 <strong>dense model</strong>，可以看到输入会激活所有的参数：</p><img src="/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2/20250224_163850.png" class=""><p><strong>图示内容</strong>：展示了一个“密集”模型，输入层的每个神经元都与隐藏层所有神经元相连，隐藏层所有神经元又与输出层神经元相连。<br><strong>图6详细说明</strong>：  </p><ol><li><strong>全连接</strong>：图中所有节点都连接到下一层的所有节点，表示无稀疏性。  </li><li><strong>所有参数被激活</strong>：没有任何“闲置”或“未激活”的参数。</li></ol><p>与之对比，<strong>sparse models</strong>（稀疏模型）只激活一部分总参数，这与 <strong>Mixture of Experts</strong> 密切相关。</p><p>为了说明这一点，我们可以把 <strong>dense model</strong> 切分成多个部分（即专家，<strong>experts</strong>），重新训练它，并且在推理（inference）时只激活其中一部分：</p><img src="/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2/20250224_164042.png" class=""><p><strong>图示内容</strong>：将原本的密集模型分割成多个专家（Expert 1、Expert 2、Expert 3、Expert 4）。在推理阶段，只选择一部分专家进行激活。  </p><ol><li><strong>模型切分</strong>：原有的大网络被拆分成多个较小的“专家”。  </li><li><strong>稀疏激活</strong>：并不是所有专家都被激活，只有部分专家在某些输入下被激活。  </li><li><strong>好处</strong>：通过稀疏激活，可以在不显著增加计算成本的情况下，拥有更多的潜在参数容量。</li></ol><p>其核心思想是：在训练期间，每个专家学习不同的信息；在推理时，只用到与当前任务最相关的那些专家。</p><p>当我们提出一个问题时，就会选择最适合该任务的专家：</p><img src="/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2/20250224_170127.png" class=""><p><strong>图示内容</strong>：展示了一个示例：当输入是 “What is 1 + 1?” 这样的数字相关问题时，路由器只激活与数字相关的专家。  </p><ol><li><strong>输入</strong>：一个表示算术问题的句子或 token。  </li><li><strong>专家选择</strong>：只激活 “Numbers” 领域的专家。  </li><li><strong>输出</strong>：专家给出结果 “2”。</li></ol><h3 id="What-does-an-Expert-Learn"><a href="#What-does-an-Expert-Learn" class="headerlink" title="What does an Expert Learn?"></a>What does an Expert Learn?</h3><p>正如前面所提到的，专家（<strong>Experts</strong>）往往学习到比整个领域更细致的知识。有人会觉得称它们为“专家”可能会带来误解，但这是因为每个专家往往只专注于某些特定类型的输入特征或上下文。</p><img src="/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2/20250224_170229.png" class=""><p><strong>图示内容</strong>：展示了一个表格或对照，说明在某些情况下，不同的专家可能学习到不同的特征（比如标点符号、动词、数字等）。  </p><ol><li><strong>示例化专家</strong>：Punctuation、Conjunctions、Verbs、Numbers 等。  </li><li><strong>分层位置</strong>：不同专家可能出现在模型的不同层。  </li><li><strong>分配</strong>：某些 token 会路由到某些专家，以获得更有效的处理。</li></ol><p>在 <strong>decoder</strong> 模型中，专家之间可能没有那么明显的领域分工。然而，这并不意味着所有专家都完全相同。<br>在 <strong>Mixtral 8x7B</strong> 这篇论文中，有一个很好的示例：每个 token 会被标记为其首选专家，这些专家并不一定对应直观的语义领域，但在统计上表现出某些倾向。</p><img src="/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2/20250224_182219.png" class=""><p>这张可视化示例还展示了，experts（专家）更倾向于关注句法（syntax），而不是特定的领域（domain）。因此，虽然 decoder experts（解码器专家）似乎并没有明确的“专业领域（specialism）”，但它们似乎会在某些特定类型的 tokens（标记）上被持续地使用。</p><p>在[图1]中，展示了一段关于 MoELayer 的示例代码或可视化结果，色块区分了不同部分，强调了<strong>专家（experts）与路由器（router）</strong>之间的关系。通过色块可以看出：</p><ul><li>experts 列表（在代码中用 nn.ModuleList 表示）包含了多个子网络（即多个 FFNN，Feed-Forward Neural Network，前馈神经网络）。</li><li>gate（门控网络，也称 router）负责选择哪些专家会被激活。</li><li>整体上可以看到，这些专家通常关注到输入句子的句法层面，而非特定主题或领域。</li></ul><h3 id="专家的架构（Architecture-of-Experts）"><a href="#专家的架构（Architecture-of-Experts）" class="headerlink" title="专家的架构（Architecture of Experts）"></a>专家的架构（Architecture of Experts）</h3>]]></content>
      
      
      <categories>
          
          <category> NLP Insights </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>推理 LLM 的可视化指南：探索推理时计算技术与 DeepSeek-R1</title>
      <link href="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/"/>
      <url>/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/</url>
      
        <content type="html"><![CDATA[<h1 id="推理-LLM-的可视化指南：探索推理时计算技术与-DeepSeek-R1"><a href="#推理-LLM-的可视化指南：探索推理时计算技术与-DeepSeek-R1" class="headerlink" title="推理 LLM 的可视化指南：探索推理时计算技术与 DeepSeek-R1"></a>推理 LLM 的可视化指南：探索推理时计算技术与 DeepSeek-R1</h1><p><strong>原文地址</strong>：<a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-reasoning-llms">A Visual Guide to Reasoning LLMs</a></p><p>📅 作者：Maarten Grootendorst</p><p>📆 日期：2025 年 2 月 3 日</p><hr><h2 id="📌-引言"><a href="#📌-引言" class="headerlink" title="📌 引言"></a>📌 引言</h2><p>DeepSeek-R1、OpenAI o3-mini 和 Google Gemini 2.0 Flash Thinking 是如何通过“推理”框架将 <strong>LLM（大型语言模型, Large Language Models）</strong> 扩展到新高度的典型示例。</p><p>它们标志着从 <strong>扩展训练时计算（train-time compute）</strong> 到 <strong>扩展推理时计算（test-time compute）</strong> 的范式转变。</p><p>在本篇文章中，我们提供了 <strong>超过 40 张定制可视化图表</strong>，带你深入探索：</p><ul><li><strong>推理 LLM（Reasoning LLMs）</strong> 领域</li><li><strong>推理时计算（Test-Time Compute）</strong> 机制</li><li><strong>DeepSeek-R1</strong> 的核心思想</li></ul><p>我们将逐步介绍相关概念，帮助你建立对这一新范式的直觉理解。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/i24pmg2.png" class=""><hr><h2 id="📖-什么是推理-LLM？"><a href="#📖-什么是推理-LLM？" class="headerlink" title="📖 什么是推理 LLM？"></a>📖 什么是推理 LLM？</h2><p>与普通 <strong>LLM（Large Language Models，大型语言模型）</strong> 相比，<strong>推理 LLM</strong> 在回答问题之前，往往会将问题 <strong>分解为更小的步骤</strong>（通常称为 <strong>推理步骤（Reasoning Steps）</strong> 或 <strong>思考过程（Thought Process）</strong>）。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_143007.png" class=""><h3 id="🧠-“推理步骤”-或-“思考过程”-是什么？"><a href="#🧠-“推理步骤”-或-“思考过程”-是什么？" class="headerlink" title="🧠 “推理步骤” 或 “思考过程” 是什么？"></a>🧠 “推理步骤” 或 “思考过程” 是什么？</h3><p>尽管我们可以哲学化地探讨 LLM 是否真的能够像人类一样思考，但这些推理步骤实际上是将推理过程 分解为更小、更结构化的推断。<strong>推理 LLM 采用的是结构化推理方式</strong>，即：</p><ul><li><strong>普通 LLM</strong>：直接输出答案</li><li><strong>推理 LLM</strong>：通过系统性推理生成答案</li></ul><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_143054.png" class=""><p>换句话说，推理 LLM 不是<strong>学习“回答什么”</strong>，而是<strong>学习“如何回答”</strong>！</p><p>要理解推理 LLM 的构建原理，我们首先需要探讨 <strong>训练时计算（Train-Time Compute）</strong> 和 <strong>推理时计算（Test-Time Compute）</strong> 之间的差异。</p><hr><h2 id="🔍-什么是训练时计算（Train-time-Compute）？"><a href="#🔍-什么是训练时计算（Train-time-Compute）？" class="headerlink" title="🔍 什么是训练时计算（Train-time Compute）？"></a>🔍 什么是训练时计算（Train-time Compute）？</h2><p>直到 2024 年年中，为了在 <strong>预训练（Pretraining）</strong> 期间提高 LLM 的性能，研究人员通常会扩大以下规模：</p><ul><li><strong>模型参数数量（# of Parameters）</strong></li><li><strong>数据集规模（# of Tokens）</strong></li><li><strong>计算量（# of FLOPs, Floating Point Operations）</strong></li></ul><p>这些合称为 <strong>训练时计算（Train-time Compute）</strong>，即 <strong>“AI 的化石燃料”</strong>，指的是：</p><blockquote><p><strong>预训练预算越大，最终得到的模型就越好。</strong></p><p>训练时计算（Train-Time Compute）包括<strong>训练（training）</strong>所需的计算，以及<strong>微调（fine-tuning）</strong>所需的计算。长期以来，一直是提高 LLM 性能的主要关注点。</p></blockquote><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_143927.png" class=""><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_144121.png" class=""><h3 id="🔢-规模定律（Scaling-Laws）"><a href="#🔢-规模定律（Scaling-Laws）" class="headerlink" title="🔢 规模定律（Scaling Laws）"></a>🔢 规模定律（Scaling Laws）</h3><p>在 <strong>LLM（大型语言模型）</strong> 研究领域，<strong>模型规模（Scale）</strong> 与 <strong>模型性能（Performance）</strong> 之间的关系被称为 <strong>规模定律（Scaling Laws）</strong>。这些定律通常用于描述 <strong>计算资源、数据规模和模型参数</strong> 如何影响模型的整体表现。</p><p>这些关系通常以 <strong>对数-对数（log-log）</strong> 方式呈现，并且在图表上通常显示为一条 <strong>近似直线</strong>，以突出计算量的巨大增长。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_144846.png" class=""><p>这张图片展示了<strong>不同坐标尺度（线性 vs. 对数）对计算资源（Compute）和模型性能（Performance）之间关系的影响</strong>，强调了大模型增长的幂律关系（Power Law）。</p><ul><li><p><strong>左图（普通线性尺度 - Normal Scale）</strong></p><ul><li>横轴（X 轴）：计算资源（Compute），<strong>线性刻度</strong>。</li><li>纵轴（Y 轴）：性能（Performance），<strong>线性刻度</strong>。</li><li>曲线显示<strong>递减收益（Diminishing Returns）</strong>，即：<strong>随着计算资源的增加，性能增长趋缓</strong>，但仍然在上升。</li></ul></li><li><p><strong>右图（对数-对数尺度 - Log-log Scale）</strong></p><ul><li>横轴（X 轴）：计算资源（Compute），<strong>对数刻度</strong>。</li><li>纵轴（Y 轴）：性能（Performance），<strong>对数刻度</strong>。</li><li>在对数-对数尺度下，原本弯曲的曲线变成<strong>一条直线</strong>，说明计算资源和性能之间呈<strong>幂律关系（Power Law Relationship）</strong>。</li></ul></li></ul><p>这些定律通常遵循 <strong>幂律（Power Laws）</strong>，即：</p><blockquote><p><strong>某个变量（如计算量）增加，会导致另一个变量（如性能）按一定比例变化。</strong></p></blockquote><p>最著名的 <strong>规模定律</strong> 包括：</p><ul><li><strong>Kaplan 规模定律</strong>（Kaplan Scaling Law）：当计算资源一定时，<strong>增加模型的参数规模比增加数据规模更有效</strong>。表明模型性能与参数量、计算量和训练数据（Tokens）之间存在幂律关系，即 更多参数、更多计算资源能提升性能（GPT-3 论文提出）。</li><li><strong>Chinchilla 规模定律</strong>（Chinchilla Scaling Law）：模型的大小和数据规模同样重要，二者需 <strong>同步扩展</strong> 才能实现最佳性能（DeepMind 提出）。</li></ul><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_145639.png" class=""><p>这张图展示了<strong>大规模 AI 训练中的 Scaling Laws（缩放定律）</strong>，表明<strong>计算资源（Compute）、数据集规模（Dataset Size）和参数量（Parameters）</strong>对模型性能的影响。关键信息如下：</p><hr><p><strong>1. 纵轴（Y轴）：测试损失（Test Loss）</strong></p><ul><li><strong>目标是降低测试损失（Test Loss）</strong>，即提高模型的泛化性能。</li><li><strong>损失（L）越小，模型性能越好</strong>。</li></ul><p><strong>2. 横轴（X轴）：三种关键变量</strong></p><ul><li><p><strong>左图（Compute，计算资源）</strong>：</p><ul><li>X 轴是计算资源（PF-days, 非 embedding）。</li><li>计算资源越多，测试损失降低（性能提升）。</li><li>公式：<br>$$<br>L = \left( \frac{C_{\text{min}}}{2.3 \times 10^8} \right)^{-0.050}<br>$$</li><li><strong>体现计算资源的幂律关系</strong>：计算资源增加，损失减少，但收益递减（指数 -0.050）。</li></ul></li><li><p><strong>中图（Dataset Size，数据集规模）</strong>：</p><ul><li>X 轴是训练数据的 Token 数量。</li><li>数据规模越大，测试损失降低（性能提升）。</li><li>公式：<br>$$<br>L = \left( \frac{D}{5.4 \times 10^{13}} \right)^{-0.095}<br>$$</li><li><strong>数据规模对损失的影响较大</strong>（指数 -0.095）。</li></ul></li><li><p><strong>右图（Parameters，参数量）</strong>：</p><ul><li>X 轴是模型参数量（非 embedding）。</li><li>参数数量越大，测试损失降低（性能提升）。</li><li>公式：<br>$$<br>L = \left( \frac{N}{8.8 \times 10^{13}} \right)^{-0.076}<br>$$</li><li><strong>参数对损失的影响介于计算资源和数据规模之间</strong>（指数 -0.076）。</li></ul></li></ul><p>这些研究表明，<strong>模型规模、数据规模和计算资源必须协同扩展，才能最大化模型的性能</strong>。</p><ul><li><strong>计算资源增加 → 训练更强大模型</strong></li><li><strong>更多 Tokens → 更好泛化能力</strong></li><li><strong>参数增加 → 但需要与数据匹配，否则过拟合</strong></li></ul><p>Kaplan 规模定律认为，在 <strong>固定计算资源</strong> 的情况下，<strong>优先增加模型参数</strong> 通常比增加数据规模更有效。而 Chinchilla 规模定律则指出，<strong>模型参数和数据规模都应同步增长</strong>，以获得更优的模型性能。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_145814.png" class=""><p>然而，在 <strong>2024 年</strong>，研究人员发现，尽管计算资源、数据规模和模型参数 <strong>持续增长</strong>，但性能提升的 <strong>边际收益（Marginal Return）</strong> 却在 <strong>逐渐降低</strong>。</p><p>这引发了一个重要的问题：</p><p>❓ <strong>“我们是否已经遇到了 LLM 发展的瓶颈？”</strong></p><hr><h2 id="🚀-什么是推理时计算（Test-time-Compute）？"><a href="#🚀-什么是推理时计算（Test-time-Compute）？" class="headerlink" title="🚀 什么是推理时计算（Test-time Compute）？"></a>🚀 什么是推理时计算（Test-time Compute）？</h2><p>由于 <strong>训练时计算的成本极其昂贵</strong>，研究人员开始关注 <strong>推理时计算（Test-time Compute）</strong>，即：</p><blockquote><p><strong>让 LLM 在推理时“思考更长时间”</strong>，而非单纯依赖更大的模型和数据集。</p></blockquote><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_145856.png" class=""><p>对于<strong>非推理模型</strong>，它们通常 <strong>直接输出答案</strong>：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Q: 8 + 5 = ?</span><br><span class="line">A: 13</span><br></pre></td></tr></tbody></table></figure><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_145936.png" class=""><p>而<strong>推理模型</strong>则会 <strong>使用更多 token 进行推理</strong>，形成系统化的“思考”过程：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Q: 8 + 5 = ?</span><br><span class="line">A: 8 + 5 可拆解为 8 + 2 + 3 = 10 + 3 = 13</span><br></pre></td></tr></tbody></table></figure><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_150033.png" class=""><p>LLM 需要消耗计算资源（如显存计算）来生成答案。然而，如果所有计算资源都用于直接生成答案，那将会是低效的！</p><p>相反，通过提前生成包含额外信息、关系和新思考的更多 token，模型可以在推理过程中分配更多计算资源以生成最终答案。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_151252.png" class=""><p>这张图片展示了 <strong>大语言模型（LLM）</strong> 在计算过程中如何分配 <strong>token</strong>（标记）来优化推理能力和最终的回答质量。核心思想是：<strong>如果计算资源（如 GPU/VRAM 计算量）全部用于直接生成答案，而没有用于思考，那么效率会受到影响</strong>。相反，增加 <strong>思考过程</strong>（即生成更多的中间 token），可以提高模型的 <strong>推理能力</strong>，从而提升 <strong>最终的回答质量</strong>。</p><p><strong>1. Token 的使用与计算量</strong></p><ul><li><strong>LLM 生成答案是按 token 逐步输出的</strong>，每个 token 都会占用计算资源。</li><li><strong>分配更多的 token 进行思考</strong>，意味着模型可以在得出最终答案之前有更多的推理步骤，从而提高正确率。</li></ul><p> <strong>2. 三种不同的计算方式</strong></p><ul><li><p><strong>场景 1（1 个 token：最少计算）</strong></p><ul><li>直接输出 <strong>“5”</strong> 作为答案。</li><li><strong>计算量最少</strong>，速度最快。</li><li><strong>如果问题较复杂，可能会出错</strong>，因为模型没有足够的计算时间来思考。</li></ul></li><li><p><strong>场景 2（6 个 token：中等计算）</strong></p><ul><li>模型生成一个简短的推理过程：<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Adding 3 and 2 gives 5</span><br></pre></td></tr></tbody></table></figure></li><li><strong>比第一种方法多了一些计算量</strong>，但仍然较为简洁。</li><li>这种方式适用于<strong>简单的数学运算或逻辑推理</strong>，但在更复杂的情况下仍可能出现错误。</li></ul></li><li><p><strong>场景 3（15 个 token：完整推理）</strong></p><ul><li>模型先进行详细的逐步推理：<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">3 + 1 = 4 , 4 + 1 = 5</span><br></pre></td></tr></tbody></table></figure>然后，模型再明确地总结：<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">the total is 5</span><br></pre></td></tr></tbody></table></figure></li><li><strong>推理过程更详细，占用的计算量最大</strong>。</li><li><strong>适用于需要多步推理的任务，如数学题、逻辑推理题等</strong>。</li></ul></li></ul><h3 id="🔢-规模定律（Scaling-Laws）-1"><a href="#🔢-规模定律（Scaling-Laws）-1" class="headerlink" title="🔢 规模定律（Scaling Laws）"></a>🔢 规模定律（Scaling Laws）</h3><p>相比于训练时计算，推理时计算的规模定律仍然较为新颖。值得注意的是，有两项研究揭示了推理时计算规模与训练时计算规模的关系。</p><p>首先，OpenAI 发表的一篇文章表明，推理时计算可能遵循与训练时计算相同的扩展趋势。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_151722.png" class=""><blockquote><p><strong>来自“学习如何推理的 LLM”一文的注释图</strong>：红色虚线显示了 OpenAI 提出的新范式可能是推理时计算。<br>这张图展示了 <strong>训练时间计算（train-time compute）和测试时间计算（test-time compute）</strong> 对模型 <strong>pass@1 准确率（accuracy）</strong> 的影响，具体来说，它强调了 <strong>测试时间计算可能比训练时间计算更有利于扩展模型性能</strong>。</p></blockquote><ol><li><p><strong>左图：训练时间计算 vs. 准确率</strong></p><ul><li><strong>X 轴（横轴）：训练时间计算（log scale，指数刻度）</strong>。</li><li><strong>Y 轴（纵轴）：pass@1 准确率</strong>（即模型在一次尝试中得到正确答案的概率）。</li><li><strong>黑色点</strong> 代表不同计算量下的模型表现，粉色虚线展示了大致的趋势。</li><li>可以看到，随着 <strong>训练计算量的增加，准确率逐渐提高</strong>，但增长趋势相对平稳。</li></ul></li><li><p><strong>右图：测试时间计算 vs. 准确率</strong></p><ul><li><strong>X 轴（横轴）：测试时间计算（log scale）</strong>。</li><li><strong>Y 轴（纵轴）：pass@1 准确率</strong>。</li><li>同样，黑色点代表不同计算量下的模型表现，粉色虚线展示了大致的趋势。</li><li>这里可以看到，随着 <strong>测试时计算量增加，模型的准确率增长更显著，甚至超过了训练计算量的效果</strong>。<br>因此，他们认为，推理时计算的扩展可能代表着新的研究范式。</li></ul></li></ol><p>其次，一篇名为《Scaling Scaling Laws with Board Games》的论文研究了 AlphaZero 在不同计算量下玩 Hex 游戏的表现。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_152050.png" class=""><blockquote><p><strong>来自“Scaling Scaling Laws with Board Games”一文的注释图</strong>：该图展示了他们如何构建不同规模的训练时计算和推理时计算。- <strong>AlphaZero</strong> 是 <strong>DeepMind</strong> 开发的一个 <strong>强化学习（Reinforcement Learning, RL）</strong> 训练的 AI。</p></blockquote><ul><li>该算法通过 <strong>自我对弈（self-play）</strong> 训练，无需人为规则输入，即可掌握<strong>围棋、国际象棋、将棋等游戏</strong>。</li><li>它结合了 <strong>神经网络预测</strong> 和 <strong>蒙特卡洛树搜索（MCTS, Monte Carlo Tree Search）</strong> 来进行决策。</li></ul><p>这张图片展示了 <strong>AlphaZero 算法</strong> 在<strong>训练阶段（train-time compute）和测试阶段（test-time compute）</strong>计算资源的不同应用。主要强调了：</p><ul><li><strong>训练时</strong>：依赖于<strong>更多参数和更长的训练时间</strong>来优化模型。</li><li><strong>测试时</strong>：依靠 <strong>更深入的树搜索（tree search）</strong> 来提升决策能力。</li></ul><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_152430.png" class=""><blockquote><p>来自“Scaling Scaling Laws with Board Games”一文的注释图：该图展示了训练时计算与推理时计算之间的关系。<br>研究结果表明，训练时计算和推理时计算紧密相关。每条虚线表示达到特定 ELO 分数所需的最小计算量。<br><strong>1. 坐标轴含义</strong></p></blockquote><ul><li><strong>X 轴（横轴）：训练时计算量（Train-time Compute，FLOP-seconds）</strong></li><li><strong>Y 轴（纵轴）：推理时计算量（Test-time Compute，FLOP-seconds）</strong></li><li><strong>对数刻度（log scale）：计算量的增长呈指数级，而不是线性增长。</strong></li></ul><p><strong>2. 关键数据趋势</strong></p><ul><li>不同颜色的曲线分别表示<strong>不同的 ELO 分数水平</strong>（-1500、-1250、-1000、-750、-500、-250）。</li><li><strong>虚线和实线</strong>：<ul><li><strong>虚线</strong> 表示某个 ELO 分数下的最优计算边界。</li><li><strong>实线</strong> 代表实际数据趋势。</li></ul></li></ul><ol><li><p><strong>训练计算和推理计算可以互相替代</strong></p><ul><li><strong>如果推理计算量增加（左上区域）</strong>，那么所需的训练计算量减少。</li><li><strong>如果训练计算量增加（右下区域）</strong>，那么所需的推理计算量减少。</li><li><strong>两者呈现负相关关系</strong>。</li></ul></li><li><p><strong>低训练计算 vs. 高推理计算</strong></p><ul><li>在 <strong>训练计算较少</strong> 的情况下（如左侧的红色圈），模型仍然可以达到相同的 ELO 水平，但需要 <strong>在推理时增加计算量</strong>（如更深的搜索树、更长的思考路径）。</li></ul></li><li><p><strong>高训练计算 vs. 低推理计算</strong></p><ul><li>在 <strong>训练计算充足</strong> 的情况下（如右侧的红色圈），模型可以<strong>减少推理计算需求</strong>，即 <strong>即使使用较少的搜索深度，仍然能获得较高的性能</strong>。</li></ul></li><li><p><strong>公式解释</strong></p><ul><li>公式：<br>$$<br>\log_{10}(\text{test compute}) = -1.2 \cdot \log_{10}(\text{train compute}) + 0.004 \cdot \text{elo} + 29<br>$$</li><li>这说明：<ul><li><strong>训练计算（train compute）增加时，推理计算（test compute）减少（系数 -1.2）</strong>。</li><li><strong>更高的 ELO（更强的 AI）需要额外的计算（系数 0.004）</strong>。</li></ul></li></ul></li></ol><p>随着推理时计算扩展类似于训练时计算，研究范式正朝着“推理”模型利用更多推理时计算的方向发展。通过这种范式转变，这些“推理”模型不再单纯关注训练时计算（预训练和微调），而是平衡训练与推理。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_152824.png" class=""><p>推理时计算甚至可以随长度扩展：</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_153922.png" class=""><p>这是我们在 DeepSeek-R1 研究中也将探讨的内容！</p><h3 id="📌-推理时计算的类别（Categories-of-Test-time-Compute）"><a href="#📌-推理时计算的类别（Categories-of-Test-time-Compute）" class="headerlink" title="📌 推理时计算的类别（Categories of Test-time Compute）"></a>📌 推理时计算的类别（Categories of Test-time Compute）</h3><p>推理模型（如 <strong>DeepSeek-R1</strong> 和 <strong>OpenAI o1</strong>）的成功表明，在推理过程中，除了简单地“思考更长时间”之外，还有更多的优化技术。</p><p>在本文中，我们将探讨 <strong>推理时计算（Test-time Compute）</strong> 的多种实现方式，包括：</p><ul><li><strong>链式思维（Chain-of-Thought）</strong></li><li><strong>答案修订（Revising Answers）</strong></li><li><strong>回溯推理（Backtracking）</strong></li><li><strong>多样性采样（Sampling）</strong></li><li><strong>其他方法</strong></li></ul><p>总体而言，推理时计算可归纳为以下 <strong>两大类别</strong>：</p><ol><li><p><strong>基于验证器的搜索（Search against Verifiers）</strong>  </p><ul><li>通过 <strong>采样多个答案</strong> 并 <strong>选择最佳答案</strong> 来优化推理。</li></ul></li><li><p><strong>修改提议分布（Modifying Proposal Distribution）</strong>  </p><ul><li>通过训练 <strong>“思考”过程</strong> 来提高推理能力。Proposal Distribution（提议分布，指在模型生成答案时，对不同可能答案的概率分布进行调整）</li></ul></li></ol><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_154456.png" class=""><p>从本质上讲：</p><ul><li><strong>基于验证器的搜索</strong> 更关注 <strong>输出质量</strong>（Output-focused）。</li><li><strong>修改提议分布</strong> 关注 <strong>输入结构</strong>（Input-focused）。</li></ul><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_154547.png" class=""><h3 id="🔍-两种主要验证器类型"><a href="#🔍-两种主要验证器类型" class="headerlink" title="🔍 两种主要验证器类型"></a>🔍 两种主要验证器类型</h3><p>为了更好地筛选和评估推理答案，我们引入了两种 <strong>验证器（Verifiers）</strong>：</p><ol><li><p><strong>结果奖励模型（Outcome Reward Models, ORM）</strong>  </p><ul><li>仅对最终答案进行评分，而不考虑推理过程。</li></ul></li><li><p><strong>过程奖励模型（Process Reward Models, PRM）</strong>  </p><ul><li>既评估最终答案，也对推理过程进行评分。</li></ul></li></ol><p>在接下来的部分，我们将详细探讨 <strong>如何将 ORM 和 PRM 应用于不同的验证方法</strong>！</p><p>顾名思义，<strong>结果奖励模型（Outcome Reward Model, ORM）</strong> 仅评估最终的答案质量，而不关注答案背后的推理过程：</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_160242.png" class=""><ul><li>ORM 只看最终输出，而不关心模型是如何得出这个答案的。</li></ul><p>相比之下，<strong>过程奖励模型（Process Reward Model, PRM）</strong> 则会评估推理过程本身：</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_160258.png" class=""><ul><li>PRM 既评估答案的正确性，也关注推理路径的合理性。</li></ul><h3 id="🧐-PRM-如何评估推理过程？"><a href="#🧐-PRM-如何评估推理过程？" class="headerlink" title="🧐 PRM 如何评估推理过程？"></a>🧐 PRM 如何评估推理过程？</h3><p>为了更清楚地说明推理步骤的重要性，让我们来看一个示例：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">问题：某个方程的解是多少？</span><br><span class="line"></span><br><span class="line">推理步骤 1：首先展开方程，得到 x = 3。</span><br><span class="line">推理步骤 2：错误地将 x = 3 改写为 x = 5。</span><br><span class="line">推理步骤 3：最终输出 x = 5。</span><br></pre></td></tr></tbody></table></figure><p>在上述示例中，虽然最终答案（x = 5）是错误的，但 ORM 仅评估最终输出，不会关注中间的错误推理。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_160332.png" class=""><p>或者在这个例子中，PRM 会发现 <strong>推理步骤 2 是错误的</strong>，并对此步骤给予低分，从而避免错误答案的出现。</p><hr><h3 id="🔍-ORM-vs-PRM-在推理中的应用"><a href="#🔍-ORM-vs-PRM-在推理中的应用" class="headerlink" title="🔍 ORM vs. PRM 在推理中的应用"></a>🔍 ORM vs. PRM 在推理中的应用</h3><p>现在你已经掌握了 <strong>结果奖励模型（ORM）</strong> 和 <strong>过程奖励模型（PRM）</strong> 之间的区别，我们接下来探讨如何将它们应用于各种 <strong>验证技术（Verification Techniques）</strong>。</p><h2 id="📌-基于验证器的搜索（Search-against-Verifiers）"><a href="#📌-基于验证器的搜索（Search-against-Verifiers）" class="headerlink" title="📌 基于验证器的搜索（Search against Verifiers）"></a>📌 基于验证器的搜索（Search against Verifiers）</h2><p>推理时计算的第一大类别是 <strong>基于验证器的搜索</strong>，它通常包含两个步骤：</p><ol><li><strong>生成多个推理过程和答案样本</strong></li><li><strong>使用验证器（奖励模型）对生成的输出进行评分</strong><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_160918.png" class=""></li></ol><h3 id="🤖-验证器的作用"><a href="#🤖-验证器的作用" class="headerlink" title="🤖 验证器的作用"></a>🤖 验证器的作用</h3><p>验证器通常是一个大型语言模型（LLM），经过微调以评估结果（ORM）或过程（PRM）。 使用验证器的一个主要优势是，无需重新训练或微调用于回答问题的大型语言模型（LLM），仅通过评分机制选择最佳答案。</p><hr><h3 id="✅-多数投票法（Majority-Voting）"><a href="#✅-多数投票法（Majority-Voting）" class="headerlink" title="✅ 多数投票法（Majority Voting）"></a>✅ 多数投票法（Majority Voting）</h3><p>最简单的方法是 <strong>不使用奖励模型或验证器</strong>，而是执行 <strong>多数投票（Majority Voting）</strong>。</p><p>📌 <strong>方法：</strong> 让 LLM 生成多个答案，选择出现次数最多的答案作为最终答案。</p><p>📌 <strong>示例：</strong></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Q: 15 × 3 = ?</span><br><span class="line">A1: 45</span><br><span class="line">A2: 42</span><br><span class="line">A3: 45</span><br><span class="line">最终答案: 45（因其出现频率最高）</span><br></pre></td></tr></tbody></table></figure><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_161249.png" class=""><p>这种方法也称为 <strong>自一致性（Self-Consistency）</strong>，强调 <strong>生成多个答案和推理步骤</strong> 的重要性。</p><hr><h3 id="🔢-Best-of-N-采样法（Best-of-N-Samples）"><a href="#🔢-Best-of-N-采样法（Best-of-N-Samples）" class="headerlink" title="🔢 Best-of-N 采样法（Best-of-N Samples）"></a>🔢 Best-of-N 采样法（Best-of-N Samples）</h3><p>Best-of-N 采样是第一个涉及验证器（Verifier）的方法，它的基本思想是生成 N 个样本答案，然后使用 奖励模型（Reward Model, RM） 对这些答案进行评分，并选择得分最高的答案。</p><p>📌 <strong>步骤：</strong></p><ol><li><strong>生成多个答案</strong>（使用较高或者不同的温度参数生成 N 个样本）。<img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_161825.png" class=""></li><li><strong>结果奖励模型（ORM, Outcome Reward Model）</strong>，每个答案都会通过 ORM 进行评分。选取得分最高的答案作为最终输出。<img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_161833.png" class="">📌 <strong>示例：</strong></li></ol><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Q: 8 + 5 = ?</span><br><span class="line">A1: 12 (得分 0.2)</span><br><span class="line">A2: 13 (得分 0.9)</span><br><span class="line">A3: 14 (得分 0.4)</span><br><span class="line">最终选择: A2（因其得分最高）</span><br></pre></td></tr></tbody></table></figure><h2 id="📌-进一步优化：-若使用-PRM，则不只评估答案，还评估整个推理过程。与仅评估最终答案的结果奖励模型（ORM）不同，过程奖励模型（PRM）会评估推理过程的质量。PRM-关注推理的每个步骤，确保推理过程合理、连贯，并最终选择总评分最高的候选答案。-加权-Best-of-N-采样（Weighted-Best-of-N-samples）-结合-ORM-和-PRM-两种验证方式，我们可以对所有候选答案进行加权评分，并选择总权重最高的答案。这种方法称为-加权-Best-of-N-采样（Weighted-Best-of-N-samples）：。"><a href="#📌-进一步优化：-若使用-PRM，则不只评估答案，还评估整个推理过程。与仅评估最终答案的结果奖励模型（ORM）不同，过程奖励模型（PRM）会评估推理过程的质量。PRM-关注推理的每个步骤，确保推理过程合理、连贯，并最终选择总评分最高的候选答案。-加权-Best-of-N-采样（Weighted-Best-of-N-samples）-结合-ORM-和-PRM-两种验证方式，我们可以对所有候选答案进行加权评分，并选择总权重最高的答案。这种方法称为-加权-Best-of-N-采样（Weighted-Best-of-N-samples）：。" class="headerlink" title="📌 进一步优化：- 若使用 PRM，则不只评估答案，还评估整个推理过程。与仅评估最终答案的结果奖励模型（ORM）不同，过程奖励模型（PRM）会评估推理过程的质量。PRM 关注推理的每个步骤，确保推理过程合理、连贯，并最终选择总评分最高的候选答案。- 加权 Best-of-N 采样（Weighted Best-of-N samples）:结合 ORM 和 PRM 两种验证方式，我们可以对所有候选答案进行加权评分，并选择总权重最高的答案。这种方法称为 加权 Best-of-N 采样（Weighted Best-of-N samples）：。"></a>📌 <strong>进一步优化：</strong><br>- 若使用 <strong>PRM</strong>，则不只评估答案，还评估整个推理过程。与仅评估最终答案的结果奖励模型（ORM）不同，过程奖励模型（PRM）会评估推理过程的质量。PRM 关注推理的每个步骤，确保推理过程合理、连贯，并最终选择总评分最高的候选答案。<br><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_161922.png" class=""><br>- <strong>加权 Best-of-N 采样（Weighted Best-of-N samples）</strong>:结合 ORM 和 PRM 两种验证方式，我们可以对所有候选答案进行加权评分，并选择总权重最高的答案。这种方法称为 加权 Best-of-N 采样（Weighted Best-of-N samples）：。<br><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_162139.png" class=""></h2><h3 id="🚀-使用过程奖励模型（PRM）的束搜索（Beam-Search）"><a href="#🚀-使用过程奖励模型（PRM）的束搜索（Beam-Search）" class="headerlink" title="🚀 使用过程奖励模型（PRM）的束搜索（Beam Search）"></a>🚀 使用过程奖励模型（PRM）的束搜索（Beam Search）</h3><p>在生成答案及其中间推理步骤的过程中，我们可以使用 <strong>束搜索（Beam Search）</strong> 进一步优化推理路径。</p><p>📌 <strong>束搜索的核心思想：</strong></p><ul><li>在推理过程中，生成多个可能的推理路径（称为“束”）。</li><li>使用 <strong>过程奖励模型（PRM, Process Reward Model）</strong> 对每条路径进行评分。</li><li>类似于 <strong>Tree of Thought</strong> 方法，始终保留得分最高的 <strong>前 3 条推理路径</strong>，并在推理过程中持续跟踪这些路径。</li><li>如果某条路径的得分较低（PRM 评分低），则提前停止该推理路径，以避免不必要的计算开销。</li></ul><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_162508.png" class=""><p>📌 <strong>优化后的答案筛选方式：</strong><br>最终，生成的所有答案将使用 <strong>Best-of-N 采样</strong> 方法进行加权评分，确保选出最佳推理路径的最终答案。</p><p>🚀 <strong>优势：</strong></p><ul><li>避免计算资源浪费，快速淘汰低质量推理路径。</li><li>结合 PRM，可以确保模型的推理过程更连贯、更符合逻辑。</li><li>通过 Best-of-N 方法进一步优化答案质量，使最终答案更加可靠。</li></ul><hr><h3 id="🎲-蒙特卡洛树搜索（Monte-Carlo-Tree-Search-MCTS）"><a href="#🎲-蒙特卡洛树搜索（Monte-Carlo-Tree-Search-MCTS）" class="headerlink" title="🎲 蒙特卡洛树搜索（Monte Carlo Tree Search, MCTS）"></a>🎲 蒙特卡洛树搜索（Monte Carlo Tree Search, MCTS）</h3><p>蒙特卡洛树搜索（Monte Carlo Tree Search, <strong>MCTS</strong>）是一种常用于决策树搜索的算法，在 LLM 的推理优化中也可以采用该方法。MCTS 通过四个步骤来优化推理路径：<br>📌 <strong>主要步骤：</strong></p><ol><li><strong>选择（Selection）：</strong> 根据预定义的公式，从当前搜索树中选择一个叶节点 进行扩展。</li><li><strong>扩展（Expand）：</strong> 在所选叶节点的基础上 创建新的子节点，以探索更多可能的推理路径。</li><li><strong>模拟（Rollouts）：</strong> 通过随机生成新的推理路径，持续扩展节点，直到达到终点（即得到最终答案）。</li><li><strong>回溯（Backpropagation）：</strong> 根据最终输出结果 更新父节点的评分，从而优化未来的搜索决策。</li></ol><p>在大语言模型（LLM）的推理过程中，我们通常希望找到最佳的推理路径，使其最终生成的答案最优。但在这个过程中，需要在 <strong>探索（Exploration）</strong> 和 <strong>利用（Exploitation）</strong> 之间取得平衡：</p><ul><li><strong>利用（Exploitation）</strong>：选择当前看起来最优的路径，以利用已知的高质量推理步骤。</li><li><strong>探索（Exploration）</strong>：选择访问次数较少的路径，以发现可能更优的推理步骤。</li></ul><h4 id="选择分数（Selection-Score）"><a href="#选择分数（Selection-Score）" class="headerlink" title="选择分数（Selection Score）"></a>选择分数（Selection Score）</h4><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_163149.png" class=""><p>在选择推理路径时，我们使用 <strong>选择分数（Selection Score）</strong> 计算每个推理步骤（即树的节点）的优先级，公式如下：</p><p>$$<br>\text{Selection Score} = \frac{\text{Total Node Reward}}{\text{Number of Node Visits}} + C \times \sqrt{\frac{\text{Number of Parent Node Visits}}{\text{Number of Node Visits}}}<br>$$</p><p>其中：</p><ul><li><p><strong>第一项</strong>：$$\frac{\text{Total Node Reward}}{\text{Number of Node Visits}}$$（利用项，Exploitation Term）</p><ul><li><strong>Total Node Reward</strong>：该节点累计获得的奖励值（表示其历史表现）。</li><li><strong>Number of Node Visits</strong>：该节点被访问的次数。</li><li>这项计算的是该节点的 <strong>平均奖励值</strong>，高奖励的节点会被优先选择。</li></ul></li><li><p><strong>第二项</strong>：$$C \times \sqrt{\frac{\text{Number of Parent Node Visits}}{\text{Number of Node Visits}}}$$（探索项，Exploration Term）</p><ul><li><strong># of Parent Node Visits</strong>：父节点被访问的次数。</li><li><strong># of Node Visits</strong>：当前节点被访问的次数。</li><li><strong>C</strong>：一个超参数，控制探索与利用的平衡。</li><li>这项鼓励探索访问次数较少的节点，以防止过早陷入局部最优解。</li></ul></li></ul><p>总结：</p><ul><li><strong>第一项（Exploitation Term）</strong> 让算法倾向于选择 <strong>历史表现较好的路径</strong>。</li><li><strong>第二项（Exploration Term）</strong> 让算法倾向于 <strong>探索访问较少的路径</strong>，避免陷入局部最优。</li><li><strong>参数 C</strong> 控制这两者的平衡。</li></ul><h4 id="2-选择（Selection）与扩展（Expand）"><a href="#2-选择（Selection）与扩展（Expand）" class="headerlink" title="2. 选择（Selection）与扩展（Expand）"></a><strong>2. 选择（Selection）与扩展（Expand）</strong></h4><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_164346.png" class=""><p>这一阶段，我们使用 <strong>选择分数</strong> 来决定哪条推理路径值得继续扩展：</p><p><strong>（1）选择（Selection）</strong></p><ul><li><strong>输入：问题（Question）</strong></li><li><strong>LLM 生成多个推理步骤（Reasoning Steps）</strong><ul><li>例如，在图片中，LLM 生成了 3 个推理步骤：<ul><li><strong>Thought 1</strong>（评分 0.4）</li><li><strong>Thought 2</strong>（评分 0.2）</li><li><strong>Thought 3</strong>（评分 0.1）</li></ul></li></ul></li><li><strong>使用选择分数（Selection Score）选择最优路径</strong>（随机初始化）<ul><li>在示例中，评分最高的 <strong>Thought 1（0.4）</strong> 被选中。</li></ul></li></ul><p><strong>（2）扩展（Expand）</strong></p><ul><li><strong>在选中的推理路径上，生成新的推理步骤</strong></li><li>这些新推理步骤的初始值设为 0，表示它们还没有经过评估。</li></ul><p>这个过程类似于 <strong>MCTS 的拓展（Expansion）阶段</strong>，即：</p><ol><li>选择当前最优路径（使用 <strong>选择分数</strong>）。</li><li>在该路径下，扩展新的推理步骤（未评分的子节点）。</li></ol><h4 id="3-Rollouts（模拟）与-Backpropagation（反向传播）"><a href="#3-Rollouts（模拟）与-Backpropagation（反向传播）" class="headerlink" title="3. Rollouts（模拟）与 Backpropagation（反向传播）"></a><strong>3. Rollouts（模拟）与 Backpropagation（反向传播）</strong></h4><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_165052.png" class=""><p>一旦扩展了推理步骤，我们需要继续探索，并利用 <strong>模拟（Rollouts）</strong> 和 <strong>反向传播（Backpropagation）</strong> 来优化整个搜索过程。</p><h3 id="（3）Rollouts（模拟）"><a href="#（3）Rollouts（模拟）" class="headerlink" title="（3）Rollouts（模拟）"></a><strong>（3）Rollouts（模拟）</strong></h3><ul><li>选定路径后，我们继续展开推理步骤，直到 <strong>生成最终答案</strong>。</li><li>这个过程类似于 <strong>在 MCTS 中随机模拟游戏到结束</strong>：<ul><li>我们从当前节点出发，进行一系列推理，直到模型生成最终的答案。</li><li>在图片中，我们沿着 Thought 1（0.4） 继续展开推理步骤。</li><li>这些推理步骤最终会 <strong>生成多个答案</strong>（图片中紫色框）。</li></ul></li></ul><h3 id="（4）Backpropagation（反向传播）"><a href="#（4）Backpropagation（反向传播）" class="headerlink" title="（4）Backpropagation（反向传播）"></a><strong>（4）Backpropagation（反向传播）</strong></h3><ul><li>通过对 <strong>最终答案</strong> 进行评分，我们可以更新前面所有参与推理的节点分数：<ul><li><strong>PRM（Process Reward Model）</strong>：对推理步骤本身进行评分，衡量其合理性。</li><li><strong>ORM（Output Reward Model）</strong>：对最终答案进行评分，衡量其正确性。</li><li>这些评分 <strong>向上传播</strong>，更新 <strong>所有经过的节点</strong> 的奖励值。</li></ul></li><li>例如：<ul><li>在图片中，最终答案的评分导致 <strong>Thought 1</strong> 的评分从 0.4 提高到 <strong>0.8</strong>。</li><li>进一步向上传播，使得 <strong>父节点的选择分数也随之更新</strong>。</li></ul></li></ul><p>这个过程保证了：</p><ul><li><strong>较好的推理路径会逐渐获得更高的分数</strong>，提高被选中的概率。</li><li><strong>较差的推理路径会被逐渐淘汰</strong>，避免浪费计算资源。</li></ul><hr><h2 id="📌-修改提议分布（Modifying-Proposal-Distribution）"><a href="#📌-修改提议分布（Modifying-Proposal-Distribution）" class="headerlink" title="📌 修改提议分布（Modifying Proposal Distribution）"></a>📌 修改提议分布（Modifying Proposal Distribution）</h2><p><strong>修改提议分布（Modifying Proposal Distribution）</strong></p><p>在大语言模型（LLM）的推理过程中，我们可以通过修改提议分布（Modifying Proposal Distribution）来优化模型的推理能力。这种方法的核心思想是：</p><ul><li><strong>不再单纯依赖模型搜索正确推理步骤</strong>（基于输出的优化），</li><li><strong>而是让模型主动生成更优的推理步骤</strong>（基于输入的优化）。</li></ul><p>换句话说，我们不是在输出结果后进行检验，而是直接修改模型在推理过程中如何选择 token，让它更倾向于选择能够引导推理的 token，而不是立即输出最终答案。修改了用于采样补全（completions）、思维（thoughts）或标记（tokens）的概率分布。这种方法可以让模型生成的答案更加准确、可解释，并且在面对复杂问题时更具有鲁棒性（robustness）。</p><p><strong>1. 直接选择最高概率 Token（Greedy 选择）</strong></p><p>在默认情况下，LLM 生成多个可能的 token 作为输出候选项，并根据其概率进行排序，最终选择最高概率的 token 进行输出。这种方法称为<strong>贪心选择（Greedy Selection）</strong>。</p><p>你可以想象，我们有一个问题（question）和一个用于采样 token 的概率分布（distribution）。常见的策略是选择得分最高的 token。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_170357.png" class=""><ul><li>例如，给定问题 <code>What is 3 + 2?</code>，LLM 可能会生成如下候选 token：<ul><li><code>5</code>（最高概率）</li><li><code>3</code></li><li><code>Adding</code></li><li><code>4</code></li><li><code>If</code></li></ul></li><li>在贪心策略下，模型会直接选择 <code>5</code> 作为最终答案，而不会进行推理。</li></ul><p>这种方法虽然快速，但存在如下问题：</p><ul><li><strong>缺乏推理能力</strong>：模型可能直接输出错误答案，因为它没有进行推理。</li><li><strong>可解释性差</strong>：对于复杂问题，用户无法理解模型是如何得出答案的。</li></ul><p><strong>2. 通过推理（Reasoning Before Answering）提高答案质量</strong></p><p>然而，请注意上图中有一些<strong>标记（tokens</strong>被标红。这些token更有可能引导模型进入一个合理的推理过程。虽然选择贪心（greedy）策略下得分最高的 token 不一定是错误的，但选择那些能引导模型进入推理过程的 token，通常会得到更好的答案。<br>让 LLM <strong>先进行推理，再给出答案</strong>，即：</p><ul><li>选择推理 token（如 <code>Adding</code>）</li><li>逐步生成推理过程，如：<ul><li><code>Adding → 3 and 2 gives → 5</code></li><li><code>If → 3 + 1 = 4, 4 + 1 = 5 → 5</code></li><li><code>The total is → 5</code></li></ul></li><li>通过推理链条逐步推导出 <code>5</code>，相比直接选择 <code>5</code>，这种方法更加可解释，并且能在复杂问题上表现更好。</li></ul><p><strong>3. 通过修改提议分布（Re-Ranking Token Probabilities）引导推理过程</strong></p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_171002.png" class=""><p>当我们<strong>修改提议分布（proposal distribution，即 token 的概率分布）</strong>时，实际上是在<strong>重新排序（re-rank）</strong>这个分布，使得“推理相关”的 token 被选中的概率更高。<br>在这种方法下，我们调整 LLM 的提议分布，使其更倾向于选择推理 token，而非直接选择答案：</p><ul><li>默认情况下，<code>5</code> 具有最高概率，而 <code>Adding</code>、<code>If</code> 等推理 token 的概率较低。</li><li>通过修改提议分布，我们提高 <code>Adding</code>、<code>If</code> 的概率，使模型倾向于进行推理。</li></ul><p><strong>4. 如何实现修改提议分布？</strong></p><p>主要有两种方式：</p><ol><li><strong>通过 Prompt Engineering</strong><ul><li>修改 Prompt，引导模型生成推理步骤。</li><li>例如：<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Q: What is 3 + 2?</span><br><span class="line">A: Let's think step by step.</span><br></pre></td></tr></tbody></table></figure></li></ul></li><li><strong>训练模型更倾向于推理</strong><ul><li>在微调过程中，提供更多具有推理链的训练数据，让模型习惯生成推理 token。</li></ul></li></ol><p><strong>总结</strong></p><ul><li><strong>贪心选择（Greedy Selection）</strong>：快速，但缺乏推理，可解释性差。</li><li><strong>推理后回答（Reasoning Before Answering）</strong>：提高答案质量和可解释性。</li><li><strong>修改提议分布（Modifying Proposal Distribution）</strong>：调整 token 选择的概率，使模型更倾向于选择推理 token，提高整体答案的合理性。</li></ul><p>这种方法在<strong>数学计算、逻辑推理、法律推理等任务</strong>上尤为重要，使得 LLM <strong>不仅能“答对”，还能“说明白”</strong>。</p><h3 id="Prompting"><a href="#Prompting" class="headerlink" title="Prompting"></a><strong>Prompting</strong></h3><p>随着我们使用 <strong>prompt engineering</strong>（提示工程）来改进输出，我们会通过更新提示（prompt）来尝试提升模型的表现。这个过程也可能推动模型去展示先前我们看到的一些<strong>reasoning</strong>（推理）过程。</p><p><strong>1. 改变 Proposal Distribution</strong></p><p>在更改 <strong>proposal distribution</strong>时，我们可以给模型提供示例（也叫做 <strong>in-context learning</strong>），让它在生成答案时模仿类似的推理风格。下面的图就展示了一个示例的情形：</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_172130.png" class=""><blockquote><ul><li><strong>图示内容</strong>：左侧是一个简单的问题 “What is 3 + 2?”，模型内部用 “Thoughts” 表示隐藏的思考过程，比如：<ol><li>First, 3 and 1 gives 4.</li><li>Then, 4 and 1 gives 5.</li><li>I believe the answer is 5.</li></ol></li><li><strong>Answer</strong>（答案）：5</li><li>右侧用红色、蓝色等不同颜色的条形或方块表示推理过程的不同部分，示意有一部分属于隐藏的推理过程（红色），以及输出结果或若干中间步骤（蓝色）。</li></ul></blockquote><p>通过类似的示例，模型在推理时就可能模仿类似的格式来进行<strong>reasoning</strong>并给出最终答案。</p><p><strong>2. “Let’s think step-by-step” 的影响</strong></p><p>我们也可以通过在提示中直接使用 “Let’s think step-by-step” 来简化上述流程。这会改变模型的 <strong>proposal distribution</strong>，让 <strong>LLM</strong>（大型语言模型）倾向于在回答之前分步骤思考。如下图所示：</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_172232.png" class=""><blockquote><ul><li><strong>图示内容</strong>：这里将提示换成 “Let’s think step-by-step”，问题仍然是 “What is 3 + 2?”。</li><li>模型产生更显式的推理过程（用红色块示意），再输出正确答案 5。</li><li>整个思路类似图1，但更加突出“分步骤思考”对最终答案生成的影响。</li></ul></blockquote><p>然而，这并不意味着模型本身已经内化了这种推理能力——它<strong>并没有从根本上学会</strong>去“反思”或“修正”错误。如果模型一开始的推理过程是错误的，那么在这种静态且线性的流程中，它往往会一直延续这个错误，而不是对自身推理进行修正。</p><hr><h3 id="STaR（Self-Taught-Reasoner）"><a href="#STaR（Self-Taught-Reasoner）" class="headerlink" title="STaR（Self-Taught Reasoner）"></a><strong>STaR（Self-Taught Reasoner）</strong></h3><p>除了通过 <strong>prompting</strong>（提示）让模型临时展示推理步骤，我们还可以让模型在训练中因为“产生正确推理步骤”而得到奖励，从而让它真正“学会”推理。这通常需要在<strong>大量带有推理过程的数据</strong>上进行训练，并结合 <strong>reinforcement learning</strong>（强化学习）来奖励特定的行为。</p><p>一个颇受争议（“much-debated”）的技术就是 <strong>STaR</strong>，即 <strong>Self-Taught Reasoner</strong>。它是让 <strong>LLM</strong> 生成自己的推理数据，再把这些数据用于对模型进行<strong>精调</strong>（<em>fine-tuning</em>）的过程。</p><p><strong>1. STaR 的流程概述</strong></p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_172455.png" class=""><ul><li>这幅图概括了 STaR 的工作原理：<ol><li><strong>Generate reasoning + answer</strong>：模型先针对输入问题生成一段 <strong>reasoning</strong>（推理）和一个 <strong>answer</strong>（答案）；<br>2a. 如果答案正确（Correct answer），则将 <strong>Question, Reasoning, Answer</strong> 作为训练样本添加到三元组数据集中（3a）；<br>  3b. 利用这些三元组数据进行 <strong>supervised fine-tuning</strong>（监督微调），让模型学会在类似情形下产出正确推理与答案。</li></ol></li></ul><p>如果模型给出了错误答案，则会触发另一条路径：</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_172608.png" class=""><ul><li>当 (2b) 模型答案错误时，我们提供正确答案作为 <strong>hint</strong>（提示），并让模型去思考“为什么这个答案是正确的”；</li><li>也就是 <strong>Generate reasoning only</strong> (why this answer is correct?)；</li><li>得到的这段新的推理依旧会被加入到三元组数据中，然后再进行 <strong>supervised fine-tuning</strong>。</li></ul><p>这里的关键要点是，我们可以通过这种方法<strong>显式</strong>地训练模型“应该如何进行推理”，而不仅仅是让它临时地模仿推理过程。我们要对模型的推理方式进行<strong>监督</strong>（<em>supervised fine-tuning</em>），从而把我们想要的推理模式“灌输”给模型。</p><p><strong>2. 自动生成合成训练样本</strong></p><p>STaR 的整个流程非常有趣，因为它会<strong>自动生成合成训练样本</strong>（<em>synthetic training examples</em>）。这些样本不仅包含问题和答案，还包含一系列推理步骤，能够帮助模型更好地学习如何“思考”。在其他研究中（例如 <strong>DeepSeek R-1</strong>），我们可以利用这些合成样本来<strong>蒸馏</strong>（<em>distill</em>，意为“提炼和保留关键信息”）推理过程到其它模型上。也就是说，一个掌握了推理能力的模型可以帮助另一个模型更快地学会类似的推理。</p><hr><p><strong>重点：</strong></p><ul><li><strong>Prompting</strong>（提示）能够影响模型的输出风格和思维过程，比如使用 “Let’s think step-by-step” 让模型显式给出推理步骤，但并不保证模型自动纠正错误。</li><li><strong>STaR</strong>（<strong>Self-Taught Reasoner</strong>）等方法则通过<strong>生成推理数据、监督微调和奖励机制</strong>，帮助模型真正学会按照指定的推理方式去思考和回答问题。</li><li>无论是哪一种方法，都可以视为对 <strong>proposal distribution</strong> 的调节：要么是提示时临时<strong>nudge</strong>（引导），要么是从训练根源上进行调教，让模型内化这种推理过程。</li><li>利用 <strong>in-context learning</strong> 提供示例，能够让模型模仿推理风格。</li><li>用 <strong>reinforcement learning</strong> 或<strong>监督微调</strong>（<strong>supervised fine-tuning</strong>）可以使模型逐渐掌握我们期望的推理模式。</li><li><strong>STaR</strong> 方法会自动收集“正确推理”数据并进行训练，使得模型在后续回答中更可能产生正确且符合要求的推理步骤。</li></ul><hr><h2 id="DeepSeek-R1"><a href="#DeepSeek-R1" class="headerlink" title="DeepSeek-R1"></a>DeepSeek-R1</h2><hr><h3 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h3><p><strong>DeepSeek-R1</strong> 是一个在推理（reasoning）模型领域的重大版本，其权重已经开源。它直接与 OpenAI 的 <strong>o1</strong> 推理模型展开竞争，并在这一领域产生了重大影响。</p><p>DeepSeek 项目在将推理功能优雅地整合进其基础模型（<strong>DeepSeek-V3-Base</strong>）方面成就卓著，采用了多种技术来完成这一目标。</p><p>有趣的是，该项目在训练过程中并未依赖额外的验证器（verifier），而且并不是单纯地依靠监督微调（supervised fine-tuning）来提炼推理行为。相反，<strong>强化学习（Reinforcement Learning, RL）</strong> 在其中扮演了重要角色。</p><p>以下我们将一起探究他们是如何在模型中训练出推理行为的！</p><hr><h3 id="2-DeepSeek-R1-Zero：推理的关键探索"><a href="#2-DeepSeek-R1-Zero：推理的关键探索" class="headerlink" title="2. DeepSeek-R1 Zero：推理的关键探索"></a>2. DeepSeek-R1 Zero：推理的关键探索</h3><p>在通往 <strong>DeepSeek-R1</strong> 的道路上，有一个名为 <strong>DeepSeek-R1 Zero</strong> 的实验性模型为这次突破打下了基础。它从 <strong>DeepSeek-V3-Base</strong> 出发，完全不使用大规模监督微调来加入推理数据，而是只依靠 <strong>强化学习</strong> 来获得推理能力。</p><h4 id="训练过程与系统提示（Prompt）"><a href="#训练过程与系统提示（Prompt）" class="headerlink" title="训练过程与系统提示（Prompt）"></a>训练过程与系统提示（Prompt）</h4><p>在此过程中，他们首先准备了一个非常直接的提示（prompt），其形式类似于系统提示（system prompt），用来作为推理管线的一部分。下文即展示了相关提示。请注意，其中明确指出了推理过程要写在 <code>&lt;think&gt;</code> 标签内、答案要写在 <code>&lt;answer&gt;</code> 标签内，但没有进一步规定推理过程应如何具体呈现或组织。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_183150.png" class=""><p>在上图中，可以看到一个简化版的对话示例（System prompt 与 User prompt）以及模型如何将<strong>推理</strong>（reasoning）放在 <code>&lt;think&gt;</code> 标签内、将<strong>答案</strong>（answer）放在 <code>&lt;answer&gt;</code> 标签内。该图突出展示了在提示（prompt）中对模型的约束：</p><ul><li><em>“The assistant first thinks about the reasoning process in the mind and then provides the user with the answer.”</em></li><li>要求使用 <code>&lt;think&gt;</code> 进行推理，使用 <code>&lt;answer&gt;</code> 进行回答。</li></ul><p>这里并未提供关于“推理过程”格式的其他例子或模板——完全由模型自己在训练中摸索出要如何输出“Chain-of-Thought”式的推理文字。</p><h4 id="强化学习奖励"><a href="#强化学习奖励" class="headerlink" title="强化学习奖励"></a>强化学习奖励</h4><p>在训练中，采用了两个基于规则（rule-based）的奖励机制：</p><ol><li><strong>准确性奖励（Accuracy rewards）</strong><br>通过测试给出的答案是否正确来进行奖励。若模型输出的答案正确，就会增加奖励。</li><li><strong>格式奖励（Format rewards）</strong><br>奖励模型对 <code>&lt;think&gt;</code> 和 <code>&lt;answer&gt;</code> 标签的正确使用。</li></ol><p>他们所使用的强化学习算法名为 <strong>Group Relative Policy Optimization（GRPO）</strong>。此算法的直观想法在于：使所有导致正确或错误答案的决策更易或更难再次出现。这些决策可能包括模型生成的某些标记（token）序列，也可能包括推理步骤本身（即思考过程）。下文给出了这一训练阶段的示意图。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_183402.png" class=""><p>在图中，重点展示了在 RL（强化学习）过程中所使用的两类奖励：</p><ul><li>“is <code>&lt;think&gt;</code> used?” —— 为使用 <code>&lt;think&gt;</code> 标签而打分。</li><li>“is <code>&lt;answer&gt;</code> used?” —— 为使用 <code>&lt;answer&gt;</code> 标签而打分。</li></ul><p>除此之外，还有对答案<strong>正确性</strong>的奖励（accuracy reward）。图中箭头所示的循环代表了在训练中不断迭代更新模型，使之越来越倾向于正确的推理方式并合乎格式要求。</p><h4 id="自发推理行为的出现"><a href="#自发推理行为的出现" class="headerlink" title="自发推理行为的出现"></a>自发推理行为的出现</h4><p>值得一提的是，研究人员并没有向模型提供任何示例来告诉它 <code>&lt;think&gt;</code> 标签中的内容应该如何书写或展开。他们仅仅告诉模型：</p><blockquote><p>“It should use <code>&lt;think&gt;</code> tags, and nothing more!”</p></blockquote><p>通过对“Chain-of-Thought”相关行为进行<strong>间接奖励</strong>（即只要推理正确、使用正确格式，就鼓励输出更完整的推理内容），模型在训练中自发地学会了越写越长的推理过程，也更易产生正确答案。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_183551.png" class=""><p>上图呈现了模型在训练过程中输出的推理长度随训练步数增加而逐渐变长的趋势。纵轴是每个响应的平均长度，横轴是训练步数。可以看到，曲线整体是向上攀升的，这表明模型不断倾向于输出更长、更详细的思考内容（Chain-of-Thought），并因此获得更高奖励。这种做法将大部分计算消耗从训练阶段（train-time compute）转移到了推理阶段（test-time compute），也就是在推理时才生成更长的思考过程。</p><p>根据研究，他们发现通过这种训练策略，模型能够自行发现最优的 Chain-of-Thought 风格的思考方式，并展现出高级的推理能力，例如：<strong>自我反思（self-reflection）</strong> 和 <strong>自我验证（self-verification）</strong>。</p><p>不过，DeepSeek-R1 Zero 的模型输出仍存在一些问题，比如可读性欠佳，且有时会混用多种语言。为了在产品化或发布级别进一步完善，研究人员提出了另一个选项，也就是在正式版本中使用的 <strong>DeepSeek R1</strong>。</p><hr><h3 id="3-深入了解-DeepSeek-R1"><a href="#3-深入了解-DeepSeek-R1" class="headerlink" title="3. 深入了解 DeepSeek-R1"></a>3. 深入了解 DeepSeek-R1</h3><p>要构建 <strong>DeepSeek-R1</strong>，作者共进行了以下五个关键步骤：</p><ol><li><strong>冷启动（Cold Start）</strong></li><li><strong>以推理为导向的强化学习（Reasoning-oriented Reinforcement Learning）</strong></li><li><strong>拒绝采样（Rejection Sampling）</strong></li><li><strong>监督微调（Supervised Fine-Tuning）</strong></li><li><strong>在所有场景下进行强化学习（Reinforcement Learning for all Scenarios）</strong></li></ol><p>接下来我们依次展开说明。</p><hr><h4 id="第一步：冷启动"><a href="#第一步：冷启动" class="headerlink" title="第一步：冷启动"></a>第一步：冷启动</h4><p>在第一步中，研究人员先使用了一个约 5000 个tokens的高质量推理数据集对 <strong>DeepSeek-V3-Base</strong> 进行微调，以避免产生可读性不佳的<strong>冷启动问题（cold start problem）</strong>。这个微调步骤可以让模型的输出更加可读，不至于在一开始就产生混乱的推理文本。下文展示了这一过程的示意图。</p><hr><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_184058.png" class=""><p>在图中可以看到：</p><ul><li>“DeepSeek-V3-Base” 通过<strong>监督微调（Supervised Fine-Tuning）</strong>的方式，引入了约 5000 条高质量推理样本。</li><li>这些样本包含了<strong>Reasoning</strong>（推理）和<strong>Answer</strong>（答案）两种部分。</li><li>该步骤目的是“防止冷启动”，即让模型在一开始就掌握基础的可读性推理。</li></ul><hr><h4 id="第二步：推理导向的强化学习"><a href="#第二步：推理导向的强化学习" class="headerlink" title="第二步：推理导向的强化学习"></a>第二步：推理导向的强化学习</h4><p>在得到一个初步微调后的模型后（上一步的成果），作者使用与 <strong>DeepSeek-V3-Zero</strong> 类似的强化学习流程对模型进行训练，但额外加入了<strong>目标语言一致性</strong>的奖励，以确保模型在推理和回答时不会混用多种语言。</p><p>除了之前提到的准确性（accuracy reward）和格式（format reward）等，还增加了<strong>语言奖励（language reward）</strong>来保证生成的语言风格或语言类型保持一致，不至于出现“中英文混杂”或“风格不稳”的现象。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_184229.png" class=""><ul><li><strong>Format reward</strong>：依旧关注 <code>&lt;think&gt;</code> 和 <code>&lt;answer&gt;</code> 的使用。</li><li><strong>Accuracy reward</strong>：检查答案是否正确，以及是否能通过相应的“单元测试”。</li><li><strong>Language reward</strong>：检查语言是否一致、通顺以及是否符合目标语言要求。</li></ul><p>这些奖励综合起来，通过强化学习（RL）循环使模型的推理和答案在可读性、准确度和语言风格方面逐渐优化。</p><hr><h4 id="第三步：拒绝采样"><a href="#第三步：拒绝采样" class="headerlink" title="第三步：拒绝采样"></a>第三步：拒绝采样</h4><p>在这一阶段，作者用<strong>第 2 步</strong>强化学习后得到的模型，来大规模生成<strong>合成推理数据</strong>，并配合 <strong>DeepSeek-V3-Base</strong> 模型来进行“评估”和“规则过滤”，最终产生约 60 万条高质量的推理样本可用于后续监督微调。同时，他们还另外生成了约 20 万条<strong>非推理样本</strong>，包含了写作、简单问答、自我认知、翻译等多种任务数据。下文总结了这一过程。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_184407.png" class=""><ul><li>左边展示了<strong>DeepSeek-V3-2</strong> 如何采样到大量<strong>Reasoning</strong>（推理）和<strong>Answer</strong>（答案），再利用基于规则的筛选和 <strong>DeepSeek-V3-Base</strong> 的判断（判断生成的内容质量），保留质量更好的推理数据（约 600,000 条）。</li><li>右边展示了<strong>非推理</strong>（non-reasoning）数据采样流程，来自 DeepSeek-V3-Base 所使用的一部分数据，总共约 200,000 条，这些数据主要涉及写作、事实性问答（factual QA）、自我认知、翻译等方面。</li></ul><p>由此，研究人员得到规模约 80 万条的“混合”数据，其中既有推理样本，也有非推理样本。</p><hr><h4 id="第四步：监督微调"><a href="#第四步：监督微调" class="headerlink" title="第四步：监督微调"></a>第四步：监督微调</h4><p>在得到上述 80 万条数据后，研究人员再次对 <strong>DeepSeek-V3-Base</strong> 进行监督微调，具体过程如下图所示。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_184526.png" class=""><ul><li>在图中，我们看到“DeepSeek-V3-Base”被用于执行<strong>监督微调（Supervised Fine-Tuning）</strong>，使用的正是前文所提到的 800,000 条<strong>高质量推理与非推理样本</strong>。</li><li>这一阶段使得模型在更大规模的数据基础上，学习到更广泛、更多样的推理形式和任务形式。</li></ul><hr><h4 id="第五步：在所有场景下的强化学习"><a href="#第五步：在所有场景下的强化学习" class="headerlink" title="第五步：在所有场景下的强化学习"></a>第五步：在所有场景下的强化学习</h4><p>在监督微调完成后，研究人员继续采用类似 <strong>DeepSeek-R1-Zero</strong> 的方法进行 <strong>RL（强化学习）</strong> 训练。但是，为了让模型更符合人类偏好，他们在这个阶段引入了更多的 <strong>“有益与无害”（helpfulness and harmlessness）</strong> 奖励信号，用来约束模型的回答。</p><p>同时，模型也被要求<strong>对推理过程进行总结（summarize）</strong>，以防止在最终输出时显示出过长、难以阅读的推理文本。这一步骤解决了前述提到的可读性问题。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_184907.png" class=""><ol><li><strong>Format reward（格式奖励）</strong>  <ul><li>是否正确使用 <code>&lt;think&gt;</code> 标签书写推理内容  </li><li>是否正确使用 <code>&lt;answer&gt;</code> 标签输出答案</li></ul></li><li><strong>Accuracy reward（准确性奖励）</strong>  <ul><li>测试输出是否能编译（“does it compile?”）  </li><li>是否能通过单元测试（“does it pass unit tests?”）</li></ul></li><li><strong>Preference rewards（偏好奖励）</strong>  <ul><li>关注 <strong>Helpfulness（有益）</strong>、<strong>Harmlessness（无害）</strong>、<strong>Human preference（人类偏好）</strong> 等  </li><li>由 RM（Reward Model） 模块来评估这些偏好指标</li></ul></li></ol><p>图中可以看到，<strong>Reasoning</strong>（推理）阶段和 <strong>Answer</strong>（答案）阶段需要分别用 <code>&lt;think&gt;</code> 和 <code>&lt;answer&gt;</code> 标签进行明确区分。同时，为了输出更为精简、可读的内容，模型也可能产生一个 <strong>Summary</strong>（总结）片段。强化学习的迭代过程会同时考虑多种奖励信号，从而不断更新模型并得到最终版本的 <strong>DeepSeek-R1</strong>。</p><p>上图中，“RM” 即 Reward Model，用于对偏好进行打分（如对话是否友善、是否符合伦理要求等），再把结果反馈给模型。</p><p>“<strong>And that’s it!<strong>”这意味着 <strong>DeepSeek-R1</strong> 实际上是 <strong>DeepSeek-V3-Base</strong> 经过监督微调（Supervised Fine-Tuning）和强化学习（RL）进一步优化而成。大量的工作都用于保证</strong>高质量数据</strong>的生成与使用，进而训练出这样一个具备强大推理能力的模型。</p><hr><h2 id="将推理知识从-DeepSeek-R1-蒸馏到其他模型"><a href="#将推理知识从-DeepSeek-R1-蒸馏到其他模型" class="headerlink" title="将推理知识从 DeepSeek-R1 蒸馏到其他模型"></a>将推理知识从 DeepSeek-R1 蒸馏到其他模型</h2><p><strong>DeepSeek-R1</strong> 拥有 <strong>6710 亿（671B）</strong> 参数。这一规模的模型在普通消费级硬件上运行存在较大难度。出于实用性考虑，作者们研究了如何将 <strong>DeepSeek-R1</strong> 的推理能力“蒸馏（distill）”到更小的模型（如 <strong>Qwen-32B</strong>）上，以便能在消费级硬件上部署和使用。</p><h3 id="蒸馏过程：Teacher-Student-框架"><a href="#蒸馏过程：Teacher-Student-框架" class="headerlink" title="蒸馏过程：Teacher-Student 框架"></a>蒸馏过程：Teacher-Student 框架</h3><p>在蒸馏过程中，<strong>DeepSeek-R1</strong> 作为教师模型（Teacher），而规模更小的模型（如 Qwen-32B）作为学生模型（Student）。二者面对相同的提示（prompt）时，分别会输出一组<strong>词元概率分布（token probability distribution）</strong>。训练时，学生模型会尽量学习并接近教师模型的输出分布。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_185245.png" class=""><ul><li>教师（DeepSeek-R1）给出自己的“proposal distribution”。例如在回答“What is 3 + 2?”时，教师模型可能倾向输出“Adding”“If”“5”“3”“4”等标记，并赋予各自不同的概率。  </li><li>学生（Qwen-32B）则会在训练中不断更新自己的概率分布，使之更接近教师的分布。</li></ul><blockquote><p><strong>额外解释</strong>：  </p><ol><li><strong>概率分布（proposal distribution）</strong>：语言模型在生成下一个词元（token）时，会输出对所有可能词元的概率估计。  </li><li><strong>蒸馏（distillation）</strong>：通过比较教师和学生的分布差异，学生会逐步调整自身参数，使其输出更接近教师模型的风格和推理倾向。</li></ol></blockquote><p>训练所使用的数据，正是之前提到的那 <strong>80 万条高质量样本</strong>——其中包含约 60 万推理样本和 20 万非推理样本。下图展示了这一数据流向： </p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_185323.png" class=""><ul><li>左侧的 <strong>Reasoning</strong>（推理）和 <strong>Answer</strong>（答案）数据，合计 80 万条。  </li><li>由 <strong>DeepSeek-R1</strong>（Teacher）生成或评估，得到对应的概率分布。  </li><li>学生模型 <strong>Qwen-32B</strong> 则根据教师的分布进行学习，最终得到一个蒸馏版本 <strong>DeepSeek-R1-Distill-Qwen-32B</strong>。</li></ul><p><img src="https://user-images.githubusercontent.com/your-image-url.png" alt="使用 80 万条高质量样本蒸馏的流程（图10）"></p><blockquote><p><strong>额外解释</strong>：  </p><ul><li>学生模型不仅仅学习了那 80 万条样本本身的输入-输出模式，也学习到 <strong>DeepSeek-R1</strong> 在面对这些数据时所“倾向”采用的推理策略和概率分布，从而在更小模型上复现类似的推理能力。  </li><li>“Distilled” 模型往往会在推理质量与计算资源之间找到更好的平衡：虽然可能在性能上略逊色于老师模型，但依然能在大多数常见任务上达到令人满意的结果，并且所需资源更低。</li></ul></blockquote><hr><h2 id="其他未成功的尝试"><a href="#其他未成功的尝试" class="headerlink" title="其他未成功的尝试"></a>其他未成功的尝试</h2><p>在研究过程中，DeepSeek 团队也曾尝试过 <strong>Process Reward Models（PRMs）</strong> 和 <strong>Monte Carlo Tree Search（MCTS）</strong> 等方法来注入推理能力，但结果并不理想：</p><ol><li><p><strong>使用 MCTS</strong>  </p><ul><li>面临的主要问题是搜索空间过于庞大，只能对节点展开进行严格限制。这样一来，效果就大打折扣。  </li><li>此外，精细化训练 Reward Model 也相当困难。</li></ul></li><li><p><strong>使用 PRMs 进行 Best-of-N 策略</strong>  </p><ul><li>如果不断重训练 Reward Model 以防止模型出现“投机取巧”（reward hacking）行为，会带来高昂的计算开销。</li></ul></li></ol><p>这些结果并不意味着这些技术无效，而是说明它们在当前大规模语言模型上的实践还有诸多限制与难点。<strong>DeepSeek-R1</strong> 之所以取得成功，更多依赖于<strong>强化学习 + 监督微调</strong>的组合，以及对大规模高质量数据的挖掘与利用。</p><hr><h2 id="总结与展望"><a href="#总结与展望" class="headerlink" title="总结与展望"></a>总结与展望</h2><p>至此，我们已经大致回顾了 <strong>DeepSeek-R1</strong> 的推理训练之旅。希望以上内容能够让你更好地理解：  </p><ul><li><strong>Test-time compute（推理时计算）</strong> 可以通过模型输出更长、更精细的思考过程（Chain-of-Thought）来取得更佳效果。  </li><li>大规模“<strong>先监督微调，再强化学习</strong>”的训练流程，以及<strong>蒸馏</strong>到更小模型的技术路线，也展现了在硬件资源和推理性能间取得平衡的方法。</li></ul><p>如前所述，<strong>DeepSeek-R1</strong> 引入了多种奖励机制，尤其是针对格式和人类偏好的奖励，来保证回答既正确又易读。“总结推理过程”（Summary）的做法也在很大程度上改善了纯文本Chain-of-Thought过长而导致的可读性问题。</p><hr><h2 id="更多资源"><a href="#更多资源" class="headerlink" title="更多资源"></a>更多资源</h2><p>如果你对 <strong>Large Language Models（LLMs）</strong> 中的推理话题感兴趣，以下资源值得参考：</p><ol><li><a href="https://newsletter.languagemodels.co/p/the-illustrated-deepseek-r1"><strong>The Illustrated DeepSeek-R1</strong></a>  <ul><li>Jay Alammar 制作的高质量可视化指南，详细介绍了 DeepSeek-R1 模型背后的原理与实现细节。</li></ul></li><li><a href="https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute"><strong>Hugging Face 的一篇博文</strong></a>  <ul><li>重点讨论了在推理阶段如何对计算量进行扩展，并给出了有趣的实验。</li></ul></li><li><a href="https://www.youtube.com/watch?v=6PEJ96k1kiw"><strong>视频 “Speculations on Test-Time Scaling”</strong></a>  <ul><li>深入探讨了在推理阶段进行各种计算扩展的常用技术细节。</li></ul></li></ol><p>此外，作者在文中也提到了一本关于大型语言模型的著作，内含更多可视化和实验结果，是想进一步研究推理 LLMs 的朋友可以深入阅读的好资料。</p><ul><li><strong>Official Website of the Book</strong>: <a href="https://www.llm-book.com/">llm-book.com</a>  </li><li><strong>Amazon 购买链接</strong>: <a href="https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961">Hands-On Large Language Models: Understanding, Building, and Optimizing LLMs</a>  </li><li><strong>GitHub 代码仓库</strong>: <a href="https://github.com/handsOnLLM/Hands-On-Large-Language-Models">handsOnLLM/Hands-On-Large-Language-Models</a></li></ul><hr><h3 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h3><p>感谢你阅读本篇关于 <strong>DeepSeek-R1</strong> 的介绍文档。通过对所有图片与文字内容的依次解读，以及对每个环节所涉及的关键技术进行了更多解释，我们希望让你对 <strong>DeepSeek-R1</strong> 的训练流程、蒸馏方法和未成功的尝试都有更加全面的了解。</p><p>在未来，随着硬件性能的提升与更成熟的训练技术出现，<strong>深度推理</strong>与<strong>模型蒸馏</strong>必将在更多实际应用场景中发挥巨大作用。让我们拭目以待！</p>]]></content>
      
      
      <categories>
          
          <category> NLP Insights </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/12/06/NLP%20Insights/Evaluation%20of%20Generation-Based%20Large%20Language%20Models%20(LLMs):%20Opportunities%20and%20Challenges%20from%20Generation%20to%20Judgment/"/>
      <url>/2024/12/06/NLP%20Insights/Evaluation%20of%20Generation-Based%20Large%20Language%20Models%20(LLMs):%20Opportunities%20and%20Challenges%20from%20Generation%20to%20Judgment/</url>
      
        <content type="html"><![CDATA[<h1 id="Evaluation-of-Generation-Based-Large-Language-Models-LLMs-Opportunities-and-Challenges-from-Generation-to-Judgment"><a href="#Evaluation-of-Generation-Based-Large-Language-Models-LLMs-Opportunities-and-Challenges-from-Generation-to-Judgment" class="headerlink" title="Evaluation of Generation-Based Large Language Models (LLMs): Opportunities and Challenges from Generation to Judgment"></a>Evaluation of Generation-Based Large Language Models (LLMs): Opportunities and Challenges from Generation to Judgment</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>Evaluation tasks in artificial intelligence (AI) and natural language processing (NLP) have long been challenging. Traditional evaluation methods, such as those based on matching or embeddings, are limited in assessing complex attributes. The recent development of large language models (LLMs) has given rise to the “LLM-as-a-Judge” paradigm, which utilizes LLMs for scoring, ranking, or selection tasks. This paper provides a comprehensive review of LLM evaluation methodologies, including their definitions, classification frameworks, benchmarks, and future research directions.</p><hr><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><h3 id="1-1-Background"><a href="#1-1-Background" class="headerlink" title="1.1 Background"></a>1.1 Background</h3><p>Evaluation is one of the core issues in machine learning and NLP. Traditional evaluation methods such as BLEU and ROUGE often rely on text overlap and lack applicability in complex scenarios. With the development of deep learning and LLMs (e.g., GPT-4), researchers have proposed the “LLM-as-a-Judge” paradigm to address the limitations of traditional evaluation methods.</p><h3 id="1-2-Research-Questions"><a href="#1-2-Research-Questions" class="headerlink" title="1.2 Research Questions"></a>1.2 Research Questions</h3><p>This paper aims to explore the following questions:</p><ul><li><strong>What do LLMs evaluate?</strong></li><li><strong>How is evaluation conducted?</strong></li><li><strong>Where are LLMs applied for evaluation?</strong></li></ul><hr><h2 id="2-Preliminary-Knowledge"><a href="#2-Preliminary-Knowledge" class="headerlink" title="2. Preliminary Knowledge"></a>2. Preliminary Knowledge</h2><h3 id="2-1-Input-Formats"><a href="#2-1-Input-Formats" class="headerlink" title="2.1 Input Formats"></a>2.1 Input Formats</h3><p>Evaluation inputs can be categorized as follows:</p><ul><li><strong>Point-Wise</strong>: Evaluation of a single sample.</li><li><strong>Pair/List-Wise</strong>: Comparative evaluation of multiple samples.</li></ul><h3 id="2-2-Output-Formats"><a href="#2-2-Output-Formats" class="headerlink" title="2.2 Output Formats"></a>2.2 Output Formats</h3><p>Evaluation outputs include:</p><ul><li><strong>Scores</strong>: Quantitative scoring of samples.</li><li><strong>Ranking</strong>: Ordering based on merit.</li><li><strong>Selection</strong>: Choosing the best option among candidates.</li></ul><hr><h2 id="3-Evaluation-Attributes"><a href="#3-Evaluation-Attributes" class="headerlink" title="3. Evaluation Attributes"></a>3. Evaluation Attributes</h2><h3 id="3-1-Helpfulness"><a href="#3-1-Helpfulness" class="headerlink" title="3.1 Helpfulness"></a>3.1 Helpfulness</h3><p>LLMs evaluate the helpfulness of responses by guiding user tasks and generating feedback, which is crucial in AI alignment.</p><h3 id="3-2-Harmlessness"><a href="#3-2-Harmlessness" class="headerlink" title="3.2 Harmlessness"></a>3.2 Harmlessness</h3><p>Evaluating the harmlessness of text is key to generating safe content. LLMs assist in data labeling or directly assess potential harmful content.</p><h3 id="3-3-Reliability"><a href="#3-3-Reliability" class="headerlink" title="3.3 Reliability"></a>3.3 Reliability</h3><p>LLMs detect factual accuracy and consistency, e.g., generating supporting evidence or conducting conversation-level reliability evaluations.</p><h3 id="3-4-Relevance"><a href="#3-4-Relevance" class="headerlink" title="3.4 Relevance"></a>3.4 Relevance</h3><p>LLMs assess the relevance of generated or retrieved content, applicable in scenarios like conversations and retrieval-augmented generation (RAG).</p><h3 id="3-5-Feasibility"><a href="#3-5-Feasibility" class="headerlink" title="3.5 Feasibility"></a>3.5 Feasibility</h3><p>In complex tasks, LLMs judge the feasibility of candidate steps or actions to optimize decision paths.</p><h3 id="3-6-Overall-Quality"><a href="#3-6-Overall-Quality" class="headerlink" title="3.6 Overall Quality"></a>3.6 Overall Quality</h3><p>By scoring across multiple dimensions, LLMs provide an overall evaluation, suitable for comprehensive comparisons in generation tasks.</p><hr><h3 id="4-Methodology"><a href="#4-Methodology" class="headerlink" title="4. Methodology"></a>4. Methodology</h3><h4 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h4><p>The methodology section focuses on optimizing the capabilities of LLMs as evaluators (LLM-as-a-Judge) through two approaches: fine-tuning and prompt engineering.</p><ol><li><strong>Fine-Tuning Techniques</strong>: Enhancing LLM judgment capabilities using supervised fine-tuning (SFT) and preference learning with labeled or synthetic feedback.</li><li><strong>Prompt Engineering</strong>: Designing effective prompt strategies, such as operation swapping, rule enhancement, and multi-agent collaboration, to improve inference and evaluation accuracy and reliability.</li></ol><hr><h4 id="4-1-Fine-Tuning-Techniques"><a href="#4-1-Fine-Tuning-Techniques" class="headerlink" title="4.1 Fine-Tuning Techniques"></a>4.1 Fine-Tuning Techniques</h4><h5 id="Data-Sources"><a href="#Data-Sources" class="headerlink" title="Data Sources"></a>Data Sources</h5><h6 id="1-Human-Labeled-Data"><a href="#1-Human-Labeled-Data" class="headerlink" title="1. Human-Labeled Data"></a>1. <strong>Human-Labeled Data</strong></h6><p>Human-labeled data provides high-quality training samples that help LLMs learn human preferences. Key studies and innovations include:</p><ol><li><p><strong>PandaLM</strong> [Wang et al., 2024h]:</p><ul><li>Collected a diverse dataset with 300,000 samples for instruction-generation tasks.</li><li>Enhanced generalization by integrating data sources like open-domain QA and dialogue generation.</li><li>Introduced standardized annotation workflows for consistency and emphasized multilingual support.</li></ul></li><li><p><strong>AspectInstruct</strong> [Liu et al., 2024a]:</p><ul><li>Introduced a dataset tailored for multi-dimensional evaluation, covering 65 tasks and 27 evaluation dimensions.</li><li>Designed a unique task segmentation mechanism for contextual understanding and dimension prioritization.</li></ul></li></ol><h6 id="2-Synthetic-Data"><a href="#2-Synthetic-Data" class="headerlink" title="2. Synthetic Data"></a>2. <strong>Synthetic Data</strong></h6><p>Synthetic data generated by LLMs reduces dependency on human labeling and expands data coverage. Key studies and innovations include:</p><ol><li><p><strong>JudgeLM</strong> [Zhu et al., 2023]:</p><ul><li>Generated a dataset with 100,000 samples, covering various instruction-generation scenarios.</li><li>Introduced task-seeding methods to ensure diversity and specificity.</li></ul></li><li><p><strong>Meta-Rewarding</strong> [Wu et al., 2024]:</p><ul><li>Proposed “meta-rewarding,” using LLM self-evaluation signals to enhance training effectiveness.</li></ul></li></ol><h5 id="Fine-Tuning-Methods"><a href="#Fine-Tuning-Methods" class="headerlink" title="Fine-Tuning Methods"></a>Fine-Tuning Methods</h5><h6 id="1-Supervised-Fine-Tuning-SFT"><a href="#1-Supervised-Fine-Tuning-SFT" class="headerlink" title="1. Supervised Fine-Tuning (SFT)"></a>1. <strong>Supervised Fine-Tuning (SFT)</strong></h6><p>SFT trains LLMs using human-labeled or synthetic data to learn evaluation criteria. Key studies include:</p><ol><li><p><strong>FLAMe</strong> [Vu et al., 2024]:</p><ul><li>Leveraged a multi-task learning framework with 5 million samples for multi-task SFT.</li><li>Unified evaluation standards across diverse tasks.</li></ul></li><li><p><strong>JSFT</strong> [Lee et al., 2024]:</p><ul><li>Combined SFT with preference learning to optimize performance on diverse evaluation tasks.</li></ul></li></ol><h6 id="2-Preference-Learning"><a href="#2-Preference-Learning" class="headerlink" title="2. Preference Learning"></a>2. <strong>Preference Learning</strong></h6><p>Preference learning optimizes LLM comparison and ranking capabilities for complex evaluations. Key studies include:</p><ol><li><p><strong>HALU-J</strong> [Wang et al., 2024a]:</p><ul><li>Employed directed preference optimization (DPO) with multi-evidence selection mechanisms.</li></ul></li><li><p><strong>Self-Taught Evaluators</strong> [Wang et al., 2024f]:</p><ul><li>Used self-generated suboptimal responses as negative samples for dynamic improvement.</li></ul></li></ol><hr><h3 id="5-Applications"><a href="#5-Applications" class="headerlink" title="5. Applications"></a>5. Applications</h3><h4 id="Overview-1"><a href="#Overview-1" class="headerlink" title="Overview"></a>Overview</h4><p>The applications of LLM-as-a-Judge have expanded from generation evaluation to alignment, retrieval, and reasoning. This section systematically introduces these applications, their specific tasks, and representative studies.</p><hr><h4 id="5-1-Evaluation"><a href="#5-1-Evaluation" class="headerlink" title="5.1 Evaluation"></a>5.1 Evaluation</h4><h5 id="Overview-2"><a href="#Overview-2" class="headerlink" title="Overview"></a>Overview</h5><p>LLM-as-a-Judge was initially applied to evaluation tasks like dialogue generation and summarization. Key studies include:</p><ol><li><p><strong>MD-Judge</strong> [Li et al., 2024f]:</p><ul><li>Evaluated safety-related Q&amp;A frameworks, focusing on harmfulness and ethical risks.</li></ul></li><li><p><strong>Chan Framework</strong> [Chan et al., 2023]:</p><ul><li>Introduced a multi-agent debate framework for improved evaluation quality.</li></ul></li><li><p><strong>ICE</strong> [Jain et al., 2023b]:</p><ul><li>Used few-shot examples for interactive multi-dimensional evaluation.</li></ul></li></ol><hr><h3 id="7-Challenges-and-Future-Directions"><a href="#7-Challenges-and-Future-Directions" class="headerlink" title="7. Challenges and Future Directions"></a>7. Challenges and Future Directions</h3><h4 id="Overview-3"><a href="#Overview-3" class="headerlink" title="Overview"></a>Overview</h4><p>Despite its powerful capabilities, LLM-as-a-Judge faces challenges such as evaluation bias, adaptability to dynamic tasks, and the potential of human-AI collaborative evaluation. This section explores these challenges and outlines future research directions.</p><h5 id="7-1-Bias-and-Vulnerabilities"><a href="#7-1-Bias-and-Vulnerabilities" class="headerlink" title="7.1 Bias and Vulnerabilities"></a>7.1 Bias and Vulnerabilities</h5><ol><li><strong>OffsetBias</strong> [Park et al., 2024]:<ul><li>Proposed a de-biasing framework to mitigate positional and content biases.</li></ul></li></ol><h5 id="7-2-Dynamic-and-Complex-Evaluations"><a href="#7-2-Dynamic-and-Complex-Evaluations" class="headerlink" title="7.2 Dynamic and Complex Evaluations"></a>7.2 Dynamic and Complex Evaluations</h5><ol><li><strong>Tree of Thought (ToT)</strong> [Yao et al., 2023a]:<ul><li>Enhanced multi-step reasoning with dynamic state evaluation mechanisms.</li></ul></li></ol><h5 id="7-3-Self-Evaluation-and-Human-AI-Collaboration"><a href="#7-3-Self-Evaluation-and-Human-AI-Collaboration" class="headerlink" title="7.3 Self-Evaluation and Human-AI Collaboration"></a>7.3 Self-Evaluation and Human-AI Collaboration</h5><ol><li><p><strong>Self-Taught Evaluators</strong> [Wang et al., 2024f]:</p><ul><li>Highlighted the potential for models to improve through self-learning mechanisms.</li></ul></li><li><p><strong>Meta-Rewarding</strong> [Wu et al., 2024]:</p><ul><li>Demonstrated the advantages of integrating self-evaluation signals into optimization.</li></ul></li></ol><hr>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>基于生成的大语言模型（LLM）评估：从生成到判断的机遇与挑战</title>
      <link href="/2024/12/06/NLP%20Insights/%E5%9F%BA%E4%BA%8E%E7%94%9F%E6%88%90%E7%9A%84%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%88LLM%EF%BC%89%E8%AF%84%E4%BC%B0%EF%BC%9A%E4%BB%8E%E7%94%9F%E6%88%90%E5%88%B0%E5%88%A4%E6%96%AD%E7%9A%84%E6%9C%BA%E9%81%87%E4%B8%8E%E6%8C%91%E6%88%98/"/>
      <url>/2024/12/06/NLP%20Insights/%E5%9F%BA%E4%BA%8E%E7%94%9F%E6%88%90%E7%9A%84%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%88LLM%EF%BC%89%E8%AF%84%E4%BC%B0%EF%BC%9A%E4%BB%8E%E7%94%9F%E6%88%90%E5%88%B0%E5%88%A4%E6%96%AD%E7%9A%84%E6%9C%BA%E9%81%87%E4%B8%8E%E6%8C%91%E6%88%98/</url>
      
        <content type="html"><![CDATA[<hr><h1 id="基于生成的大语言模型（LLM）评估：从生成到判断的机遇与挑战"><a href="#基于生成的大语言模型（LLM）评估：从生成到判断的机遇与挑战" class="headerlink" title="基于生成的大语言模型（LLM）评估：从生成到判断的机遇与挑战"></a>基于生成的大语言模型（LLM）评估：从生成到判断的机遇与挑战</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>人工智能（AI）与自然语言处理（NLP）领域中的评估任务长期面临挑战。传统的评估方法（如基于匹配或嵌入的技术）在判断复杂属性时效果有限。近期大语言模型（LLM）的发展催生了“LLM-as-a-Judge”范式，利用LLM对任务进行评分、排序或选择。本论文对LLM评估方法进行了全面综述，包括其定义、分类框架、评估基准，以及未来的研究方向。</p><hr><h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><h3 id="1-1-背景"><a href="#1-1-背景" class="headerlink" title="1.1 背景"></a>1.1 背景</h3><p>评估是机器学习和NLP的核心问题之一，传统评估方法如BLEU和ROUGE通常基于文本重叠，缺乏对复杂场景的适用性。随着深度学习和LLM的发展（如GPT-4），研究者提出了“LLM-as-a-Judge”模式，以解决传统评估的局限。</p><h3 id="1-2-研究问题"><a href="#1-2-研究问题" class="headerlink" title="1.2 研究问题"></a>1.2 研究问题</h3><p>本论文旨在探讨以下问题：</p><ul><li><strong>评估内容：LLM评估什么？</strong></li><li><strong>评估方法：如何进行评估？</strong></li><li><strong>应用场景：LLM在哪里评估？</strong></li></ul><hr><h2 id="2-预备知识"><a href="#2-预备知识" class="headerlink" title="2. 预备知识"></a>2. 预备知识</h2><h3 id="2-1-输入格式"><a href="#2-1-输入格式" class="headerlink" title="2.1 输入格式"></a>2.1 输入格式</h3><p>评估输入可分为：</p><ul><li><strong>点对点（Point-Wise）</strong>：单个样本评估。</li><li><strong>对/列表评估（Pair/List-Wise）</strong>：多个样本的比较评估。</li></ul><h3 id="2-2-输出格式"><a href="#2-2-输出格式" class="headerlink" title="2.2 输出格式"></a>2.2 输出格式</h3><p>评估输出包括：</p><ul><li><strong>评分（Score）</strong>：对样本进行量化评分。</li><li><strong>排序（Ranking）</strong>：根据优劣排序。</li><li><strong>选择（Selection）</strong>：从多个候选中选取最佳方案。</li></ul><hr><h2 id="3-评估属性"><a href="#3-评估属性" class="headerlink" title="3. 评估属性"></a>3. 评估属性</h2><h3 id="3-1-有用性（Helpfulness）"><a href="#3-1-有用性（Helpfulness）" class="headerlink" title="3.1 有用性（Helpfulness）"></a>3.1 有用性（Helpfulness）</h3><p>LLM通过指导用户任务和生成反馈，对响应的有用性进行评估。这在AI对齐（Alignment）中尤为重要。</p><h3 id="3-2-无害性（Harmlessness）"><a href="#3-2-无害性（Harmlessness）" class="headerlink" title="3.2 无害性（Harmlessness）"></a>3.2 无害性（Harmlessness）</h3><p>评估文本的无害性是生成安全内容的关键。LLM可辅助数据标注或直接评估潜在的有害内容。</p><h3 id="3-3-可靠性（Reliability）"><a href="#3-3-可靠性（Reliability）" class="headerlink" title="3.3 可靠性（Reliability）"></a>3.3 可靠性（Reliability）</h3><p>LLM可检测事实性和一致性。例如，通过生成辅助证据或进行对话级别的可靠性评估。</p><h3 id="3-4-相关性（Relevance）"><a href="#3-4-相关性（Relevance）" class="headerlink" title="3.4 相关性（Relevance）"></a>3.4 相关性（Relevance）</h3><p>LLM可评估生成或检索内容的相关性，适用于会话、检索增强生成（RAG）等场景。</p><h3 id="3-5-可行性（Feasibility）"><a href="#3-5-可行性（Feasibility）" class="headerlink" title="3.5 可行性（Feasibility）"></a>3.5 可行性（Feasibility）</h3><p>在复杂任务中，LLM可对候选步骤或行动进行可行性判断，从而优化决策路径。</p><h3 id="3-6-总体质量（Overall-Quality）"><a href="#3-6-总体质量（Overall-Quality）" class="headerlink" title="3.6 总体质量（Overall Quality）"></a>3.6 总体质量（Overall Quality）</h3><p>LLM通过多维度评分生成整体评价，适用于生成任务的综合比较。</p><hr><h3 id="4-方法论"><a href="#4-方法论" class="headerlink" title="4. 方法论"></a>4. 方法论</h3><h4 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h4><p>方法论部分主要探讨如何优化LLM作为评估者（LLM-as-a-Judge）的能力，从调优和提示技术两个方面进行阐述：</p><ol><li><strong>调优技术</strong>：通过监督微调（SFT）和偏好学习等方法，利用人工标注数据或合成反馈来增强LLM的判断能力。</li><li><strong>提示技术</strong>：设计高效的提示策略（如操作交换、规则增强、多代理协作等）以提升LLM在推理和评估过程中的准确性和可靠性。</li></ol><hr><h4 id="4-1-调优技术"><a href="#4-1-调优技术" class="headerlink" title="4.1 调优技术"></a>4.1 调优技术</h4><h5 id="数据来源"><a href="#数据来源" class="headerlink" title="数据来源"></a>数据来源</h5><h6 id="1-人工标注数据"><a href="#1-人工标注数据" class="headerlink" title="1. 人工标注数据"></a>1. <strong>人工标注数据</strong></h6><p>人工标注数据提供了高质量的训练样本，帮助LLM学习人类偏好。以下是核心研究及其创新点：</p><ol><li><p><strong>PandaLM</strong>【Wang et al., 2024h】：</p><ul><li>PandaLM项目收集了多样化的人工标注数据集，涵盖指令生成任务的300,000个样本。</li><li>作者通过整合多种数据源（如开放领域问答和对话生成）来增强模型的泛化能力。</li><li>该研究的关键创新在于引入了标准化的标注流程，以确保数据质量与一致性。</li><li>此外，PandaLM强调多语言支持，通过跨文化的数据标注提高模型的适用性。</li><li>最终，PandaLM被证明在多个评估任务上表现优异，其输出与人工评估高度相关。</li></ul></li><li><p><strong>AspectInstruct</strong>【Liu et al., 2024a】：</p><ul><li>该研究首次提出了一个针对多维度评估的指令调优数据集，涵盖65个任务和27个评估维度。</li><li>数据集中包含对话生成、摘要和数据到文本转换等复杂任务的多方面评分。</li><li>作者设计了独特的任务分割机制，使模型能够根据上下文理解并优先评估特定维度。</li><li>研究的亮点在于数据集的多样性和全面性，为多任务评估提供了新的基准。</li><li>最终，该数据集显著提升了LLM在不同评估场景中的多维度理解和评估能力。</li></ul></li></ol><h6 id="2-合成数据"><a href="#2-合成数据" class="headerlink" title="2. 合成数据"></a>2. <strong>合成数据</strong></h6><p>合成数据通过LLM生成训练样本，减少了对人工标注的依赖，同时扩展了数据覆盖范围。以下是核心研究及其创新点：</p><ol><li><p><strong>JudgeLM</strong>【Zhu et al., 2023】：</p><ul><li>研究者利用GPT-4生成包含任务种子、生成答案及相关评估的高质量数据集。</li><li>数据集中包含10万个样本，覆盖了指令生成任务的多种场景。</li><li>核心创新点在于引入了生成任务种子的方法，确保生成数据的多样性和针对性。</li><li>作者还设计了一种基于偏好学习的优化方法，以提高LLM对细粒度任务的判断能力。</li><li>研究表明，经过这种优化后的JudgeLM在多个基准测试中超越了传统方法。</li></ul></li><li><p><strong>Meta-Rewarding</strong>【Wu et al., 2024】：</p><ul><li>提出了一种新颖的“元奖励”（Meta-Rewarding）方法，通过LLM自我评估生成的判断信号增强训练效果。</li><li>该方法要求模型在生成答案后对自己的输出进行评分，从而生成偏好数据。</li><li>创新点在于采用策略模型作为评估者，显著提高了数据生成效率和质量。</li><li>此外，该研究通过逐步改进的偏好数据训练LLM，提高了其评估任务的鲁棒性。</li><li>最终，Meta-Rewarding展示了LLM自我增强能力的潜力，成为偏好学习领域的重要进展。</li></ul></li></ol><h5 id="调优方法"><a href="#调优方法" class="headerlink" title="调优方法"></a>调优方法</h5><h6 id="1-监督微调（SFT）"><a href="#1-监督微调（SFT）" class="headerlink" title="1. 监督微调（SFT）"></a>1. <strong>监督微调（SFT）</strong></h6><p>监督微调通过使用人工标注或合成数据，让LLM从示例中学习判断标准。以下是核心研究及其创新点：</p><ol><li><p><strong>FLAMe</strong>【Vu et al., 2024】：</p><ul><li>该研究提出了Foundational Large Autorater Models (FLAMe)，利用超过500万个样本进行大规模多任务监督微调。</li><li>FLAMe在多任务数据中引入了统一的评价标准，提高了模型在多样化任务中的评估能力。</li><li>创新点在于采用多任务学习框架，将多个评估维度集成到一个模型中。</li><li>作者还设计了任务分层训练策略，使模型能够逐步掌握复杂的评估任务。</li><li>实验结果表明，FLAMe在多个生成任务上的表现优于传统评估指标。</li></ul></li><li><p><strong>JSFT</strong>【Lee et al., 2024】：</p><ul><li>提出了Judge-augmented Supervised Fine-Tuning（JSFT）方法，通过扩展偏好学习数据增强微调效果。</li><li>数据集中包含点对点和对比评估任务，以全面覆盖多种评估场景。</li><li>创新点在于引入了多阶段训练策略，结合监督学习和偏好学习优化模型性能。</li><li>此外，研究者设计了简化提示机制，显著提高了模型处理复杂输入的能力。</li><li>JSFT的实验结果显示，其生成的评估结果在多个基准上超过了现有方法。</li></ul></li></ol><h6 id="2-偏好学习"><a href="#2-偏好学习" class="headerlink" title="2. 偏好学习"></a>2. <strong>偏好学习</strong></h6><p>偏好学习通过优化LLM的比较和排序能力，适用于复杂评估任务。以下是核心研究及其创新点：</p><ol><li><p><strong>HALU-J</strong>【Wang et al., 2024a】：</p><ul><li>提出了一种基于批评的偏好学习方法，专注于选择相关证据并生成详细批评。</li><li>创新点在于设计了多证据选择机制，提高了LLM的可靠性评估能力。</li><li>该方法通过Directed Preference Optimization（DPO）进行优化，使模型能够更准确地判断任务间的优劣。</li><li>HALU-J还结合了上下文推理，扩展了偏好学习的应用场景。</li><li>实验表明，HALU-J显著提升了复杂任务的评估准确性，尤其是在事实性和逻辑性判断上。</li></ul></li><li><p><strong>Self-Taught Evaluators</strong>【Wang et al., 2024f】：</p><ul><li>该研究提出了一种自学习的评估者方法，利用被扰乱的指令生成低质量数据作为偏好学习的负样本。</li><li>自学习方法通过自动生成的次优响应，提供了丰富的训练数据。</li><li>创新点在于通过动态调整偏好信号，提升了模型的适应性和通用性。</li><li>作者还设计了基于多轮交互的学习策略，使模型能够在动态环境中自我优化。</li><li>实验结果显示，Self-Taught Evaluators在多个开放式生成任务中表现优异。</li></ul></li></ol><h3 id="4-2-提示技术"><a href="#4-2-提示技术" class="headerlink" title="4.2 提示技术"></a>4.2 提示技术</h3><h4 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h4><p>提示技术（Prompting）通过设计高效的提示策略和推理流程优化LLM的评估能力。这部分探讨如何在推理阶段利用提示技术提升判断精度，减少偏差，并增强模型的评估鲁棒性。主要方法包括操作交换、规则增强、多代理协作、演示、多轮交互以及比较加速。</p><hr><h4 id="4-2-1-操作交换（Swapping-Operation）"><a href="#4-2-1-操作交换（Swapping-Operation）" class="headerlink" title="4.2.1 操作交换（Swapping Operation）"></a>4.2.1 操作交换（Swapping Operation）</h4><h5 id="概述-2"><a href="#概述-2" class="headerlink" title="概述"></a>概述</h5><p>操作交换技术通过更改候选项顺序减少评估的偏置性，确保LLM对输入顺序不敏感，从而提高评估的公平性和可靠性。</p><h6 id="1-MT-Bench【Zheng-et-al-2023】："><a href="#1-MT-Bench【Zheng-et-al-2023】：" class="headerlink" title="1. MT-Bench【Zheng et al., 2023】："></a>1. <strong>MT-Bench</strong>【Zheng et al., 2023】：</h6><ul><li>本研究首次系统性地提出操作交换技术，通过多轮评估减少LLM的顺序敏感性。</li><li>创新点在于引入“对称性检查”机制：将候选项顺序互换，若评分结果一致，则标记为稳定，否则标记为不稳定。</li><li>作者发现操作交换能够有效减少由于位置偏差导致的错误判断。</li><li>该技术应用于多任务评估中，尤其是在复杂生成任务的排序中表现突出。</li><li>MT-Bench为后续的LLM评估技术提供了一个重要的公平性基准。</li></ul><h6 id="2-Starling【Zhu-et-al-2024a】："><a href="#2-Starling【Zhu-et-al-2024a】：" class="headerlink" title="2. Starling【Zhu et al., 2024a】："></a>2. <strong>Starling</strong>【Zhu et al., 2024a】：</h6><ul><li>提出一种类似链式推理（Chain-of-Thought, CoT）的提示技术，通过全面评估所有候选项的两两关系，再总结为最终排序。</li><li>创新点在于强制模型生成所有可能的对比结果，确保评估全面且无偏。</li><li>作者还设计了一种交叉验证机制，进一步提高评估稳定性。</li><li>实验显示，这种方法显著减少了位置偏差带来的误差，特别是在排序任务中表现优异。</li><li>Starling验证了链式思维结合操作交换技术的潜力，尤其在复杂对比任务中的效果显著。</li></ul><hr><h4 id="4-2-2-规则增强（Rule-Augmentation）"><a href="#4-2-2-规则增强（Rule-Augmentation）" class="headerlink" title="4.2.2 规则增强（Rule Augmentation）"></a>4.2.2 规则增强（Rule Augmentation）</h4><h5 id="概述-3"><a href="#概述-3" class="headerlink" title="概述"></a>概述</h5><p>规则增强技术通过在提示中嵌入明确的原则、标准或参考内容，使模型能够更加系统地评估任务，从而提升评估的准确性和一致性。</p><h6 id="1-Constitutional-AI【Bai-et-al-2022】："><a href="#1-Constitutional-AI【Bai-et-al-2022】：" class="headerlink" title="1. Constitutional AI【Bai et al., 2022】："></a>1. <strong>Constitutional AI</strong>【Bai et al., 2022】：</h6><ul><li>本研究引入了“原则驱动”的规则增强方法，利用帮助性、无害性和诚实性等标准指导模型评估。</li><li>创新点在于为每个评估维度定义详细的评分标准，并通过原则约束生成内容。</li><li>作者采用多层提示设计，使LLM能够逐步推理并给出最终评估。</li><li>实验表明，这种方法显著提升了模型在复杂场景中的判断一致性。</li><li>Constitutional AI成为后续研究的重要基石，为基于规则的评估技术奠定了基础。</li></ul><h6 id="2-OAIF【Guo-et-al-2024】："><a href="#2-OAIF【Guo-et-al-2024】：" class="headerlink" title="2. OAIF【Guo et al., 2024】："></a>2. <strong>OAIF</strong>【Guo et al., 2024】：</h6><ul><li>提出了在线AI反馈（Online AI Feedback, OAIF）框架，通过实时原则指导提升模型评估的灵活性。</li><li>核心创新点在于动态调整评估规则，使模型能够适应多变的任务需求。</li><li>OAIF引入了细粒度的多维评分策略，为每个候选项生成独立的评估报告。</li><li>作者验证了这种方法在实时决策中的潜力，尤其在对话和生成任务中表现突出。</li><li>OAIF展现了规则增强的实时适应能力，为实时评估任务提供了新方向。</li></ul><hr><h4 id="4-2-3-多代理协作（Multi-agent-Collaboration）"><a href="#4-2-3-多代理协作（Multi-agent-Collaboration）" class="headerlink" title="4.2.3 多代理协作（Multi-agent Collaboration）"></a>4.2.3 多代理协作（Multi-agent Collaboration）</h4><h5 id="概述-4"><a href="#概述-4" class="headerlink" title="概述"></a>概述</h5><p>多代理协作通过组合多个LLM的评估结果，减少单一模型的偏差，提高评估的准确性和鲁棒性。这种方法强调模型之间的角色分工和合作。</p><h6 id="1-Peer-Rank-PR-【Li-et-al-2023】："><a href="#1-Peer-Rank-PR-【Li-et-al-2023】：" class="headerlink" title="1. **Peer Rank (PR)**【Li et al., 2023】："></a>1. **Peer Rank (PR)**【Li et al., 2023】：</h6><ul><li>提出了同行排名算法，整合多个LLM的对比偏好生成最终排序。</li><li>创新点在于设计了“加权投票”机制，根据模型之间的评分一致性调整权重。</li><li>该研究还探讨了代理间的协作效率和鲁棒性，提出了优化协作路径的方法。</li><li>PR的实验结果显示，其生成的评估结果在排序准确性上优于传统单模型方法。</li><li>该研究为多模型协作技术奠定了理论基础，是后续研究的重要参考。</li></ul><h6 id="2-Cascaded-Selective-Evaluation【Jung-et-al-2024】："><a href="#2-Cascaded-Selective-Evaluation【Jung-et-al-2024】：" class="headerlink" title="2. Cascaded Selective Evaluation【Jung et al., 2024】："></a>2. <strong>Cascaded Selective Evaluation</strong>【Jung et al., 2024】：</h6><ul><li>设计了级联选择评估框架，首先由较弱的模型进行初步评估，仅在需要时调用更强大的模型。</li><li>创新点在于通过分级策略优化计算成本，同时确保评估结果的高质量。</li><li>作者提出了一种交叉验证机制，结合多个代理的结果生成最终判断。</li><li>研究表明，这种级联策略在复杂任务中表现出显著的资源效率提升。</li><li>Cascaded Selective Evaluation展示了多代理协作在资源有限情况下的潜力。</li></ul><hr><h4 id="4-2-4-演示（Demonstration）"><a href="#4-2-4-演示（Demonstration）" class="headerlink" title="4.2.4 演示（Demonstration）"></a>4.2.4 演示（Demonstration）</h4><h5 id="概述-5"><a href="#概述-5" class="headerlink" title="概述"></a>概述</h5><p>演示技术利用具体的示例作为提示，帮助LLM学习评估标准。这种方法通过少量高质量样例显著提高模型的评估能力。</p><h6 id="1-ALLURE【Hasanbeig-et-al-2023】："><a href="#1-ALLURE【Hasanbeig-et-al-2023】：" class="headerlink" title="1. ALLURE【Hasanbeig et al., 2023】："></a>1. <strong>ALLURE</strong>【Hasanbeig et al., 2023】：</h6><ul><li>提出了迭代演示技术，通过在提示中加入显著偏差的示例提高模型的鲁棒性。</li><li>创新点在于采用动态演示方法，逐步更新提示以适应不同的评估任务。</li><li>研究表明，这种方法在低资源场景中表现出色，尤其是在新任务的适应性上有显著提升。</li><li>作者还探讨了如何选择代表性样例以最大化演示效果。</li><li>ALLURE验证了高质量演示样例在提升评估能力方面的重要性。</li></ul><h6 id="2-ICE【Jain-et-al-2023b】："><a href="#2-ICE【Jain-et-al-2023b】：" class="headerlink" title="2. ICE【Jain et al., 2023b】："></a>2. <strong>ICE</strong>【Jain et al., 2023b】：</h6><ul><li>提出了交互式多维评估框架，通过少量上下文示例指导LLM评估。</li><li>创新点在于将评估任务分解为多个独立维度，每个维度都有针对性的示例支持。</li><li>研究表明，ICE框架显著减少了模型在多维任务中的评估偏差。</li><li>实验结果显示，其生成的评估结果在与人工评价的一致性上达到高水平。</li><li>ICE为多维度评估任务的提示设计提供了新思路。</li></ul><hr><h4 id="4-2-5-多轮交互（Multi-turn-Interaction）"><a href="#4-2-5-多轮交互（Multi-turn-Interaction）" class="headerlink" title="4.2.5 多轮交互（Multi-turn Interaction）"></a>4.2.5 多轮交互（Multi-turn Interaction）</h4><h5 id="概述-6"><a href="#概述-6" class="headerlink" title="概述"></a>概述</h5><p>多轮交互通过动态调整提示和上下文信息，为LLM提供更全面的评估依据，适用于需要多步推理的复杂任务。</p><h6 id="1-KIEval【Yu-et-al-2024】："><a href="#1-KIEval【Yu-et-al-2024】：" class="headerlink" title="1. KIEval【Yu et al., 2024】："></a>1. <strong>KIEval</strong>【Yu et al., 2024】：</h6><ul><li>提出了知识交互式评估框架，通过动态问答生成丰富的上下文信息。</li><li>创新点在于引入了“交互者”角色，模拟用户和模型之间的动态交互。</li><li>作者设计了一种鲁棒性检测机制，避免因上下文污染导致的错误评估。</li><li>研究表明，KIEval在复杂任务中的表现优于传统静态评估方法。</li><li>此框架适用于多维度评估，特别是在需要动态调整上下文的场景中。</li></ul><h6 id="2-Auto-Arena【Zhao-et-al-2024c】："><a href="#2-Auto-Arena【Zhao-et-al-2024c】：" class="headerlink" title="2. Auto-Arena【Zhao et al., 2024c】："></a>2. <strong>Auto-Arena</strong>【Zhao et al., 2024c】：</h6><ul><li>设计了一种多轮辩论框架，允许多个模型围绕特定任务进行交互讨论。</li><li>创新点在于结合多轮问答和动态评分机制，从不同角度对候选答案进行评估。</li><li>研究表明，这种方法能够揭示候选答案间的深层次差异。</li><li>作者还探讨了如何通过动态调整辩论内容提高评估效率。</li><li>Auto-Arena展示了多轮交互在复杂评估任务中的潜力。</li></ul><hr><h4 id="4-2-6-比较加速（Comparison-Acceleration）"><a href="#4-2-6-比较加速（Comparison-Acceleration）" class="headerlink" title="4.2.6 比较加速（Comparison Acceleration）"></a>4.2.6 比较加速（Comparison Acceleration）</h4><h5 id="概述-7"><a href="#概述-7" class="headerlink" title="概述"></a>概述</h5><p>比较加速技术通过优化比较流程，减少多候选排序任务的计算成本，提高评估效率。</p><h6 id="1-Ranked-Pairing【Zhai-et-al-2024】："><a href="#1-Ranked-Pairing【Zhai-et-al-2024】：" class="headerlink" title="1. Ranked Pairing【Zhai et al., 2024】："></a>1. <strong>Ranked Pairing</strong>【Zhai et al., 2024】：</h6><ul><li>提出了一种基于基线比较的排序方法，通过对所有候选项与基线进行比较确定优劣。</li><li>创新点在于避免传统两两比较的高计算开销，显著提高了评估效率。</li><li>作者还设计了一种自适应比较策略，进一步优化排序性能。</li><li>研究表明，Ranked Pairing在大规模排序任务中表现出极高的效率。</li><li>此方法特别适用于需要快速生成排序结果的场景。</li></ul><h6 id="2-Tournament-based-Comparison【Lee-et-al-2024】："><a href="#2-Tournament-based-Comparison【Lee-et-al-2024】：" class="headerlink" title="2. Tournament-based Comparison【Lee et al., 2024】："></a>2. <strong>Tournament-based Comparison</strong>【Lee et al., 2024】：</h6><ul><li>采用锦标赛式的比较方法，构建树状结构逐层筛选最佳</li></ul><p>候选。</p><ul><li>创新点在于结合拒绝采样和多轮比较，减少了低质量候选的影响。</li><li>作者探讨了不同树结构设计对评估效率和准确性的影响。</li><li>实验结果显示，该方法在多候选任务中显著提高了计算效率。</li><li>Tournament-based Comparison展示了基于结构化比较的潜在优势。</li></ul><hr><h3 id="5-应用场景"><a href="#5-应用场景" class="headerlink" title="5. 应用场景"></a>5. 应用场景</h3><h4 id="概述-8"><a href="#概述-8" class="headerlink" title="概述"></a>概述</h4><p>LLM-as-a-Judge的应用场景已从最初的生成任务评估扩展到多个领域，包括评估、对齐（Alignment）、检索和推理（Reasoning）。这一部分系统性地介绍这些应用场景，讨论每种应用的具体任务和代表性研究。</p><hr><h4 id="5-1-评估"><a href="#5-1-评估" class="headerlink" title="5.1 评估"></a>5.1 评估</h4><h5 id="概述-9"><a href="#概述-9" class="headerlink" title="概述"></a>概述</h5><p>LLM-as-a-Judge最初的核心应用是评估任务，包括开放式生成任务（如对话生成、摘要生成）、推理任务，以及其他新兴任务。通过LLM评估，能够更精准地捕捉复杂生成任务中的质量、相关性及逻辑性等维度。</p><h6 id="1-MD-Judge【Li-et-al-2024f】："><a href="#1-MD-Judge【Li-et-al-2024f】：" class="headerlink" title="1. MD-Judge【Li et al., 2024f】："></a>1. <strong>MD-Judge</strong>【Li et al., 2024f】：</h6><ul><li>提出了专门针对安全性相关问答的评估框架，用于检测LLM在生成敏感内容时的可靠性。</li><li>创新点在于设计了多维度的安全性评估标准，包括潜在伤害性、道德风险以及语言误导性。</li><li>作者通过对比多个LLM的评估能力，验证了MD-Judge框架的鲁棒性。</li><li>此框架在评估复杂场景（如恶意问题）的生成效果方面表现突出。</li><li>MD-Judge为生成模型的安全性评估提供了一个新的基准。</li></ul><h6 id="2-Chan框架【Chan-et-al-2023】："><a href="#2-Chan框架【Chan-et-al-2023】：" class="headerlink" title="2. Chan框架【Chan et al., 2023】："></a>2. <strong>Chan框架</strong>【Chan et al., 2023】：</h6><ul><li>提出了一个多代理辩论框架，通过让多个LLM角色分别生成答案并彼此评估，提升生成任务的评估质量。</li><li>创新点在于设计了角色分工机制，不同模型在辩论中扮演不同的立场，从多角度评估候选答案。</li><li>研究表明，该框架能够显著提升评估结果的细粒度和多样性。</li><li>作者还探讨了模型间的交互如何影响评估的一致性和公平性。</li><li>Chan框架在开放式文本生成任务中的应用表明，模型之间的协作能够显著改进评估质量。</li></ul><h6 id="3-ICE【Jain-et-al-2023b】："><a href="#3-ICE【Jain-et-al-2023b】：" class="headerlink" title="3. ICE【Jain et al., 2023b】："></a>3. <strong>ICE</strong>【Jain et al., 2023b】：</h6><ul><li>提出了交互式多维评估框架，通过少量上下文示例指导LLM评估。</li><li>创新点在于将评估任务分解为多个独立维度，每个维度都有针对性的示例支持。</li><li>研究表明，ICE框架显著减少了模型在多维任务中的评估偏差。</li><li>实验结果显示，其生成的评估结果在与人工评价的一致性上达到高水平。</li><li>ICE为多维度评估任务的提示设计提供了新思路。</li></ul><hr><h4 id="5-2-对齐（Alignment）"><a href="#5-2-对齐（Alignment）" class="headerlink" title="5.2 对齐（Alignment）"></a>5.2 对齐（Alignment）</h4><h5 id="概述-10"><a href="#概述-10" class="headerlink" title="概述"></a>概述</h5><p>对齐任务的目标是通过训练或微调使LLM的生成内容更符合人类的价值观和偏好。LLM-as-a-Judge被广泛用于生成对齐数据和评估对齐效果。</p><h6 id="1-Constitutional-AI【Bai-et-al-2022】：-1"><a href="#1-Constitutional-AI【Bai-et-al-2022】：-1" class="headerlink" title="1. Constitutional AI【Bai et al., 2022】："></a>1. <strong>Constitutional AI</strong>【Bai et al., 2022】：</h6><ul><li>提出了基于原则对齐的框架，通过定义帮助性、无害性和诚实性等原则，优化生成模型的输出。</li><li>创新点在于将原则融入奖励建模过程，利用LLM生成的偏好信号构建对齐数据集。</li><li>作者通过多轮实验验证了这种基于规则的对齐方法对生成质量的显著提升。</li><li>此框架适用于各种生成任务，尤其在减少有害输出方面效果显著。</li><li>Constitutional AI的成功展示了基于规则的对齐方法的潜力。</li></ul><h6 id="2-DIRECT-RLAIF【Lee-et-al-2023】："><a href="#2-DIRECT-RLAIF【Lee-et-al-2023】：" class="headerlink" title="2. DIRECT-RLAIF【Lee et al., 2023】："></a>2. <strong>DIRECT-RLAIF</strong>【Lee et al., 2023】：</h6><ul><li>提出了一种直接强化学习对齐反馈（DIRECT-RLAIF）方法，通过较大的LLM生成偏好信号指导较小模型。</li><li>核心创新点在于利用较强的LLM模型作为动态评估者，避免传统奖励模型中存在的“奖励陈旧性”问题。</li><li>作者验证了这种方法在对齐生成任务中的有效性，特别是在开放式对话中的显著改进。</li><li>DIRECT-RLAIF为更高效的对齐方法提供了理论基础。</li><li>研究结果表明，这种方法可以在较少人工干预的情况下生成符合人类偏好的内容。</li></ul><h6 id="3-OAIF【Guo-et-al-2024】："><a href="#3-OAIF【Guo-et-al-2024】：" class="headerlink" title="3. OAIF【Guo et al., 2024】："></a>3. <strong>OAIF</strong>【Guo et al., 2024】：</h6><ul><li>提出了在线AI反馈（Online AI Feedback, OAIF）框架，通过实时原则指导提升模型评估的灵活性。</li><li>核心创新点在于动态调整评估规则，使模型能够适应多变的任务需求。</li><li>OAIF引入了细粒度的多维评分策略，为每个候选项生成独立的评估报告。</li><li>作者验证了这种方法在实时决策中的潜力，尤其在对话和生成任务中表现突出。</li><li>OAIF展现了规则增强的实时适应能力，为实时评估任务提供了新方向。</li></ul><hr><h4 id="5-3-检索（Retrieval）"><a href="#5-3-检索（Retrieval）" class="headerlink" title="5.3 检索（Retrieval）"></a>5.3 检索（Retrieval）</h4><h5 id="概述-11"><a href="#概述-11" class="headerlink" title="概述"></a>概述</h5><p>在检索场景中，LLM-as-a-Judge主要用于提升文档排序的精度和检索增强生成（RAG）的效果。通过更高效的排序算法，LLM能够在传统检索和复杂生成任务中提供更高质量的相关性评估。</p><h6 id="1-Ranked-Pairing【Zhai-et-al-2024】：-1"><a href="#1-Ranked-Pairing【Zhai-et-al-2024】：-1" class="headerlink" title="1. Ranked Pairing【Zhai et al., 2024】："></a>1. <strong>Ranked Pairing</strong>【Zhai et al., 2024】：</h6><ul><li>提出了一种基于基线比较的排序方法，通过对所有候选项与基线进行比较确定优劣。</li><li>创新点在于避免传统两两比较的高计算开销，显著提高了评估效率。</li><li>作者还设计了一种自适应比较策略，进一步优化排序性能。</li><li>研究表明，Ranked Pairing在大规模排序任务中表现出极高的效率。</li><li>此方法特别适用于需要快速生成排序结果的场景。</li></ul><h6 id="2-LLM-Eval【Lin-and-Chen-2023a】："><a href="#2-LLM-Eval【Lin-and-Chen-2023a】：" class="headerlink" title="2. LLM-Eval【Lin and Chen, 2023a】："></a>2. <strong>LLM-Eval</strong>【Lin and Chen, 2023a】：</h6><ul><li>提出了在对话生成中的相关性评估框架，利用LLM替代人工标注。</li><li>创新点在于设计了结合上下文和生成内容的提示技术，确保评估更加精确。</li><li>作者通过对比实验验证了LLM在会话相关性评估中的潜力，结果与人工标注高度一致。</li><li>此框架显著减少了评估成本，同时提升了效率。</li><li>LLM-Eval在对话生成任务中的应用表明，模型在生成评估中的角色日益重要。</li></ul><h6 id="3-ToT-Tree-of-Thought-【Yao-et-al-2023a】："><a href="#3-ToT-Tree-of-Thought-【Yao-et-al-2023a】：" class="headerlink" title="3. **ToT (Tree of Thought)**【Yao et al., 2023a】："></a>3. **ToT (Tree of Thought)**【Yao et al., 2023a】：</h6><ul><li>提出了通过树状结构增强推理能力的方法，并结合LLM进行评估。</li><li>创新点在于引入了状态评估模块，通过逐步筛选最优推理路径提升检索和生成任务的精度。</li><li>研究表明，ToT框架显著提升了复杂任务的解决能力，尤其在多步推理和决策中表现优异。</li><li>作者还提出了评估路径的动态调整机制，使LLM能够更灵活地应对多样化任务。</li><li>ToT验证了结构化评估框架在复杂任务中的有效性。</li></ul><hr><h4 id="5-4-推理（Reasoning）"><a href="#5-4-推理（Reasoning）" class="headerlink" title="5.4 推理（Reasoning）"></a>5.4 推理（Reasoning）</h4><h5 id="概述-12"><a href="#概述-12" class="headerlink" title="概述"></a>概述</h5><p>推理任务的核心是评估LLM的中间推理过程和最终答案的正确性。LLM-as-a-Judge在数学推理、时间推理和复杂逻辑推理任务中展示了显著的评估能力。</p><h6 id="1-HALU-J【Wang-et-al-2024a】："><a href="#1-HALU-J【Wang-et-al-2024a】：" class="headerlink" title="1. HALU-J【Wang et al., 2024a】："></a>1. <strong>HALU-J</strong>【Wang et al., 2024a】：</h6><ul><li>提出了一种基于批评的偏好学习方法，专注于选择相关证据并生成详细批评。</li><li>创新点在于设计了多证据选择机制，提高了LLM的可靠性评估能力。</li><li>该方法通过Directed Preference Optimization（DPO）进行优化，使模型能够更准确地判断任务间的优劣。</li><li>HALU-J还结合了上下文推理，扩展了偏好学习的应用场景。</li><li>实验表明，HALU-J显著提升了复杂任务的评估准确性，尤其是在事实性和逻辑性判断上。</li></ul><h6 id="2-KIEval【Yu-et-al-2024】："><a href="#2-KIEval【Yu-et-al-2024】：" class="headerlink" title="2. KIEval【Yu et al., 2024】："></a>2. <strong>KIEval</strong>【Yu et al., 2024】：</h6><ul><li>提出了知识交互式评估框架，通过动态问答生成丰富的上下文信息。</li><li>创新点在于引入了“交互者”角色，模拟用户和模型之间的动态交互。</li><li>作者设计了一种鲁棒性检测机制，避免因上下文污染导致的错误评估。</li><li>研究表明，KIEval在复杂任务中的表现优于传统静态评估方法。</li><li>此框架适用于多维度评估，特别是在需要动态调整上下文的场景中。</li></ul><hr><h3 id="6-评估基准"><a href="#6-评估基准" class="headerlink" title="6. 评估基准"></a>6. 评估基准</h3><h4 id="概述-13"><a href="#概述-13" class="headerlink" title="概述"></a>概述</h4><p>评估基准是验证LLM-as-a-Judge能力的重要工具。本节整理并介绍当前用于不同评估维度的基准，包括有用性、无害性、可靠性等方面的具体框架和其核心思想。这些基准覆盖了从对话生成到复杂任务推理的广泛应用场景，为后续研究提供了关键数据支持。</p><hr><h5 id="6-1-综合评估基准"><a href="#6-1-综合评估基准" class="headerlink" title="6.1 综合评估基准"></a>6.1 综合评估基准</h5><h6 id="1-SORRY-Bench【Xie-et-al-2024a】："><a href="#1-SORRY-Bench【Xie-et-al-2024a】：" class="headerlink" title="1. SORRY-Bench【Xie et al., 2024a】："></a>1. <strong>SORRY-Bench</strong>【Xie et al., 2024a】：</h6><ul><li>设计了一个专注于安全性和无害性评估的综合基准，重点测试LLM对潜在有害内容的拒绝能力。</li><li>创新点在于提供了一个多模型对比框架，包括开源和专有LLM的表现分析。</li><li>基准数据集涵盖多种潜在危险场景，如政治敏感内容和虚假信息生成。</li><li>作者还引入了动态拒绝率作为衡量指标，展示了不同模型在拒绝任务中的细粒度表现。</li><li>实验表明，小型LLM经过微调后可以在安全性评估中达到与大型模型相当的水平。</li></ul><h6 id="2-HalluJudge【Luo-et-al-2024】："><a href="#2-HalluJudge【Luo-et-al-2024】：" class="headerlink" title="2. HalluJudge【Luo et al., 2024】："></a>2. <strong>HalluJudge</strong>【Luo et al., 2024】：</h6><ul><li>提出了一个专门用于对话级事实性评估的基准，涵盖大规模对话数据集。</li><li>核心创新在于设计了一种细粒度的事实性评分机制，通过引入上下文验证生成内容的准确性。</li><li>数据集中包括多种类型的事实性错误，如数据遗漏、模糊表述和直接虚假信息。</li><li>HalluJudge还整合了自动化和人工评估方法，提高了基准的覆盖面和可靠性。</li><li>实验结果表明，HalluJudge能够显著提高LLM在对话场景中的事实性检测能力。</li></ul><hr><h5 id="6-2-专用领域评估基准"><a href="#6-2-专用领域评估基准" class="headerlink" title="6.2 专用领域评估基准"></a>6.2 专用领域评估基准</h5><h6 id="1-FaithScore【Jing-et-al-2024】："><a href="#1-FaithScore【Jing-et-al-2024】：" class="headerlink" title="1. FaithScore【Jing et al., 2024】："></a>1. <strong>FaithScore</strong>【Jing et al., 2024】：</h6><ul><li>FaithScore是第一个跨模态的可靠性评估框架，适用于文本和图像生成任务。</li><li>创新点在于设计了多模态评估方法，结合语言和视觉信号来验证生成内容的真实性。</li><li>数据集覆盖了从事实描述到跨模态推理的多个任务，测试了模型的全局一致性和细节准确性。</li><li>FaithScore还引入了多阶段评分机制，逐步分解任务以提高评估的精细化程度。</li><li>实验显示，FaithScore在多模态生成任务中的评估结果与人工评分高度一致。</li></ul><h6 id="2-GEMBA【Kocmi-and-Federmann-2023】："><a href="#2-GEMBA【Kocmi-and-Federmann-2023】：" class="headerlink" title="2. GEMBA【Kocmi and Federmann, 2023】："></a>2. <strong>GEMBA</strong>【Kocmi and Federmann, 2023】：</h6><ul><li>GEMBA基准专注于机器翻译和文本摘要任务的整体质量评估。</li><li>核心创新点在于结合BLEU等传统指标和LLM生成的综合评分，提供更全面的评估结果。</li><li>数据集中包含多种语言和领域的真实文本，覆盖多样化的任务需求。</li><li>作者设计了一种动态反馈机制，允许LLM在评估过程中进行自适应调整。</li><li>GEMBA基准的引入显著推动了机器翻译和摘要任务中LLM-as-a-Judge的应用。</li></ul><h6 id="3-Just-Eval【Lin-et-al-2023】："><a href="#3-Just-Eval【Lin-et-al-2023】：" class="headerlink" title="3. Just-Eval【Lin et al., 2023】："></a>3. <strong>Just-Eval</strong>【Lin et al., 2023】：</h6><ul><li>提出了一个基于生成内容有用性和无害性的综合基准，适用于广泛的开放式任务。</li><li>创新点在于为不同任务设计了定制化的评估标准，并结合多维评分系统生成最终评价。</li><li>数据集中涵盖了对话、问答和复杂推理等任务，验证了基准的通用性。</li><li>作者还分析了模型在不同任务和领域上的表现，提供了详细的对比结果。</li><li>Just-Eval的应用表明，评估框架需要结合任务特点进行优化，才能最大化评估的准确性。</li></ul><hr><h4 id="6-3-动态评估基准"><a href="#6-3-动态评估基准" class="headerlink" title="6.3 动态评估基准"></a>6.3 动态评估基准</h4><h6 id="1-RevisEval【Zhang-et-al-2024e】："><a href="#1-RevisEval【Zhang-et-al-2024e】：" class="headerlink" title="1. RevisEval【Zhang et al., 2024e】："></a>1. <strong>RevisEval</strong>【Zhang et al., 2024e】：</h6><ul><li>RevisEval通过引入动态自我修正机制，让LLM在生成评估之前对输出进行多次调整。</li><li>核心创新在于结合LLM的自我纠错能力，将最终输出用于多维度评估。</li><li>数据集中覆盖了对话生成、摘要和复杂推理任务，验证了基准的动态适应能力。</li><li>RevisEval引入了多轮反馈机制，允许模型在评估过程中迭代改进。</li><li>实验结果表明，动态评估能够显著提升复杂任务中评估的精确性和稳定性。</li></ul><h6 id="2-Meta-ranking【Liu-et-al-2024c】："><a href="#2-Meta-ranking【Liu-et-al-2024c】：" class="headerlink" title="2. Meta-ranking【Liu et al., 2024c】："></a>2. <strong>Meta-ranking</strong>【Liu et al., 2024c】：</h6><ul><li>Meta-ranking框架通过弱模型生成初步排序，再由强模型进行最终评估。</li><li>创新点在于使用多阶段的排名方法，提高评估效率并降低计算开销。</li><li>数据集中包含了多种任务类型，并通过实验验证了Meta-ranking的通用性。</li><li>该框架特别适用于大规模排序任务，显著减少了评估时间。</li><li>Meta-ranking展示了弱模型和强模型协作评估的潜力，是多模型评估的新方向。</li></ul><hr><h3 id="7-挑战与未来方向"><a href="#7-挑战与未来方向" class="headerlink" title="7. 挑战与未来方向"></a>7. 挑战与未来方向</h3><h4 id="概述-14"><a href="#概述-14" class="headerlink" title="概述"></a>概述</h4><p>尽管LLM-as-a-Judge在评估任务中展现了强大能力，但依然面临着多方面的挑战。主要问题包括评估偏差与脆弱性、动态与复杂任务中的适应性，以及人机协同评估的潜力。本节探讨这些挑战并提出未来的研究方向。</p><hr><h5 id="7-1-偏差与脆弱性"><a href="#7-1-偏差与脆弱性" class="headerlink" title="7.1 偏差与脆弱性"></a>7.1 偏差与脆弱性</h5><h6 id="1-OffsetBias【Park-et-al-2024】："><a href="#1-OffsetBias【Park-et-al-2024】：" class="headerlink" title="1. OffsetBias【Park et al., 2024】："></a>1. <strong>OffsetBias</strong>【Park et al., 2024】：</h6><ul><li>OffsetBias通过设计一个去偏优化框架，减少LLM在评估任务中的位置偏差和内容偏见。</li><li>创新点在于使用合成数据生成“坏”样本，通过训练模型识别并修正偏差。</li><li>作者提出了一种多维度的去偏学习机制，确保评估在不同场景下的一致性。</li><li>研究表明，OffsetBias能够显著降低模型在生成任务中的不公平表现。</li><li>此方法为减少LLM评估中的偏差问题提供了重要方向。</li></ul><h6 id="2-SORRY-Bench【Xie-et-al-2024a】："><a href="#2-SORRY-Bench【Xie-et-al-2024a】：" class="headerlink" title="2. SORRY-Bench【Xie et al., 2024a】："></a>2. <strong>SORRY-Bench</strong>【Xie et al., 2024a】：</h6><ul><li>进一步研究了模型在拒绝有害内容时可能出现的误拒绝问题。</li><li>创新点在于结合动态评分机制和拒绝数据集，分析模型在多种任务中的拒绝倾向。</li><li>作者指出，小型模型在特定场景中可能比大型模型更高效。</li><li>实验结果表明，SORRY-Bench能够帮助识别并减轻评估偏差。</li><li>此基准成为探讨评估脆弱性的一个重要工具。</li></ul><hr><h5 id="7-2-动态与复杂评估"><a href="#7-2-动态与复杂评估" class="headerlink" title="7.2 动态与复杂评估"></a>7.2 动态与复杂评估</h5><h6 id="1-Tree-of-Thought-ToT-【Yao-et-al-2023a】："><a href="#1-Tree-of-Thought-ToT-【Yao-et-al-2023a】：" class="headerlink" title="1. **Tree of Thought (ToT)**【Yao et al., 2023a】："></a>1. **Tree of Thought (ToT)**【Yao et al., 2023a】：</h6><ul><li>ToT通过树状结构优化复杂任务的多步推理和评估。</li><li>创新点在于结合动态状态评估机制，使评估更加适应复杂多变的任务需求。</li><li>数据集中覆盖了需要多步推理的复杂任务，如问答和决策优化。</li><li>实验表明，ToT框架显著提升了复杂任务的解决能力和评估准确性。</li><li>该研究为动态评估提供了新的理论和实践支持。</li></ul><h6 id="2-RAIN【Li-et-al-2024】："><a href="#2-RAIN【Li-et-al-2024】：" class="headerlink" title="2. RAIN【Li et al., 2024】："></a>2. <strong>RAIN</strong>【Li et al., 2024】：</h6><ul><li>RAIN提出了可回溯的自回归推理机制，让LLM能够在评估过程中动态修正错误。</li><li>创新点在于结合自我评估和多轮推理机制，确保最终输出的高质量。</li><li>作者还设计了一种动态调整机制，使模型能够适应不同任务的变化。</li><li>实验显示，RAIN在复杂任务中的评估能力优于传统静态方法。</li><li>此框架展示了动态评估在复杂场景中的潜力。</li></ul><hr><h5 id="7-3-自我评估与人机协同"><a href="#7-3-自我评估与人机协同" class="headerlink" title="7.3 自我评估与人机协同"></a>7.3 自我评估与人机协同</h5><h6 id="1-Self-Taught-Evaluators【Wang-et-al-2024f】："><a href="#1-Self-Taught-Evaluators【Wang-et-al-2024f】：" class="headerlink" title="1. Self-Taught Evaluators【Wang et al., 2024f】："></a>1. <strong>Self-Taught Evaluators</strong>【Wang et al., 2024f】：</h6><ul><li>提出了一种自我学习框架，模型通过生成低质量数据对自身进行动态优化。</li><li>创新点在于引入了一种动态评估机制，让模型能够逐步提升自身评估能力。</li><li>数据集中包括了多种类型的任务，为自我评估提供了广泛支持。</li><li>Self-Taught Evaluators展示了模型在无需人工干预情况下的自我提升能力。</li><li>此框架为自动化评估任务提供了新思路。</li></ul><h6 id="2-Meta-Rewarding【Wu-et-al-2024】："><a href="#2-Meta-Rewarding【Wu-et-al-2024】：" class="headerlink" title="2. Meta-Rewarding【Wu et al., 2024】："></a>2. <strong>Meta-Rewarding</strong>【Wu et al., 2024】：</h6><ul><li>Meta-Rewarding通过将LLM的自评估信号作为偏好数据，用于进一步优化模型。</li><li>创新点在于结合策略模型自我反馈，增强模型的自适应能力。</li><li>作者还探讨了如何动态调整评估策略以提高鲁棒性。</li><li>实验表明，Meta-Rewarding能够显著提升复杂任务中的评估效果。</li><li>该研究展示了人机协同评估的潜在优势。</li></ul><hr>]]></content>
      
      
      <categories>
          
          <category> NLP Insights </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Onnx </tag>
            
            <tag> Deployment </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Reflections on Identity and Subjectivity</title>
      <link href="/2024/12/03/Life%20Reflections/Reflections%20on%20Identity%20and%20Subjectivity/"/>
      <url>/2024/12/03/Life%20Reflections/Reflections%20on%20Identity%20and%20Subjectivity/</url>
      
        <content type="html"><![CDATA[<h1 id="PR-Application-Rejected-Reflections-on-Identity-and-Subjectivity"><a href="#PR-Application-Rejected-Reflections-on-Identity-and-Subjectivity" class="headerlink" title="PR Application Rejected: Reflections on Identity and Subjectivity"></a>PR Application Rejected: Reflections on Identity and Subjectivity</h1><p>When I received the news of my PR application being rejected, after a brief moment of shock, what arose within me was not merely frustration but a peculiar sense of “existential dilemma.” On the surface, it seemed like just an administrative outcome, yet it profoundly mirrored the multiple tensions between the structure of contemporary global mobility and the construction of subjectivity.</p><ul><li>Amid the tension between globalization and national sovereignty, is it even possible to affirm an individual’s identity?</li><li>Does the rejection of a PR application symbolically exclude an individual from a collective sense of belonging?</li></ul><hr><h2 id="PR-Application-From-the-Fantasy-of-Rights-to-the-Maze-of-Identity"><a href="#PR-Application-From-the-Fantasy-of-Rights-to-the-Maze-of-Identity" class="headerlink" title="PR Application: From the Fantasy of Rights to the Maze of Identity"></a>PR Application: From the Fantasy of Rights to the Maze of Identity</h2><p>Within the theoretical framework of Anthony Giddens’ <em>Modernity and Self-Identity</em>, applying for PR is not merely a pursuit of residency rights but a symbolic quest for identity stability and future possibilities. However, in the context of globalization, this pursuit often falls into what Derrida describes as the structure of <em>différance</em>: the realization of rights is perpetually deferred, and the confirmation of identity remains suspended.</p><p>In this context, rejection is tantamount to a form of <strong>symbolic violence</strong>. It not only disrupts my plans for the future but also shatters the illusion of subjectivity I held within this domain.</p><hr><h2 id="Subjectivity-vs-Institutional-Discipline"><a href="#Subjectivity-vs-Institutional-Discipline" class="headerlink" title="Subjectivity vs. Institutional Discipline"></a>Subjectivity vs. Institutional Discipline</h2><p>Bourdieu’s field theory reveals the distribution of power in social practices, and the practice of PR applications is a concrete field where power disciplines individuals. Rejection is not merely an administrative outcome but an invisible disciplining of the subject, hinting at the imbalance of power between individuals and institutions in the era of platform capitalism.</p><p>Through Foucault’s lens of discipline, this process not only constrains individuals’ <strong>physical mobility</strong> but also profoundly affects the <strong>emotional and mental freedom</strong> of individuals.</p><hr><h2 id="From-Loss-to-Reflection"><a href="#From-Loss-to-Reflection" class="headerlink" title="From Loss to Reflection"></a>From Loss to Reflection</h2><p>In a sense, rejection is not an end but an opportunity for <strong>reconstruction</strong>. Bauman’s concept of <em>liquid modernity</em> might help me interpret this failure: in a constantly fluid world, fixed identities and stable senses of belonging are scarce resources. Perhaps I need to redefine my position amid this loss and find my own meaning in the fragments of grand narratives.</p><p>As Žižek puts it: <strong>“True freedom is not about getting what you want but about confronting the trauma of reality.”</strong> The failure of my PR application may not be the end of identity but a challenge to how I reconstruct subjectivity in the face of uncertainty.</p><p>Thus, this is not an ending but a dialectical transformation: in the moment of shattered stability, perhaps lies the beginning of transcending grand narratives and rediscovering the meaning of one’s existence.</p>]]></content>
      
      
      <categories>
          
          <category> Life Reflections </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Living in Singapore </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>身份与主体性的反思</title>
      <link href="/2024/12/03/Life%20Reflections/%E8%BA%AB%E4%BB%BD%E4%B8%8E%E4%B8%BB%E4%BD%93%E6%80%A7%E7%9A%84%E5%8F%8D%E6%80%9D/"/>
      <url>/2024/12/03/Life%20Reflections/%E8%BA%AB%E4%BB%BD%E4%B8%8E%E4%B8%BB%E4%BD%93%E6%80%A7%E7%9A%84%E5%8F%8D%E6%80%9D/</url>
      
        <content type="html"><![CDATA[<h1 id="永居申请被拒：身份与主体性的反思"><a href="#永居申请被拒：身份与主体性的反思" class="headerlink" title="永居申请被拒：身份与主体性的反思"></a>永居申请被拒：身份与主体性的反思</h1><p>当我接到永居申请被拒的消息时，短暂的愣神之后，内心涌动的却并非单纯的挫败，而是一种奇异的“生存论困境”感。表面上，这似乎只是一次行政结果的体现，但其背后却深刻折射了当代全球流动性结构与主体性建构之间的多重张力。</p><ul><li>在全球化与国家主权的张力下，个体身份的确认究竟是否可能？</li><li>当永居申请被拒时，是否意味着个体被象征性地排除在某种集体意义之外？</li></ul><hr><h2 id="永居申请：从权利幻想到身份迷宫"><a href="#永居申请：从权利幻想到身份迷宫" class="headerlink" title="永居申请：从权利幻想到身份迷宫"></a>永居申请：从权利幻想到身份迷宫</h2><p>在吉登斯的“现代性与自我认同”理论框架下，永居申请不仅是一种居留权的争取，更是一种对身份稳定性与未来可能性的符号化追求。然而，在全球化语境下，这种追求往往陷入德里达所描述的“延异”结构：权利的实现总是被推迟，身份的确认总是悬置。</p><p>在此情境中，申请被拒的结果无异于一种<strong>符号暴力</strong>。它不仅断裂了我对未来的规划，也撕裂了我在这一场域中的主体性幻象。</p><hr><h2 id="主体性与制度规训的对抗"><a href="#主体性与制度规训的对抗" class="headerlink" title="主体性与制度规训的对抗"></a>主体性与制度规训的对抗</h2><p>布尔迪厄的场域理论揭示了权力在社会实践中的分布方式，而永居申请这一制度实践正是权力规训个体的具体化场域。拒绝不仅是一种行政结果，更是一种对主体的隐形规训，暗示了平台资本主义时代个体与制度之间的权力失衡。</p><p>福柯的规训视角让我们看到，这一过程不仅限制了个体的<strong>物理流动性</strong>，也深刻影响了<strong>情感与精神的自由流动</strong>。</p><hr><h2 id="从失落到反思"><a href="#从失落到反思" class="headerlink" title="从失落到反思"></a>从失落到反思</h2><p>从某种意义上说，被拒并非一种终结，而是一种<strong>重构的契机</strong>。鲍曼提出的“液态现代性”或许能帮助我理解这次失败：在一个不断流动的世界中，固定的身份和稳定的归属感本就是稀缺资源。或许，我需要在失落中重新定义自己的位置，从宏大叙事的破碎中找到属于自己的意义。</p><p>正如齐泽克所言：<strong>“真正的自由不是得到你想要的，而是面对现实的创伤。”</strong> 永居申请的失败也许不是身份的终结，而是对我如何在不确定性中重新构建主体性的终极挑战。</p><p>因此，这并非终结，而是一次辩证的转化：在稳定性破碎的瞬间，或许恰是我们超越宏大叙事、重新发现自我存在意义的开端。</p>]]></content>
      
      
      <categories>
          
          <category> Life Reflections </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 坡岛生活指北 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【Leetcode Python题解】「1346. Check If N and Its Double Exist」</title>
      <link href="/2024/12/02/Code%20Chronicles/Leetcode%20Python%E9%A2%98%E8%A7%A3%E3%80%91%E3%80%8C1346.%20Check%20If%20N%20and%20Its%20Double%20Exist%E3%80%8D/"/>
      <url>/2024/12/02/Code%20Chronicles/Leetcode%20Python%E9%A2%98%E8%A7%A3%E3%80%91%E3%80%8C1346.%20Check%20If%20N%20and%20Its%20Double%20Exist%E3%80%8D/</url>
      
        <content type="html"><![CDATA[<h1 id="【Leetcode-Python题解】「1346-Check-If-N-and-Its-Double-Exist」"><a href="#【Leetcode-Python题解】「1346-Check-If-N-and-Its-Double-Exist」" class="headerlink" title="【Leetcode Python题解】「1346. Check If N and Its Double Exist」"></a>【Leetcode Python题解】「1346. Check If N and Its Double Exist」</h1><h2 id="题目：1346-Check-If-N-and-Its-Double-Exist"><a href="#题目：1346-Check-If-N-and-Its-Double-Exist" class="headerlink" title="题目：1346. Check If N and Its Double Exist"></a>题目：<a href="https://leetcode.com/problems/check-if-n-and-its-double-exist/description/?envType=daily-question&amp;envId=2024-12-01">1346. Check If N and Its Double Exist</a></h2><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><p>给定一个整数数组 <code>arr</code>，检查是否存在两个不同的索引 <code>i</code> 和 <code>j</code>，满足：</p><ul><li><code>i != j</code></li><li><code>0 &lt;= i, j &lt; arr.length</code></li><li><code>arr[i] == 2 * arr[j]</code></li></ul><h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><p><strong>示例 1:</strong></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">输入：arr = [10,2,5,3]</span><br><span class="line">输出：true</span><br><span class="line">解释：对于 i = 0 和 j = 2，arr[i] = 10 等于 2 * 5 = 2 * arr[j]</span><br></pre></td></tr></tbody></table></figure><p><strong>示例 2:</strong></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">输入：arr = [3,1,7,11]</span><br><span class="line">输出：false</span><br><span class="line">解释：不存在满足条件的 i 和 j。</span><br></pre></td></tr></tbody></table></figure><h3 id="约束条件"><a href="#约束条件" class="headerlink" title="约束条件"></a>约束条件</h3><ul><li><code>2 &lt;= arr.length &lt;= 500</code></li><li><code>-10³ &lt;= arr[i] &lt;= 10³</code></li></ul><h2 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h2><p>这道题可以用多种方法解决，我们来分析两种主要的解法：暴力解法和哈希表解法。</p><h3 id="1-暴力解法"><a href="#1-暴力解法" class="headerlink" title="1. 暴力解法"></a>1. 暴力解法</h3><p>最直观的解法是使用两层循环，遍历所有可能的数对。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">checkIfExist</span>(<span class="params">arr</span>):</span><br><span class="line">    n = <span class="built_in">len</span>(arr)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            <span class="keyword">if</span> i != j <span class="keyword">and</span> arr[i] == <span class="number">2</span> * arr[j]:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></tbody></table></figure><p><strong>复杂度分析：</strong></p><ul><li>时间复杂度：O(n²)，其中 n 是数组长度</li><li>空间复杂度：O(1)，只使用了常数额外空间</li></ul><h3 id="2-哈希表解法"><a href="#2-哈希表解法" class="headerlink" title="2. 哈希表解法"></a>2. 哈希表解法</h3><p>使用哈希表可以显著优化时间复杂度。我们只需要一次遍历数组，同时用哈希表记录已经遇到的数字。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">checkIfExist</span>(<span class="params">arr</span>):</span><br><span class="line">    seen = <span class="built_in">set</span>()</span><br><span class="line">    <span class="keyword">for</span> num <span class="keyword">in</span> arr:</span><br><span class="line">        <span class="keyword">if</span> num * <span class="number">2</span> <span class="keyword">in</span> seen <span class="keyword">or</span> (num % <span class="number">2</span> == <span class="number">0</span> <span class="keyword">and</span> num // <span class="number">2</span> <span class="keyword">in</span> seen):</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        seen.add(num)</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></tbody></table></figure><p><strong>复杂度分析：</strong></p><ul><li>时间复杂度：O(n)，只需要遍历一次数组</li><li>空间复杂度：O(n)，需要额外的哈希表空间</li></ul><h3 id="代码优化案例"><a href="#代码优化案例" class="headerlink" title="代码优化案例"></a>代码优化案例</h3><p>让我们看一个初始版本的代码，以及如何优化它：</p><p><strong>原始版本：</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">checkIfExist</span>(<span class="params">self, arr: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        hashmap = {}</span><br><span class="line">        <span class="keyword">for</span> i, item <span class="keyword">in</span> <span class="built_in">enumerate</span>(arr):</span><br><span class="line">            hashmap[i] = item</span><br><span class="line">            <span class="keyword">if</span> item * <span class="number">2</span> <span class="keyword">in</span> hashmap.values():</span><br><span class="line">                j = <span class="built_in">next</span>(k <span class="keyword">for</span> k, v <span class="keyword">in</span> hashmap.items() <span class="keyword">if</span> v == item * <span class="number">2</span>)</span><br><span class="line">                <span class="keyword">if</span> i != j:</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">            <span class="keyword">if</span> item//<span class="number">2</span> <span class="keyword">in</span> hashmap.values() <span class="keyword">and</span> item%<span class="number">2</span>==<span class="number">0</span>:</span><br><span class="line">                j = <span class="built_in">next</span>(k <span class="keyword">for</span> k, v <span class="keyword">in</span> hashmap.items() <span class="keyword">if</span> v == item//<span class="number">2</span>)</span><br><span class="line">                <span class="keyword">if</span> i != j:</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></tbody></table></figure><p><strong>优化版本：</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">checkIfExist</span>(<span class="params">self, arr: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        seen = <span class="built_in">set</span>()</span><br><span class="line">        <span class="keyword">for</span> num <span class="keyword">in</span> arr:</span><br><span class="line">            <span class="keyword">if</span> num * <span class="number">2</span> <span class="keyword">in</span> seen <span class="keyword">or</span> (num % <span class="number">2</span> == <span class="number">0</span> <span class="keyword">and</span> num // <span class="number">2</span> <span class="keyword">in</span> seen):</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">            seen.add(num)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></tbody></table></figure><h3 id="优化要点"><a href="#优化要点" class="headerlink" title="优化要点"></a>优化要点</h3><ol><li><p><strong>数据结构选择</strong></p><ul><li>使用集合(set)替代字典(dict)</li><li>不需要存储索引信息，只关注值的存在性</li></ul></li><li><p><strong>代码简化</strong></p><ul><li>合并重复的检查逻辑</li><li>移除不必要的变量和计算</li><li>使用更简洁的条件判断</li></ul></li><li><p><strong>性能提升</strong></p><ul><li>避免使用 <code>hashmap.values()</code> 遍历</li><li>使用集合的 O(1) 查找特性</li><li>减少重复计算</li></ul></li></ol><h2 id="关键注意点"><a href="#关键注意点" class="headerlink" title="关键注意点"></a>关键注意点</h2><ol><li><p><strong>边界情况处理</strong></p><ul><li>考虑数组中有 0 的情况（0 的两倍仍然是 0）</li><li>注意负数的处理</li><li>确保不使用同一个索引（i != j）</li></ul></li><li><p><strong>数值检查</strong></p><ul><li>需要同时检查一个数的两倍和一半</li><li>检查一半时要确保数字是偶数</li></ul></li><li><p><strong>性能优化</strong></p><ul><li>使用恰当的数据结构（集合）</li><li>避免不必要的计算和遍历</li></ul></li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这道题展示了如何通过选择适当的数据结构和优化代码逻辑来提升算法的性能。从初始的暴力解法到使用哈希表，再到代码的优化，每一步都带来了显著的改进。最终的解决方案不仅运行效率高，而且代码简洁易懂。</p><p>关键是要理解：</p><ol><li>暴力解法虽然直观，但效率低下</li><li>哈希表提供了最优的时空权衡</li><li>代码优化不仅是为了效率，也是为了可读性和可维护性</li></ol>]]></content>
      
      
      <categories>
          
          <category> Code Chronicles </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> Leetcode </tag>
            
            <tag> 每日一题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Leetcode Python Solution - 2097. Valid Arrangement of Pairs</title>
      <link href="/2024/12/01/Code%20Chronicles/%5BLeetcode%20Python%20Solution%5D%202097.%20Valid%20Arrangement%20of%20Pairs/"/>
      <url>/2024/12/01/Code%20Chronicles/%5BLeetcode%20Python%20Solution%5D%202097.%20Valid%20Arrangement%20of%20Pairs/</url>
      
        <content type="html"><![CDATA[<h1 id="Leetcode-Python-Solution-2097-Valid-Arrangement-of-Pairs"><a href="#Leetcode-Python-Solution-2097-Valid-Arrangement-of-Pairs" class="headerlink" title="[Leetcode Python Solution] 2097. Valid Arrangement of Pairs"></a>[Leetcode Python Solution] 2097. Valid Arrangement of Pairs</h1><p>In this technical blog, we’ll dive deep into Leetcode Problem 2097 — <em>Valid Arrangement of Pairs</em>. We will break down the solution step by step, from understanding the problem, modeling it as a graph theory problem, to implementing the solution.</p><p>Problem Link: <a href="https://leetcode.com/problems/valid-arrangement-of-pairs/description/">2097. Valid Arrangement of Pairs</a></p><hr><h2 id="Problem-Description"><a href="#Problem-Description" class="headerlink" title="Problem Description"></a>Problem Description</h2><p>Given a 2D array <code>pairs</code> where <code>pairs[i] = [start, end]</code>, you need to rearrange these pairs so that for adjacent pairs <code>[start1, end1]</code> and <code>[start2, end2]</code>, the condition <code>end1 == start2</code> holds.</p><p>It is guaranteed that at least one valid arrangement exists.</p><h3 id="Examples"><a href="#Examples" class="headerlink" title="Examples"></a>Examples</h3><h4 id="Example-1"><a href="#Example-1" class="headerlink" title="Example 1"></a>Example 1</h4><p><strong>Input:</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pairs = [[<span class="number">5</span>,<span class="number">1</span>],[<span class="number">4</span>,<span class="number">5</span>],[<span class="number">11</span>,<span class="number">9</span>],[<span class="number">9</span>,<span class="number">4</span>]]</span><br></pre></td></tr></tbody></table></figure><p><strong>Output:</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">11</span>,<span class="number">9</span>],[<span class="number">9</span>,<span class="number">4</span>],[<span class="number">4</span>,<span class="number">5</span>],[<span class="number">5</span>,<span class="number">1</span>]]</span><br></pre></td></tr></tbody></table></figure><p><strong>Explanation:</strong><br>The arrangement satisfies:</p><ul><li><code>end0 = 9 == 9 = start1</code></li><li><code>end1 = 4 == 4 = start2</code></li><li><code>end2 = 5 == 5 = start3</code></li></ul><h4 id="Example-2"><a href="#Example-2" class="headerlink" title="Example 2"></a>Example 2</h4><p><strong>Input:</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pairs = [[<span class="number">1</span>,<span class="number">3</span>],[<span class="number">3</span>,<span class="number">2</span>],[<span class="number">2</span>,<span class="number">1</span>]]</span><br></pre></td></tr></tbody></table></figure><p><strong>Output:</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">1</span>,<span class="number">3</span>],[<span class="number">3</span>,<span class="number">2</span>],[<span class="number">2</span>,<span class="number">1</span>]]</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="Problem-Modeling-Eulerian-Path-Problem"><a href="#Problem-Modeling-Eulerian-Path-Problem" class="headerlink" title="Problem Modeling: Eulerian Path Problem"></a>Problem Modeling: Eulerian Path Problem</h2><p>This problem can be modeled as an <strong>Eulerian Path Problem</strong> in graph theory. Each <code>pair [start, end]</code> is treated as a directed edge from <code>start</code> to <code>end</code>, and we aim to find a path that traverses all edges while satisfying the given condition.</p><h3 id="What-is-an-Eulerian-Path"><a href="#What-is-an-Eulerian-Path" class="headerlink" title="What is an Eulerian Path?"></a>What is an Eulerian Path?</h3><ul><li><strong>Definition</strong>: An Eulerian path is a path in a graph that visits every edge exactly once.</li><li><strong>Conditions</strong>:<ol><li>An Eulerian path exists if and only if there are exactly two nodes in the graph with unbalanced in-degrees and out-degrees:<ul><li>Start node: the node where <code>out-degree - in-degree = 1</code>.</li><li>End node: the node where <code>in-degree - out-degree = 1</code>.</li></ul></li><li>If all nodes have equal in-degrees and out-degrees, the graph contains an Eulerian circuit, and the path can start at any node.</li></ol></li></ul><hr><h2 id="Solution-Approach"><a href="#Solution-Approach" class="headerlink" title="Solution Approach"></a>Solution Approach</h2><h3 id="1-Graph-Construction"><a href="#1-Graph-Construction" class="headerlink" title="1. Graph Construction"></a>1. Graph Construction</h3><p>Model each <code>pair [start, end]</code> as a directed edge:</p><ul><li>Nodes: <code>start</code> and <code>end</code>.</li><li>Edges: Directed edge from <code>start</code> to <code>end</code>.</li></ul><p>Simultaneously, calculate the <strong>in-degrees</strong> and <strong>out-degrees</strong> of each node to identify the starting node.</p><h3 id="2-Finding-the-Start-Node"><a href="#2-Finding-the-Start-Node" class="headerlink" title="2. Finding the Start Node"></a>2. Finding the Start Node</h3><p>Using the in-degree and out-degree counts:</p><ul><li>A node with <code>out-degree - in-degree = 1</code> is the start node.</li><li>If no such node exists, the graph contains an Eulerian circuit, and we can start from any node.</li></ul><h3 id="3-Using-Hierholzer’s-Algorithm-to-Find-the-Path"><a href="#3-Using-Hierholzer’s-Algorithm-to-Find-the-Path" class="headerlink" title="3. Using Hierholzer’s Algorithm to Find the Path"></a>3. Using Hierholzer’s Algorithm to Find the Path</h3><p><strong>Hierholzer’s Algorithm</strong> is used to find an Eulerian path:</p><ol><li>Start at the chosen node and follow any unvisited edge.</li><li>Continue until reaching a dead end (a node with no outgoing edges).</li><li>Backtrack and record the path.</li><li>Reverse the recorded path to get the correct order.</li></ol><hr><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h2><p>Here’s the complete Python solution:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">validArrangement</span>(<span class="params">pairs</span>):</span><br><span class="line">    <span class="comment"># Construct the graph</span></span><br><span class="line">    graph = defaultdict(<span class="built_in">list</span>)</span><br><span class="line">    in_degree = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">    out_degree = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> start, end <span class="keyword">in</span> pairs:</span><br><span class="line">        graph[start].append(end)</span><br><span class="line">        out_degree[start] += <span class="number">1</span></span><br><span class="line">        in_degree[end] += <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Find the starting node</span></span><br><span class="line">    start_node = pairs[<span class="number">0</span>][<span class="number">0</span>]  <span class="comment"># Default start node</span></span><br><span class="line">    <span class="keyword">for</span> node <span class="keyword">in</span> graph:</span><br><span class="line">        <span class="keyword">if</span> out_degree[node] - in_degree[node] == <span class="number">1</span>:</span><br><span class="line">            start_node = node</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Hierholzer's Algorithm</span></span><br><span class="line">    path = []</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">current_node</span>):</span><br><span class="line">        <span class="keyword">while</span> graph[current_node]:</span><br><span class="line">            next_node = graph[current_node].pop()</span><br><span class="line">            dfs(next_node)</span><br><span class="line">            path.append([current_node, next_node])</span><br><span class="line">    </span><br><span class="line">    dfs(start_node)</span><br><span class="line">    <span class="keyword">return</span> path[::-<span class="number">1</span>]</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="Code-Explanation"><a href="#Code-Explanation" class="headerlink" title="Code Explanation"></a>Code Explanation</h2><h3 id="Graph-Construction"><a href="#Graph-Construction" class="headerlink" title="Graph Construction"></a>Graph Construction</h3><p>Using <code>defaultdict</code> to store the adjacency list and count the in-degrees and out-degrees:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">graph = defaultdict(<span class="built_in">list</span>)</span><br><span class="line">in_degree = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">out_degree = defaultdict(<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> start, end <span class="keyword">in</span> pairs:</span><br><span class="line">    graph[start].append(end)</span><br><span class="line">    out_degree[start] += <span class="number">1</span></span><br><span class="line">    in_degree[end] += <span class="number">1</span></span><br></pre></td></tr></tbody></table></figure><h3 id="Finding-the-Start-Node"><a href="#Finding-the-Start-Node" class="headerlink" title="Finding the Start Node"></a>Finding the Start Node</h3><p>Based on the rules for Eulerian paths:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">start_node = pairs[<span class="number">0</span>][<span class="number">0</span>]  <span class="comment"># Default value</span></span><br><span class="line"><span class="keyword">for</span> node <span class="keyword">in</span> graph:</span><br><span class="line">    <span class="keyword">if</span> out_degree[node] - in_degree[node] == <span class="number">1</span>:</span><br><span class="line">        start_node = node</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></tbody></table></figure><h3 id="Hierholzer’s-Algorithm"><a href="#Hierholzer’s-Algorithm" class="headerlink" title="Hierholzer’s Algorithm"></a>Hierholzer’s Algorithm</h3><p>Using a recursive DFS to find the Eulerian path:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">path = []  <span class="comment"># To store the path</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">current_node</span>):</span><br><span class="line">    <span class="keyword">while</span> graph[current_node]:  <span class="comment"># While there are outgoing edges</span></span><br><span class="line">        next_node = graph[current_node].pop()  <span class="comment"># Remove edge</span></span><br><span class="line">        dfs(next_node)</span><br><span class="line">        path.append([current_node, next_node])  <span class="comment"># Record path</span></span><br></pre></td></tr></tbody></table></figure><hr><h2 id="Example-Execution"><a href="#Example-Execution" class="headerlink" title="Example Execution"></a>Example Execution</h2><p>For <code>pairs = [[5,1],[4,5],[11,9],[9,4]]</code>:</p><h3 id="Graph-State"><a href="#Graph-State" class="headerlink" title="Graph State"></a>Graph State</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">11 → 9</span><br><span class="line"> 9 → 4</span><br><span class="line"> 4 → 5</span><br><span class="line"> 5 → 1</span><br></pre></td></tr></tbody></table></figure><h3 id="Execution-Process"><a href="#Execution-Process" class="headerlink" title="Execution Process"></a>Execution Process</h3><ol><li><p>Start DFS from node <code>11</code>:</p><ul><li>Visit <code>11 → 9</code> and remove the edge.</li><li>Visit <code>9 → 4</code> and remove the edge.</li><li>Visit <code>4 → 5</code> and remove the edge.</li><li>Visit <code>5 → 1</code> and remove the edge.</li></ul></li><li><p>Backtrack to record the path:</p><ul><li><code>path = [[5,1], [4,5], [9,4], [11,9]]</code>.</li></ul></li><li><p>Reverse the path for the final result:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">11</span>,<span class="number">9</span>], [<span class="number">9</span>,<span class="number">4</span>], [<span class="number">4</span>,<span class="number">5</span>], [<span class="number">5</span>,<span class="number">1</span>]]</span><br></pre></td></tr></tbody></table></figure></li></ol><hr><h2 id="Time-and-Space-Complexity"><a href="#Time-and-Space-Complexity" class="headerlink" title="Time and Space Complexity"></a>Time and Space Complexity</h2><ul><li><strong>Time Complexity</strong>: <code>O(E)</code>, where <code>E</code> is the number of edges.<ul><li>Constructing the graph: <code>O(E)</code>.</li><li>DFS traversal: <code>O(E)</code>.</li></ul></li><li><strong>Space Complexity</strong>: <code>O(E)</code> for storing the adjacency list and result path.</li></ul><hr><h2 id="Python-Tips-and-Tricks"><a href="#Python-Tips-and-Tricks" class="headerlink" title="Python Tips and Tricks"></a>Python Tips and Tricks</h2><h3 id="1-Closures"><a href="#1-Closures" class="headerlink" title="1. Closures"></a>1. Closures</h3><p>A <strong>closure</strong> allows inner functions to access variables from the outer function, even after the outer function has finished executing.</p><h4 id="Example"><a href="#Example" class="headerlink" title="Example:"></a>Example:</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">outer</span>():</span><br><span class="line">    x = <span class="number">10</span>  <span class="comment"># Outer variable</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">inner</span>():</span><br><span class="line">        <span class="keyword">nonlocal</span> x  <span class="comment"># Use the outer variable</span></span><br><span class="line">        x += <span class="number">1</span></span><br><span class="line">        <span class="built_in">print</span>(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> inner</span><br><span class="line"></span><br><span class="line">closure_func = outer()  <span class="comment"># Returns the inner function</span></span><br><span class="line">closure_func()  <span class="comment"># Outputs 11</span></span><br><span class="line">closure_func()  <span class="comment"># Outputs 12</span></span><br></pre></td></tr></tbody></table></figure><p>In the solution, <code>dfs()</code> uses a closure to access and modify the <code>path</code> list without passing it explicitly.</p><hr><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>This problem showcases how to model a graph problem as an Eulerian path and use Hierholzer’s algorithm for an efficient solution. Such graph theory techniques provide a robust framework for solving similar problems.</p><p>Feel free to leave questions or share your thoughts!</p>]]></content>
      
      
      <categories>
          
          <category> Code Chronicles </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> Leetcode </tag>
            
            <tag> Daily Challenge </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【Leetcode Python题解】「2097. Valid Arrangement of Pairs」</title>
      <link href="/2024/12/01/Code%20Chronicles/%E3%80%90Leetcode%20Python%E9%A2%98%E8%A7%A3%E3%80%91%E3%80%8C2097.%20Valid%20Arrangement%20of%20Pairs%E3%80%8D/"/>
      <url>/2024/12/01/Code%20Chronicles/%E3%80%90Leetcode%20Python%E9%A2%98%E8%A7%A3%E3%80%91%E3%80%8C2097.%20Valid%20Arrangement%20of%20Pairs%E3%80%8D/</url>
      
        <content type="html"><![CDATA[<h1 id="【Leetcode-Python题解】「2097-Valid-Arrangement-of-Pairs」"><a href="#【Leetcode-Python题解】「2097-Valid-Arrangement-of-Pairs」" class="headerlink" title="【Leetcode Python题解】「2097. Valid Arrangement of Pairs」"></a>【Leetcode Python题解】「2097. Valid Arrangement of Pairs」</h1><h2 id="在这篇技术博客中，我们将深入解析-LeetCode-的第-2097-题-——-Valid-Arrangement-of-Pairs，并全面介绍如何从题意理解、图论建模到算法实现逐步解决问题。题目：2097-Valid-Arrangement-of-Pairs"><a href="#在这篇技术博客中，我们将深入解析-LeetCode-的第-2097-题-——-Valid-Arrangement-of-Pairs，并全面介绍如何从题意理解、图论建模到算法实现逐步解决问题。题目：2097-Valid-Arrangement-of-Pairs" class="headerlink" title="在这篇技术博客中，我们将深入解析 LeetCode 的第 2097 题 —— Valid Arrangement of Pairs，并全面介绍如何从题意理解、图论建模到算法实现逐步解决问题。题目：2097. Valid Arrangement of Pairs"></a>在这篇技术博客中，我们将深入解析 LeetCode 的第 2097 题 —— <em>Valid Arrangement of Pairs</em>，并全面介绍如何从题意理解、图论建模到算法实现逐步解决问题。<br>题目：<a href="https://leetcode.com/problems/valid-arrangement-of-pairs/description/">2097. Valid Arrangement of Pairs</a></h2><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>给定一个二维数组 <code>pairs</code>，其中 <code>pairs[i] = [start, end]</code>，我们需要重新排列这些数字对，使得相邻的两个数字对 <code>[start1, end1]</code> 和 <code>[start2, end2]</code> 满足以下条件：</p><ul><li><code>end1 == start2</code>。</li></ul><p>输入数据保证一定存在这样一种合法的排列方式。</p><h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><h4 id="示例-1"><a href="#示例-1" class="headerlink" title="示例 1"></a>示例 1</h4><p><strong>输入：</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pairs = [[<span class="number">5</span>,<span class="number">1</span>],[<span class="number">4</span>,<span class="number">5</span>],[<span class="number">11</span>,<span class="number">9</span>],[<span class="number">9</span>,<span class="number">4</span>]]</span><br></pre></td></tr></tbody></table></figure><p><strong>输出：</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">11</span>,<span class="number">9</span>],[<span class="number">9</span>,<span class="number">4</span>],[<span class="number">4</span>,<span class="number">5</span>],[<span class="number">5</span>,<span class="number">1</span>]]</span><br></pre></td></tr></tbody></table></figure><p><strong>解释：</strong><br>排列后满足条件：</p><ul><li><code>end0 = 9 == 9 = start1</code></li><li><code>end1 = 4 == 4 = start2</code></li><li><code>end2 = 5 == 5 = start3</code></li></ul><h4 id="示例-2"><a href="#示例-2" class="headerlink" title="示例 2"></a>示例 2</h4><p><strong>输入：</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pairs = [[<span class="number">1</span>,<span class="number">3</span>],[<span class="number">3</span>,<span class="number">2</span>],[<span class="number">2</span>,<span class="number">1</span>]]</span><br></pre></td></tr></tbody></table></figure><p><strong>输出：</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">1</span>,<span class="number">3</span>],[<span class="number">3</span>,<span class="number">2</span>],[<span class="number">2</span>,<span class="number">1</span>]]</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="问题建模：欧拉路径问题"><a href="#问题建模：欧拉路径问题" class="headerlink" title="问题建模：欧拉路径问题"></a>问题建模：欧拉路径问题</h2><p>这道题的本质是一个图论中的欧拉路径问题。我们将每个 <code>pair [start, end]</code> 看作一条从 <code>start</code> 到 <code>end</code> 的有向边，并试图找到一条路径能遍历所有边且满足条件。</p><h3 id="什么是欧拉路径？"><a href="#什么是欧拉路径？" class="headerlink" title="什么是欧拉路径？"></a>什么是欧拉路径？</h3><ul><li><strong>定义</strong>：欧拉路径是一条路径，它能遍历图中每条边恰好一次。</li><li><strong>条件</strong>：<ol><li>如果图中有且仅有两个节点的入度和出度不相等，则可以存在欧拉路径。<ul><li>起点：出度比入度大 1 的节点。</li><li>终点：入度比出度大 1 的节点。</li></ul></li><li>如果所有节点的入度等于出度，则图中存在欧拉回路。</li></ol></li></ul><hr><h2 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h2><h3 id="1-图的构建"><a href="#1-图的构建" class="headerlink" title="1. 图的构建"></a>1. 图的构建</h3><p>将每个 <code>pair [start, end]</code> 建模为有向边：</p><ul><li>节点为 <code>start</code> 和 <code>end</code>。</li><li>边为从 <code>start</code> 到 <code>end</code>。</li></ul><p>同时统计每个节点的 <strong>入度</strong> 和 <strong>出度</strong>，用于后续判断起点。</p><h3 id="2-寻找起点"><a href="#2-寻找起点" class="headerlink" title="2. 寻找起点"></a>2. 寻找起点</h3><p>通过入度和出度的统计：</p><ul><li>出度 - 入度 = 1 的节点是路径的起点。</li><li>如果没有这样的节点，说明图中存在欧拉回路，可从任意节点开始。</li></ul><h3 id="3-Hierholzer-算法找路径"><a href="#3-Hierholzer-算法找路径" class="headerlink" title="3. Hierholzer 算法找路径"></a>3. Hierholzer 算法找路径</h3><p><strong>Hierholzer 算法</strong> 用于寻找欧拉路径，其核心步骤：</p><ol><li>从起点开始，任意选择一条未访问的边走。</li><li>一直走直到走到死胡同（当前节点没有出边）。</li><li>回溯过程中记录路径。</li><li>最终记录的路径需要反转才能得到正确的顺序。</li></ol><hr><h2 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a>算法实现</h2><p>下面是 Python 实现的完整代码：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">validArrangement</span>(<span class="params">pairs</span>):</span><br><span class="line">    <span class="comment"># 构建图</span></span><br><span class="line">    graph = defaultdict(<span class="built_in">list</span>)</span><br><span class="line">    in_degree = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">    out_degree = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> start, end <span class="keyword">in</span> pairs:</span><br><span class="line">        graph[start].append(end)</span><br><span class="line">        out_degree[start] += <span class="number">1</span></span><br><span class="line">        in_degree[end] += <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 寻找起点</span></span><br><span class="line">    start_node = pairs[<span class="number">0</span>][<span class="number">0</span>]  <span class="comment"># 默认起点</span></span><br><span class="line">    <span class="keyword">for</span> node <span class="keyword">in</span> graph:</span><br><span class="line">        <span class="keyword">if</span> out_degree[node] - in_degree[node] == <span class="number">1</span>:</span><br><span class="line">            start_node = node</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Hierholzer算法</span></span><br><span class="line">    path = []</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">current_node</span>):</span><br><span class="line">        <span class="keyword">while</span> graph[current_node]:</span><br><span class="line">            next_node = graph[current_node].pop()</span><br><span class="line">            dfs(next_node)</span><br><span class="line">            path.append([current_node, next_node])</span><br><span class="line">    </span><br><span class="line">    dfs(start_node)</span><br><span class="line">    <span class="keyword">return</span> path[::-<span class="number">1</span>]</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="代码解析"><a href="#代码解析" class="headerlink" title="代码解析"></a>代码解析</h2><h3 id="图的构建"><a href="#图的构建" class="headerlink" title="图的构建"></a>图的构建</h3><p>使用 <code>defaultdict</code> 来存储图的邻接表，以及统计每个节点的入度和出度：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">graph = defaultdict(<span class="built_in">list</span>)</span><br><span class="line">in_degree = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">out_degree = defaultdict(<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> start, end <span class="keyword">in</span> pairs:</span><br><span class="line">    graph[start].append(end)</span><br><span class="line">    out_degree[start] += <span class="number">1</span></span><br><span class="line">    in_degree[end] += <span class="number">1</span></span><br></pre></td></tr></tbody></table></figure><h3 id="寻找起点"><a href="#寻找起点" class="headerlink" title="寻找起点"></a>寻找起点</h3><p>根据入度和出度的统计规则：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">start_node = pairs[<span class="number">0</span>][<span class="number">0</span>]  <span class="comment"># 默认值</span></span><br><span class="line"><span class="keyword">for</span> node <span class="keyword">in</span> graph:</span><br><span class="line">    <span class="keyword">if</span> out_degree[node] - in_degree[node] == <span class="number">1</span>:</span><br><span class="line">        start_node = node</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></tbody></table></figure><h3 id="Hierholzer算法"><a href="#Hierholzer算法" class="headerlink" title="Hierholzer算法"></a>Hierholzer算法</h3><p>通过深度优先搜索（DFS）找到路径：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">path = []  <span class="comment"># 存储路径</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">current_node</span>):</span><br><span class="line">    <span class="keyword">while</span> graph[current_node]:  <span class="comment"># 当前节点还有出边</span></span><br><span class="line">        next_node = graph[current_node].pop()  <span class="comment"># 获取并删除边</span></span><br><span class="line">        dfs(next_node)</span><br><span class="line">        path.append([current_node, next_node])  <span class="comment"># 记录路径</span></span><br></pre></td></tr></tbody></table></figure><hr><h2 id="示例运行"><a href="#示例运行" class="headerlink" title="示例运行"></a>示例运行</h2><p>以 <code>pairs = [[5,1],[4,5],[11,9],[9,4]]</code> 为例：</p><h3 id="图的状态"><a href="#图的状态" class="headerlink" title="图的状态"></a>图的状态</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">11 → 9</span><br><span class="line"> 9 → 4</span><br><span class="line"> 4 → 5</span><br><span class="line"> 5 → 1</span><br></pre></td></tr></tbody></table></figure><h3 id="执行过程"><a href="#执行过程" class="headerlink" title="执行过程"></a>执行过程</h3><ol><li><p>从节点 <code>11</code> 开始，DFS 遍历：</p><ul><li>走 <code>11 → 9</code>，移除边。</li><li>走 <code>9 → 4</code>，移除边。</li><li>走 <code>4 → 5</code>，移除边。</li><li>走 <code>5 → 1</code>，移除边。</li></ul></li><li><p>回溯记录路径：</p><ul><li><code>path = [[5,1], [4,5], [9,4], [11,9]]</code></li></ul></li><li><p>反转路径得到最终结果：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">11</span>,<span class="number">9</span>], [<span class="number">9</span>,<span class="number">4</span>], [<span class="number">4</span>,<span class="number">5</span>], [<span class="number">5</span>,<span class="number">1</span>]]</span><br></pre></td></tr></tbody></table></figure></li></ol><hr><h2 id="算法复杂度"><a href="#算法复杂度" class="headerlink" title="算法复杂度"></a>算法复杂度</h2><ul><li><strong>时间复杂度</strong>：<code>O(E)</code>，其中 <code>E</code> 是边的数量。<ul><li>构建图需要 <code>O(E)</code>。</li><li>DFS 遍历每条边需要 <code>O(E)</code>。</li></ul></li><li><strong>空间复杂度</strong>：<code>O(E)</code>，用于存储图的邻接表和结果路径。</li></ul><hr><h2 id="Python-技巧补充"><a href="#Python-技巧补充" class="headerlink" title="Python 技巧补充"></a>Python 技巧补充</h2><p>最后，我们补充一些代码中用到的一些 Python 技巧。</p><h3 id="1-闭包（Closure）"><a href="#1-闭包（Closure）" class="headerlink" title="1. 闭包（Closure）"></a>1. 闭包（Closure）</h3><p><strong>闭包</strong> 是指在一个函数内部定义另一个函数时，内部函数可以访问外部函数的变量，即使外部函数已经执行完毕。 </p><p>在代码中，闭包的作用是通过内部函数访问和修改外部作用域的变量，而无需通过函数参数显式传递。</p><h4 id="示例："><a href="#示例：" class="headerlink" title="示例："></a>示例：</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">outer</span>():</span><br><span class="line">    x = <span class="number">10</span>  <span class="comment"># 外部变量</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">inner</span>():</span><br><span class="line">        <span class="keyword">nonlocal</span> x  <span class="comment"># 指定使用外部变量</span></span><br><span class="line">        x += <span class="number">1</span></span><br><span class="line">        <span class="built_in">print</span>(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> inner</span><br><span class="line"></span><br><span class="line">closure_func = outer()  <span class="comment"># 返回 inner 函数</span></span><br><span class="line">closure_func()  <span class="comment"># 输出 11</span></span><br><span class="line">closure_func()  <span class="comment"># 输出 12</span></span><br></pre></td></tr></tbody></table></figure><p>在本文的算法中，<code>dfs()</code> 函数利用闭包访问 <code>path</code> 列表，避免了通过参数显式传递路径的复杂操作。</p><h4 id="为什么不需要将-path-作为参数？"><a href="#为什么不需要将-path-作为参数？" class="headerlink" title="为什么不需要将 path 作为参数？"></a>为什么不需要将 <code>path</code> 作为参数？</h4><ul><li><strong>易读性</strong>：闭包让代码更加直观，避免传递多个参数。</li><li><strong>效率</strong>：闭包直接操作外部变量，避免在递归过程中传递和合并路径。</li></ul><h3 id="2-defaultdict-与-Counter"><a href="#2-defaultdict-与-Counter" class="headerlink" title="2. defaultdict 与 Counter"></a>2. <code>defaultdict</code> 与 <code>Counter</code></h3><p>Python 的 <code>collections</code> 模块提供了许多工具类，其中 <code>defaultdict</code> 和 <code>Counter</code> 是本题的核心工具。</p><h4 id="defaultdict"><a href="#defaultdict" class="headerlink" title="defaultdict"></a><code>defaultdict</code></h4><p><code>defaultdict</code> 是字典的一个子类，可以为不存在的键提供默认值，从而避免访问不存在键时抛出 <code>KeyError</code>。</p><h5 id="使用方式："><a href="#使用方式：" class="headerlink" title="使用方式："></a>使用方式：</h5><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line"><span class="comment"># 默认值是列表</span></span><br><span class="line">graph = defaultdict(<span class="built_in">list</span>)</span><br><span class="line">graph[<span class="number">1</span>].append(<span class="number">2</span>)</span><br><span class="line">graph[<span class="number">2</span>].append(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(graph)  <span class="comment"># 输出：{1: [2], 2: [3]}</span></span><br><span class="line"><span class="built_in">print</span>(graph[<span class="number">3</span>])  <span class="comment"># 输出：[]，不会报错</span></span><br></pre></td></tr></tbody></table></figure><p>在本文中，<code>defaultdict</code> 被用作图的邻接表，简化了图的构建和更新操作。</p><h4 id="Counter"><a href="#Counter" class="headerlink" title="Counter"></a><code>Counter</code></h4><p><code>Counter</code> 是字典的子类，用于统计元素的出现次数。它将每个元素作为键，出现次数作为值。</p><h5 id="使用方式：-1"><a href="#使用方式：-1" class="headerlink" title="使用方式："></a>使用方式：</h5><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line">counts = Counter([<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(counts)  <span class="comment"># 输出：Counter({3: 3, 2: 2, 1: 1})</span></span><br></pre></td></tr></tbody></table></figure><p>在本文中，可以通过 <code>Counter</code> 快速统计每个节点的入度和出度。</p><hr><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这道题通过将数字对建模为图的欧拉路径问题，使用 Hierholzer 算法高效地找到合法排列。这种图论问题的建模方法不仅提升了题目理解，还为类似问题提供了通用解法。</p><p>希望这篇博客能帮助你彻底掌握这道题！如果你有其他问题，欢迎交流！</p>]]></content>
      
      
      <categories>
          
          <category> Code Chronicles </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> Leetcode </tag>
            
            <tag> 每日一题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Detailed Explanation of LoRA, DPO, KTO, and SFT Technologies</title>
      <link href="/2024/10/23/NLP%20Insights/Introduction%20to%20LLM%20Training%20Terminology:%20LoRA,%20DPO,%20KTO,%20and%20SFT%20Technologies/"/>
      <url>/2024/10/23/NLP%20Insights/Introduction%20to%20LLM%20Training%20Terminology:%20LoRA,%20DPO,%20KTO,%20and%20SFT%20Technologies/</url>
      
        <content type="html"><![CDATA[<h3 id="Introduction-to-LLM-Training-Terminology-LoRA-DPO-KTO-and-SFT-Technologies"><a href="#Introduction-to-LLM-Training-Terminology-LoRA-DPO-KTO-and-SFT-Technologies" class="headerlink" title="Introduction to LLM Training Terminology:LoRA, DPO, KTO, and SFT Technologies"></a><strong>Introduction to LLM Training Terminology:LoRA, DPO, KTO, and SFT Technologies</strong></h3><p>This document provides a detailed introduction to several important techniques used in fine-tuning and optimizing large language models (such as LLAMA3), including <strong>SFT (Supervised Fine-Tuning)</strong>, <strong>LoRA (Low-Rank Adaptation)</strong>, <strong>Alignment</strong> technologies, <strong>KTO (Kahneman-Tversky Optimization)</strong>, and <strong>DPO (Direct Preference Optimization)</strong>. The document also elaborates on the principles of each technique, specific implementation methods, as well as the selection of corresponding loss functions and optimizers.</p><hr><h2 id="1-SFT-Supervised-Fine-Tuning"><a href="#1-SFT-Supervised-Fine-Tuning" class="headerlink" title="1. SFT (Supervised Fine-Tuning)"></a>1. <strong>SFT (Supervised Fine-Tuning)</strong></h2><h3 id="1-1-Principle"><a href="#1-1-Principle" class="headerlink" title="1.1 Principle"></a>1.1 <strong>Principle</strong></h3><p>SFT is a traditional fine-tuning method that adjusts the parameters of a pre-trained model through supervised learning to improve its performance on specific tasks. SFT is typically used to fine-tune models on specific labeled datasets, with the training process resembling standard supervised learning.</p><h3 id="1-2-Implementation-Method"><a href="#1-2-Implementation-Method" class="headerlink" title="1.2 Implementation Method"></a>1.2 <strong>Implementation Method</strong></h3><ul><li><strong>Select a Pre-trained Model</strong>: Such as GPT, BERT, and other language models.</li><li><strong>Prepare a Labeled Dataset</strong>: The dataset includes input-output pairs.</li><li><strong>Train the Model</strong>: Use a standard cross-entropy loss function to train the model, optimizing parameters through gradient descent.</li></ul><h3 id="1-3-Core-Code"><a href="#1-3-Core-Code" class="headerlink" title="1.3 Core Code"></a>1.3 <strong>Core Code</strong></h3><p>Using Hugging Face’s <code>Trainer</code> interface for SFT:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Trainer, TrainingArguments, AutoModelForSeq2SeqLM, AutoTokenizer</span><br><span class="line"></span><br><span class="line">model_name = <span class="string">"gpt2"</span></span><br><span class="line">model = AutoModelForSeq2SeqLM.from_pretrained(model_name)</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_name)</span><br><span class="line"></span><br><span class="line">train_dataset = load_dataset(<span class="string">"my_dataset"</span>, split=<span class="string">"train"</span>)</span><br><span class="line"></span><br><span class="line">training_args = TrainingArguments(</span><br><span class="line">    output_dir=<span class="string">"./results"</span>,</span><br><span class="line">    evaluation_strategy=<span class="string">"epoch"</span>,</span><br><span class="line">    learning_rate=<span class="number">2e-5</span>,</span><br><span class="line">    per_device_train_batch_size=<span class="number">16</span>,</span><br><span class="line">    num_train_epochs=<span class="number">3</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model=model,</span><br><span class="line">    args=training_args,</span><br><span class="line">    train_dataset=train_dataset,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer.train()</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="2-LoRA-Low-Rank-Adaptation"><a href="#2-LoRA-Low-Rank-Adaptation" class="headerlink" title="2. LoRA (Low-Rank Adaptation)"></a>2. <strong>LoRA (Low-Rank Adaptation)</strong></h2><h3 id="2-1-Principle"><a href="#2-1-Principle" class="headerlink" title="2.1 Principle"></a>2.1 <strong>Principle</strong></h3><p>LoRA is a parameter-efficient fine-tuning technique that performs low-rank decomposition of the weight matrices in large models. It decomposes the original weight matrix $W$ into two low-rank matrices $B$ and $A$, and only fine-tunes these low-rank matrices. The design goal of LoRA is to reduce the number of fine-tuning parameters while retaining the pre-trained model weights, optimizing model performance by adjusting the low-rank matrices.</p><h3 id="2-2-Implementation-Method"><a href="#2-2-Implementation-Method" class="headerlink" title="2.2 Implementation Method"></a>2.2 <strong>Implementation Method</strong></h3><ul><li><strong>Weight Decomposition</strong>: For the model’s linear layers (such as the <code>q_proj</code> and <code>v_proj</code> layers in the attention mechanism), decompose the weight matrix into two low-rank matrices $B$ and $A$.</li><li><strong>Fine-Tune Specific Layers</strong>: Apply LoRA only to these specific linear layers, keeping other layers in the model unchanged.</li></ul><h3 id="2-3-Layers-to-Fine-Tune-vs-Layers-to-Keep-Unchanged"><a href="#2-3-Layers-to-Fine-Tune-vs-Layers-to-Keep-Unchanged" class="headerlink" title="2.3 Layers to Fine-Tune vs. Layers to Keep Unchanged"></a>2.3 <strong>Layers to Fine-Tune vs. Layers to Keep Unchanged</strong></h3><h4 id="Layers-to-Fine-Tune"><a href="#Layers-to-Fine-Tune" class="headerlink" title="Layers to Fine-Tune"></a><strong>Layers to Fine-Tune</strong></h4><p>LoRA is typically applied to the linear projection layers in Transformer models, especially several key layers in the multi-head attention mechanism:</p><ul><li><strong>q_proj</strong> (Query Projection Layer)</li><li><strong>k_proj</strong> (Key Projection Layer)</li><li><strong>v_proj</strong> (Value Projection Layer)</li><li><strong>o_proj</strong> (Output Projection Layer)</li><li><strong>ffn_up_proj</strong> and <strong>ffn_down_proj</strong> (Up and Down Projection Layers of the Feedforward Neural Network)</li></ul><h4 id="Layers-to-Keep-Unchanged"><a href="#Layers-to-Keep-Unchanged" class="headerlink" title="Layers to Keep Unchanged"></a><strong>Layers to Keep Unchanged</strong></h4><ul><li><strong>Embedding Layers</strong>: Responsible for encoding inputs and outputs, usually do not require fine-tuning.</li><li><strong>LayerNorm Layers</strong>: These layers are mainly used for normalization, do not contain many parameters, and are typically kept unchanged.</li><li><strong>Activation Function Layers</strong>: Non-linear activation functions like ReLU or GELU do not involve parameters and do not require fine-tuning.</li></ul><h3 id="2-4-Loss-Function"><a href="#2-4-Loss-Function" class="headerlink" title="2.4 Loss Function"></a>2.4 <strong>Loss Function</strong></h3><p>The loss function for LoRA is usually task-specific. In language generation tasks, LoRA uses <strong>cross-entropy loss</strong> to measure the difference between the generated text and the target text:</p><p>$$<br>\mathcal{L}<em>{\text{LoRA}} = - \sum</em>{i} y_i \log(\hat{y}_i)<br>$$</p><p>where $y_i$ is the true label, and $\hat{y}_i$ is the model’s output probability.</p><h3 id="2-5-Optimizer"><a href="#2-5-Optimizer" class="headerlink" title="2.5 Optimizer"></a>2.5 <strong>Optimizer</strong></h3><p>LoRA fine-tuning typically uses the <strong>AdamW</strong> optimizer, as shown in the following code:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.AdamW(lora_model.parameters(), lr=<span class="number">5e-5</span>)</span><br></pre></td></tr></tbody></table></figure><h3 id="2-6-Core-Code"><a href="#2-6-Core-Code" class="headerlink" title="2.6 Core Code"></a>2.6 <strong>Core Code</strong></h3><p>Implementing LoRA using the <code>peft</code> library:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> peft <span class="keyword">import</span> LoraConfig, get_peft_model</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForSeq2SeqLM</span><br><span class="line"></span><br><span class="line">model_name = <span class="string">"gpt2"</span></span><br><span class="line">model = AutoModelForSeq2SeqLM.from_pretrained(model_name)</span><br><span class="line"></span><br><span class="line">lora_config = LoraConfig(</span><br><span class="line">    r=<span class="number">8</span>,                </span><br><span class="line">    lora_alpha=<span class="number">32</span>,      </span><br><span class="line">    target_modules=[<span class="string">"q_proj"</span>, <span class="string">"v_proj"</span>],  </span><br><span class="line">    lora_dropout=<span class="number">0.1</span>,   </span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">lora_model = get_peft_model(model, lora_config)</span><br><span class="line">lora_model.train()</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="3-Alignment-Alignment-Techniques"><a href="#3-Alignment-Alignment-Techniques" class="headerlink" title="3. Alignment (Alignment Techniques)"></a>3. <strong>Alignment (Alignment Techniques)</strong></h2><p>Before introducing KL divergence, we first need to clarify how LLM alignment is achieved, along with the underlying principles and mathematical formulas.</p><h3 id="1-What-is-Model-Alignment"><a href="#1-What-is-Model-Alignment" class="headerlink" title="1. What is Model Alignment?"></a><strong>1. What is Model Alignment?</strong></h3><p>The core objective of model alignment is to ensure that the language model’s outputs meet human expectations or preferences. Typically, the model is initially trained through large-scale supervised learning (SFT, Supervised Fine-Tuning) to generate a model with basic capabilities. Subsequently, through alignment techniques, the model is further adjusted to ensure that its generated content better aligns with human preferences or avoids producing harmful or erroneous information.</p><p><strong>Core Mechanism of Alignment</strong>:</p><ul><li><strong>Positive Samples</strong>: Outputs that meet human expectations (e.g., correct answers).</li><li><strong>Negative Samples</strong>: Outputs that do not meet human expectations (e.g., incorrect answers).</li></ul><p>By using paired preference data or labels (correct/incorrect), the model’s outputs are further fine-tuned to generate more positive samples while reducing the probability of generating negative samples.</p><hr><h3 id="2-Mathematical-Principles-of-Model-Alignment"><a href="#2-Mathematical-Principles-of-Model-Alignment" class="headerlink" title="2. Mathematical Principles of Model Alignment"></a><strong>2. Mathematical Principles of Model Alignment</strong></h3><p>During the alignment process, the model generates outputs through a <strong>policy model</strong>, which is typically an SFT-trained language model used to generate outputs given an input. To optimize the model’s outputs to better align with human preferences, the following loss functions and optimization methods are commonly used:</p><h4 id="2-1-Policy-Model"><a href="#2-1-Policy-Model" class="headerlink" title="2.1 Policy Model"></a><strong>2.1 Policy Model</strong></h4><p>Assume the current policy of the model is $\pi_\theta$, which represents the probability of the model generating output $y$ given input $x$:</p><p>$$<br>\pi_\theta(y|x)<br>$$</p><p>The objective of the policy model is to adjust the parameters $\theta$ to increase the probability of generating correct outputs (positive samples) and decrease the probability of generating incorrect outputs (negative samples).</p><h4 id="2-2-Mechanism-for-Increasing-Positive-Sample-Probability-and-Decreasing-Negative-Sample-Probability"><a href="#2-2-Mechanism-for-Increasing-Positive-Sample-Probability-and-Decreasing-Negative-Sample-Probability" class="headerlink" title="2.2 Mechanism for Increasing Positive Sample Probability and Decreasing Negative Sample Probability"></a><strong>2.2 Mechanism for Increasing Positive Sample Probability and Decreasing Negative Sample Probability</strong></h4><p>To achieve this goal, loss functions with preference comparisons or labels are typically used for optimization:</p><ol><li><p><strong>Optimization of Positive Samples</strong>: By increasing the loss weight of positive samples, the model is guided to generate positive samples with higher probability when faced with the same problem.</p><ul><li>The loss function for positive samples guides the model to produce more outputs that meet human expectations.</li></ul></li><li><p><strong>Penalty for Negative Samples</strong>: By applying higher loss weights to negative samples, the model learns to reduce the probability of generating these incorrect outputs.</p><ul><li>The loss function for negative samples aims to penalize the model more when it generates incorrect answers, thereby reducing the likelihood of such outputs.</li></ul></li></ol><p>In some methods, such as DPO and KTO, <strong>KL divergence</strong> between the current policy model and a reference model is calculated to prevent the model from deviating excessively from the original pre-trained model during optimization.</p><hr><h3 id="3-Role-of-Loss-Functions-and-KL-Divergence"><a href="#3-Role-of-Loss-Functions-and-KL-Divergence" class="headerlink" title="3. Role of Loss Functions and KL Divergence"></a><strong>3. Role of Loss Functions and KL Divergence</strong></h3><p>In the model alignment process, the loss function typically consists of two parts:</p><ol><li><strong>Preference Loss</strong> or <strong>Label Loss</strong>, used to optimize the model to generate outputs that meet human expectations.</li><li><strong>KL Divergence</strong>, used to constrain the model from deviating from the reference model.</li></ol><h4 id="3-1-Role-of-KL-Divergence"><a href="#3-1-Role-of-KL-Divergence" class="headerlink" title="3.1 Role of KL Divergence"></a><strong>3.1 Role of KL Divergence</strong></h4><p>KL divergence (Kullback-Leibler Divergence) measures the difference between two probability distributions. In model alignment, KL divergence is used to limit the distribution difference between the current model $\pi_\theta$ and the reference model $\pi_{\text{ref}}$, ensuring that the model’s outputs do not deviate excessively from the pre-trained model during optimization. The specific formula is:</p><p>$$<br>\text{KL}(\pi_\theta(y|x) | \pi_{\text{ref}}(y|x)) = \sum_y \pi_\theta(y|x) \log \frac{\pi_\theta(y|x)}{\pi_{\text{ref}}(y|x)}<br>$$</p><ul><li>If the KL divergence is large, it indicates that the current model’s generated distribution significantly differs from the reference model, which may mean the model is producing unreasonable outputs.</li><li>By minimizing KL divergence, the model can be further optimized while ensuring the reasonableness of its outputs.</li></ul><h4 id="3-2-Loss-Function-Formulas"><a href="#3-2-Loss-Function-Formulas" class="headerlink" title="3.2 Loss Function Formulas"></a><strong>3.2 Loss Function Formulas</strong></h4><p>Based on preferences or labels, the model’s loss function can be expressed in the following forms:</p><h5 id="Loss-Function-in-DPO"><a href="#Loss-Function-in-DPO" class="headerlink" title="Loss Function in DPO:"></a><strong>Loss Function in DPO</strong>:</h5><p>$$<br>L_{DPO} = -\mathbb{E}_{x, y_w, y_l \sim D} [\log \sigma(\beta (\log(\pi_\theta(y_w|x)) - \log(\pi_\theta(y_l|x))))]<br>$$</p><ul><li>$y_w$: Higher-preference answer.</li><li>$y_l$: Lower-preference answer.</li></ul><p>In DPO, KL divergence can be introduced as a regularization term:<br>$$<br>L_{DPO} = -\log \sigma(\log(\pi_\theta(y_w|x)) - \log(\pi_\theta(y_l|x)) + \beta \cdot \text{KL}(\pi_\theta | \pi_{\text{ref}}))<br>$$<br>By controlling KL divergence, the model’s outputs do not deviate too much from the reference model.</p><h5 id="Loss-Function-in-KTO"><a href="#Loss-Function-in-KTO" class="headerlink" title="Loss Function in KTO:"></a><strong>Loss Function in KTO</strong>:</h5><p>The loss function in KTO is based on prospect theory and incorporates KL divergence as a core component:<br>$$<br>L_{KTO} = \lambda_U \cdot \sigma(\beta \cdot (\text{KL}(\pi_\theta(\text{Answer 2}) | \pi_{\text{ref}}) - r_{\theta}(\text{Answer 2})))<br>$$</p><ul><li>$r_{\theta}(x, y)$: The current policy’s confidence in negative samples (incorrect answers).</li><li>KL divergence is used to measure the difference between the current model and the reference model, ensuring that while reducing the generation of negative samples, the model does not deviate from the original reference model.</li></ul><p>By increasing the loss for negative samples (i.e., increasing the value of $\lambda_U$), the model reduces the confidence in negative samples, thereby decreasing the probability of generating similar incorrect answers in the future.</p><hr><h3 id="4-How-to-Optimize-the-Model"><a href="#4-How-to-Optimize-the-Model" class="headerlink" title="4. How to Optimize the Model"></a><strong>4. How to Optimize the Model</strong></h3><p>Through the loss functions introduced above, model optimization is typically performed using <strong>Gradient Descent</strong>. The gradients of the loss function reflect the differences between the model’s outputs and the expected outputs, and the optimization goal is to minimize the loss function.</p><p><strong>Gradient Update Formula</strong>:<br>$$<br>\theta_{\text{new}} = \theta_{\text{old}} - \eta \nabla_{\theta} L<br>$$<br>where:</p><ul><li>$\eta$ is the learning rate, determining the step size of each parameter update.</li><li>$\nabla_{\theta} L$ is the gradient of the loss function with respect to the model parameters, indicating the contribution of the current parameters to the loss.</li></ul><p>Through continuous iteration, the model gradually increases the probability of generating positive samples and decreases the probability of generating negative samples, ultimately achieving model alignment.</p><ul><li>The core objective of <strong>Model Alignment</strong> is to optimize the model’s outputs to meet human expectations through preference or label data.</li><li>The <strong>Policy Model</strong> ($\pi_\theta$) generates outputs, and KL divergence is used to control the degree of deviation from the reference model, preventing unreasonable biases during optimization.</li><li>The <strong>Probability of Positive Samples</strong> is gradually increased through the optimization of the loss function, while the <strong>Probability of Negative Samples</strong> is reduced by increasing loss weights and lowering confidence.</li><li>Gradient descent is used to update model parameters, ultimately achieving model alignment.</li></ul><hr><h2 id="4-DPO-Direct-Preference-Optimization"><a href="#4-DPO-Direct-Preference-Optimization" class="headerlink" title="4. DPO (Direct Preference Optimization)"></a>4. <strong>DPO (Direct Preference Optimization)</strong></h2><h3 id="4-1-Principle"><a href="#4-1-Principle" class="headerlink" title="4.1 Principle"></a>4.1 <strong>Principle</strong></h3><p>DPO directly optimizes the model’s output preference function to make the model’s outputs more aligned with human preferences. It compares different outputs generated by the model and uses a preference function to evaluate which of the two outputs is better, thereby guiding the optimization of the model parameters.</p><h3 id="4-2-Loss-Function"><a href="#4-2-Loss-Function" class="headerlink" title="4.2 Loss Function"></a>4.2 <strong>Loss Function</strong></h3><p>DPO uses a preference loss function to compare the quality of two outputs:</p><p>$$<br>\mathcal{L}_{\text{DPO}} = \log(1 + \exp(-\sigma \cdot (\hat{y}_a - \hat{y}_b) \cdot p))<br>$$</p><ul><li>$ \hat{y}_a $ and $ \hat{y}_b $ are the model’s predictions for two samples.</li><li>$ p $ is the human preference (1 indicates preference for $a$, -1 indicates preference for $b$).</li><li>$ \sigma $ is a smoothing parameter.</li></ul><h3 id="4-3-Optimizer"><a href="#4-3-Optimizer" class="headerlink" title="4.3 Optimizer"></a>4.3 <strong>Optimizer</strong></h3><p>DPO typically uses the <strong>AdamW</strong> optimizer, which is suitable for optimizing large-scale parameter models. The code is as follows:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.AdamW(model.parameters(), lr=<span class="number">1e-5</span>)</span><br></pre></td></tr></tbody></table></figure><h3 id="4-4-Core-Code"><a href="#4-4-Core-Code" class="headerlink" title="4.4 Core Code"></a>4.4 <strong>Core Code</strong></h3><p>The following are the training steps for DPO:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preference_loss</span>(<span class="params">output_a, output_b, human_preference</span>):</span><br><span class="line">    preference = human_preference(output_a, output_b)</span><br><span class="line">    loss = torch.log(<span class="number">1</span> + torch.exp(-preference * (output_a - output_b)))</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dpo_training_step</span>(<span class="params">model, data_a, data_b, human_preference</span>):</span><br><span class="line">    output_a = model(data_a)</span><br><span class="line">    output_b = model(data_b)</span><br><span class="line">    </span><br><span class="line">    loss = preference_loss(output_a, output_b, human_preference)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> batch_a, batch_b <span class="keyword">in</span> training_data:</span><br><span class="line">    dpo_training_step(model, batch_a, batch_b, human_preference_function)</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="5-KTO-Kahneman-Tversky-Optimization"><a href="#5-KTO-Kahneman-Tversky-Optimization" class="headerlink" title="5. KTO (Kahneman-Tversky Optimization)"></a>5. <strong>KTO (Kahneman-Tversky Optimization)</strong></h2><h3 id="5-1-Principle"><a href="#5-1-Principle" class="headerlink" title="5.1 Principle"></a>5.1 <strong>Principle</strong></h3><p>KTO is based on Kahneman and Tversky’s Prospect Theory, which uses an asymmetric utility function to measure the model’s gains and losses. It aims to optimize the model’s performance, especially in scenarios with asymmetric risks and rewards. The utility function is defined as follows:</p><p>$$<br>\mathcal{U}(x) =<br>\begin{cases}<br>x^{\alpha}, &amp; x \geq 0 \<br>-\lambda (-x)^{\alpha}, &amp; x &lt; 0<br>\end{cases}<br>$$</p><ul><li>$x$ is the difference between the model’s prediction and the true value.</li><li>$\alpha$ is the non-linear coefficient, typically 0.88.</li><li>$\lambda$ is the loss penalty weight, typically 2.25.</li></ul><h3 id="5-2-Loss-Function"><a href="#5-2-Loss-Function" class="headerlink" title="5.2 Loss Function"></a>5.2 <strong>Loss Function</strong></h3><p>The loss function for KTO is based on the utility function from Prospect Theory and is used to penalize the model’s prediction errors:</p><p>$$<br>\mathcal{L}<em>{\text{KTO}} = -\mathbb{E}[\mathcal{U}(y</em>{\text{pred}} - y_{\text{true}})]<br>$$</p><h3 id="5-3-Optimizer"><a href="#5-3-Optimizer" class="headerlink" title="5.3 Optimizer"></a>5.3 <strong>Optimizer</strong></h3><p>KTO commonly uses the <strong>AdamW</strong> optimizer to ensure stability during the training process:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.AdamW(model.parameters(), lr=<span class="number">3e-5</span>)</span><br></pre></td></tr></tbody></table></figure><h3 id="5-4-Core-Code"><a href="#5-4-Core-Code" class="headerlink" title="5.4 Core Code"></a>5.4 <strong>Core Code</strong></h3><p>The following is the code for calculating the KTO loss function:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">prospect_utility</span>(<span class="params">value, alpha=<span class="number">0.88</span>, lambda_=<span class="number">2.25</span></span>):</span><br><span class="line">    <span class="keyword">if</span> value &gt;= <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> torch.<span class="built_in">pow</span>(value, alpha)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> -lambda_ * torch.<span class="built_in">pow</span>(-value, alpha)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">kto_loss</span>(<span class="params">predictions, targets</span>):</span><br><span class="line">    value = predictions - targets</span><br><span class="line">    utility = prospect_utility(value)</span><br><span class="line">    loss = -torch.mean(utility)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">    predictions = model(data)</span><br><span class="line">    loss = kto_loss(predictions, targets)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a><strong>Summary</strong></h3><table><thead><tr><th>Method</th><th>Loss Function</th><th>Optimizer</th></tr></thead><tbody><tr><td><strong>SFT</strong></td><td>Cross-Entropy Loss</td><td>AdamW, RMSprop, SGD</td></tr><tr><td><strong>LoRA</strong></td><td>Cross-Entropy Loss</td><td>AdamW, RMSprop, SGD</td></tr><tr><td><strong>DPO</strong></td><td>Preference Loss Function: $\log(1 + \exp(-\sigma (\hat{y}_a - \hat{y}_b)p))$</td><td>AdamW</td></tr><tr><td><strong>KTO</strong></td><td>Prospect Theory Utility Function: $-\mathbb{E}[\mathcal{U}(y_{\text{pred}} - y_{\text{true}})]$</td><td>AdamW</td></tr></tbody></table><p>Through the organization of this document, readers can clearly understand the principles, specific implementation steps, loss function designs, and optimizer selections for technologies such as SFT, LoRA, DPO, and KTO, especially in the context of fine-tuning large-scale pre-trained models like LLAMA3.</p>]]></content>
      
      
      <categories>
          
          <category> NLP Insights </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LoRA, DPO, KTO 与 SFT 技术详解</title>
      <link href="/2024/10/23/NLP%20Insights/LLM%E8%AE%AD%E7%BB%83%E6%9C%AF%E8%AF%AD%E4%BB%8B%E7%BB%8D%EF%BC%9ASFT%E3%80%81LoRA%E3%80%81Alignment%E3%80%81KTO%E5%92%8CDPO/"/>
      <url>/2024/10/23/NLP%20Insights/LLM%E8%AE%AD%E7%BB%83%E6%9C%AF%E8%AF%AD%E4%BB%8B%E7%BB%8D%EF%BC%9ASFT%E3%80%81LoRA%E3%80%81Alignment%E3%80%81KTO%E5%92%8CDPO/</url>
      
        <content type="html"><![CDATA[<h3 id="LoRA-DPO-KTO-与-SFT-技术详解"><a href="#LoRA-DPO-KTO-与-SFT-技术详解" class="headerlink" title="LoRA, DPO, KTO 与 SFT 技术详解"></a><strong>LoRA, DPO, KTO 与 SFT 技术详解</strong></h3><p>本篇文档将详细介绍几种在大型语言模型（如 LLAMA3）微调和优化中的重要技术，包括 <strong>SFT（Supervised Fine-Tuning）</strong>、<strong>LoRA（Low-Rank Adaptation）</strong>、<strong>Alignment</strong> 技术、<strong>KTO（Kahneman-Tversky Optimization）</strong> 和 <strong>DPO（Direct Preference Optimization）</strong>。文中还将详细阐述每种技术的原理、具体实现方法以及相应的损失函数与优化器选择。</p><hr><h2 id="1-SFT（Supervised-Fine-Tuning）"><a href="#1-SFT（Supervised-Fine-Tuning）" class="headerlink" title="1. SFT（Supervised Fine-Tuning）"></a>1. <strong>SFT（Supervised Fine-Tuning）</strong></h2><h3 id="1-1-原理"><a href="#1-1-原理" class="headerlink" title="1.1 原理"></a>1.1 <strong>原理</strong></h3><p>SFT 是一种传统的微调方法，通过监督学习对预训练模型进行微调，调整模型的参数使其在特定任务上表现更好。SFT 通常用于针对特定的标注数据进行模型微调，训练的过程类似于常规的监督学习。</p><h3 id="1-2-实现方法"><a href="#1-2-实现方法" class="headerlink" title="1.2 实现方法"></a>1.2 <strong>实现方法</strong></h3><ul><li><strong>选择预训练模型</strong>：如 GPT、BERT 等语言模型。</li><li><strong>准备标注数据集</strong>：数据集包含输入和输出对。</li><li><strong>训练模型</strong>：使用标准的交叉熵损失函数对模型进行训练，通过梯度下降优化参数。</li></ul><h3 id="1-3-核心代码"><a href="#1-3-核心代码" class="headerlink" title="1.3 核心代码"></a>1.3 <strong>核心代码</strong></h3><p>使用 Hugging Face 的 <code>Trainer</code> 接口进行 SFT：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Trainer, TrainingArguments, AutoModelForSeq2SeqLM, AutoTokenizer</span><br><span class="line"></span><br><span class="line">model_name = <span class="string">"gpt2"</span></span><br><span class="line">model = AutoModelForSeq2SeqLM.from_pretrained(model_name)</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_name)</span><br><span class="line"></span><br><span class="line">train_dataset = load_dataset(<span class="string">"my_dataset"</span>, split=<span class="string">"train"</span>)</span><br><span class="line"></span><br><span class="line">training_args = TrainingArguments(</span><br><span class="line">    output_dir=<span class="string">"./results"</span>,</span><br><span class="line">    evaluation_strategy=<span class="string">"epoch"</span>,</span><br><span class="line">    learning_rate=<span class="number">2e-5</span>,</span><br><span class="line">    per_device_train_batch_size=<span class="number">16</span>,</span><br><span class="line">    num_train_epochs=<span class="number">3</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model=model,</span><br><span class="line">    args=training_args,</span><br><span class="line">    train_dataset=train_dataset,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer.train()</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="2-LoRA（Low-Rank-Adaptation）"><a href="#2-LoRA（Low-Rank-Adaptation）" class="headerlink" title="2. LoRA（Low-Rank Adaptation）"></a>2. <strong>LoRA（Low-Rank Adaptation）</strong></h2><h3 id="2-1-原理"><a href="#2-1-原理" class="headerlink" title="2.1 原理"></a>2.1 <strong>原理</strong></h3><p>LoRA 是一种参数高效的微调技术，通过对大模型中的权重矩阵进行低秩分解，将原始权重矩阵 $W$ 分解为两个低秩矩阵 $B$ 和 $A$，并仅对这些低秩矩阵进行微调。LoRA 的设计目标是减少微调参数的数量，在保留预训练模型权重的同时，通过调整低秩矩阵来优化模型表现。</p><h3 id="2-2-实现方法"><a href="#2-2-实现方法" class="headerlink" title="2.2 实现方法"></a>2.2 <strong>实现方法</strong></h3><ul><li><strong>权重分解</strong>：对于模型的线性层（如注意力机制中的 <code>q_proj</code> 和 <code>v_proj</code> 层），将权重矩阵分解为两个低秩矩阵 $B$ 和 $A$。</li><li><strong>微调特定层</strong>：仅对这些特定的线性层应用 LoRA，而模型中的其他层保持不变。</li></ul><h3 id="2-3-可微调的层与不变的层"><a href="#2-3-可微调的层与不变的层" class="headerlink" title="2.3 可微调的层与不变的层"></a>2.3 <strong>可微调的层与不变的层</strong></h3><h4 id="可微调的层"><a href="#可微调的层" class="headerlink" title="可微调的层"></a><strong>可微调的层</strong></h4><p>LoRA 通常应用于 Transformer 模型中的线性投影层，尤其是多头注意力机制中的几个关键层：</p><ul><li><strong>q_proj</strong>（Query 投影层）</li><li><strong>k_proj</strong>（Key 投影层）</li><li><strong>v_proj</strong>（Value 投影层）</li><li><strong>o_proj</strong>（Output 投影层）</li><li><strong>ffn_up_proj</strong> 和 <strong>ffn_down_proj</strong>（前馈神经网络的上下投影层）</li></ul><h4 id="不变的层"><a href="#不变的层" class="headerlink" title="不变的层"></a><strong>不变的层</strong></h4><ul><li><strong>Embedding 层</strong>：负责输入和输出的编码，通常不需要微调。</li><li><strong>LayerNorm 层</strong>：这些层主要用于归一化，不含大量参数，通常保持不变。</li><li><strong>激活函数层</strong>：如 ReLU 或 GELU 等非线性激活函数不涉及参数，不需要进行微调。</li></ul><h3 id="2-4-损失函数"><a href="#2-4-损失函数" class="headerlink" title="2.4 损失函数"></a>2.4 <strong>损失函数</strong></h3><p>LoRA 的损失函数通常与具体任务相关。在语言生成任务中，LoRA 使用<strong>交叉熵损失</strong>来度量生成文本和目标文本之间的差异：</p><p>$$<br>\mathcal{L}<em>{\text{LoRA}} = - \sum</em>{i} y_i \log(\hat{y}_i)<br>$$</p><p>其中 $y_i$ 是真实标签，$\hat{y}_i$ 是模型的输出概率。</p><h3 id="2-5-优化器"><a href="#2-5-优化器" class="headerlink" title="2.5 优化器"></a>2.5 <strong>优化器</strong></h3><p>LoRA 微调通常使用 <strong>AdamW</strong> 优化器，具体代码如下：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.AdamW(lora_model.parameters(), lr=<span class="number">5e-5</span>)</span><br></pre></td></tr></tbody></table></figure><h3 id="2-6-核心代码"><a href="#2-6-核心代码" class="headerlink" title="2.6 核心代码"></a>2.6 <strong>核心代码</strong></h3><p>使用 <code>peft</code> 库实现 LoRA：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> peft <span class="keyword">import</span> LoraConfig, get_peft_model</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForSeq2SeqLM</span><br><span class="line"></span><br><span class="line">model_name = <span class="string">"gpt2"</span></span><br><span class="line">model = AutoModelForSeq2SeqLM.from_pretrained(model_name)</span><br><span class="line"></span><br><span class="line">lora_config = LoraConfig(</span><br><span class="line">    r=<span class="number">8</span>,                </span><br><span class="line">    lora_alpha=<span class="number">32</span>,      </span><br><span class="line">    target_modules=[<span class="string">"q_proj"</span>, <span class="string">"v_proj"</span>],  </span><br><span class="line">    lora_dropout=<span class="number">0.1</span>,   </span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">lora_model = get_peft_model(model, lora_config)</span><br><span class="line">lora_model.train()</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="3-Alignment（对齐技术）"><a href="#3-Alignment（对齐技术）" class="headerlink" title="3. Alignment（对齐技术）"></a>3. <strong>Alignment（对齐技术）</strong></h2><p>在引入KL散度之前，我们首先需要明确LLM对齐（Alignment）是如何实现的，以及背后的原理和数学公式。</p><h3 id="1-什么是模型对齐（Alignment）？"><a href="#1-什么是模型对齐（Alignment）？" class="headerlink" title="1. 什么是模型对齐（Alignment）？"></a><strong>1. 什么是模型对齐（Alignment）？</strong></h3><p>模型对齐的核心目标是让语言模型的输出符合人类的期望或偏好。通常，模型最初通过大规模监督学习（SFT，Supervised Fine-Tuning）训练，生成具有基础能力的模型。接下来，通过对齐技术，进一步调整模型，使其生成的内容更符合人类偏好或避免产生有害、错误的信息。</p><p><strong>对齐的核心机制</strong>：</p><ul><li><strong>正样本</strong>：符合人类预期的输出（如正确回答）。</li><li><strong>负样本</strong>：不符合人类预期的输出（如错误回答）。</li></ul><p>通过使用成对偏好数据或标签（正确/错误），对模型的输出进行进一步微调，使模型能够生成更多的正样本，同时减少负样本的生成概率。</p><hr><h3 id="2-模型对齐的数学原理"><a href="#2-模型对齐的数学原理" class="headerlink" title="2. 模型对齐的数学原理"></a><strong>2. 模型对齐的数学原理</strong></h3><p>在对齐过程中，模型会通过<strong>策略模型</strong>（Policy Model）来生成输出，策略模型通常是经过SFT训练的语言模型，用来在给定输入下生成输出。为了优化模型的输出，使其更加符合人类偏好，常常使用以下损失函数和优化方法：</p><h4 id="2-1-策略模型"><a href="#2-1-策略模型" class="headerlink" title="2.1 策略模型"></a><strong>2.1 策略模型</strong></h4><p>假设当前模型的策略为 $\pi_\theta$，它表示在给定输入 $x$ 时，模型生成输出 $y$ 的概率：<br>$$<br>\pi_\theta(y|x)<br>$$<br>策略模型的目标是通过调整参数 $\theta$，提高生成正确输出（正样本）的概率，降低生成错误输出（负样本）的概率。</p><h4 id="2-2-提高正样本概率与降低负样本概率的机制"><a href="#2-2-提高正样本概率与降低负样本概率的机制" class="headerlink" title="2.2 提高正样本概率与降低负样本概率的机制"></a><strong>2.2 提高正样本概率与降低负样本概率的机制</strong></h4><p>为了实现这个目标，通常使用带有偏好比较或标签的损失函数进行优化：</p><ol><li><p><strong>正样本的优化</strong>：通过增加正样本的损失权重，使得模型生成正样本的概率更高。</p><ul><li>正样本的损失函数会引导模型在面对相同问题时，生成更多符合人类期望的答案。</li></ul></li><li><p><strong>负样本的惩罚</strong>：对负样本施加更高的损失权重，模型会学习到减少这些错误输出的概率。</p><ul><li>负样本的损失函数旨在让模型在生成错误答案时感知到更大的惩罚，从而减少这些输出的生成。</li></ul></li></ol><p>在某些方法中，例如DPO和KTO，还会通过计算当前策略模型与参考模型之间的<strong>KL散度</strong>，来防止模型在优化过程中过度偏离原始预训练模型。</p><hr><h3 id="3-损失函数与KL散度的作用"><a href="#3-损失函数与KL散度的作用" class="headerlink" title="3. 损失函数与KL散度的作用"></a><strong>3. 损失函数与KL散度的作用</strong></h3><p>在模型对齐的过程中，损失函数通常包含两部分：</p><ol><li><strong>偏好损失</strong>或<strong>标签损失</strong>，用于优化模型生成符合人类期望的输出。</li><li><strong>KL散度</strong>，用于约束模型不要偏离参考模型。</li></ol><h4 id="3-1-KL散度的作用"><a href="#3-1-KL散度的作用" class="headerlink" title="3.1 KL散度的作用"></a><strong>3.1 KL散度的作用</strong></h4><p>KL散度（Kullback-Leibler Divergence）衡量的是两个概率分布之间的差异。在模型对齐中，KL散度用于限制当前模型 \(\pi_\theta\) 和参考模型 \(\pi_{\text{ref}}\) 的分布差异，确保在优化过程中模型的输出不会过度偏离预训练模型。具体公式为：<br>$$<br>\text{KL}(\pi_\theta(y|x) | \pi_{\text{ref}}(y|x)) = \sum_y \pi_\theta(y|x) \log \frac{\pi_\theta(y|x)}{\pi_{\text{ref}}(y|x)}<br>$$</p><ul><li>如果KL散度较大，表示当前模型生成的分布与参考模型有较大的差异，这可能意味着模型生成了不合理的输出。</li><li>通过最小化KL散度，模型能够在保证输出合理性的基础上，进行进一步的优化。</li></ul><h4 id="3-2-损失函数公式"><a href="#3-2-损失函数公式" class="headerlink" title="3.2 损失函数公式"></a><strong>3.2 损失函数公式</strong></h4><p>根据偏好或标签，模型的损失函数可以表达为以下形式：</p><h5 id="DPO中的损失函数："><a href="#DPO中的损失函数：" class="headerlink" title="DPO中的损失函数："></a><strong>DPO中的损失函数</strong>：</h5><p>$$<br>L_{DPO} = -\mathbb{E}_{x, y_w, y_l \sim D} [\log \sigma(\beta (\log(\pi_\theta(y_w|x)) - \log(\pi_\theta(y_l|x))))]<br>$$</p><ul><li>$y_w$：偏好较高的答案。</li><li>$y_l$：偏好较低的答案。</li></ul><p>DPO中可以引入KL散度作为正则化项：<br>$$<br>L_{DPO} = -\log \sigma(\log(\pi_\theta(y_w|x)) - \log(\pi_\theta(y_l|x)) + \beta \cdot \text{KL}(\pi_\theta | \pi_{\text{ref}}))<br>$$<br>通过控制KL散度，模型的输出不会偏离参考模型太多。</p><h5 id="KTO中的损失函数："><a href="#KTO中的损失函数：" class="headerlink" title="KTO中的损失函数："></a><strong>KTO中的损失函数</strong>：</h5><p>KTO的损失函数基于前景理论，并将KL散度作为核心部分，表达为：<br>$$<br>L_{KTO} = \lambda_U \cdot \sigma(\beta \cdot (\text{KL}(\pi_\theta(\text{Answer 2}) | \pi_{\text{ref}}) - r_{\theta}(\text{Answer 2})))<br>$$</p><ul><li>$r_{\theta}(x, y)$：当前策略对负样本（错误答案）的置信度。</li><li>KL散度用于衡量当前模型与参考模型的差异，确保模型在减少负样本生成的同时，不偏离原始参考模型。</li></ul><p>通过增加负样本的损失（即增加 $\lambda_U$ 的值），模型会降低负样本的置信度，使未来生成类似错误答案的概率变小。</p><hr><h3 id="4-如何优化模型"><a href="#4-如何优化模型" class="headerlink" title="4. 如何优化模型"></a><strong>4. 如何优化模型</strong></h3><p>通过上面介绍的损失函数，模型的优化通常是通过<strong>梯度下降</strong>（Gradient Descent）来完成的。损失函数的梯度反映了模型输出与期望输出之间的差异，优化目标是最小化损失函数。</p><p><strong>梯度更新公式</strong>：<br>$$<br>\theta_{\text{new}} = \theta_{\text{old}} - \eta \nabla_{\theta} L<br>$$<br>其中：</p><ul><li>$\eta$ 是学习率，决定每次参数更新的步长。</li><li>$\nabla_{\theta} L$ 是损失函数对模型参数的梯度，表示当前参数对损失的贡献。</li></ul><p>通过不断迭代，模型会逐渐提高生成正样本的概率，减少负样本的生成概率，最终实现模型对齐。</p><ul><li>模型对齐（Alignment）的核心目标是通过偏好或标签数据，优化模型的输出，使其符合人类期望。</li><li><strong>策略模型</strong>（$\pi_\theta$）生成输出，KL散度用于控制模型与参考模型的偏离程度，避免模型在优化过程中产生不合理的偏差。</li><li><strong>正样本的概率</strong>通过损失函数的优化逐步提升，<strong>负样本的概率</strong>通过增加损失权重和降低置信度来减少。</li><li>梯度下降用于更新模型参数，最终实现模型对齐</li></ul><hr><h2 id="4-DPO（Direct-Preference-Optimization）"><a href="#4-DPO（Direct-Preference-Optimization）" class="headerlink" title="4. DPO（Direct Preference Optimization）"></a>4. <strong>DPO（Direct Preference Optimization）</strong></h2><h3 id="4-1-原理"><a href="#4-1-原理" class="headerlink" title="4.1 原理"></a>4.1 <strong>原理</strong></h3><p>DPO 通过直接优化模型输出的偏好函数，使模型的输出更加符合人类偏好。它比较模型的不同输出，并通过偏好函数评估这两个输出哪个更好，从而指导模型参数的优化。</p><h3 id="4-2-损失函数"><a href="#4-2-损失函数" class="headerlink" title="4.2 损失函数"></a>4.2 <strong>损失函数</strong></h3><p>DPO 使用偏好损失函数（Preference Loss），用于比较两个输出的优劣：</p><p>$$<br>\mathcal{L}_{\text{DPO}} = \log(1 + \exp(-\sigma \cdot (\hat{y}_a - \hat{y}_b) \cdot p))<br>$$</p><ul><li>$ \hat{y}_a $ 和 $ \hat{y}_b $ 是模型对两个样本的预测值。</li><li>$ p $ 是人类偏好（1 表示偏好 $a$，-1 表示偏好 $b$）。</li><li>$ \sigma $ 是平滑参数。</li></ul><h3 id="4-3-优化器"><a href="#4-3-优化器" class="headerlink" title="4.3 优化器"></a>4.3 <strong>优化器</strong></h3><p>DPO 通常使用 <strong>AdamW</strong> 优化器，适用于大规模参数模型的优化，代码如下：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.AdamW(model.parameters(), lr=<span class="number">1e-5</span>)</span><br></pre></td></tr></tbody></table></figure><h3 id="4-4-核心代码"><a href="#4-4-核心代码" class="headerlink" title="4.4 核心代码"></a>4.4 <strong>核心代码</strong></h3><p>以下是 DPO 的训练步骤：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preference_loss</span>(<span class="params">output_a, output_b, human_preference</span>):</span><br><span class="line">    preference = human_preference(output_a, output_b)</span><br><span class="line">    loss = torch.log(<span class="number">1</span> + torch.exp(-preference * (output_a - output_b)))</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dpo_training_step</span>(<span class="params">model, data_a, data_b, human_preference</span>):</span><br><span class="line">    output_a = model(data_a)</span><br><span class="line">    output_b = model(data_b)</span><br><span class="line">    </span><br><span class="line">    loss = preference_loss(output_a, output_b, human_preference)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> batch_a, batch_b <span class="keyword">in</span> training_data:</span><br><span class="line">    dpo_training_step(model, batch_a, batch_b, human_preference_function)</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="5-KTO（Kahneman-Tversky-Optimization）"><a href="#5-KTO（Kahneman-Tversky-Optimization）" class="headerlink" title="5. KTO（Kahneman-Tversky Optimization）"></a>5. <strong>KTO（Kahneman-Tversky Optimization）</strong></h2><h3 id="5-1-原理"><a href="#5-1-原理" class="headerlink" title="5.1 原理"></a>5.1 <strong>原理</strong></h3><p>KTO 基于 Kahneman 和 Tversky 的前景理论（Prospect Theory），通过非对称效用函数衡量模型的增益和损失，旨在优化模型的表现，尤其在风险和收益不对称的场景下。效用函数定义如下：</p><p>$$<br>\mathcal{U}(x) =<br>\begin{cases}<br>x^{\alpha}, &amp; x \geq 0 \<br>-\lambda (-x)^{\alpha}, &amp; x &lt; 0<br>\end{cases}<br>$$</p><ul><li>$x$ 是模型预测与真实值的差异。</li><li>$\alpha$ 是非线性系数，通常为 0</li></ul><p>.88。</p><ul><li>$\lambda$ 是损失的惩罚权重，通常为 2.25。</li></ul><h3 id="5-2-损失函数"><a href="#5-2-损失函数" class="headerlink" title="5.2 损失函数"></a>5.2 <strong>损失函数</strong></h3><p>KTO 的损失函数基于前景理论的效用函数，用于惩罚模型的预测误差：</p><p>$$<br>\mathcal{L}<em>{\text{KTO}} = -\mathbb{E}[\mathcal{U}(y</em>{\text{pred}} - y_{\text{true}})]<br>$$</p><h3 id="5-3-优化器"><a href="#5-3-优化器" class="headerlink" title="5.3 优化器"></a>5.3 <strong>优化器</strong></h3><p>KTO 常使用 <strong>AdamW</strong> 优化器，以确保训练过程的稳定性：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.AdamW(model.parameters(), lr=<span class="number">3e-5</span>)</span><br></pre></td></tr></tbody></table></figure><h3 id="5-4-核心代码"><a href="#5-4-核心代码" class="headerlink" title="5.4 核心代码"></a>5.4 <strong>核心代码</strong></h3><p>以下是 KTO 损失函数的计算代码：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">prospect_utility</span>(<span class="params">value, alpha=<span class="number">0.88</span>, lambda_=<span class="number">2.25</span></span>):</span><br><span class="line">    <span class="keyword">if</span> value &gt;= <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> torch.<span class="built_in">pow</span>(value, alpha)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> -lambda_ * torch.<span class="built_in">pow</span>(-value, alpha)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">kto_loss</span>(<span class="params">predictions, targets</span>):</span><br><span class="line">    value = predictions - targets</span><br><span class="line">    utility = prospect_utility(value)</span><br><span class="line">    loss = -torch.mean(utility)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">    predictions = model(data)</span><br><span class="line">    loss = kto_loss(predictions, targets)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h3><table><thead><tr><th>方法</th><th>损失函数</th><th>优化器</th></tr></thead><tbody><tr><td><strong>SFT</strong></td><td>交叉熵损失</td><td>AdamW，RMSprop，SGD</td></tr><tr><td><strong>LoRA</strong></td><td>交叉熵损失</td><td>AdamW，RMSprop，SGD</td></tr><tr><td><strong>DPO</strong></td><td>偏好损失函数： $\log(1 + \exp(-\sigma (\hat{y}_a - \hat{y}_b)p))$</td><td>AdamW</td></tr><tr><td><strong>KTO</strong></td><td>前景理论效用函数： $-\mathbb{E}[\mathcal{U}(y_{\text{pred}} - y_{\text{true}})]$</td><td>AdamW</td></tr></tbody></table><p>通过本文档的整理，读者能够清晰理解 SFT、LoRA、DPO 和 KTO 等技术的原理、具体实现步骤、损失函数设计和优化器选择，特别是在 LLAMA3 这种大规模预训练模型的微调场景下的实际应用。</p>]]></content>
      
      
      <categories>
          
          <category> NLP Insights </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用压缩有限状态机进行本地 LLM 的快速 JSON 解码</title>
      <link href="/2024/08/13/NLP%20Insights/%E4%BD%BF%E7%94%A8%E5%8E%8B%E7%BC%A9%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA%E8%BF%9B%E8%A1%8C%E6%9C%AC%E5%9C%B0%20LLM%20%E7%9A%84%E5%BF%AB%E9%80%9F%20JSON%20%E8%A7%A3%E7%A0%81/"/>
      <url>/2024/08/13/NLP%20Insights/%E4%BD%BF%E7%94%A8%E5%8E%8B%E7%BC%A9%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA%E8%BF%9B%E8%A1%8C%E6%9C%AC%E5%9C%B0%20LLM%20%E7%9A%84%E5%BF%AB%E9%80%9F%20JSON%20%E8%A7%A3%E7%A0%81/</url>
      
        <content type="html"><![CDATA[<h2 id="使用压缩有限状态机进行本地-LLM-的快速-JSON-解码"><a href="#使用压缩有限状态机进行本地-LLM-的快速-JSON-解码" class="headerlink" title="使用压缩有限状态机进行本地 LLM 的快速 JSON 解码"></a>使用压缩有限状态机进行本地 LLM 的快速 JSON 解码</h2><p><strong>作者</strong>: Liangsheng Yin, Ying Sheng, Lianmin Zheng<br><strong>日期</strong>: 2024 年 2 月 5 日</p><hr><p>本文内容基于 LMSYS Org 发布的一篇博客文章，原文链接：<a href="https://lmsys.org/blog/2024-02-05-compressed-fsm/">LMSYS Org 博客</a>。相关的代码库可以在以下链接找到：<a href="https://github.com/sgl-project/sglang/tree/main?tab=readme-ov-file#json-decoding">SGLang 代码库</a>。</p><p>让一个 LLM 始终生成符合特定模式的有效 JSON 或 YAML，对于许多应用来说是一个关键特性。在这篇博客文章中，我们介绍了一种显著加速这种约束解码的优化方法。我们的方法利用了压缩的有限状态机，并且兼容任何正则表达式，因此可以适用于任何 JSON 或 YAML 模式。与现有系统逐步解码一个标记的方式不同，我们的方法分析了正则表达式的有限状态机，压缩了单一的转换路径，并在可能的情况下一次性解码多个标记。与最先进的系统（guidance + llama.cpp，outlines + vLLM）相比，我们的方法可以将延迟减少最多 2 倍，并提高吞吐量最多 2.5 倍。这一优化还使得约束解码比普通解码更快。你可以在 SGLang 上试用它。</p><p><img src="https://lmsys.org/images/blog/compressed_fsm/demo.gif" alt="图1：SGLang和Outlines + vLLM在JSON解码中的比较"></p><p>图一展示了 SGLang 和 Outlines + vLLM 在 JSON 解码任务中的性能比较。这是一个动态对比，目的是展示两者在相同任务下的速度差异。SGLang 采用了一种新的跳跃前进解码算法，通过压缩有限状态机来加速解码过程。相比之下，Outlines + vLLM 使用了传统的逐步解码方法。图中的动画演示了 SGLang 在处理多字符（或标记）解码时的优势，显著减少了解码时间。</p><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>JSON 是数据交换中最重要的格式之一。要求 LLM 始终生成有效的 JSON 可以使 LLM 的输出以结构化方式轻松解析。认识到其重要性，OpenAI 引入了 JSON 模式，它约束模型始终返回有效的 JSON 对象。然而，通常需要更细粒度的控制，以确保生成的 JSON 对象符合特定的模式，例如：</p><img src="/2024/08/13/NLP%20Insights/%E4%BD%BF%E7%94%A8%E5%8E%8B%E7%BC%A9%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA%E8%BF%9B%E8%A1%8C%E6%9C%AC%E5%9C%B0%20LLM%20%E7%9A%84%E5%BF%AB%E9%80%9F%20JSON%20%E8%A7%A3%E7%A0%81/image-1.png" class="" title="图2：遵循JSON模式的约束生成示例"><p>图二展示了一个受限生成的例子，利用大语言模型（LLMs）来生成符合特定 JSON 模式的对象。在这个例子中，左侧的 JSON 模式定义了一个对象，其中包含了 name、age 和 house 三个属性，分别是字符串和整数类型。右侧则显示了受限生成的输出对象，模型通过约束生成技术，生成了符合这些属性的具体实例，如“Harry”的名字、15 岁的年龄以及属于“Gryffindor”的房子。这展示了 LLMs 在生成结构化数据时的能力，同时确保了生成内容符合预定的格式。</p><p>对于本地 LLM，有两种主要方法来引导模型生成符合特定模式的 JSON 对象。</p><h3 id="方法-1：基于有限状态机"><a href="#方法-1：基于有限状态机" class="headerlink" title="方法 1：基于有限状态机"></a>方法 1：基于有限状态机</h3><p>这种方法涉及将 JSON 模式转换为正则表达式。然后，我们可以基于正则表达式构建一个有限状态机（FSM）。FSM 用于引导 LLM 的生成。在 FSM 的每个状态中，我们可以计算允许的转换并识别可接受的下一个标记。这使我们能够在解码过程中跟踪当前状态，并通过对输出应用 logit 偏差来过滤掉无效的标记。你可以在 outlines 论文中了解更多关于这种方法的信息。</p><img src="/2024/08/13/NLP%20Insights/%E4%BD%BF%E7%94%A8%E5%8E%8B%E7%BC%A9%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA%E8%BF%9B%E8%A1%8C%E6%9C%AC%E5%9C%B0%20LLM%20%E7%9A%84%E5%BF%AB%E9%80%9F%20JSON%20%E8%A7%A3%E7%A0%81/image-2.png" class="" title="图3：基于FSM和Logits屏蔽的约束解码。在第一次约束解码过程中，仅允许age。在第二次过程中，由于正则表达式需要数字，因此允许0和1，但LLM更有可能采样1"><p>图三展示了如何利用有限状态机（FSM）来实现受限解码。在这个过程中，首先将 JSON 模式转换为正则表达式，然后利用 FSM 来引导 LLM 的生成。在图中，FSM 状态图展示了 age 字段的受限生成过程，其中只有合法的数字（如 0-9）会被允许。每个状态的转换由正则表达式的规则定义，确保生成的 JSON 数据始终有效。这种方法通过在生成过程中施加限制，来控制 LLM 生成特定的输出。</p><p>FSM 方法利用广义的正则表达式来定义低层次规则，可以应用于广泛的语法，例如 JSON 模式、IP 地址和电子邮件。</p><h4 id="限制："><a href="#限制：" class="headerlink" title="限制："></a>限制：</h4><p>由于 FSM 是在标记级别构建的，因此它只能在每一步通过一个标记来转换状态。因此，它一次只能解码一个标记，导致解码速度较慢。</p><h3 id="方法-2：基于交织"><a href="#方法-2：基于交织" class="headerlink" title="方法 2：基于交织"></a>方法 2：基于交织</h3><p>除了将整个 JSON 模式转换为正则表达式之外，另一种方法是使用基于交织的解码。在这种方法中，给定的 JSON 模式可以分解为几个部分，每个部分包含一个分块预填充部分或一个约束解码部分。这些不同的部分由推理系统交织执行。由于分块预填充可以在一个前向传递中处理多个标记，它比逐标记解码更快。</p><p>Guidance 提供了一套基于交织解码的语法规则，使用 llama.cpp 作为后端。</p><img src="/2024/08/13/NLP%20Insights/%E4%BD%BF%E7%94%A8%E5%8E%8B%E7%BC%A9%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA%E8%BF%9B%E8%A1%8C%E6%9C%AC%E5%9C%B0%20LLM%20%E7%9A%84%E5%BF%AB%E9%80%9F%20JSON%20%E8%A7%A3%E7%A0%81/image-3.png" class="" title="图4：Guidance中的交织JSON解码"><p>图四展示了 Guidance 框架中的交织语法，如何利用交织语法来进行 JSON 的解码。图中的代码片段定义了一个函数，使用 Guidance 语法生成一个包含 name、age 和 house 的 JSON 对象。交织语法通过将不同部分的解码与预填充部分交替进行，能够提高解码速度。图下方展示了这一过程的工作原理，绿色和蓝色的条形代表不同部分的处理过程，展示了交织解码在不同阶段的执行情况。</p><h4 id="限制：-1"><a href="#限制：-1" class="headerlink" title="限制："></a>限制：</h4><ul><li>基于交织的方法需要自定义语法，使其不如单个正则表达式灵活和表达力强。</li><li>由于解码和分块预填充段之间可能存在冲突，处理标记边界时存在困难。</li><li>解释器与后端之间的频繁通信带来了额外的开销。</li></ul><h3 id="我们的方法：使用压缩有限状态机的跳跃前进解码"><a href="#我们的方法：使用压缩有限状态机的跳跃前进解码" class="headerlink" title="我们的方法：使用压缩有限状态机的跳跃前进解码"></a>我们的方法：使用压缩有限状态机的跳跃前进解码</h3><p>通过引入基于压缩有限状态机的新解码算法——跳跃前进解码，我们可以结合 FSM 和交织方法的优点。</p><p>在由 JSON 模式转换的正则表达式引导的解码过程中，当我们达到特定节点时，可以预测即将到来的字符串：</p><p>在图 3 中，解码开始时，根据正则表达式，我们可以预见到接下来的字符串是：</p><figure class="highlight json"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">{</span></span><br><span class="line">  <span class="attr">"name"</span><span class="punctuation">:</span></span><br></pre></td></tr></tbody></table></figure><p>然后进入实际的解码部分。<br>同样，当 LLM 在为角色填写房子属性时输出了 G，我们可以自信地预测下一个字符串将是 ryffindor，从而完成整个字符串为 Gryffindor。</p><p>这正是跳跃前进解码算法加速解码的方式。在跳跃前进算法中，我们检查给定正则表达式的有限状态机，识别所有单一的转换边，并将连续的转换路径压缩为单一路径。我们可以直接预填充（扩展）这些单一路径，跳过逐标记解码，直到下一个分支点。</p><img src="/2024/08/13/NLP%20Insights/%E4%BD%BF%E7%94%A8%E5%8E%8B%E7%BC%A9%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA%E8%BF%9B%E8%A1%8C%E6%9C%AC%E5%9C%B0%20LLM%20%E7%9A%84%E5%BF%AB%E9%80%9F%20JSON%20%E8%A7%A3%E7%A0%81/image-4.png" class="" title="图5：跳跃前进解码与压缩FSM和普通解码的比较"><p>图五展示了跳跃前进解码与普通解码的对比。跳跃前进解码利用压缩的有限状态机，通过提前预测并预填充可能的字符串，减少了逐标记解码的次数。例如，在为 house 字段生成值时，模型在解码过程中直接跳跃并预填充了“Gryffindor”这个字符串，而无需逐字符生成。图中的流程展示了如何通过这种方法提高解码效率，同时避免了不必要的重复计算。<br>图五展示了<strong>压缩有限状态机的跳跃前进解码</strong>与<strong>普通解码</strong>的对比，特别是在生成 JSON 数据时的性能差异。为了更详细地理解这张图，我们需要分步骤分析图中的各个部分。</p><ol><li><p><strong>输入提示</strong>（左侧的绿色部分）：提示模型生成一个符合 JSON 模式的对象。这里的 JSON 对象包括“name”、“age”和“house”三个属性，分别代表名字、年龄和学院。</p></li><li><p><strong>跳跃前进解码过程</strong>（中间部分的蓝色和橙色方块）：</p><ul><li><strong>橙色方块</strong>代表需要约束解码的部分。例如，生成“name”属性时，模型通过跳跃前进解码算法可以直接生成完整的字符串“Harry”。</li><li><strong>蓝色方块</strong>代表模型在跳跃前进过程中逐字符（或逐标记）解码的部分。这种解码方式在遇到非确定性时（例如多个可能的值）才会出现。</li></ul></li><li><p><strong>普通解码过程</strong>（中间部分的蓝色方块）：普通解码需要逐字符或逐标记地生成整个 JSON 对象。相比之下，普通解码方式在处理每一个字符或标记时都需要进行预测和选择，显著降低了解码速度。</p></li><li><p><strong>对比结果</strong>（右侧部分）：</p><ul><li><strong>跳跃前进解码</strong>生成的 JSON 对象展示在最上方，这种方法通过预测并预填充可能的字符串，大大加速了解码过程。例如，在生成“Gryffindor”这个字符串时，模型直接跳过了逐字符生成的步骤。</li><li><strong>普通解码</strong>生成的 JSON 对象展示在最下方，这种方法逐字符解码，虽然能够保证生成的准确性，但效率较低，尤其是在处理长字符串或复杂结构时。</li></ul></li></ol><h3 id="详细解读："><a href="#详细解读：" class="headerlink" title="详细解读："></a>详细解读：</h3><ol><li><p><strong>跳跃前进解码的工作原理</strong>：</p><ul><li>在解码的过程中，模型使用压缩后的有限状态机（FSM）来预测和识别即将生成的字符串。如果模型能在当前上下文中准确预测出接下来要生成的字符串，那么它可以跳过这些字符串的逐标记解码，直接生成整个字符串（例如“Gryffindor”）。</li><li>这种方法利用了正则表达式的结构特点，将连续的转换路径压缩成一个单一路径，从而避免了不必要的逐标记解码步骤。</li></ul></li><li><p><strong>普通解码的限制</strong>：</p><ul><li>普通解码方法需要逐步解码每一个字符或标记，因此在处理复杂的 JSON 对象时效率较低。每一步都需要模型重新计算可能的输出，并从中选择最优解，这会大幅增加解码时间。</li></ul></li><li><p><strong>性能差异</strong>：</p><ul><li>由于跳跃前进解码减少了逐字符解码的次数，并且利用了 FSM 的压缩特性，它在时间和计算资源上的开销都显著低于普通解码。尤其在需要生成大量数据或处理复杂结构时，跳跃前进解码的优势更加明显。</li></ul></li></ol><p>SGLang 的 RadixAttention 机制极大地简化了跳跃前进解码算法的实现。当执行跳跃前进时，我们可以简单地终止当前请求并排入新请求。SGLang 运行时的 RadixAttention 和高效的扩展原语将自动重用前一组标记的 KV 缓存，从而避免冗余计算。</p><h2 id="标记边界处理"><a href="#标记边界处理" class="headerlink" title="标记边界处理"></a>标记边界处理</h2><p>在实现约束解码时，由于字符与标记之间复杂的可能映射关系，处理标记边界总是很棘手。</p><p>在 LLM 解码过程中，它可能更倾向（意味着概率更高）于将多个字符组合成一个标记。例如，在 JSON 解码的上下文中解码”Hello”时，LLM 可能会输出如下标记：<br>“ He llo “，</p><p>而不是解码最后的” ，它总是倾向于将其与后续字符组合成更常见的标记”， 这种效果可能导致一些奇怪的行为。例如，在上述情况下，如果正则表达式设置为”[\w\d\s]*“（不包含最后的”， ），这可能会导致无限解码，因为 LLM 想要停止于”，但该标记是不允许的。</p><p>此外，在跳跃前进解码过程中，我们发现对跳跃前进部分使用不同的标记策略可能会导致后续标记的 logit 分布不同。简单地将标记化的跳跃前进部分附加到当前的标记序列中可能会产生意外的结果。</p><p>为了解决这些问题，我们提出了以下解决方案：</p><ul><li>我们在跳跃前进阶段实施了重新标记化机制。这包括附加字符串而不是标记，然后重新标记整个文本。这种方法有效地解决了大多数标记化问题，并且仅导致计算开销增加约 4%。</li><li><strong>建议</strong>使用综合正则表达式引导整个解码过程，而不是使用多个连接的正则表达式。这种方法确保 FSM 和 LLM 都了解整个解码过程，从而尽量减少与边界相关的问题。<br>你还可以在这篇博客文章中阅读一些额外的讨论。</li></ul><h2 id="基准测试结果"><a href="#基准测试结果" class="headerlink" title="基准测试结果"></a>基准测试结果</h2><p>我们在两个任务上对我们的跳跃前进解码进行了基准测试：</p><ol><li>使用简短的提示生成 JSON 格式的角色数据。</li><li>从长文档中提取城市信息并以 JSON 格式输出。</li></ol><p>我们在 NVIDIA A10 GPU（24GB）上测试了 llama-7B，使用了 vllm v0.2.7，guidance v0.1.0，outlines v0.2.5 和 llama.cpp v0.2.38（Python 绑定）。下图显示了这些方法的吞吐量（使用每个系统支持的最大批次大小）和延迟（批次大小为 1）：</p><img src="/2024/08/13/NLP%20Insights/%E4%BD%BF%E7%94%A8%E5%8E%8B%E7%BC%A9%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA%E8%BF%9B%E8%A1%8C%E6%9C%AC%E5%9C%B0%20LLM%20%E7%9A%84%E5%BF%AB%E9%80%9F%20JSON%20%E8%A7%A3%E7%A0%81/image-5.png" class="" title="图6：基准测试结果"><p>结果表明，使用我们的解码算法的 SGLang 显著优于所有其他系统。它可以将延迟减少最多 2 倍，并将吞吐量提高最多 2.5 倍。在角色生成任务中，即使不使用跳跃前进的 SGLang 也比 Outlines+vLLM 实现了更高的吞吐量；我们怀疑这是由于 Outlines 中的某些开销所致。</p><h2 id="用例"><a href="#用例" class="headerlink" title="用例"></a>用例</h2><p>我们已经与 Boson.ai 测试了这个功能两周，他们正在将这个功能引入他们的生产用例中，因为它保证了更高的解码吞吐量和可靠的响应。</p><p>此外，另一位用户使用此功能通过视觉语言模型 LLaVA 从图像中提取结构化信息。</p><img src="/2024/08/13/NLP%20Insights/%E4%BD%BF%E7%94%A8%E5%8E%8B%E7%BC%A9%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA%E8%BF%9B%E8%A1%8C%E6%9C%AC%E5%9C%B0%20LLM%20%E7%9A%84%E5%BF%AB%E9%80%9F%20JSON%20%E8%A7%A3%E7%A0%81/image-6.png" class="" title="图7：使用SGLang和LLaVA从图像中提取结构化信息">]]></content>
      
      
      <categories>
          
          <category> NLP Insights </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SGLang </tag>
            
            <tag> Structured LLM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Detailed Steps for Running Fine-tuned Gemma-2-2b-it with vLLM</title>
      <link href="/2024/08/07/NLP%20Insights/Running%20Fine-tuned%20Gemma-2-2b-it%20with%20vLLM/"/>
      <url>/2024/08/07/NLP%20Insights/Running%20Fine-tuned%20Gemma-2-2b-it%20with%20vLLM/</url>
      
        <content type="html"><![CDATA[<p>In this post, I will share the steps to run the fine-tuned Gemma-2-2b-it model using vLLM. This guide will cover the installation process, environment configuration, and common troubleshooting tips.</p><h2 id="Installation-and-Verification-of-vLLM"><a href="#Installation-and-Verification-of-vLLM" class="headerlink" title="Installation and Verification of vLLM"></a>Installation and Verification of vLLM</h2><p>First, ensure that you have installed and verified vLLM version 0.5.3.</p><ol><li><p>Install vLLM:</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!pip install vllm==0.5.3</span><br></pre></td></tr></tbody></table></figure></li><li><p>Verify the installation:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> vllm</span><br><span class="line"><span class="built_in">print</span>(vllm.__version__)</span><br><span class="line"><span class="comment"># Output: 0.5.3</span></span><br></pre></td></tr></tbody></table></figure></li></ol><h2 id="Installing-Flashinfer"><a href="#Installing-Flashinfer" class="headerlink" title="Installing Flashinfer"></a>Installing Flashinfer</h2><p>Follow these steps to install Flashinfer, ensuring compatibility with your torch version and CUDA.</p><ol><li><p>Check the torch version and CUDA compatibility:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="built_in">print</span>(torch.__version__)  <span class="comment"># Should output: 2.3.1+cu121</span></span><br><span class="line"><span class="built_in">print</span>(torch.version.cuda) <span class="comment"># Should output: 12.1</span></span><br></pre></td></tr></tbody></table></figure></li><li><p>Install Flashinfer:<br>According to the documentation, Gemma runs on version 0.0.8. vLLM requires FlashInfer v0.0.8 (refer to <a href="https://github.com/vllm-project/vllm/issues/7060">vLLM Version and Flashinfer Documentation</a> for details on Gemma 2).</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!pip install flashinfer==0.0.8 -i https://flashinfer.ai/whl/cu121/torch2.3/</span><br></pre></td></tr></tbody></table></figure></li></ol><h2 id="Updating-Environment-Variables-for-vLLM-Backend"><a href="#Updating-Environment-Variables-for-vLLM-Backend" class="headerlink" title="Updating Environment Variables for vLLM Backend"></a>Updating Environment Variables for vLLM Backend</h2><p>Ensure that Flashinfer is set as the attention mechanism backend for vLLM:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">"VLLM_ATTENTION_BACKEND"</span>] = <span class="string">"FLASHINFER"</span></span><br></pre></td></tr></tbody></table></figure><h2 id="Testing-vLLM"><a href="#Testing-vLLM" class="headerlink" title="Testing vLLM"></a>Testing vLLM</h2><p>Here is the test code to generate text using vLLM:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> vllm <span class="keyword">import</span> LLM, SamplingParams</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">llm = LLM(model=<span class="string">"gemma-2-2b-model"</span>, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">sampling_params = SamplingParams(</span><br><span class="line">    temperature=<span class="number">0.8</span>,</span><br><span class="line">    max_tokens=<span class="number">512</span>,</span><br><span class="line">    top_p=<span class="number">0.95</span>,</span><br><span class="line">    top_k=<span class="number">1</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Example test data</span></span><br><span class="line">test_data = [{<span class="string">"text"</span>: <span class="string">"Input test text 1"</span>}, {<span class="string">"text"</span>: <span class="string">"Input test text 2"</span>}]</span><br><span class="line"></span><br><span class="line">prompts = [</span><br><span class="line">    test_data[random.randint(<span class="number">0</span>, <span class="built_in">len</span>(test_data) - <span class="number">1</span>)][<span class="string">"text"</span>],</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">outputs = llm.generate(</span><br><span class="line">    prompts,</span><br><span class="line">    sampling_params</span><br><span class="line">)</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><p>By following these steps, you should be able to successfully run the fine-tuned Gemma-2-2b-it model.</p><h2 id="Common-Errors-and-Solutions"><a href="#Common-Errors-and-Solutions" class="headerlink" title="Common Errors and Solutions"></a>Common Errors and Solutions</h2><p>Here are some common errors you might encounter and their solutions:</p><ol><li><p><strong>RuntimeError: <code>CHECK_EQ(paged_kv_indptr.size(0), batch_size + 1) failed. 1 vs 257</code></strong></p><ul><li><strong>Cause</strong>: Incorrect Flashinfer version.</li><li><strong>Solution</strong>: Ensure you have installed the correct version of Flashinfer.</li></ul></li><li><p><strong>TypeError: <code>'NoneType' object is not callable</code></strong></p><ul><li><strong>Cause</strong>: Flashinfer is not installed.</li><li><strong>Solution</strong>: Install Flashinfer following the steps above.</li></ul></li><li><p><strong>ValueError: <code>Please use Flashinfer backend for models with logits_soft_cap (i.e., Gemma-2). Otherwise, the output might be wrong. Set Flashinfer backend by export VLLM_ATTENTION_BACKEND=FLASHINFER.</code></strong></p><ul><li><strong>Cause</strong>: Flashinfer backend is not set.</li><li><strong>Solution</strong>: Set the environment variable <code>VLLM_ATTENTION_BACKEND</code> to <code>FLASHINFER</code>.</li></ul></li></ol><p>By following these detailed steps and solutions, you should be able to successfully run and debug the fine-tuned Gemma-2-2b-it model. If you encounter any issues, refer to the relevant documentation or seek help from the community.</p>]]></content>
      
      
      <categories>
          
          <category> NLP Insights </category>
          
      </categories>
      
      
        <tags>
            
            <tag> vLLM </tag>
            
            <tag> Gemma-2-2b-it </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用vLLM运行微调后的Gemma-2</title>
      <link href="/2024/08/07/NLP%20Insights/%E4%BD%BF%E7%94%A8vLLM%E8%BF%90%E8%A1%8C%E5%BE%AE%E8%B0%83%E5%90%8E%E7%9A%84Gemma-2/"/>
      <url>/2024/08/07/NLP%20Insights/%E4%BD%BF%E7%94%A8vLLM%E8%BF%90%E8%A1%8C%E5%BE%AE%E8%B0%83%E5%90%8E%E7%9A%84Gemma-2/</url>
      
        <content type="html"><![CDATA[<h1 id="使用vLLM运行微调后的Gemma-2-2b-it的详细步骤"><a href="#使用vLLM运行微调后的Gemma-2-2b-it的详细步骤" class="headerlink" title="使用vLLM运行微调后的Gemma-2-2b-it的详细步骤"></a>使用vLLM运行微调后的Gemma-2-2b-it的详细步骤</h1><p>在这里分享一下我运行微调后的Gemma-2-2b-it模型并使用vLLM的步骤，希望对其他人有所帮助。本文将详细介绍安装过程、环境配置以及常见问题的解决方法。</p><h2 id="安装和验证vLLM"><a href="#安装和验证vLLM" class="headerlink" title="安装和验证vLLM"></a>安装和验证vLLM</h2><p>首先，确保安装并验证vLLM的版本是0.5.3。</p><ol><li><p>安装vLLM：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install vllm==0.5.3</span><br></pre></td></tr></tbody></table></figure></li><li><p>验证安装：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> vllm</span><br><span class="line"><span class="built_in">print</span>(vllm.__version__)</span><br><span class="line"><span class="comment"># 输出: 0.5.3</span></span><br></pre></td></tr></tbody></table></figure></li></ol><h2 id="安装Flashinfer"><a href="#安装Flashinfer" class="headerlink" title="安装Flashinfer"></a>安装Flashinfer</h2><p>按照以下步骤安装Flashinfer，并确保您的torch版本和CUDA兼容性。</p><ol><li><p>检查torch版本和CUDA兼容性：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="built_in">print</span>(torch.__version__)  <span class="comment"># 应输出: 2.3.1+cu121</span></span><br><span class="line"><span class="built_in">print</span>(torch.version.cuda) <span class="comment"># 应输出: 12.1</span></span><br></pre></td></tr></tbody></table></figure></li><li><p>安装Flashinfer：<br>根据文档，Gemma运行在版本0.08。vLLM需要FlashInfer v0.0.8（请参阅<a href="https://github.com/vllm-project/vllm/issues/7060">vLLM版本和Flashinfer文档</a>中关于Gemma 2的部分）。</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!pip install flashinfer==0.0.8 -i https://flashinfer.ai/whl/cu121/torch2.3/</span><br></pre></td></tr></tbody></table></figure></li></ol><h2 id="更新环境中的VLLM后端变量"><a href="#更新环境中的VLLM后端变量" class="headerlink" title="更新环境中的VLLM后端变量"></a>更新环境中的VLLM后端变量</h2><p>确保设置Flashinfer为vLLM的注意力机制后端：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">"VLLM_ATTENTION_BACKEND"</span>] = <span class="string">"FLASHINFER"</span></span><br></pre></td></tr></tbody></table></figure><h2 id="测试vLLM"><a href="#测试vLLM" class="headerlink" title="测试vLLM"></a>测试vLLM</h2><p>以下是使用vLLM生成文本的测试代码：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> vllm <span class="keyword">import</span> LLM, SamplingParams</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">llm = LLM(model=<span class="string">"gemma-2-2b-model"</span>, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">sampling_params = SamplingParams(</span><br><span class="line">    temperature=<span class="number">0.8</span>,</span><br><span class="line">    max_tokens=<span class="number">512</span>,</span><br><span class="line">    top_p=<span class="number">0.95</span>,</span><br><span class="line">    top_k=<span class="number">1</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例测试数据</span></span><br><span class="line">test_data = [{<span class="string">"text"</span>: <span class="string">"输入测试文本1"</span>}, {<span class="string">"text"</span>: <span class="string">"输入测试文本2"</span>}]</span><br><span class="line"></span><br><span class="line">prompts = [</span><br><span class="line">    test_data[random.randint(<span class="number">0</span>, <span class="built_in">len</span>(test_data) - <span class="number">1</span>)][<span class="string">"text"</span>],</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">outputs = llm.generate(</span><br><span class="line">    prompts,</span><br><span class="line">    sampling_params</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预期输出:</span></span><br><span class="line"><span class="comment"># Processed prompts: 100%|██████████| 1/1 [00:01&lt;00:00,  1.24s/it, est. speed input: 991.44 toks/s, output: 87.79 toks/s]</span></span><br></pre></td></tr></tbody></table></figure><p>通过上述步骤，您应该能够成功运行微调后的Gemma-2-2b-it模型。</p><h2 id="常见错误及解决方法"><a href="#常见错误及解决方法" class="headerlink" title="常见错误及解决方法"></a>常见错误及解决方法</h2><p>在运行过程中，可能会遇到以下常见错误：</p><ol><li><p><strong>RuntimeError: <code>CHECK_EQ(paged_kv_indptr.size(0), batch_size + 1) failed. 1 vs 257</code></strong></p><ul><li><strong>原因</strong>：Flashinfer版本错误。</li><li><strong>解决方法</strong>：请确保安装了正确版本的Flashinfer。</li></ul></li><li><p><strong>TypeError: <code>'NoneType' object is not callable</code></strong></p><ul><li><strong>原因</strong>：没有安装Flashinfer。</li><li><strong>解决方法</strong>：按照上述步骤安装Flashinfer。</li></ul></li><li><p><strong>ValueError: <code>Please use Flashinfer backend for models with logits_soft_cap (i.e., Gemma-2). Otherwise, the output might be wrong. Set Flashinfer backend by export VLLM_ATTENTION_BACKEND=FLASHINFER.</code></strong></p><ul><li><strong>原因</strong>：未设置Flashinfer后端。</li><li><strong>解决方法</strong>：设置环境变量<code>VLLM_ATTENTION_BACKEND</code>为<code>FLASHINFER</code>。</li></ul></li></ol><p>通过上述详细步骤和解决方法，您应该能够成功运行并调试微调后的Gemma-2-2b-it模型。如果您在任何一步遇到问题，请参考相应的文档或在社区中寻求帮助。</p>]]></content>
      
      
      <categories>
          
          <category> NLP Insights </category>
          
      </categories>
      
      
        <tags>
            
            <tag> vLLM </tag>
            
            <tag> Gemma-2 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>如何准确计算固定长度模型的困惑度（PPL）</title>
      <link href="/2024/04/17/NLP%20Insights/%20%E5%9B%B0%E6%83%91%E5%BA%A6(PPL)/"/>
      <url>/2024/04/17/NLP%20Insights/%20%E5%9B%B0%E6%83%91%E5%BA%A6(PPL)/</url>
      
        <content type="html"><![CDATA[<h1 id="如何计算固定长度模型的困惑度（PPL）"><a href="#如何计算固定长度模型的困惑度（PPL）" class="headerlink" title="如何计算固定长度模型的困惑度（PPL）"></a>如何计算固定长度模型的困惑度（PPL）</h1><p>困惑度（PPL）是评估语言模型最常用的指标之一。在深入探讨之前，我们应该注意这个指标特别适用于传统语言模型（有时被称为自回归或因果语言模型），而对于像 BERT 这样的 masked language models 则没有明确定义（见<a href="https://huggingface.co/docs/transformers/main/en/model_summary">模型总结</a>）。</p><p>困惑度被定义为序列的指数化平均负对数似然。如果我们有一个标记化序列 $X = (x_0, x_1, \dots, x_t)$，那么 $X$ 的困惑度为，</p><p>$$<br>\text{PPL}(X) = \exp \left{ -\frac{1}{t}\sum*{i=1}^t \log p<em>\theta (x</em>i|x*{&lt;i}) \right}<br>$$</p><p>其中 $\log p<em>\theta (x_i|x</em>{&lt;i})$ 是第 i 个标记的对数似然，条件是根据我们的模型前面的标记 $x_{&lt;i}$。直观上，它可以被认为是评估模型在语料库中指定标记集合上预测均匀性的能力。重要的是，这意味着标记化程序直接影响模型的困惑度，这在比较不同模型时应始终考虑。</p><p>这也相当于数据和模型预测之间的交叉熵的指数化。想要了解更多关于困惑度及其与每字符位数（BPC）和数据压缩的关系的直觉，可以查看这篇在 The Gradient 上的<a href="https://thegradient.pub/understanding-evaluation-metrics-for-language-models/">精彩博客文章</a>。</p><h2 id="Calculating-PPL-with-fixed-length-models"><a href="#Calculating-PPL-with-fixed-length-models" class="headerlink" title="Calculating PPL with fixed-length models"></a>Calculating PPL with fixed-length models</h2><p>如果我们不受模型上下文大小的限制，我们会通过自回归地分解序列并在每一步都基于整个前序子序列来条件化，从而评估模型的困惑度，如下图所示。</p><img width="600" alt="Full decomposition of a sequence with unlimited context length" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/ppl_full.gif"><p>然而，在处理近似模型时，我们通常受到模型可以处理的标记数量的限制。例如，<a href="https://huggingface.co/docs/transformers/main/en/model_doc/gpt2">GPT-2</a>的最大版本有固定的 1024 个标记长度，所以当 $t$ 大于 1024 时，我们无法直接计算 $p<em>\theta(x_t|x</em>{&lt;t})$。</p><p>相反，序列通常被分解成等于模型最大输入大小的子序列。如果模型的最大输入大小是 $k$，那么我们通过只条件化前 $k-1$ 个标记（而不是整个上下文）来近似计算一个标记 $x_t$ 的似然。在评估模型序列的困惑度时，一种诱人但次优的方法是将序列分解成不相交的块，并独立地累加每个段的分解对数似然。</p><img width="600" alt="Suboptimal PPL not taking advantage of full available context" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/ppl_chunked.gif"><p>这种计算很快，因为每个段的困惑度可以在一次前向传递中计算出来，但这是一个较差的完全分解困惑度的近似，并且通常会产生更高（更差）的 PPL，因为模型在大多数预测步骤中的上下文较少。</p><p>相反，应该使用滑动窗口策略来评估固定长度模型的 PPL。这涉及到重复滑动上下文窗口，使模型在做出每个预测时拥有更多的上下文。</p><img width="600" alt="Sliding window PPL taking advantage of all available context" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/ppl_sliding.gif"><ol><li><p><strong>无限上下文分解：</strong> 如果没有对模型输入长度的限制，我们可以在每一步都使用整个前序子序列来预测下一个标记。这样可以最准确地评估模型的性能，因为每次预测都考虑了所有先前的信息。</p></li><li><p><strong>固定长度限制：</strong> 实际中，大多数模型如 GPT-2 有固定的输入长度限制（例如 1024 个标记）。当序列长度超过这个限制时，不能直接计算每个标记的条件概率，因为不能将整个序列作为条件。</p></li><li><p><strong>分块近似：</strong> 一种处理长序列的方法是将序列分解成多个与模型最大输入长度相等的子序列。每个子序列单独评估，但这种方法可能会因为没有使用完整的上下文而导致更高的困惑度。</p></li><li><p><strong>滑动窗口策略：</strong> 为了更好地利用可用的上下文，可以使用滑动窗口策略。这种方法通过不断移动上下文窗口来尝试在每次预测时为模型提供更多的上下文信息，从而更接近于使用完整上下文的理想情况。</p></li><li><p><strong>跨步滑动窗口：</strong> 一个实际的折中方法是使用跨步滑动窗口，这样可以在保证一定效率的同时，为每次模型预测提供足够的上下文，从而改善困惑度的计算和模型预测的准确性。</p></li></ol><p>这些方法都是为了解决因模型输入长度限制而不能直接评估整个序列的问题，试图通过不同的技术使评估更加准确，同时考虑到计算资源的有效使用。</p>]]></content>
      
      
      <categories>
          
          <category> NLP Insights </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Language Modeling </tag>
            
            <tag> Perplexity </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【Python题解】2834. 找出美丽数组的最小和</title>
      <link href="/2024/03/08/Code%20Chronicles/2834.%20%E6%89%BE%E5%87%BA%E7%BE%8E%E4%B8%BD%E6%95%B0%E7%BB%84%E7%9A%84%E6%9C%80%E5%B0%8F%E5%92%8C/"/>
      <url>/2024/03/08/Code%20Chronicles/2834.%20%E6%89%BE%E5%87%BA%E7%BE%8E%E4%B8%BD%E6%95%B0%E7%BB%84%E7%9A%84%E6%9C%80%E5%B0%8F%E5%92%8C/</url>
      
        <content type="html"><![CDATA[<h1 id="2834-找出美丽数组的最小和"><a href="#2834-找出美丽数组的最小和" class="headerlink" title="2834. 找出美丽数组的最小和"></a>2834. 找出美丽数组的最小和</h1><blockquote><p>Problem: <a href="https://leetcode.cn/problems/find-the-minimum-possible-sum-of-a-beautiful-array/description/">2834. 找出美丽数组的最小和</a></p></blockquote><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><p>给定两个正整数 <code>n</code> 和 <code>target</code>，目标是找到一个长度为 <code>n</code> 的数组，满足以下条件：</p><ul><li>数组由两两不同的正整数组成。</li><li>不存在两个不同下标 <code>i</code> 和 <code>j</code> 使得 <code>nums[i] + nums[j] == target</code>。<br>返回符合条件的美丽数组所可能具备的最小和，并对结果进行 <code>10^9 + 7</code> 取模。</li></ul><h3 id="测试用例"><a href="#测试用例" class="headerlink" title="测试用例"></a>测试用例</h3><p>示例 1：</p><ul><li>输入：n = 2, target = 3</li><li>输出：4</li></ul><p>示例 2：</p><ul><li>输入：n = 3, target = 3</li><li>输出：8</li></ul><p>示例 3：</p><ul><li>输入：n = 1, target = 1</li><li>输出：1</li></ul><h2 id="原始思路"><a href="#原始思路" class="headerlink" title="原始思路"></a>原始思路</h2><h3 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h3><p>初始方案是从最小的数字开始，逐个检查每个数字是否可以被添加到数组中，同时确保不会存在两个数字之和等于 <code>target</code>。</p><ul><li>从 <code>1</code> 开始逐个尝试添加数字到数组。</li><li>对于每个数字，检查是否与数组中已有的数字相加会得到 <code>target</code>。</li><li>如果不会，将其添加到数组中。</li><li>继续此过程，直到数组长度达到 <code>n</code>。</li></ul><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">minimumPossibleSum</span>(<span class="params">n, target</span>):</span><br><span class="line">    selected_nums = <span class="built_in">set</span>([<span class="number">1</span>])</span><br><span class="line">    total_sum = <span class="number">1</span></span><br><span class="line">    current_num = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> <span class="built_in">len</span>(selected_nums) &lt; n:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">all</span>((current_num + num != target) <span class="keyword">for</span> num <span class="keyword">in</span> selected_nums):</span><br><span class="line">            selected_nums.add(current_num)</span><br><span class="line">            total_sum += current_num</span><br><span class="line">        current_num += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> total_sum % (<span class="number">10</span>**<span class="number">9</span> + <span class="number">7</span>)</span><br></pre></td></tr></tbody></table></figure><h3 id="复杂度分析"><a href="#复杂度分析" class="headerlink" title="复杂度分析"></a>复杂度分析</h3><ul><li>时间复杂度：O(n^2)，因为每个数字的添加都需要遍历已选择的数字集合。</li><li>空间复杂度：O(n)，用于存储选择的数字集合。</li></ul><h2 id="贪心优化"><a href="#贪心优化" class="headerlink" title="贪心优化"></a>贪心优化</h2><h3 id="方案-1"><a href="#方案-1" class="headerlink" title="方案"></a>方案</h3><ul><li><p><strong>优化策略</strong>：</p><ul><li><strong>避免集合的使用</strong>：<ul><li>引入“避免”集合，存储所有与已选数字相加得到 <code>target</code> 的数字。</li><li>这样可以快速检查新数字是否会导致和为 <code>target</code> 的情况。</li></ul></li><li><strong>直接检查</strong>：<ul><li>每次选择一个新数字时，仅检查它是否在“避免”集合中。</li><li>不在集合中的数字被认为是安全的，可以直接添加。</li></ul></li><li><strong>动态更新避免集合</strong>：<ul><li>当新数字被添加到美丽数组时，相应的 <code>target - 新数字</code> 也被添加到“避免”集合中。</li><li>这确保任何可能与新数字组成 <code>target</code> 的数字在未来都会被避免。</li></ul></li></ul></li><li><p><strong>优化后的时间复杂度</strong>：</p><ul><li>每个数字只需进行一次集合检查。</li><li>时间复杂度降低为 O(n)，显著提高了算法效率。</li></ul></li></ul><h3 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">minimumPossibleSum_optimized</span>(<span class="params">n, target</span>):</span><br><span class="line">    selected_nums = <span class="built_in">set</span>()</span><br><span class="line">    avoid_nums = <span class="built_in">set</span>()</span><br><span class="line">    total_sum = <span class="number">0</span></span><br><span class="line">    current_num = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> <span class="built_in">len</span>(selected_nums) &lt; n:</span><br><span class="line">        <span class="keyword">if</span> current_num <span class="keyword">not</span> <span class="keyword">in</span> avoid_nums:</span><br><span class="line">            selected_nums.add(current_num)</span><br><span class="line">            total_sum += current_num</span><br><span class="line">            avoid_nums.add(target - current_num)</span><br><span class="line">        current_num += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> total_sum % (<span class="number">10</span>**<span class="number">9</span> + <span class="number">7</span>)</span><br></pre></td></tr></tbody></table></figure><h3 id="复杂度分析-1"><a href="#复杂度分析-1" class="headerlink" title="复杂度分析"></a>复杂度分析</h3><ul><li>时间复杂度：O(n)，因为每个数字只需检查一次。</li><li>空间复杂度：O(n)，用于存储选择的数字和避免数字集合。</li></ul><h2 id="数学方法"><a href="#数学方法" class="headerlink" title="数学方法"></a>数学方法</h2><h3 id="方案-2"><a href="#方案-2" class="headerlink" title="方案"></a>方案</h3><p>针对上述问题，我们采用了一种更高效的数学方法来解决这个问题。该方法通过分析问题的数学本质，减少了必要的计算量，特别适用于处理大规模数据。</p><ol><li><p><strong>问题分解</strong>：</p><ul><li>首先，我们将问题分解为两部分。由于数组中的数字都是唯一的，且两个不同的数字之和不能等于 <code>target</code>，我们首先从最小的数字开始选择，直到我们不能再选择更多的数字而不违反和的规则。</li></ul></li><li><p><strong>选择前半部分的数字</strong>：</p><ul><li>在 <code>1</code> 到 <code>target-1</code> 的范围内，某些数字不能同时出现。例如，如果 <code>target</code> 是 <code>6</code>，则 <code>1</code> 和 <code>5</code>、<code>2</code> 和 <code>4</code> 不能同时出现，因为它们的和等于 <code>6</code>。但是，<code>3</code>（当 <code>target</code> 是偶数）或 <code>3</code> 和 <code>2</code>（当 <code>target</code> 是奇数）是可以被选择的。</li><li>这意味着我们可以自由选择从 <code>1</code> 到 <code>m</code> 的数字，其中 <code>m = min(⌊target/2⌋, n)</code>。对于这部分数字，我们可以直接使用等差数列的求和公式来计算它们的总和，即 <code>m * (m + 1) / 2</code>。</li></ul></li><li><p><strong>选择后半部分的数字</strong>：</p><ul><li>一旦我们选择了前 <code>m</code> 个数字，剩下需要选择的数字的数量就是 <code>n - m</code>。由于我们已经选择了 <code>1</code> 到 <code>m</code>，我们现在需要从 <code>target</code> 开始选择剩下的数字。</li><li>如果 <code>n</code> 大于 <code>m</code>，那么我们将从 <code>target</code> 开始连续选择 <code>n - m</code> 个数字。这些数字的总和可以用等差数列的求和公式来计算，公式为：<code>(2 * target + n - m - 1) * (n - m) / 2</code>。</li></ul></li><li><p><strong>计算总和并取模</strong>：</p><ul><li>我们将两部分的和相加，并对结果进行 <code>10^9 + 7</code> 取模，以得到最终答案。</li></ul></li></ol><h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><ul><li>第一部分和：从 <code>1</code> 到 <code>min(target // 2, n)</code> 的和。</li><li>第二部分和（如果需要）：从 <code>target</code> 开始，选择的 <code>n - min(target // 2, n)</code> 个数字的和。</li><li>将这两部分的和相加，即得到符合条件的美丽数组的最小和。</li></ul><ol><li><p><strong>选择小于 <code>target // 2</code> 的数字</strong>：</p><ul><li>当我们从 1 开始逐渐增加数字，直到 <code>target // 2</code>，这些数字不可能与数组中的其他数字相加得到 <code>target</code>。</li><li>例如，如果 <code>target</code> 是 10，那么 <code>target // 2</code> 是 5。在这种情况下，1 到 5 之间的任何两个数字相加都不会等于 10。</li><li>因此，这部分的选择是安全的，并且由于我们需要最小和，所以我们从 1 开始逐一增加。</li></ul></li><li><p>**当 <code>n</code> 大于 <code>target // 2</code>**：</p><ul><li>如果 <code>n</code> 大于 <code>target // 2</code>，这意味着仅仅选择小于 <code>target // 2</code> 的数字不足以填满数组。</li><li>在这种情况下，我们需要继续选择更多的数字，但为了避免和为 <code>target</code> 的组合，我们需要从 <code>target</code> 本身开始选择。</li><li>我们继续逐一增加，直到数组长度达到 <code>n</code>。</li></ul></li><li><p><strong>计算总和</strong>：</p><ul><li>第一部分是从 1 到 <code>min(target // 2, n)</code> 的和。</li><li>第二部分（如果需要）是从 <code>target</code> 开始，选择剩下的 <code>n - min(target // 2, n)</code> 个数字。</li><li>最后，将这两部分的和加起来，就是我们要找的最小和。</li></ul></li></ol><p>这是一个通过数学方法来解决问题的典型例子，它避免了复杂的编程逻辑，提供了一种更简洁高效的解决方案。</p><h3 id="代码-2"><a href="#代码-2" class="headerlink" title="代码"></a>代码</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">minimumPossibleSum_math_approach</span>(<span class="params">n, target</span>):</span><br><span class="line">    MOD = <span class="number">10</span>**<span class="number">9</span> + <span class="number">7</span></span><br><span class="line">    m = <span class="built_in">min</span>(target // <span class="number">2</span>, n)</span><br><span class="line">    first_half_sum = m * (m + <span class="number">1</span>) // <span class="number">2</span></span><br><span class="line">    remaining = n - m</span><br><span class="line">    second_half_sum = (<span class="number">2</span> * target + remaining - <span class="number">1</span>) * remaining // <span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> (first_half_sum + second_half_sum) % MOD</span><br></pre></td></tr></tbody></table></figure><h3 id="复杂度分析-2"><a href="#复杂度分析-2" class="headerlink" title="复杂度分析"></a>复杂度分析</h3><ul><li>时间复杂度：O(1)，因为结果是通过直接计算得出的。</li><li>空间复杂度：O(1)，只使用了固定数量的变量。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Code Chronicles </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> Leetcode </tag>
            
            <tag> 每日一题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>跟着GPT老师学小聊：如何做一个好的捧哏</title>
      <link href="/2024/03/05/Life%20Reflections/%E8%B7%9F%E7%9D%80GPT%E8%80%81%E5%B8%88%E5%AD%A6%E5%B0%8F%E8%81%8A/"/>
      <url>/2024/03/05/Life%20Reflections/%E8%B7%9F%E7%9D%80GPT%E8%80%81%E5%B8%88%E5%AD%A6%E5%B0%8F%E8%81%8A/</url>
      
        <content type="html"><![CDATA[<p>英语小聊 (small talk) 是日常生活中不可或缺的交流形式，它不仅有助于打破沉默，也能在轻松的氛围中促进理解和友谊。在这篇文章中，我们将结合 10 个主题，提供相关的词汇、短语，并分享生活故事的开场白。同时，学习如何成为一名优秀的捧哏，通过提问和接话，让对话更加流畅和有趣。</p><h3 id="1-旅游体验-Travel-Experiences"><a href="#1-旅游体验-Travel-Experiences" class="headerlink" title="1. 旅游体验 (Travel Experiences)"></a>1. 旅游体验 (Travel Experiences)</h3><ul><li>词汇：Itinerary (行程), off the beaten path (人迹罕至), picturesque (如画的), excursion (远足), landmark (地标)。</li><li>短语：Cultural immersion (文化沉浸), travel off the beaten path (走偏僻的路), soak up the atmosphere (沉浸在气氛中)。</li><li>故事开场： “One place I really enjoyed visiting was…” (我非常喜欢去的一个地方是…)</li><li>追问： “That sounds amazing! What was the most unforgettable part of your trip?” (听起来太棒了！你旅行中最难忘的部分是什么？)</li><li>接话： “I’ve heard that place is beautiful. Did you take a lot of photos?” (我听说那个地方很美。你拍了很多照片吗？)</li></ul><h3 id="2-食物与美食-Food-and-Cuisine"><a href="#2-食物与美食-Food-and-Cuisine" class="headerlink" title="2. 食物与美食 (Food and Cuisine)"></a>2. 食物与美食 (Food and Cuisine)</h3><ul><li>词汇：Gastronomy (美食学), palate (味觉), savory (可口的), gourmet (美食家), culinary (烹饪的)。</li><li>短语：Acquired taste (后天品味), comfort food (安慰食物), fusion cuisine (融合菜肴), culinary delights (烹饪乐趣)。</li><li>故事开场： “I recently tried cooking…” (我最近尝试烹饪…)</li><li>追问： “Oh, how did it turn out? What ingredients did you use?” (哦，结果怎样？你用了哪些食材？)</li><li>接话： “I love trying new recipes too. Do you have any recommendations?” (我也喜欢尝试新食谱。你有什么推荐吗？)</li></ul><h3 id="3-爱好与兴趣-Hobbies-and-Interests"><a href="#3-爱好与兴趣-Hobbies-and-Interests" class="headerlink" title="3. 爱好与兴趣 (Hobbies and Interests)"></a>3. 爱好与兴趣 (Hobbies and Interests)</h3><ul><li>词汇：Amateur (业余爱好者), pastime (消遣), dabble (涉猎), proficiency (熟练), knack (诀窍)。</li><li>短语：Pursue a hobby (追求一个爱好), hone skills (磨练技能), leisure activities (休闲活动), broaden horizons (开阔视野)。</li><li>故事开场： “In my free time, I like to…” (在我空闲的时候，我喜欢…)</li><li>追问： “That’s interesting! How did you get started with that hobby?” (真有趣！你是怎么开始这个爱好的？)</li><li>接话： “It sounds like a great way to relax. I’ve been looking for a new hobby myself.” (听起来是放松的好方式。我自己也在找新的爱好。)</li></ul><h3 id="4-电影、电视节目和书籍-Movies-TV-Shows-and-Books"><a href="#4-电影、电视节目和书籍-Movies-TV-Shows-and-Books" class="headerlink" title="4. 电影、电视节目和书籍 (Movies, TV Shows, and Books)"></a>4. 电影、电视节目和书籍 (Movies, TV Shows, and Books)</h3><ul><li>词汇：Plot (情节), genre (类型), protagonist (主角), cliffhanger (悬念), screenplay (剧本)。</li><li>短语：Twist in the tale (故事的转折), page-turner (扣人心弦的书), critically acclaimed (广受好评), binge-watch (连续看剧)。</li><li>故事开场： “I watched a movie recently, and I found it…” (我最近看了一部电影，我觉得它…)</li><li>追问： “What did you like most about it? Any particular scene or character?” (你最喜欢它的哪个部分？有特别喜欢的场景或角色吗？)</li><li>接话： “I’ve been looking for something good to watch/read. Would you recommend it?” (我一直在找好看/好读的东西。你会推荐它吗？)</li></ul><h3 id="5-当前事件-Current-Events"><a href="#5-当前事件-Current-Events" class="headerlink" title="5. 当前事件 (Current Events)"></a>5. 当前事件 (Current Events)</h3><ul><li>词汇：Geopolitics (地缘政治), humanitarian (人道主义的), legislation (立法), diplomacy (外交), fiscal (财政的)。</li><li>短语：Political turmoil (政治动荡), economic sanctions (经济制裁), diplomatic relations (外交关系), social unrest (社会动乱)。</li><li>故事开场： “I read an interesting news article about…” (我读到了一个有趣的新闻文章，关于…)</li><li>追问： “That does sound interesting. How do you think it will affect us?” (那确实很有趣。你认为它会如何影响我们？)</li><li>接话： “I read something similar. It’s fascinating how quickly things are changing.” (我读过类似的东西。事物变化之快真是令人着迷。)</li></ul><h3 id="6-日常生活与日程-Daily-Life-and-Routine"><a href="#6-日常生活与日程-Daily-Life-and-Routine" class="headerlink" title="6. 日常生活与日程 (Daily Life and Routine)"></a>6. 日常生活与日程 (Daily Life and Routine)</h3><ul><li>词汇：Mundane (平凡的), routine (日常的), chores (杂务), errand (差事), regimen (规律)。</li><li>短语：Daily grind (日常琐事), run errands (做杂事), stick to a routine (遵守日常), day-to-day life (日常生活)。</li><li>故事开场： “A typical day for me involves…” (我的典型一天包括…)</li><li>追问： “Sounds like a busy day. What do you enjoy most in your daily routine?” (听起来是忙碌的一天。你最喜欢日常生活中的哪个部分？)</li><li>接话： “I can relate to that. My mornings are pretty similar. Do you have any morning rituals?” (我能理解。我的早晨也差不多。你有什么晨间仪式吗？)</li></ul><h3 id="7-语言学习-Language-Learning"><a href="#7-语言学习-Language-Learning" class="headerlink" title="7. 语言学习 (Language Learning)"></a>7. 语言学习 (Language Learning)</h3><ul><li>词汇：Fluency (流利), proficiency (精通), bilingual (双语的), immersion (沉浸式), linguistics (语言学)。</li><li>短语：Gain proficiency (提高熟练度), language barrier (语言障碍), mother tongue (母语), pick up a language (学习一种语言)。</li><li>故事开场： “One challenge I face in learning English is…” (我在学习英语时面临的一个挑战是…)</li><li>追问： “I see. What strategies are you using to overcome that challenge?” (我明白了。你用什么策略来克服这个挑战？)</li><li>接话： “Learning a language can be tough. I’m also trying to improve my [language].” (学习一门语言可能很难。我也在努力提高我的[语言]水平。)</li></ul><h3 id="8-文化差异-Cultural-Differences"><a href="#8-文化差异-Cultural-Differences" class="headerlink" title="8. 文化差异 (Cultural Differences)"></a>8. 文化差异 (Cultural Differences)</h3><ul><li>词汇：Etiquette (礼仪), customs (风俗), heritage (遗产), assimilate (同化), diversity (多样性)。</li><li>短语：Cultural exchange (文化交流), societal norms (社会规范), cross-cultural (跨文化), traditional values (传统价值)。</li><li>故事开场： “One thing I find different here compared to my home country is…” (我发现这里和我的祖国相比有一点不同是…)</li><li>追问： “That’s quite interesting. How do you feel about that difference?” (这很有趣。你对这种差异有什么感觉？)</li><li>接话： “Cultural differences are so intriguing. I’ve noticed something similar when I traveled to [country].” (文化差异真的很有趣。我在去[国家]旅行时也注意到了类似的事情。)</li></ul><h3 id="9-科技与趋势-Technology-and-Trends"><a href="#9-科技与趋势-Technology-and-Trends" class="headerlink" title="9. 科技与趋势 (Technology and Trends)"></a>9. 科技与趋势 (Technology and Trends)</h3><ul><li>词汇：Innovative (创新的), cutting-edge (尖端的), algorithm (算法), virtual reality (虚拟现实), automation (自动化)。</li><li>短语：Stay ahead of the curve (保持领先), technological advancements (技术进步), digital age (数字时代), the latest trend (最新趋势)。</li><li>故事开场： “I’m curious about how…” (我对…感到好奇)</li><li>追问： “Why does that interest you? Have you tried it out yourself?” (为什么那会引起你的兴趣？你自己试过了吗？)</li><li>接话： “Technology is advancing so fast. I’m also curious about [specific technology or trend].” (科技进步太快了。我也对[特定科技或趋势]感到好奇。)</li></ul><h3 id="10-个人发展-Personal-Development"><a href="#10-个人发展-Personal-Development" class="headerlink" title="10. 个人发展 (Personal Development)"></a>10. 个人发展 (Personal Development)</h3><ul><li>词汇：Self-improvement (自我提升), mindfulness (正念), resilience (韧性), aspiration (抱负), introspection (反省)。</li><li>短语：Set goals (设定目标), personal growth(个人成长), step out of comfort zone (走出舒适区), life-long learning (终身学习)。</li><li>故事开场： “I’ve been trying to…” (我一直在尝试…)</li><li>追问： “That’s a great goal. How are you planning to achieve it?” (那是个很好的目标。你打算如何实现它？)</li><li>接话： “Self-improvement is so important. I’m also working on [your goal or habit].” (自我提升非常重要。我也在努力[你的目标或习惯]。)</li></ul><p>成为一个好的捧哏不仅能使对话更加深入和有意义，还能展现你的倾听和理解能力。这些问题和评论可以帮助你在各种话题中更好地参与和维持英语对话。试试看，你会发现每次小聊都充满新的发现和乐趣！🌟🗣️💬</p>]]></content>
      
      
      <categories>
          
          <category> Life Reflections </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Language Learning </tag>
            
            <tag> Small Talk </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【Python题解】100226. 在带权树网络中统计可连接服务器对数目</title>
      <link href="/2024/03/04/Code%20Chronicles/%E3%80%90Python%E9%A2%98%E8%A7%A3%E3%80%91100226.%20%E5%9C%A8%E5%B8%A6%E6%9D%83%E6%A0%91%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%BB%9F%E8%AE%A1%E5%8F%AF%E8%BF%9E%E6%8E%A5%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AF%B9%E6%95%B0%E7%9B%AE/"/>
      <url>/2024/03/04/Code%20Chronicles/%E3%80%90Python%E9%A2%98%E8%A7%A3%E3%80%91100226.%20%E5%9C%A8%E5%B8%A6%E6%9D%83%E6%A0%91%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%BB%9F%E8%AE%A1%E5%8F%AF%E8%BF%9E%E6%8E%A5%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AF%B9%E6%95%B0%E7%9B%AE/</url>
      
        <content type="html"><![CDATA[<h3 id="题目：100226-在带权树网络中统计可连接服务器对数目"><a href="#题目：100226-在带权树网络中统计可连接服务器对数目" class="headerlink" title="题目：100226. 在带权树网络中统计可连接服务器对数目"></a>题目：100226. 在带权树网络中统计可连接服务器对数目</h3><h4 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h4><p>你被给定一个未定根的加权树，它有 <code>n</code> 个顶点，代表从 0 到 <code>n - 1</code> 编号的服务器，一个数组 <code>edges</code>，其中 <code>edges[i] = [ai, bi, weighti]</code> 代表顶点 <code>ai</code> 和 <code>bi</code> 之间的双向边，边的权重为 <code>weighti</code>。你还被给定一个整数 <code>signalSpeed</code>。</p><p>如果满足以下条件，两个服务器 <code>a</code> 和 <code>b</code> 可以通过服务器 <code>c</code> 连接：</p><ul><li><code>a &lt; b</code>，<code>a != c</code> 且 <code>b != c</code>。</li><li>从 <code>c</code> 到 <code>a</code> 的距离可被 <code>signalSpeed</code> 整除。</li><li>从 <code>c</code> 到 <code>b</code> 的距离可被 <code>signalSpeed</code> 整除。</li><li>从 <code>c</code> 到 <code>b</code> 和从 <code>c</code> 到 <code>a</code> 的路径不共享任何边。</li></ul><p>返回一个整数数组 <code>count</code>，长度为 <code>n</code>，其中 <code>count[i]</code> 是通过服务器 <code>i</code> 可连接的服务器对数。</p><h4 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h4><p><strong>示例 1</strong>:</p><p>输入: <code>edges = [[0,1,1],[1,2,5],[2,3,13],[3,4,9],[4,5,2]]</code>, <code>signalSpeed = 1</code><br>输出: <code>[0,4,6,6,4,0]</code><br>解释: 由于 <code>signalSpeed</code> 为 1，<code>count[c]</code> 等于从 <code>c</code> 出发且不共享任何边的路径对数。<br>在给定的路径图中，<code>count[c]</code> 等于 <code>c</code> 左侧的服务器数乘以 <code>c</code> 右侧的服务器数。</p><p><strong>示例 2</strong>:</p><p>输入: <code>edges = [[0,6,3],[6,5,3],[0,3,1],[3,2,7],[3,1,6],[3,4,2]]</code>, <code>signalSpeed = 3</code><br>输出: <code>[2,0,0,0,0,0,2]</code><br>解释: 通过服务器 0，有 2 对可连接服务器：(4, 5) 和 (4, 6)。<br>通过服务器 6，有 2 对可连接服务器：(4, 5) 和 (0, 5)。<br>可以证明，除了 0 和 6 之外的服务器无法连接任何两个服务器。</p><h4 id="限制条件"><a href="#限制条件" class="headerlink" title="限制条件"></a>限制条件</h4><ul><li><code>2 &lt;= n &lt;= 1000</code></li><li><code>edges.length == n - 1</code></li><li><code>edges[i].length == 3</code></li><li><code>0 &lt;= ai, bi &lt; n</code></li><li><code>1 &lt;= weighti &lt;= 10^6</code></li><li><code>1 &lt;= signalSpeed &lt;= 10^6</code></li><li>输入保证 <code>edges</code> 表示一个有效的树。</li></ul><hr><h3 id="问题概述"><a href="#问题概述" class="headerlink" title="问题概述"></a>问题概述</h3><p>给定一个表示服务器网络的树结构。每个服务器通过带权重的边与其他服务器连接。目标是计算在树中的每个服务器通过的可连接服务器对的数量，这些条件由<code>signalSpeed</code>定义。</p><h3 id="可连接服务器的条件"><a href="#可连接服务器的条件" class="headerlink" title="可连接服务器的条件"></a>可连接服务器的条件</h3><p>如果满足以下条件，两个服务器 <code>a</code> 和 <code>b</code> 可以通过服务器 <code>c</code> 连接：</p><ul><li><code>a &lt; b</code></li><li><code>a</code> 和 <code>b</code> 都不同于 <code>c</code>。</li><li>从 <code>c</code> 到 <code>a</code> 和从 <code>c</code> 到 <code>b</code> 的距离都能被 <code>signalSpeed</code> 整除。</li><li>从 <code>c</code> 到 <code>a</code> 和从 <code>c</code> 到 <code>b</code> 的路径不共享任何边。</li></ul><h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><ol><li><p><strong>树表示</strong>：树使用邻接表表示，每个服务器连接到其邻居以及连接边的权重。</p></li><li><p><strong>深度优先搜索（DFS）</strong>：</p><ul><li>使用修改后的 DFS 算法从每个服务器开始遍历树。</li><li>该算法计算所有其他服务器与当前服务器的距离。</li><li>DFS 确保在路径中不考虑共享边。</li></ul></li><li><p><strong>计算可连接对</strong>：</p><ul><li>对于每个服务器 <code>c</code>，该算法识别通过移除 <code>c</code> 形成的所有可能的子树。</li><li>它计算每个子树中与 <code>c</code> 的距离能被 <code>signalSpeed</code> 整除的服务器数量。</li><li>通过服务器 <code>c</code> 的可连接对的总数是通过考虑来自不同子树的对的所有可能组合计算出来的。</li></ul></li></ol><h3 id="Python-实现"><a href="#Python-实现" class="headerlink" title="Python 实现"></a>Python 实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">count_connectable_servers</span>(<span class="params">edges, signal_speed</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">build_tree</span>(<span class="params">edges</span>):</span><br><span class="line">        tree = defaultdict(<span class="built_in">list</span>)</span><br><span class="line">        <span class="keyword">for</span> a, b, weight <span class="keyword">in</span> edges:</span><br><span class="line">            tree[a].append((b, weight))</span><br><span class="line">            tree[b].append((a, weight))</span><br><span class="line">        <span class="keyword">return</span> tree</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">dfs_count_nodes</span>(<span class="params">server, parent, distance</span>):</span><br><span class="line">        <span class="keyword">if</span> distance % signal_speed == <span class="number">0</span>:</span><br><span class="line">            count[<span class="number">0</span>] += <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> neighbor, weight <span class="keyword">in</span> tree[server]:</span><br><span class="line">            <span class="keyword">if</span> neighbor != parent:</span><br><span class="line">                dfs_count_nodes(neighbor, server, distance + weight)</span><br><span class="line"></span><br><span class="line">    n = <span class="built_in">len</span>(edges) + <span class="number">1</span></span><br><span class="line">    tree = build_tree(edges)</span><br><span class="line">    counts = [<span class="number">0</span>] * n</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        subtree_counts = []</span><br><span class="line">        <span class="keyword">for</span> neighbor, weight <span class="keyword">in</span> tree[c]:</span><br><span class="line">            count = [<span class="number">0</span>]</span><br><span class="line">            dfs_count_nodes(neighbor, c, weight)</span><br><span class="line">            subtree_counts.append(count[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(subtree_counts)):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i + <span class="number">1</span>, <span class="built_in">len</span>(subtree_counts)):</span><br><span class="line">                counts[c] += subtree_counts[i] * subtree_counts[j]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> counts</span><br></pre></td></tr></tbody></table></figure><h3 id="复杂度分析"><a href="#复杂度分析" class="headerlink" title="复杂度分析"></a>复杂度分析</h3><ul><li><strong>时间复杂度</strong>：该算法对树中的每个服务器执行一次 DFS。由于每条边在每次 DFS 中被访问一次，并且有 <code>n</code> 个服务器，所以总体时间复杂度为 O(n^2)，其中 <code>n</code> 是服务器的数量。</li><li><strong>空间复杂度</strong>：由于存储树结构和在 DFS 过程中使用的辅助数据结构，空间复杂度为 O(n)。</li></ul><h3 id="测试用例"><a href="#测试用例" class="headerlink" title="测试用例"></a>测试用例</h3><p>该解决方案已经通过提供的示例和其他自定义测试用例进行了测试，以确保其正确性。</p><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>这个解决方案有效地计算了在树形网络中，每个服务器通过的可连接服务器对的数量，考虑到了与 <code>signalSpeed</code> 和连接规则相关的给定限制。</p>]]></content>
      
      
      <categories>
          
          <category> Code Chronicles </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> Leetcode </tag>
            
            <tag> 双周赛 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【Python题解】100232. 超过阈值的最少操作数 II</title>
      <link href="/2024/03/03/Code%20Chronicles/%E3%80%90Python%E9%A2%98%E8%A7%A3%E3%80%91100232.%20%E8%B6%85%E8%BF%87%E9%98%88%E5%80%BC%E7%9A%84%E6%9C%80%E5%B0%91%E6%93%8D%E4%BD%9C%E6%95%B0%20II/"/>
      <url>/2024/03/03/Code%20Chronicles/%E3%80%90Python%E9%A2%98%E8%A7%A3%E3%80%91100232.%20%E8%B6%85%E8%BF%87%E9%98%88%E5%80%BC%E7%9A%84%E6%9C%80%E5%B0%91%E6%93%8D%E4%BD%9C%E6%95%B0%20II/</url>
      
        <content type="html"><![CDATA[<h3 id="题目：100232-超过阈值的最少操作数-II"><a href="#题目：100232-超过阈值的最少操作数-II" class="headerlink" title="题目：100232. 超过阈值的最少操作数 II"></a>题目：100232. 超过阈值的最少操作数 II</h3><h3 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h3><p><strong>超过阈值的最少操作数 II</strong></p><p>给定一个从 0 开始的整数数组 <code>nums</code> 和一个整数 <code>k</code>。你可以进行如下操作：</p><ul><li>选择 <code>nums</code> 中最小的两个整数 <code>x</code> 和 <code>y</code>。</li><li>将 <code>x</code> 和 <code>y</code> 从 <code>nums</code> 中删除。</li><li>将 <code>min(x, y) * 2 + max(x, y)</code> 添加到数组中的任意位置。</li></ul><p>注意，只有当 <code>nums</code> 至少包含两个元素时，你才可以执行以上操作。</p><p>目标是使数组中的所有元素都大于或等于 <code>k</code>。请返回实现此目标所需的最少操作次数。</p><h4 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h4><ol><li><p>示例 1：</p><ul><li>输入：<code>nums = [2,11,10,1,3], k = 10</code></li><li>输出：<code>2</code></li><li>解释：第一次操作中，我们删除元素 1 和 2，然后添加 1 _ 2 + 2 到 <code>nums</code> 中，<code>nums</code> 变为 <code>[4, 11, 10, 3]</code>。第二次操作中，我们删除元素 3 和 4，然后添加 3 _ 2 + 4 到 <code>nums</code> 中，<code>nums</code> 变为 <code>[10, 11, 10]</code>。此时，数组中的所有元素都大于等于 10，所以我们停止操作。需要的最少操作次数为 2。</li></ul></li><li><p>示例 2：</p><ul><li>输入：<code>nums = [1,1,2,4,9], k = 20</code></li><li>输出：<code>4</code></li><li>解释：第一次操作后，<code>nums</code> 变为 <code>[2, 4, 9, 3]</code>。第二次操作后，<code>nums</code> 变为 <code>[7, 4, 9]</code>。第三次操作后，<code>nums</code> 变为 <code>[15, 9]</code>。第四次操作后，<code>nums</code> 变为 <code>[33]</code>。此时，数组中的所有元素都大于等于 20，所以我们停止操作。需要的最少操作次数为 4。</li></ul></li></ol><h4 id="提示"><a href="#提示" class="headerlink" title="提示"></a>提示</h4><ul><li><code>2 &lt;= nums.length &lt;= 2 * 10^5</code></li><li><code>1 &lt;= nums[i] &lt;= 10^9</code></li><li><code>1 &lt;= k &lt;= 10^9</code></li><li>输入保证答案一定存在，也就是说一定存在一个操作序列使数组中所有元素都大于等于 <code>k</code>。</li></ul><h3 id="解决策略"><a href="#解决策略" class="headerlink" title="解决策略"></a>解决策略</h3><p>这个问题可以通过贪心算法和最小堆来高效解决。我们每次从数组中选择最小的两个数进行操作，这样可以最快地增加数的总和，更快地达到或超过阈值 <code>k</code>。步骤如下：</p><ol><li><p><strong>初始化</strong>: 将数组转换成最小堆，以便快速找到最小的两个数。</p></li><li><p><strong>执行操作</strong>: 反复执行以下步骤，直到数组中的所有元素都大于或等于 <code>k</code>：</p><ul><li>从堆中弹出最小的两个元素 <code>x</code> 和 <code>y</code>。</li><li>将 <code>2 * min(x, y) + max(x, y)</code> 添加回堆中。</li><li>记录操作次数。</li></ul></li><li><p><strong>返回结果</strong>: 当所有元素都大于或等于 <code>k</code> 时，返回操作次数。</p></li></ol><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> heapq</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">min_operations_to_reach_k</span>(<span class="params">nums, k</span>):</span><br><span class="line">    <span class="comment"># 将数组转换成最小堆</span></span><br><span class="line">    heapq.heapify(nums)</span><br><span class="line">    operations = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 当最小的元素小于k时，继续操作</span></span><br><span class="line">    <span class="keyword">while</span> nums[<span class="number">0</span>] &lt; k:</span><br><span class="line">        <span class="comment"># 如果数组中只剩一个元素，则无法进行操作</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(nums) &lt; <span class="number">2</span>:</span><br><span class="line">            <span class="keyword">return</span> -<span class="number">1</span>  <span class="comment"># 返回-1表示无法达到目标</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 弹出最小的两个元素</span></span><br><span class="line">        x = heapq.heappop(nums)</span><br><span class="line">        y = heapq.heappop(nums)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将新元素插入堆中</span></span><br><span class="line">        heapq.heappush(nums, <span class="number">2</span> * <span class="built_in">min</span>(x, y) + <span class="built_in">max</span>(x, y))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 增加操作计数</span></span><br><span class="line">        operations += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> operations</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试代码</span></span><br><span class="line"><span class="built_in">print</span>(min_operations_to_reach_k([<span class="number">2</span>, <span class="number">11</span>, <span class="number">10</span>, <span class="number">1</span>, <span class="number">3</span>], <span class="number">10</span>))  <span class="comment"># 应为2</span></span><br><span class="line"><span class="built_in">print</span>(min_operations_to_reach_k([<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">9</span>], <span class="number">20</span>))  <span class="comment"># 应为4</span></span><br></pre></td></tr></tbody></table></figure><p>这段代码使用 Python 的 <code>heapq</code> 模块来有效管理最小堆，确保每次都能快速找到最小的两个数。通过这种方法，我们可以高效地解决这个问题。</p>]]></content>
      
      
      <categories>
          
          <category> Code Chronicles </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> Leetcode </tag>
            
            <tag> 双周赛 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【Python题解】100231. 超过阈值的最少操作数 I</title>
      <link href="/2024/03/02/Code%20Chronicles/%E3%80%90Python%E9%A2%98%E8%A7%A3%E3%80%91100231.%20%E8%B6%85%E8%BF%87%E9%98%88%E5%80%BC%E7%9A%84%E6%9C%80%E5%B0%91%E6%93%8D%E4%BD%9C%E6%95%B0%20I/"/>
      <url>/2024/03/02/Code%20Chronicles/%E3%80%90Python%E9%A2%98%E8%A7%A3%E3%80%91100231.%20%E8%B6%85%E8%BF%87%E9%98%88%E5%80%BC%E7%9A%84%E6%9C%80%E5%B0%91%E6%93%8D%E4%BD%9C%E6%95%B0%20I/</url>
      
        <content type="html"><![CDATA[<h1 id="超过阈值的最少操作数-I"><a href="#超过阈值的最少操作数-I" class="headerlink" title="超过阈值的最少操作数 I"></a>超过阈值的最少操作数 I</h1><h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><p>给定一个下标从 0 开始的整数数组 <code>nums</code> 和一个整数 <code>k</code>。在每次操作中，你可以删除 <code>nums</code> 中的最小元素。目标是通过最少的操作次数使数组中的所有元素都大于或等于 <code>k</code>。需要返回实现此目标所需的最少操作次数。</p><h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><p><strong>示例 1:</strong></p><ul><li>输入：<code>nums = [2, 11, 10, 1, 3], k = 10</code></li><li>输出：<code>3</code></li><li>解释：<ul><li>第一次操作后，<code>nums</code> 变为 <code>[2, 11, 10, 3]</code>。</li><li>第二次操作后，<code>nums</code> 变为 <code>[11, 10, 3]</code>。</li><li>第三次操作后，<code>nums</code> 变为 <code>[11, 10]</code>。</li><li>此时，数组中的所有元素都大于等于 <code>10</code>，所以停止操作。</li><li>使数组中所有元素都大于等于 <code>10</code> 需要的最少操作次数为 <code>3</code>。</li></ul></li></ul><p><strong>示例 2:</strong></p><ul><li>输入：<code>nums = [1, 1, 2, 4, 9], k = 1</code></li><li>输出：<code>0</code></li><li>解释：数组中的所有元素都大于等于 <code>1</code>，所以不需要对 <code>nums</code> 做任何操作。</li></ul><p><strong>示例 3:</strong></p><ul><li>输入：<code>nums = [1, 1, 2, 4, 9], k = 9</code></li><li>输出：<code>4</code></li><li>解释：<code>nums</code> 中只有一个元素大于等于 <code>9</code>，所以需要执行 <code>4</code> 次操作。</li></ul><h3 id="提示"><a href="#提示" class="headerlink" title="提示"></a>提示</h3><ul><li><code>1 &lt;= nums.length &lt;= 50</code></li><li><code>1 &lt;= nums[i] &lt;= 10^9</code></li><li><code>1 &lt;= k &lt;= 10^9</code></li><li>输入保证至少有一个满足 <code>nums[i] &gt;= k</code> 的下标 <code>i</code> 存在。</li></ul><h2 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h2><p>要解决这个问题，首先需要对数组 <code>nums</code> 进行排序。排序后，数组中的最小元素将位于数组的起始位置。从数组的最小端开始，每遇到一个小于 <code>k</code> 的元素，就将其删除，并将操作计数器加一。当遇到第一个大于或等于 <code>k</code> 的元素时，停止删除操作。</p><p>给定一个从 0 开始的整数数组 <code>nums</code> 和一个整数 <code>k</code>，目的是通过最少的操作次数使数组中的所有元素都大于或等于 <code>k</code>。操作定义为删除数组中的最小元素。</p><h3 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h3><ol><li>对数组 <code>nums</code> 进行排序。</li><li>初始化操作计数器为 <code>0</code>。</li><li>遍历排序后的数组，对于每个小于 <code>k</code> 的元素，增加操作计数。</li><li>当遇到第一个大于或等于 <code>k</code> 的元素时，停止遍历。</li><li>返回操作计数。</li></ol><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><h4 id="算法思路"><a href="#算法思路" class="headerlink" title="算法思路"></a>算法思路</h4><ol><li><strong>排序数组</strong>：首先将数组排序，这样可以确保我们总是在执行操作时删除最小的元素。</li><li><strong>计数操作</strong>：从数组的最小端开始，对于每个小于 <code>k</code> 的元素，我们将其删除，并增加操作计数器。</li><li><strong>停止条件</strong>：当数组中所有剩余的元素都大于或等于 <code>k</code> 时，操作结束。</li></ol><h4 id="算法步骤-1"><a href="#算法步骤-1" class="headerlink" title="算法步骤"></a>算法步骤</h4><ol><li>对数组 <code>nums</code> 进行排序。</li><li>初始化一个操作计数器。</li><li>遍历排序后的数组，对于每个小于 <code>k</code> 的元素，增加操作计数。</li><li>当遇到第一个大于或等于 <code>k</code> 的元素时，停止遍历。</li><li>返回操作计数。</li></ol><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">min_operations</span>(<span class="params">nums, k</span>):</span><br><span class="line">    <span class="comment"># Sort the array</span></span><br><span class="line">    nums.sort()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Count the number of operations</span></span><br><span class="line">    operations = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> num <span class="keyword">in</span> nums:</span><br><span class="line">        <span class="keyword">if</span> num &lt; k:</span><br><span class="line">            operations += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> operations</span><br></pre></td></tr></tbody></table></figure><h3 id="测试用例"><a href="#测试用例" class="headerlink" title="测试用例"></a>测试用例</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(min_operations([<span class="number">2</span>, <span class="number">11</span>, <span class="number">10</span>, <span class="number">1</span>, <span class="number">3</span>], <span class="number">10</span>))  <span class="comment"># 应输出: 3</span></span><br><span class="line"><span class="built_in">print</span>(min_operations([<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">9</span>], <span class="number">1</span>))     <span class="comment"># 应输出: 0</span></span><br><span class="line"><span class="built_in">print</span>(min_operations([<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">9</span>], <span class="number">9</span>))     <span class="comment"># 应输出: 4</span></span><br></pre></td></tr></tbody></table></figure><h3 id="复杂度分析"><a href="#复杂度分析" class="headerlink" title="复杂度分析"></a>复杂度分析</h3><ul><li><strong>时间复杂度</strong>：O(n log n)，主要由排序步骤决定。</li><li><strong>空间复杂度</strong>：O(n) 或 O(1)，取决于所使用的排序算法。如果使用原地排序算法，空间复杂度可以降低到 O(1)。</li></ul><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>这个解决方案通过排序和简单的线性遍历，有效地找出了将数组中所有元素变得大于或等于 <code>k</code> 所需的最少操作次数。它适用于不同的输入场景，并且在时间和空间效率方面表现良好。</p>]]></content>
      
      
      <categories>
          
          <category> Code Chronicles </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> Leetcode </tag>
            
            <tag> 双周赛 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>【Python题解】2369. 检查数组是否存在有效划分</title>
      <link href="/2024/03/01/Code%20Chronicles/2369.%20%E6%A3%80%E6%9F%A5%E6%95%B0%E7%BB%84%E6%98%AF%E5%90%A6%E5%AD%98%E5%9C%A8%E6%9C%89%E6%95%88%E5%88%92%E5%88%86/"/>
      <url>/2024/03/01/Code%20Chronicles/2369.%20%E6%A3%80%E6%9F%A5%E6%95%B0%E7%BB%84%E6%98%AF%E5%90%A6%E5%AD%98%E5%9C%A8%E6%9C%89%E6%95%88%E5%88%92%E5%88%86/</url>
      
        <content type="html"><![CDATA[<h1 id="【Python-题解】2369-检查数组的有效划分方法"><a href="#【Python-题解】2369-检查数组的有效划分方法" class="headerlink" title="【Python 题解】2369. 检查数组的有效划分方法"></a>【Python 题解】2369. 检查数组的有效划分方法</h1><h2 id="题目概述"><a href="#题目概述" class="headerlink" title="题目概述"></a>题目概述</h2><p>Leetcode 的题目 2369 要求我们检查一个整数数组<code>nums</code>是否可以划分为一个或多个满足特定条件的连续子数组。有效的划分条件包括：</p><ul><li>子数组由两个相等的元素组成，如 <code>[2, 2]</code>。</li><li>子数组由三个相等的元素组成，如 <code>[3, 3, 3]</code>。</li><li>子数组由三个连续递增的元素组成，且相邻元素之间差值为 1，如 <code>[4, 5, 6]</code>。</li></ul><p>如果数组至少存在一种有效划分，则返回<code>true</code>；否则返回<code>false</code>。</p><h2 id="示例分析"><a href="#示例分析" class="headerlink" title="示例分析"></a>示例分析</h2><ul><li>示例 1：<code>nums = [4, 4, 4, 5, 6]</code>可以划分为 <code>[4, 4]</code> 和 <code>[4, 5, 6]</code>，因此返回<code>true</code>。</li><li>示例 2：<code>nums = [1, 1, 1, 2]</code>不满足任何划分条件，因此返回<code>false</code>。</li></ul><h2 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h2><p>采用<strong>动态规划</strong>策略，我们定义一个布尔数组<code>dp</code>，其中<code>dp[i]</code>表示数组的前<code>i</code>个元素是否可以有效地划分。遍历数组，对于每个位置<code>i</code>，尝试以下三种划分方式：</p><ol><li>子数组由最后两个相等的元素组成。</li><li>子数组由最后三个相等的元素组成。</li><li>子数组由最后三个连续递增的元素组成。</li></ol><h2 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h2><ol><li>初始化 dp[0] 为 true，表示空数组可以有效划分。</li><li>对于每个 i（从 2 开始），检查是否可以通过上述三种方式之一将数组划分到 i。</li><li>最终，dp[n]（其中 n 是数组的长度）给出了整个数组是否可以有效划分的答案。</li><li>如果任何一种方式可行，则将<code>dp[i]</code>设置为<code>true</code>。</li></ol><p>具体来讲<br>初始化：</p><ol><li>初始化 dp[0]为 true，表示空数组是可以被有效划分的。</li><li>状态转移：对于数组中的每个元素 nums[i]（从第二个元素开始），考虑以下几种情况：<ul><li>如果 nums[i]与 nums[i-1]相等，检查 dp[i-2]是否为 true。如果是，表示[nums[i-1], nums[i]]可以形成有效划分，设置 dp[i]为 true。</li><li>如果 nums[i]、nums[i-1]和 nums[i-2]三者相等，同样检查 dp[i-3]。如果 dp[i-3]为 true，表示[nums[i-2], nums[i-1], nums[i]]可以形成有效划分，设置 dp[i]为 true。</li><li>如果 nums[i]、nums[i-1]和 nums[i-2]形成连续递增序列（即 nums[i]-1 == nums[i-1]且 nums[i-1]-1 == nums[i-2]），检查 dp[i-3]。若为 true，则设置 dp[i]为 true。</li></ul></li><li>结果判断：<ul><li>最终，检查 dp[n]的值（其中 n 是数组 nums 的长度）。如果 dp[n]为 true，则表示整个数组可以被有效划分；否则，不可以。</li></ul></li></ol><h2 id="动态规划实现"><a href="#动态规划实现" class="headerlink" title="动态规划实现"></a>动态规划实现</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">validPartition</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        n = <span class="built_in">len</span>(nums)</span><br><span class="line">        dp = [<span class="literal">False</span>] * (n + <span class="number">1</span>)</span><br><span class="line">        dp[<span class="number">0</span>] = <span class="literal">True</span>  <span class="comment"># 空数组视为有效划分</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, n + <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> nums[i - <span class="number">1</span>] == nums[i - <span class="number">2</span>]:</span><br><span class="line">                dp[i] = dp[i] <span class="keyword">or</span> dp[i - <span class="number">2</span>]</span><br><span class="line">            <span class="keyword">if</span> i &gt; <span class="number">2</span> <span class="keyword">and</span> nums[i - <span class="number">1</span>] == nums[i - <span class="number">2</span>] == nums[i - <span class="number">3</span>]:</span><br><span class="line">                dp[i] = dp[i] <span class="keyword">or</span> dp[i - <span class="number">3</span>]</span><br><span class="line">            <span class="keyword">if</span> i &gt; <span class="number">2</span> <span class="keyword">and</span> nums[i - <span class="number">1</span>] - <span class="number">1</span> == nums[i - <span class="number">2</span>] <span class="keyword">and</span> nums[i - <span class="number">2</span>] - <span class="number">1</span> == nums[i - <span class="number">3</span>]:</span><br><span class="line">                dp[i] = dp[i] <span class="keyword">or</span> dp[i - <span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dp[n]</span><br></pre></td></tr></tbody></table></figure><h2 id="算法复杂度分析"><a href="#算法复杂度分析" class="headerlink" title="算法复杂度分析"></a>算法复杂度分析</h2><ul><li><strong>时间复杂度</strong>：O(N)，其中 N 是数组<code>nums</code>的长度，需要遍历整个数组来填充动态规划数组。</li><li><strong>空间复杂度</strong>：O(N)，用于存储动态规划数组<code>dp</code>，记录每个位置的划分情况。</li></ul><p>通过动态规划，我们提供了一种高效且直观的解决方案，确保算法的优化性能，适用于类似的数组划分问题。</p>]]></content>
      
      
      <categories>
          
          <category> Code Chronicles </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> Leetcode </tag>
            
            <tag> 每日一题 </tag>
            
            <tag> 动态规划 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Gorilla LLM 大语言模型简介</title>
      <link href="/2024/02/28/NLP%20Insights/Gorilla:%20Large%20Language%20Model%20Connected%20with%20Massive%20APIs/"/>
      <url>/2024/02/28/NLP%20Insights/Gorilla:%20Large%20Language%20Model%20Connected%20with%20Massive%20APIs/</url>
      
        <content type="html"><![CDATA[<h1 id="Gorilla-LLM-大语言模型简介"><a href="#Gorilla-LLM-大语言模型简介" class="headerlink" title="Gorilla LLM 大语言模型简介"></a>Gorilla LLM 大语言模型简介</h1><p>🦍 Gorilla: Large Language Model Connected with Massive APIs<br>Link: <a href="https://gorilla.cs.berkeley.edu/blogs/7_open_functions_v2.html">https://gorilla.cs.berkeley.edu/blogs/7_open_functions_v2.html</a></p><ul><li>Berkeley 功能调用排行榜<a href="https://gorilla.cs.berkeley.edu/leaderboard.html">Berkeley 功能调用排行榜</a></li><li>在线体验模型：<a href="https://gorilla.cs.berkeley.edu/leaderboard.html">Gorilla OpenFunctions-v2 网络演示</a></li><li>项目详情：<a href="https://github.com/ShishirPatil/gorilla/tree/main/openfunctions">GitHub</a></li><li>模型（7B 参数）在 HuggingFace 上的页面：<a href="https://huggingface.co/gorilla-llm/gorilla-openfunctions-v2">gorilla-llm/gorilla-openfunctions-v2</a></li></ul><h2 id="1-伯克利函数调用排行榜"><a href="#1-伯克利函数调用排行榜" class="headerlink" title="1. 伯克利函数调用排行榜"></a>1. 伯克利函数调用排行榜</h2><p>自 2022 年底以来，大语言模型（LLMs）凭借其执行通用任务的强大能力，成为众人关注的焦点。不仅限于聊天应用，将这些模型应用于开发各类 AI 应用和软件（如 Langchain, Llama Index, AutoGPT, Voyager）已成为一种趋势。GPT, Gemini, Llama, Mistral 等模型通过与外部世界的交互，如函数调用和执行，展现了其巨大潜力。</p><p>我们推出了<strong>伯克利函数调用排行榜（BFCL）</strong>，这是首个全面且可执行的 LLMs 函数调用评估。与之前的评估如 Anyscale 函数调用数据集不同，我们考虑了更多形式的函数调用、不同场景下的调用，以及函数调用的可执行性。我们根据实际应用场景构建了这个数据集，涵盖了大多数用户可能遇到的函数调用用例，例如在 AI 智能体或企业工作流程中的应用。为此，我们的评估数据集包含了丰富的类别，覆盖了多种语言。同时，我们还发布了 Gorilla-Openfunctions-v2 模型，这是目前最先进的开源模型，能够处理多种编程语言的函数调用，包括并行和多重函数调用。此外，我们还提供了一项特殊的调试功能，即当提供的函数不符合任务要求时，模型会输出“错误消息”。</p><p><a href="https://gorilla.cs.berkeley.edu/leaderboard#api-explorer">https://gorilla.cs.berkeley.edu/leaderboard#api-explorer</a></p><h3 id="伯克利函数调用排行榜-🏆"><a href="#伯克利函数调用排行榜-🏆" class="headerlink" title="伯克利函数调用排行榜 🏆"></a>伯克利函数调用排行榜 🏆</h3><p>伯克利函数调用</p><p>排行榜（BFCL）旨在全面研究不同 LLMs 在函数调用能力上的表现。它包含了 2000 个包含多种编程语言（Python, Java, JavaScript, REST API）的问题-函数-答案对，覆盖了多样化的应用领域和复杂的用例（如多重函数调用和并行函数调用）。我们还研究了函数相关性检测，以确定模型对不适合的函数如何作出反应（在这种情况下会提供“错误消息”）。具体来说，BFCL 包括了 100 个 Java、50 个 JavaScript、70 个 REST API、100 个 SQL 和 1680 个 Python 的各种简单、并行、多重、可执行函数调用场景以及函数相关性检测。</p><p>排行榜显示，OpenAI 的 GPT-4 在函数调用评估中仍领先，而 Gorilla OpenFunctions-v2（来自 Gorilla LLM）的表现几乎与之媲美。其后是 Mistral-medium 模型（来自 Mistral AI）和 Claude-2.1（来自 Anthropic）。这说明，一个经过微调的开源模型在函数调用任务上也可以达到与专有模型相近的水平，而无需进行复杂的链接。</p><p>我们致力于涵盖真实世界的用例和多样的语言。未来，我们将继续扩展测试领域，并探索更多创新用例。</p><p><img src="https://gorilla.cs.berkeley.edu/assets/img/blog_post_8_Leaderboard.jpg" alt="https://gorilla.cs.berkeley.edu/assets/img/blog_post_8_Leaderboard.jpg"></p><p><em>LLMs 在伯克利函数调用排行榜（BFCL）上的表现</em></p><p>为了更深入地分析和可视化结果，我们提供了一个交互式的六边形工具，供用户比较不同模型的性能。我们将测试分为 9 个类别，包括函数不相关性检测、AST 树检查和执行函数调用检查，用于简单、多重、并行多功能场景。通过这个工具，我们可以清楚地看到各个测试中模型的表现。在简单单一函数调用方面，专有模型和开源模型表现类似。但在涉及多重和并行函数调用时，GPT 系列模型的表现超过了开源模型。</p><p><img src="https://gorilla.cs.berkeley.edu/assets/img/blog_post_8_Wagon.gif" alt="https://gorilla.cs.berkeley.edu/assets/img/blog_post_8_Wagon.gif"></p><p><em>使用伯克利函数调用排行榜（BFCL）六边形图进行的详细分析</em></p><h3 id="数据集组成"><a href="#数据集组成" class="headerlink" title="数据集组成"></a>数据集组成</h3><p>Gorilla OpenFunctions 的评估数据集已从最初的 100 个条目扩展到 1900 个。评估数据集在以下方面展现了多样性：</p><ul><li>函数文档领域</li><li>函数文档和函数调用问答对的数量</li><li>不同编程语言的数据类型</li></ul><p>我们的评估 JSON 函数是从不同网站来源抓取和生成的。我们特意包含了像数学代数、体育足球、金融抵押等领域。我们在评估中包括了 40 个子领域的函数，这使我们能够了解模型性能在数据丰富的领域（如计算和云）以及体育、法律等小众领域的表现。</p><p><img src="https://gorilla.cs.berkeley.edu/assets/img/blog_post_8_data_composition.png" alt="https://gorilla.cs.berkeley.edu/assets/img/blog_post_8_data_composition.png"></p><p><em>伯克利函数调用排行榜（BFCL）数据组成</em></p><h3 id="评估类别-📊"><a href="#评估类别-📊" class="headerlink" title="评估类别 📊"></a>评估类别 📊</h3><p>我们将评估主要分为两大类：</p><ul><li>Python：简单函数、多重函数、并行函数、并行多重函数</li><li>非 Python：函数相关性检测、REST API、SQL、Java、JavaScript</li></ul><h3 id="Python-评估"><a href="#Python-评估" class="headerlink" title="Python 评估"></a>Python 评估</h3><ul><li><strong>简单函数：</strong>这一类别包含了最常见的格式：用户提供一个 JSON 函数文档，模型只调用其中一个函数。</li><li><strong>多重函数：</strong>这一类别包含了需要从 2-4 个 JSON 函数文档中选择并调用一个函数的用户问题。模型需要根据用户提供的上下文选择最合适的函数。</li><li><strong>并行函数：</strong>并行函数定义为使用一个用户查询同时调用多个函数。模型需要判断需要调用多少个函数，问题可以是单个或多个句子。</li><li><strong>并行多重函数：</strong>并行多重函数是并行函数和多重函数的结合，即模型被提供了多个函数文档，每个文档中的函数可能被调用一次或多次。</li></ul><p>每个类别都有相应的可执行类别。在这部分，我们根据一些免费的 REST API 端点（例如获取天气）和直接计算的函数（例如线性回归）编写了函数代码。可执行类别旨在判断函数调用生成是否能够在实际应用中使用。</p><h3 id="非-Python-评估"><a href="#非-Python-评估" class="headerlink" title="非 Python 评估"></a>非 Python 评估</h3><p>除了上述主要类别外，我们还包含了更具体的类别来评估模型在不同场景下的表现，并测试其对不相关问题和函数文档的应对能力。</p><ul><li>函数相关性检测：这一类别设计了一个场景，其中提供的任何函数都不相关，也不应该被调用。我们期望模型的输出是没有函数调用。这个场景帮助我们了解模型是否会在缺乏生成函数代码的信息时产生错误。</li><li><strong>REST API</strong>：现实世界中的大多数 API 调用都是 REST API 调用。Python 主要通过 requests.get(), requests.post(), requests.delete()等方法在 python requests 库中完成 REST API 调用。</li><li><strong>GET 请求：</strong>GET 请求是现实世界中最常用的。因此，我们包括了真实世界的 GET 请求来测试模型生成可执行 REST API 调用的能力。我们的评估包括两种变体：一种是需要在 URL 中传递参数的，另一种是需要将参数作为键/值对放入 requests.get()的 params 和/或 headers 中。模型需要根据情况决定如何调用。</li><li><strong>SQL：</strong>SQL 评估数据包括我们定制的 sql.execute 函数，其中包含 sql_keyword, table_name, columns 和 conditions。这些参数提供了构建简单 SQL 查询的必要信息。我们希望通过函数调用可靠地构建和使用 SQL 查询，而不是专门训练一个 SQL 模型。我们的评估数据集限制了场景，仅支持简单的关键词，如“SELECT”, “INSERT INTO”, “UPDATE”, “DELETE”, “CREATE”。</li><li><strong>Java + JavaScript：</strong>尽管大多数编程语言的函数调用格式相同，但每种编程语言都有其特有的类型。例如，C 有指针类型，Java 有 HashMap 类型。这个测试类别的目的是了解函数调用模型如何扩展到不仅仅是 JSON 和 Python 类型，还包括所有特定于语言的类型。</li></ul><p>这些类别使我们能够看到不同模型在 API 调用的流行用例中的表现，并为我们提供了关于函数调用模型潜力的洞察。</p><h3 id="评估指标-📈"><a href="#评估指标-📈" class="headerlink" title="评估指标 📈"></a>评估指标 📈</h3><p>我们使用两种流行的方法来评估模型生成答案的准确性：AST 检查器和执行检查器。理想情况下，应使用执行检查器，但由于并非所有结果都容易执行（如 Java 函数），我们使用 AST 作为执行检查器的补充。</p><ul><li>抽象语法树（AST）检查器</li><li>执行检查器</li></ul><p>AST 检查：对于可执行的函数调用答案，我们使用 AST 树进行解析。</p><p>示例：<code>[calculate_triangle_area(base=10, height=5)]</code></p><p>解析：<code>Module(body=[Expr(value=List(elts=[Call(func=Name(id='calculate_triangle_area', ctx=Load()), args=[], keywords=[keyword(arg='base', value=Constant(value=10)), keyword(arg='height', value=Constant(value=5))])], ctx=Load()))], type_ignores=[]) [calculate_triangle_area(base=10, height=5)]</code></p><p>我们从 AST 中提取变量，并检查每个参数是否在可能的答案中找到并精确匹配。对于每个可能的答案，应接受的答案包括：</p><ul><li>布尔值：<ul><li>我们检查布尔值的直接匹配，不允许对布尔值的字符串版本有宽容。</li></ul></li><li>整数、浮点数：<ul><li>答案应该是唯一的，例如 [1]</li></ul></li><li>列表：<ul><li>我们检查精确匹配，因此任何顺序的列表都应匹配。[1,2,3]==[2,3,1]</li></ul></li><li>字典：<ul><li>为简化，我们跳过检查递归 AST 字典结构。</li></ul></li><li>字符串：<ul><li>可能的日期 “20th June”, “2023-06-20”, “06/20/2023”, “Jun.20,2023”</li><li>可能的位置 [“New York City”, “NYC”]</li><li>可能的任何东西 [“Manchester United”, “Man United”, “Man U”, “MUFC”]</li></ul></li></ul><p>以下是一些可能的答案示例：</p><ul><li><code>{"calculate_triangle_area": {"base": [10], "height": [5], "unit": ["units", "unit"]}}</code></li><li><code>{"predict_house_price": {"bedrooms": [3], "bathrooms": [2], "area": [1800], "location": ["San Francisco", "San Francisco, CA"]}}</code></li></ul><p>这种检查机制适用于除了 executable_*和 REST 之外的所有内容。</p><p>可执行检查：对于 executable_*和 REST，我们有相应的函数，可以为每个问题执行。因此，在模型生成答案后，我们将直接执行这些答案。有两种类型的匹配：</p><ul><li>确定性的可执行输出：我们根据我们人类执行的结果检查精确匹配。</li><li>非确定性和现实世界相关的可执行输出：我们检查其响应类型和响应 JSON 键的一致性，看看值是否是我们期望看到的。</li></ul><h3 id="提示"><a href="#提示" class="headerlink" title="提示"></a>提示</h3><p>我们提供了用于评估我们的专有和开源模型的所有提示。对于函数调用模型，我们没有提供任何系统提示，而是直接启用函数调用模式并放置函数定义。对于聊天模型，我们提供了明确的系统消息。</p><ol><li><p>对于所有函数调用模型，我们直接启用函数调用模式并放置函数定义。</p></li><li><p>对于聊天模型，我们提供了明确的系统消息：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">SYSTEM_PROMPT_FOR_CHAT_MODEL = """"</span><br><span class="line">你是一个编写函数的专家。你会收到一个问题和一系列可能的函数。</span><br><span class="line">根据问题，你需要进行一个或多个函数/工具调用来实现目的。</span><br><span class="line">如果没有一个函数可以使用，请指出。如果给定问题缺少函数所需的参数，</span><br><span class="line">也请指出。你应该只在工具调用部分返回函数调用。</span><br><span class="line">"""</span><br><span class="line"></span><br><span class="line">SYSTEM_PROMPT_FOR_CHAT_MODEL = """"</span><br><span class="line">You are an expert in composing functions. You are given a question and a set of possible functions.</span><br><span class="line">Based on the question, you will need to make one or more function/tool calls to achieve the purpose.</span><br><span class="line">If none of the function can be used, point it out. If the given question lacks the parameters required by the function,</span><br><span class="line">also point it out. You should only return the function call in tools call sections.</span><br><span class="line">"""</span><br></pre></td></tr></tbody></table></figure><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">USER_MESSAGE_FOR_CHAT_MODEL = "Questions:{user_prompt}\\n这里是一系列你可以调用的JSON格式函数列表:\\n{functions}. 如果你决定返回函数调用，不得包含其他文本。"</span><br><span class="line">USER_MESSAGE_FOR_CHAT_MODEL = "Questions:{user_prompt}\nHere is a list of functions in JSON format that you can invoke:\n{functions}. Should you decide to return the function call(s), NO other text MUST be included."</span><br></pre></td></tr></tbody></table></figure></li></ol><h3 id="常见错误"><a href="#常见错误" class="headerlink" title="常见错误"></a>常见错误</h3><p>通过我们的基准测试 BFCL，我们能够识别 LLMs 在生成函数调用时所犯的一些常见错误。这些错误揭示了当前模型的局限性，并为如何改进</p><p>它们提供了洞察。</p><ol><li><p>GPT 的函数文档难以格式化，其类型在现实世界场景中受到限制。例如，我们需要将 float 手动转换为 number，以使函数与 OpenAI 兼容。此外，数字相比 float 在精度和类型一致性方面信息传递较少。</p><p>在 Gorilla Openfunctions-v2 中，我们通过不限制参数类型来提高函数文档的灵活性。换言之，用户可以提供 Tuple、Float，甚至 Java 中的特定类型，如 HashMap 和 LinkedList。</p></li><li><p>GPT 在需要某种隐式转换的参数场景中表现不佳。例如，当参数不是直接在用户问题中给出时。</p><figure class="highlight json"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">"Function"</span><span class="punctuation">:</span></span><br><span class="line"><span class="punctuation">{</span></span><br><span class="line">    <span class="attr">"name"</span><span class="punctuation">:</span> <span class="string">"finance.predict_future_value"</span><span class="punctuation">,</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="attr">"parameters"</span><span class="punctuation">:</span></span><br><span class="line">    <span class="punctuation">{</span></span><br><span class="line">        <span class="attr">"type"</span><span class="punctuation">:</span> <span class="string">"object"</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">"properties"</span><span class="punctuation">:</span></span><br><span class="line">        <span class="punctuation">{</span></span><br><span class="line">            <span class="attr">"present_value"</span><span class="punctuation">:</span></span><br><span class="line">            <span class="punctuation">{</span></span><br><span class="line">                <span class="attr">"type"</span><span class="punctuation">:</span> <span class="string">"number"</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">"description"</span><span class="punctuation">:</span> <span class="string">"The present value of the investment."</span></span><br><span class="line">            <span class="punctuation">}</span><span class="punctuation">,</span></span><br><span class="line"></span><br><span class="line">            <span class="attr">"annual_interest_rate"</span><span class="punctuation">:</span></span><br><span class="line">            <span class="punctuation">{</span></span><br><span class="line">                <span class="attr">"type"</span><span class="punctuation">:</span> <span class="string">"number"</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">"description"</span><span class="punctuation">:</span> <span class="string">"The annual interest rate of the investment."</span></span><br><span class="line">            <span class="punctuation">}</span><span class="punctuation">,</span></span><br><span class="line"></span><br><span class="line">            <span class="attr">"compounding_periods_per_year"</span><span class="punctuation">:</span></span><br><span class="line">            <span class="punctuation">{</span></span><br><span class="line">                <span class="attr">"type"</span><span class="punctuation">:</span> <span class="string">"integer"</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">"description"</span><span class="punctuation">:</span> <span class="string">"The number of times that interest is compounded per year."</span><span class="punctuation">,</span></span><br><span class="line">            <span class="punctuation">}</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">"time_years"</span><span class="punctuation">:</span></span><br><span class="line">            <span class="punctuation">{</span></span><br><span class="line">                <span class="attr">"type"</span><span class="punctuation">:</span> <span class="string">"integer"</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">"description"</span><span class="punctuation">:</span> <span class="string">"The investment horizon in years."</span></span><br><span class="line">            <span class="punctuation">}</span></span><br><span class="line">            ...</span><br><span class="line">        <span class="punctuation">}</span></span><br><span class="line">        <span class="attr">"required"</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">"present_value"</span><span class="punctuation">,</span> <span class="string">"annual_interest_rate"</span><span class="punctuation">,</span> <span class="string">"time_years"</span><span class="punctuation">]</span></span><br><span class="line">    <span class="punctuation">}</span></span><br><span class="line"><span class="punctuation">}</span></span><br></pre></td></tr></tbody></table></figure><p>Questions : Predict the future value of a $5000 investment with an annual interest rate of 5% in 3 years with monthly compounding.</p><figure class="highlight json"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">GPT<span class="number">-4</span> output<span class="punctuation">:</span></span><br><span class="line"><span class="punctuation">[</span><span class="punctuation">{</span></span><br><span class="line">    <span class="attr">"name"</span><span class="punctuation">:</span> <span class="string">"finance.predict_future_value"</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">"parameters"</span><span class="punctuation">:</span></span><br><span class="line">    <span class="punctuation">{</span></span><br><span class="line">        <span class="attr">"present_value"</span><span class="punctuation">:</span> <span class="number">5000</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">"annual_interest_rate"</span><span class="punctuation">:</span> <span class="number">5</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">"compounding_periods_per_year"</span><span class="punctuation">:</span> <span class="number">12</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">"time_years"</span><span class="punctuation">:</span> <span class="number">3</span></span><br><span class="line">    <span class="punctuation">}</span></span><br><span class="line"><span class="punctuation">}</span><span class="punctuation">]</span></span><br></pre></td></tr></tbody></table></figure><figure class="highlight json"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Gorilla-openfunctions-v2 output<span class="punctuation">:</span></span><br><span class="line"><span class="punctuation">[</span><span class="punctuation">{</span></span><br><span class="line">    <span class="attr">"name"</span><span class="punctuation">:</span> <span class="string">"finance.predict_future_value"</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">"parameters"</span><span class="punctuation">:</span></span><br><span class="line">    <span class="punctuation">{</span></span><br><span class="line">        <span class="attr">"present_value"</span><span class="punctuation">:</span> <span class="number">5000</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">"annual_interest_rate"</span><span class="punctuation">:</span> <span class="number">0.05</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">"compounding_periods_per_year"</span><span class="punctuation">:</span> <span class="number">12</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">"time_years"</span><span class="punctuation">:</span> <span class="number">3</span></span><br><span class="line">    <span class="punctuation">}</span></span><br><span class="line"><span class="punctuation">}</span><span class="punctuation">]</span></span><br></pre></td></tr></tbody></table></figure></li><li><p>聊天模型倾向于生成格式错误的函数调用，其中参数可以提取但无法执行。</p><p>mistral-medium 生成的结果示例如下：<code>solve_quadratic_equation(a=2, b=6, c=5)</code>。通过 gorilla-openfunctions-v2，我们能够直接输出<code>solve_quadratic_equation(a=3, b=2, c=1)</code>，该结果在接收后即可执行。</p></li><li><p>REST API 调用不一致：例如，某些情况下模型可能无法正确生成 API 调用的 URL 或参数。</p></li></ol><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>我们通过 Gorilla Open Functions 排行榜对 LLMs 函数调用进行了全面和系统性的评估。研究表明，在不涉及复杂规划和链式函数调用的简单函数调用方面，经过微调的开源模型可以与专有模型相媲美。此外，我们还推出了 Gorilla Open Functions v2，这是一个开源模型，可以帮助用户通过函数调用构建 AI 应用，并实现与 json 兼容的输出交互。</p><p>我们希望您喜欢这篇博客文章。欢迎您在<a href="https://discord.gg/SwTyuTAxX3">Discord</a>、<a href="https://twitter.com/shishirpatil_/status/1661780076277678082">Twitter (#GorillaLLM)</a>和<a href="https://github.com/ShishirPatil/gorilla/">GitHub</a>上分享您的想法。</p><p>如果您想引用 Gorilla：</p><figure class="highlight json"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">@misc<span class="punctuation">{</span>berkeley-function-calling-leaderboard<span class="punctuation">,</span></span><br><span class="line">  title=<span class="punctuation">{</span>Berkeley Function Calling Leaderboard<span class="punctuation">}</span><span class="punctuation">,</span></span><br><span class="line">  author=<span class="punctuation">{</span>Fanjia Yan and Huanzhi Mao and Charlie Cheng-Jie Ji and Tianjun Zhang and Shishir G. Patil and Ion Stoica and Joseph E. Gonzalez<span class="punctuation">}</span><span class="punctuation">,</span></span><br><span class="line">  howpublished=<span class="punctuation">{</span>\url<span class="punctuation">{</span>https<span class="punctuation">:</span><span class="comment">//gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html}},</span></span><br><span class="line">  year=<span class="punctuation">{</span><span class="number">2024</span><span class="punctuation">}</span><span class="punctuation">,</span></span><br><span class="line"><span class="punctuation">}</span></span><br></pre></td></tr></tbody></table></figure><h2 id="2-Gorilla-OpenFunctions-v2"><a href="#2-Gorilla-OpenFunctions-v2" class="headerlink" title="2. Gorilla OpenFunctions v2"></a>2. Gorilla OpenFunctions v2</h2><p><img src="https://gorilla.cs.berkeley.edu/assets/img/blog_post_7_demo.gif" alt="https://gorilla.cs.berkeley.edu/assets/img/blog_post_7_demo.gif"></p><p><em>Gorilla OpenFunctions-v2！在开源模型中技术领先（SoTA），与商业模型媲美。</em></p><p>Gorilla OpenFunctions 的最新版本——版本 2，带来了大语言模型（LLM）在开源社区中函数调用方面的重大进展。作为前一版本的升级替代，Gorilla OpenFunctions-v2 不仅保持了开源精神，还引入了令人兴奋的新功能。这包括支持 Python、Java、JavaScript 和 REST API 等</p><p>多种编程语言——这在开源和闭源模型中都是首次；同时具备处理多个和并行函数调用的能力，以及判断函数相关性的能力。这次更新巩固了 gorilla-openfunctions-v2 在 LLM 领域中函数调用能力的领先地位。而且，这种即插即用的更新方式使得 OpenFunctions 可以轻松集成到各种应用中，从社交媒体平台如 Instagram 到送货服务如 Doordash，还有包括 Google Calendar 和 Stripe 等实用工具。</p><h3 id="新功能速览-🚀"><a href="#新功能速览-🚀" class="headerlink" title="新功能速览!! 🚀"></a>新功能速览!! 🚀</h3><p>我们在 OpenFunctions-v2 中推出的五个激动人心的新功能包括：</p><p><img src="https://gorilla.cs.berkeley.edu/assets/img/blog_post_7_open_function_v2_features.png" alt="https://gorilla.cs.berkeley.edu/assets/img/blog_post_7_open_function_v2_features.png"></p><ul><li><strong>支持更多数据类型：</strong> Gorilla Open Functions v2 现在能支持多种语言，并扩展了对函数调用中参数类型的支持。例如，对于 Python，支持的类型包括<code>[string, number, boolean, list, tuple, dict, any]</code>；Java 和 Javascript 同样支持丰富的类型。相比之下，OpenAI 和许多其他公司仅支持 JSON 模式，即<code>[string, number, integer, object, array, boolean]</code>。这种对类型的原生支持意味着您现在可以更方便地使用 openfunctions-v2。</li><li><strong>支持并行和多功能：</strong> 可以处理并行和多功能调用。在多功能场景中，用户可以在不确定哪个功能最合适时输入多个功能；Gorilla 模型将从中选择一个或多个（或不选择）来响应用户的请求。在并行功能中，可以通过多次调用同一功能来响应用户的提示。Gorilla 模型不仅同时支持这两种模式，还能将它们的优势结合起来。</li><li><strong>功能相关性检测：</strong> 在没有提供功能或相关功能的情况下减少错误响应。Gorilla openfunctions v2 现在能自动判断提供给模型的功能是否能够解决用户的问题。识别到这一点后，LLM 会向用户展示一个错误信息，提供更多帮助。</li><li><strong>增强的 RESTful API 能力：</strong> 提升了格式化 RESTful API 调用的能力。RESTful API 广泛应用于网络中，为许多流行的软件服务（如 Slack、PayPal 等）提供支持。我们的模型经过特殊训练，能够高质量地处理 RESTful API 调用。</li></ul><p>快速链接：</p><ul><li>其他功能调用模型的表现：<a href="https://gorilla.cs.berkeley.edu/leaderboard.html">Berkeley 功能调用排行榜</a></li><li>在线体验模型：<a href="https://gorilla.cs.berkeley.edu/leaderboard.html">Gorilla OpenFunctions-v2 网络演示</a></li><li>项目详情：<a href="https://github.com/ShishirPatil/gorilla/tree/main/openfunctions">GitHub</a></li><li>模型（7B 参数）在 HuggingFace 上的页面：<a href="https://huggingface.co/gorilla-llm/gorilla-openfunctions-v2">gorilla-llm/gorilla-openfunctions-v2</a></li></ul><h3 id="在您的应用中集成-OpenFunctions-v2-🔨"><a href="#在您的应用中集成-OpenFunctions-v2-🔨" class="headerlink" title="在您的应用中集成 OpenFunctions-v2 🔨"></a>在您的应用中集成 OpenFunctions-v2 🔨</h3><p>使用 Gorilla OpenFunctions-v2 非常简单：</p><ol><li>为了便于快速原型开发，我们提供了一个托管的 Gorilla Openfunctions-v2 模型供推理使用。您也可以在本地运行它，或通过 HuggingFace 的页面自行托管。以下示例展示了如何调用托管的 gorilla openfunctions v2 模型：</li></ol><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> openai</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_gorilla_response</span>(<span class="params">prompt=<span class="string">""</span>, model=<span class="string">"gorilla-openfunctions-v2"</span>, functions=[]</span>):</span><br><span class="line">    openai.api_key = <span class="string">"EMPTY"</span>  <span class="comment"># 由UC Berkeley免费托管 ❤️</span></span><br><span class="line">    openai.api_base = <span class="string">"&lt;http://luigi.millennium.berkeley.edu:8000/v1&gt;"</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">    completion = openai.ChatCompletion.create(</span><br><span class="line">        model=<span class="string">"gorilla-openfunctions-v2"</span>,</span><br><span class="line">        temperature=<span class="number">0.0</span>,</span><br><span class="line">        messages=[{<span class="string">"role"</span>: <span class="string">"user"</span>, <span class="string">"content"</span>: prompt}],</span><br><span class="line">        functions=functions,</span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># completion.choices[0].message.content, 函数调用的字符串格式</span></span><br><span class="line">    <span class="comment"># completion.choices[0].message.functionsl, 函数调用的Json格式</span></span><br><span class="line">    <span class="keyword">return</span> completion.choices[<span class="number">0</span>]</span><br></pre></td></tr></tbody></table></figure><ol><li>向模型提问：<br><code>波士顿和旧金山的天气怎么样？</code></li><li>格式化您的功能调用：模型将根据您的请求返回功能调用。</li></ol><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">query = <span class="string">"波士顿和旧金山的天气怎么样？"</span></span><br><span class="line">functions = [</span><br><span class="line">    {</span><br><span class="line">        <span class="string">"name"</span>: <span class="string">"get_current_weather"</span>,</span><br><span class="line">        <span class="string">"description"</span>: <span class="string">"获取指定地点的当前天气"</span>,</span><br><span class="line">        <span class="string">"parameters"</span>: {</span><br><span class="line">            <span class="string">"type"</span>: <span class="string">"object"</span>,</span><br><span class="line">            <span class="string">"properties"</span>: {</span><br><span class="line">                <span class="string">"location"</span>: {</span><br><span class="line">                    <span class="string">"type"</span>: <span class="string">"string"</span>,</span><br><span class="line">                    <span class="string">"description"</span>: <span class="string">"城市和州，比如旧金山，CA"</span>,</span><br><span class="line">                },</span><br><span class="line">                <span class="string">"unit"</span>: {<span class="string">"type"</span>: <span class="string">"string"</span>, <span class="string">"enum"</span>: [<span class="string">"celsius"</span>, <span class="string">"fahrenheit"</span>]},</span><br><span class="line">            },</span><br><span class="line">            <span class="string">"required"</span>: [<span class="string">"location"</span>],</span><br><span class="line">        },</span><br><span class="line">    }</span><br><span class="line">]</span><br></pre></td></tr></tbody></table></figure><ol><li>获取您的功能调用：模型将根据您的请求返回一个 Python 功能调用。<br>这为开发人员和非开发人员提供了便利，使他们能够利用复杂功能而无需编写大量代码。</li></ol><p>输入：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">get_gorilla_response(prompt=query, functions=[functions])</span><br></pre></td></tr></tbody></table></figure><p>输出：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[get_current_weather(location=<span class="string">'Boston, MA'</span>), get_current_weather(location=<span class="string">'San Francisco, CA'</span>)]</span><br></pre></td></tr></tbody></table></figure><p>通过上面的示例，您可以利用 Gorilla OpenFunctions-v2 生成格式良好的输出，或用您自己的定义调用函数！然后，您可以在您的应用程序和聊天机器人中自由地使用这些功能！</p><p>注意：Gorilla 目前仅支持<code>openai==0.28.1</code>版本的托管端点。我们很快将升级以支持<code>openai==1.xx</code>版本，届时<code>functions</code>将被<code>tool_calls</code>替换。</p><h3 id="Berkeley-功能调用排行榜上的表现-🔥"><a href="#Berkeley-功能调用排行榜上的表现-🔥" class="headerlink" title="Berkeley 功能调用排行榜上的表现 🔥"></a>Berkeley 功能调用排行榜上的表现 🔥</h3><p><img src="https://gorilla.cs.berkeley.edu/assets/img/blog_post_7_open_function_v2_summary.png" alt="https://gorilla.cs.berkeley.edu/assets/img/blog_post_7_open_function_v2_summary.png"></p><p><em>我们在 Berkeley 功能调用排行榜上进行了全面和详尽的评估，我们的模型与目前技术最先进的 GPT-4-1106 预览版以及 GPT-4 和 GPT-3.5-turbo 功能调用特性进行了对比。此外，我们还将我们的模型与其他开源模型进行了比较，展示了其优越性能。我们的评估涵盖了来自不同领域（包括旅游、金融、安排会议等）和语言（java、javascript、python、restAPI）的 2000 多个不同的查询和 API 文档对。</em></p><p>要深入了解我们的模型在每个类别中的表现，请参阅下面 Berkeley 功能调用排行榜中的详细表格。与目前技术最先进的 GPT-4 功能调用相比，Gorilla OpenFunctions-v2 在 Python 中的简单功能调用类别表现更优，但在涉及多个和并行功能的功能调用上表现不如 GPT-4。这一新特性对我们和整个开源社区来说仍是一个令人兴奋的研究领域。值得一提的是，我们的模型提供了非常稳定的可执行功能调用 - 这些功能调用是通过实际执行来评估的，无需任何干预。不出所料，经过训练的 Gorilla 模型在除 Python 以外的编程语言（如 Java、Javascript 和 REST API）上的功能调用上胜过了 GPT-4。对于 REST API，我们的模型提供了更稳定的输出，其中包括了所有必需的字段，包括<strong>url</strong>、<strong>params</strong>和<strong>header</strong>，使我们的模型非常适合立即采用。</p><p><img src="https://gorilla.cs.berkeley.edu/assets/img/blog_post_7_open_function_v2_leaderboard.png" alt="https://gorilla.cs.berkeley.edu/assets/img/blog_post_7_open_function_v2_leaderboard.png"></p><p><em>左侧是 GPT-4 生成的，右侧是 openfunctions-v2 生成的。从上面的错误中可以看出，当 GPT-4 功能调用处理涉及复杂参数结构（例如字典中的字典）并带有默认值的功能时，该模型往往会遇到麻烦，尤其是在解析默认值方面。与其说是一种边缘情况，不如说上面的示例是 REST API 的一个常见范例。</em></p><h3 id="OpenFunctions-数据组成与训练-🍕"><a href="#OpenFunctions-数据组成与训练-🍕" class="headerlink" title="OpenFunctions 数据组成与训练 🍕"></a>OpenFunctions 数据组成与训练 🍕</h3><p>Gorilla openfunctions v2 是一个基于\[deepseek-coder-7b-instruct-v1.5\]大语言模型进一步训练的 7B 参数模型。为了训练该模型，我们从三个不同来源收集了共计 65,283 个问题-功能-答案对：Python 包（19,353）、Java 存储库（16,586）、Javascript 存储库（4,245）、公共 API（6,009）以及来自各种云提供商的命令行工具（19,090）。数据组成如下图所示。</p><p>在数据收集之后，我们进行了四次数据增强，以提高我们的训练数据集的多样性。首先，我们更改了函数名称，这对于确保模型不会“记住”API 映射至关重要。其次，我们添加了随机选择的、数量不等的函数，使我们的数据集与并行函数兼容。这样我们就可以从简单的函数中生成多功能数据集。第三，我们采用扰动提示的策略来生成并行功能的场景，并将其扩展到同时包括多功能和并行功能。最后，我们还包含了一些功能在输入时不足以解决任务的数据集部分，我们将这些标记为“相关性检测”场景。与大多数 LLM 训练一样，我们对每种数据增强的程度进行了广泛的变化，以训练出一个健壮的模型。</p><ul><li><strong>函数名称变换：</strong> 我们通过使用不同的函数名称来增强原始的问题-功能-答案对，避免模型记住函数名称和问题之间的相关性（例如，“uber”API 用于交通）。<br><code>query + [{'name': 'func1', 'description': 'order takeout'}] -&gt; ans1 =&gt; query + [{'name': 'func2', 'description': 'order takeout'}] -&gt; [ans2]</code></li><li><strong>并行功能变换：</strong> 为了处理选择多个功能来回答用户请求的更复杂情况，我们更改了原始问题以要求多个输出。<br><code>query + [{'name': 'func1', 'description': 'order takeout'}] -&gt; ans1 =&gt; query + [{'name': 'func1', 'description': 'order takeout'}, {'name': 'func2', 'description': 'get weather'}] -&gt; [ans1]</code></li><li><strong>多功能变换：</strong> 在训练中包含多个功能调用的原始功能变换，使模型学习选择使用哪个功能调用。<br><code>query1 + [{'name': 'func1', 'description': 'order takeout'}] -&gt; ans1 =&gt; query2 + [{'name': 'func1', 'description': 'order takeout'}] -&gt; [ans1, ans2]</code></li><li><strong>并行多功能变换：</strong> 上述并行和多功能变换的结合。<br><code>query1 + [{'name': 'func1', 'description': 'order takeout'}] -&gt; ans1 =&gt; query2 + [{'name': 'func1', 'description': 'order takeout'}, {'name': 'func2', 'description': 'get weather'}] -&gt; [ans1, ans2]</code></li><li><strong>功能相关性检测变换：</strong> 我们还包含了一些在输入时提供的功能无法解决任务的数据集部分。我们称之为“相关性检测”。<br><code>query1 + [{'name': 'func1', 'description': 'order takeout'}] -&gt; ans1 =&gt; query2 + [{'name': 'func1', 'description': 'order takeout'}] -&gt; [Error, the function cannot solve the question.]</code></li></ul><p>在整个数据增强之后，我们还使用 Rouge 得分进行了数据去重，这已经成为标准做法。</p><h3 id="结论-1"><a href="#结论-1" class="headerlink" title="结论"></a>结论</h3><p>我们很高兴发布<code>gorilla-openfunctions-v2</code>，这是一个在 Deepseek-Coder-7B-Instruct-v1.5 大语言模型基础上训练的 7B 参数模型。它接收用户的提示和多个 API 调用，并返回带有正确参数的功能。OpenFunctions 扩展了对 Python、Java 和 JavaScript 以及 RESTful API 中参数类型的原生支持。欲了解更多信息，请查看我们在 Berkeley 功能调用排行榜上的博客评估，以及我们 GitHub 页面上的模型。博客中的所有结果都是使用<code>gorilla-openfunctions-v2</code>生成的。</p><p>Licensing:</p><p>Gorilla OpenFunctions v2 is distributed under the Apache 2.0 license. This software incorporates elements from the Deepseek model. Consequently, the licensing of Gorilla OpenFunctions v2 adheres to the Apache 2.0 license, with additional terms as outlined in Appendix A of the Deepseek license.</p><hr><p>我们希望您喜欢这篇博客文章。欢迎您在<a href="https://discord.gg/3apqwwME">Discord</a>、Twitter (#GorillaLLM)和<a href="https://github.com/ShishirPatil/gorilla/">GitHub</a>上与我们分享您的想法。</p><p>如果您想引用 Gorilla：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@inproceedings{gorilla-openfunctions-v2,</span></span><br><span class="line">  title={Gorilla OpenFunctions v2},</span><br><span class="line">  author={Charlie Cheng-Jie Ji, Huanzhi Mao, Fanjia Yan, Shishir G. Patil, Tianjun Zhang, Ion Stoica, Joseph E. Gonzalez},</span><br><span class="line">  year={<span class="number">2024</span>},</span><br><span class="line">  howpublished={\url{https://gorilla.cs.berkeley.edu//blogs/7_open_functions_v2.html}},</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>]]></content>
      
      
      <categories>
          
          <category> NLP Insights </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
            <tag> Agent </tag>
            
            <tag> Tool Use </tag>
            
            <tag> Gorilla </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FastChat Training Script Code Analysis - Train.py 【FastChat Series Part 1】</title>
      <link href="/2024/02/27/NLP%20Insights/FastChat%20Training%20Script%20Code%20Analysis%20-%20Train.py%20%E3%80%90FastChat%20Series%20Part%201%E3%80%91/"/>
      <url>/2024/02/27/NLP%20Insights/FastChat%20Training%20Script%20Code%20Analysis%20-%20Train.py%20%E3%80%90FastChat%20Series%20Part%201%E3%80%91/</url>
      
        <content type="html"><![CDATA[<h1 id="FastChat-Training-Script-Code-Analysis-Train-py-【FastChat-Series-Part-1】"><a href="#FastChat-Training-Script-Code-Analysis-Train-py-【FastChat-Series-Part-1】" class="headerlink" title="FastChat Training Script Code Analysis - Train.py 【FastChat Series Part 1】"></a>FastChat Training Script Code Analysis - Train.py 【FastChat Series Part 1】</h1><p>In this article, we delve into the train.py script of FastChat (<a href="https://github.com/lm-sys/FastChat">https://github.com/lm-sys/FastChat</a>) (<a href="https://github.com/lm-sys/FastChat/blob/main/fastchat/train/train.py">https://github.com/lm-sys/FastChat/blob/main/fastchat/train/train.py</a>), a key component for training and optimizing large language models (LLMs). FastChat is an advanced open-source platform focused on developing, deploying, and evaluating chatbots based on LLMs. The platform not only supports top-tier models like Vicuna and MT-Bench but also includes a distributed multi-model service system equipped with a Web UI and RESTful API compatible with OpenAI, enabling efficient training and evaluation of models.</p><p>We provide a detailed analysis of the train.py script’s source code. This script is a training script for natural language processing models based on the transformers library, covering critical steps such as data preprocessing, model training, and saving. Our goal is to offer a detailed explanation of each class and function in train.py, including their functionality and role in the overall training process.</p><h2 id="1-Importing-Modules"><a href="#1-Importing-Modules" class="headerlink" title="1. Importing Modules"></a>1. Importing Modules</h2><h3 id="1-Built-in-Modules"><a href="#1-Built-in-Modules" class="headerlink" title="1. Built-in Modules"></a>1. Built-in Modules</h3><p>These are standard library modules that come with Python and don’t require additional installation.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> dataclasses <span class="keyword">import</span> dataclass, field</span><br></pre></td></tr></tbody></table></figure><p>Imports Python’s <code>dataclasses</code> module for creating classes with default values.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br></pre></td></tr></tbody></table></figure><p>Imports the <code>json</code> module for handling JSON format data.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br></pre></td></tr></tbody></table></figure><p>Imports the <code>math</code> module for mathematical operations.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pathlib</span><br></pre></td></tr></tbody></table></figure><p>Imports the <code>pathlib</code> module for handling file paths.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Dict</span>, <span class="type">Optional</span>, <span class="type">Sequence</span></span><br></pre></td></tr></tbody></table></figure><p>Imports the <code>typing</code> module for type annotations.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></tbody></table></figure><h3 id="2-Dependency-Libraries"><a href="#2-Dependency-Libraries" class="headerlink" title="2. Dependency Libraries"></a>2. Dependency Libraries</h3><p>These are external libraries typically installed via a package manager like pip.<br>Imports the <code>numpy</code> library, commonly used for scientific computing.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></tbody></table></figure><p>Imports <code>PyTorch</code>, a popular deep learning framework.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br></pre></td></tr></tbody></table></figure><p>Imports <code>Dataset</code> from <code>torch</code> for creating custom datasets.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> transformers</span><br></pre></td></tr></tbody></table></figure><p>Imports the <code>transformers</code> library, a popular natural language processing library.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Trainer</span><br></pre></td></tr></tbody></table></figure><p>Imports <code>Trainer</code> from <code>transformers</code> for training models.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers.trainer_pt_utils <span class="keyword">import</span> LabelSmoother</span><br></pre></td></tr></tbody></table></figure><p>Imports <code>LabelSmoother</code> from <code>transformers</code> for label smoothing.</p><h3 id="3-Project-Specific-Functions"><a href="#3-Project-Specific-Functions" class="headerlink" title="3. Project-Specific Functions"></a>3. Project-Specific Functions</h3><p>These are functions or classes custom-implemented in the Fast Chat project.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fastchat.conversation <span class="keyword">import</span> SeparatorStyle</span><br></pre></td></tr></tbody></table></figure><p>Imports <code>SeparatorStyle</code> from the <code>fastchat</code> package for defining conversation separator styles. The <code>SeparatorStyle</code> class is an enumeration class created using Python’s <code>enum</code> module, defining a series of separator styles. Enumerations are a programming concept used to define a named set of constants, making code clearer and more maintainable.</p><p>In the <code>SeparatorStyle</code> class, each member represents a specific style of separator. These styles are often used in text processing, especially in scenarios where different sections or elements need to be distinguished. For instance, in handling dialog or textual data, different methods might be needed to differentiate between user input and machine responses.</p><p>Regarding the use of the <code>auto()</code> function:</p><ul><li><code>auto()</code> is a special function provided by Python’s <code>enum</code> module. It automatically assigns a unique value to each member in an enumeration class.</li><li>Without using <code>auto()</code>, you would need to manually assign a unique value to each enumeration member. <code>auto()</code> simplifies this process by letting Python handle the assignment of these values automatically.</li><li>The values assigned by <code>auto()</code> are usually integers, starting from 1 and increasing sequentially.</li></ul><p>In the case of the <code>SeparatorStyle</code> class, <code>auto()</code> is used to automatically assign a unique integer value to each type of separator style. For example, <code>ADD_COLON_SINGLE</code>, <code>ADD_COLON_TWO</code>, etc., will be given different integer values.</p><p>The names of each enumeration member (such as <code>ADD_COLON_SINGLE</code>, <code>NO_COLON_SINGLE</code>, etc.) typically describe the characteristics of that separator style. For instance, <code>ADD_COLON_SINGLE</code> might represent adding a colon as a separator after a certain element, whereas <code>NO_COLON_SINGLE</code> means no colon is added.</p><p>This approach makes referencing and handling these separator styles in the code more convenient and clear. For example, different separator styles can be chosen based on different scenarios or requirements without having to remember their specific values.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fastchat.model.model_adapter <span class="keyword">import</span> get_conversation_template</span><br></pre></td></tr></tbody></table></figure><p>Imports <code>get_conversation_template</code> from the <code>fastchat</code> package for obtaining conversation templates. In this code segment, the call logic primarily involves obtaining the default conversation template for a specific model. The call chain is as follows:</p><ol><li><p><strong>Starting Call - <code>get_conversation_template(model_path: str)</code></strong></p><ul><li>This function is the starting point of the call chain. It accepts a parameter <code>model_path</code>, specifying the path of the model.</li><li>The purpose of this function is to obtain the default conversation template for the given model path.</li></ul></li><li><p><strong>Call <code>get_model_adapter(model_path: str)</code></strong></p><ul><li>The <code>get_conversation_template</code> function first calls <code>get_model_adapter</code>, passing in the model path.</li><li>The purpose of <code>get_model_adapter</code> is to find and return a suitable <code>BaseModelAdapter</code> object for the provided model path.</li><li>This function first tries to match the basename of <code>model_path</code>. If no match is found, it tries the full path.</li><li>If a suitable adapter is found, it is returned; otherwise, a <code>ValueError</code> is thrown.</li></ul></li><li><p><strong>Execute <code>BaseModelAdapter.get_default_conv_template(model_path: str)</code></strong></p><ul><li>Once the appropriate model adapter is obtained, <code>get_conversation_template</code> retrieves the default conversation template by calling the <code>get_default_conv_template</code> method of that adapter.</li><li>Note that this method is defined in the <code>BaseModelAdapter</code> class but might be overridden in subclasses.</li></ul></li><li><p><strong>Call <code>get_conv_template(name: str)</code></strong></p><ul><li>Inside the <code>get_default_conv_template</code> method, it calls the <code>get_conv_template</code> function, usually passing a predefined template name like <code>"one_shot"</code>.</li><li>The purpose of <code>get_conv_template</code> is to retrieve a specified name’s template from the global registry of conversation templates <code>conv_templates</code>.</li></ul></li><li><p><strong>Obtain and Return a <code>Conversation</code> Object</strong></p><ul><li>The <code>get_conv_template</code> function returns an instance of the <code>Conversation</code> class, usually copied from the <code>conv_templates</code> dictionary.</li><li>Finally, this <code>Conversation</code> instance is returned to the original call site of <code>get_conversation_template</code>.</li></ul></li></ol><p>Summarizing the call chain:</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">get_conversation_template(model_path)</span><br><span class="line">  -&gt; get_model_adapter(model_path)</span><br><span class="line">  -&gt; [BaseModelAdapter].get_default_conv_template(model_path)</span><br><span class="line">    -&gt; get_conv_template(name)</span><br><span class="line">      -&gt; Return Conversation Object</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><p>In this process, the code navigates through a series of function calls to find a suitable model adapter based on the provided model path and retrieve a specific conversation template from it. This design pattern allows flexibility in providing different conversation templates for different models, enhancing the reusability and extensibility of the code.</p><hr><h2 id="2-Configuration-Classes"><a href="#2-Configuration-Classes" class="headerlink" title="2. Configuration Classes"></a>2. Configuration Classes</h2><p>These classes are defined using Python’s <code>dataclass</code> decorator and are mainly used for storing configurations and parameters. These classes usually do not contain complex methods or logic but are used to define and store data structures. These classes include:</p><ul><li><code>ModelArguments</code>: Stores parameters related to the model, like model path, trust in remote code, etc.</li><li><code>DataArguments</code>: Stores parameters related to data, like data path, evaluation data path, and whether to use lazy preprocessing.</li><li><code>TrainingArguments</code>: Stores parameters related to training, like cache directory, optimizer type, model maximum length, etc. This class extends <code>transformers.TrainingArguments</code> and adds some custom parameters.</li></ul><p>These classes are mainly used to simplify and organize parameter management in the code, making parameter modification and access more convenient.</p><h3 id="1-ModelArguments-Class"><a href="#1-ModelArguments-Class" class="headerlink" title="1. ModelArguments Class"></a>1. ModelArguments Class</h3><h4 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ModelArguments</span>:</span><br><span class="line">    model_name_or_path: <span class="type">Optional</span>[<span class="built_in">str</span>] = field(default=<span class="string">"facebook/opt-125m"</span>)</span><br><span class="line">    trust_remote_code: <span class="built_in">bool</span> = field(</span><br><span class="line">        default=<span class="literal">False</span>,</span><br><span class="line">        metadata={</span><br><span class="line">            <span class="string">"help"</span>: <span class="string">"Whether or not to allow for custom models defined on the Hub in their own modeling files"</span></span><br><span class="line">        },</span><br><span class="line">    )</span><br><span class="line">    padding_side: <span class="built_in">str</span> = field(</span><br><span class="line">        default=<span class="string">"right"</span>, metadata={<span class="string">"help"</span>: <span class="string">"The padding side in tokenizer"</span>}</span><br><span class="line">    )</span><br></pre></td></tr></tbody></table></figure><h4 id="Explanation"><a href="#Explanation" class="headerlink" title="Explanation"></a>Explanation</h4><p><code>ModelArguments</code> is a data class (<code>dataclass</code>) used for storing model-related configuration parameters.<br><strong>Attributes:</strong></p><ol><li><code>model_name_or_path</code>: Specifies the name or path of the pretrained model.</li><li><code>trust_remote_code</code>: Whether to allow custom models that have their modeling files defined on the Hub.</li><li><code>padding_side</code>: Specifies the padding side in the tokenizer, typically right or left padding.</li></ol><details><summary> Introduction to `@dataclass` decorator, click to expand </summary>`@dataclass` is a decorator used to automate the generation of special methods like `__init__()`, `__repr__()`, `__eq__()` etc., thus simplifying the writing of data classes. This decorator is part of Python 3.7 and is in the `dataclasses` module.<p>When you use <code>@dataclass</code> before a class definition, Python automatically adds some special methods based on the fields defined in the class. This is very useful for creating classes that store a small amount of data but do not need complex methods.</p><p>Specifically, using <code>@dataclass</code>:</p><ol><li><p><strong>Automatically generates a constructor (<code>__init__</code> method)</strong>: Python creates an <code>__init__</code> method automatically based on the fields defined in the class, so you don’t need to manually write this method to initialize your class instances.</p></li><li><p><strong>Automatically generates a <code>__repr__</code> method</strong>: This makes printing the class instances provide a more readable string representation, usually including the class name and its fields and their values.</p></li><li><p><strong>Automatically generates an <code>__eq__</code> method</strong>: This allows you to use the <code>==</code> operator to compare two instances of the class, comparing the values of the instance fields.</p></li><li><p><strong>Support for type annotations</strong>: When defining fields, you can use type annotations, which not only help with clarity of code but can also be checked for type correctness using some tools.</p></li></ol><p>In the case of the <code>ModelArguments</code> class, the <code>@dataclass</code> decorator will generate the above-mentioned methods. This means you can easily create an instance of <code>ModelArguments</code>, and when printing or comparing these instances, you will get the expected behavior.</p><p>For example, when you create an instance of <code>ModelArguments</code>:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">args = ModelArguments()</span><br></pre></td></tr></tbody></table></figure><p>This will call the automatically generated <code>__init__</code> method, using the default values “facebook/opt-125m” for <code>model_name_or_path</code>, <code>False</code> for <code>trust_remote_code</code>, and “right” for <code>padding_side</code>.</p><p>When you print this instance:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(args)</span><br></pre></td></tr></tbody></table></figure><p>This will call the automatically generated <code>__repr__</code> method, showing a detailed view of the class instance, like <code>ModelArguments(model_name_or_path="facebook/opt-125m", trust_remote_code=False, padding_side="right")</code>.</p><p>Thus, the <code>@dataclass</code> decorator simplifies the process of creating classes, making the code more concise and maintainable.</p><p>Overall, the <code>@dataclass</code> decorator is a convenient tool provided by Python for quickly creating classes mainly used for storing data.</p></details><h3 id="2-DataArguments-Class"><a href="#2-DataArguments-Class" class="headerlink" title="2. DataArguments Class"></a>2. DataArguments Class</h3><h4 id="Code-1"><a href="#Code-1" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DataArguments</span>:</span><br><span class="line">    data_path: <span class="built_in">str</span> = field(</span><br><span class="line">        default=<span class="literal">None</span>, metadata={<span class="string">"help"</span>: <span class="string">"Path to the training data."</span>}</span><br><span class="line">    )</span><br><span class="line">    eval_data_path: <span class="built_in">str</span> = field(</span><br><span class="line">        default=<span class="literal">None</span>, metadata={<span class="string">"help"</span>: <span class="string">"Path to the evaluation data."</span>}</span><br><span class="line">    )</span><br><span class="line">    lazy_preprocess: <span class="built_in">bool</span> = <span class="literal">False</span></span><br></pre></td></tr></tbody></table></figure><h4 id="Explanation-1"><a href="#Explanation-1" class="headerlink" title="Explanation"></a>Explanation</h4><p><strong>DataArguments Class</strong></p><ul><li><code>DataArguments</code> is also a data class used for storing data-related configuration parameters.</li><li>Attributes:<ul><li><code>data_path</code>: Path to the training data.</li><li><code>eval_data_path</code>: Path to the evaluation data.</li><li><code>lazy_preprocess</code>: Whether to use lazy loading for data preprocessing, i.e., load and process data as needed.</li></ul></li></ul><h3 id="3-TrainingArguments-Class"><a href="#3-TrainingArguments-Class" class="headerlink" title="3. TrainingArguments Class"></a>3. TrainingArguments Class</h3><h4 id="Code-2"><a href="#Code-2" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TrainingArguments</span>(transformers.TrainingArguments):</span><br><span class="line">    cache_dir: <span class="type">Optional</span>[<span class="built_in">str</span>] = field(default=<span class="literal">None</span>)</span><br><span class="line">    optim: <span class="built_in">str</span> = field(default=<span class="string">"adamw_torch"</span>)</span><br><span class="line">    model_max_length: <span class="built_in">int</span> = field(</span><br><span class="line">        default=<span class="number">512</span>,</span><br><span class="line">        metadata={</span><br><span class="line">            <span class="string">"help"</span>: <span class="string">"Maximum sequence length. Sequences will be right padded (and possibly truncated)."</span></span><br><span class="line">        },</span><br><span class="line">    )</span><br></pre></td></tr></tbody></table></figure><h4 id="Explanation-2"><a href="#Explanation-2" class="headerlink" title="Explanation"></a>Explanation</h4><p><code>TrainingArguments</code> class extends <code>transformers.TrainingArguments</code>.</p><ol><li><p><strong>TrainingArguments Class</strong></p><ul><li><code>TrainingArguments</code> is a data class that, by extending <code>transformers.TrainingArguments</code>, gains the capability to handle training parameters.</li><li>Attributes defined in <code>TrainingArguments</code>:<ul><li><code>cache_dir</code>: Specifies the directory path for caching the model and tokenizer.</li><li><code>optim</code>: Defines the type of optimizer to use, like <code>'adamw_torch'</code>.</li><li><code>model_max_length</code>: Specifies the maximum sequence length the model can handle.</li></ul></li></ul></li><li><p><strong>transformers.TrainingArguments Class</strong></p><ul><li><code>transformers.TrainingArguments</code> is a class in the transformers library that is used for configuring various parameters in the model training process.</li><li>This class contains a plethora of attributes for controlling the training process, such as:<ul><li><code>output_dir</code>: Specifies the directory to save the model and training results.</li><li><code>num_train_epochs</code>: Number of training epochs.</li><li><code>per_device_train_batch_size</code>: Batch size per device for training.</li><li><code>save_steps</code>: Steps interval for saving the model.</li><li><code>evaluation_strategy</code>: Strategy for evaluating the model, like at the end of each epoch.</li><li><code>learning_rate</code>: Learning rate.</li><li><code>warmup_steps</code>: Steps used for warmup in the learning rate schedule.</li></ul></li><li><code>transformers.TrainingArguments</code> also</li></ul></li></ol><p> contains many other parameters for fine-tuning the training process, including logging, model saving strategies, learning rate scheduling, and more.</p><p>By extending <code>transformers.TrainingArguments</code>, the <code>TrainingArguments</code> class not only inherits all these training parameter configurations but can also add some custom training parameters, like in this case <code>cache_dir</code>, <code>optim</code>, and <code>model_max_length</code>. This approach enhances code reusability and flexibility, allowing you to adjust and extend training configurations as per the specific requirements of your project.</p><h2 id="3-Functional-Utility-Functions"><a href="#3-Functional-Utility-Functions" class="headerlink" title="3. Functional Utility Functions"></a>3. Functional Utility Functions</h2><h3 id="1-rank0-print-args"><a href="#1-rank0-print-args" class="headerlink" title="1. rank0_print(*args)"></a>1. rank0_print(*args)</h3><h4 id="Code-3"><a href="#Code-3" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">local_rank = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">rank0_print</span>(<span class="params">*args</span>):</span><br><span class="line">    <span class="keyword">if</span> local_rank == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(*args)</span><br></pre></td></tr></tbody></table></figure><h4 id="Explanation-3"><a href="#Explanation-3" class="headerlink" title="Explanation"></a>Explanation</h4><p>Defines a global variable local_rank for distributed training.<br>Defines a function rank0_print to print information only if local_rank is 0, used for controlling output in distributed training. This way, repetitive printing of the same information across multiple nodes is avoided, making the output clearer and more concise.</p><ul><li>Used to print information only on the main node (rank 0) in a distributed training environment.</li><li>Parameters: A variable number of arguments for printing.</li></ul><h3 id="2-trainer-save-model-safe-trainer-transformers-Trainer"><a href="#2-trainer-save-model-safe-trainer-transformers-Trainer" class="headerlink" title="2. trainer_save_model_safe(trainer: transformers.Trainer)"></a>2. <code>trainer_save_model_safe(trainer: transformers.Trainer)</code></h3><h4 id="Code-4"><a href="#Code-4" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">trainer_save_model_safe</span>(<span class="params">trainer: transformers.Trainer</span>):</span><br><span class="line">    <span class="keyword">from</span> torch.distributed.fsdp <span class="keyword">import</span> FullyShardedDataParallel <span class="keyword">as</span> FSDP</span><br><span class="line">    <span class="keyword">from</span> torch.distributed.fsdp <span class="keyword">import</span> StateDictType, FullStateDictConfig</span><br><span class="line"></span><br><span class="line">    save_policy = FullStateDictConfig(offload_to_cpu=<span class="literal">True</span>, rank0_only=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">with</span> FSDP.state_dict_type(</span><br><span class="line">        trainer.model, StateDictType.FULL_STATE_DICT, save_policy</span><br><span class="line">    ):</span><br><span class="line">        trainer.save_model()</span><br></pre></td></tr></tbody></table></figure><p>The function <code>trainer_save_model_safe(trainer: transformers.Trainer)</code> aims to safely save models trained with the PyTorch distributed framework. Let’s delve into the details of this function and its key components.</p><h4 id="Explanation-4"><a href="#Explanation-4" class="headerlink" title="Explanation"></a>Explanation</h4><ol><li><p>Parameters:</p><ul><li><code>trainer</code>: An instance of <code>transformers.Trainer</code>. This class is one of the core components of the Hugging Face Transformers library, used for training and evaluating models.</li></ul></li><li><p>Functionality:</p><ul><li>The main purpose of this function is to safely save models in a distributed training environment. It particularly considers the model saving strategy when using Fully Sharded Data Parallel (FSDP).</li></ul></li><li><p>FSDP</p><ul><li><strong>FullyShardedDataParallel (FSDP)</strong><ul><li>This is a component of PyTorch’s distributed training framework. FSDP helps reduce memory usage on each GPU by sharding model parameters across multiple GPUs, allowing the training of larger models.</li><li>In this context, FSDP is primarily used for handling and saving model states in distributed training.</li></ul></li><li><strong>StateDictType</strong><ul><li>This is an enumeration type that defines how to save the model’s state dictionary. In FSDP environments, saving and loading model states might require special handling.</li></ul></li><li><strong>FullStateDictConfig</strong><ul><li>This class configures parameters for saving the full state dictionary. It’s part of FSDP’s functionality and is used to control how the model state is saved.</li></ul></li></ul></li><li><p>Function Implementation</p><ul><li><strong>Setting Save Policy</strong><ul><li><code>save_policy = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)</code> creates a save policy. Here, two key parameters are specified:<ul><li><code>offload_to_cpu</code>: Offload model parameters to CPU before saving the state dictionary, which helps reduce GPU memory usage.</li><li><code>rank0_only</code>: Save the model only on rank 0 (usually the main node). In distributed training, this avoids saving the same model copy on every node, saving storage space.</li></ul></li></ul></li><li><strong>Saving the Model</strong><ul><li>Using the <code>with FSDP.state_dict_type(trainer.model, StateDictType.FULL_STATE_DICT, save_policy)</code> context manager, the type and policy for saving the model’s state dictionary are set.</li><li>Within this context, <code>trainer.save_model()</code> is called to save the model. Due to the <code>save_policy</code>, the model is saved securely following the specified configuration.</li></ul></li></ul></li></ol><p>The function <code>trainer_save_model_safe</code> encapsulates a safe model saving logic, particularly for scenarios involving PyTorch’s FSDP in distributed training. It ensures that only a complete model state is saved on one node and offloads model parameters to CPU before saving, optimizing memory usage and storage efficiency. This is crucial for training large models and managing large-scale distributed training environments.</p><h3 id="3-preprocess-sources-tokenizer-transformers-PreTrainedTokenizer-Dict"><a href="#3-preprocess-sources-tokenizer-transformers-PreTrainedTokenizer-Dict" class="headerlink" title="3.preprocess(sources,tokenizer: transformers.PreTrainedTokenizer) -> Dict"></a>3.<code>preprocess(sources,tokenizer: transformers.PreTrainedTokenizer) -&gt; Dict</code></h3><h4 id="Code-5"><a href="#Code-5" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess</span>(<span class="params"></span></span><br><span class="line"><span class="params">    sources,</span></span><br><span class="line"><span class="params">    tokenizer: transformers.PreTrainedTokenizer,</span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="type">Dict</span>:</span><br><span class="line">    conv = get_conversation_template(<span class="string">"vicuna"</span>)</span><br><span class="line">    roles = {<span class="string">"human"</span>: conv.roles[<span class="number">0</span>], <span class="string">"gpt"</span>: conv.roles[<span class="number">1</span>]}</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Apply prompt templates</span></span><br><span class="line">    conversations = []</span><br><span class="line">    <span class="keyword">for</span> i, source <span class="keyword">in</span> <span class="built_in">enumerate</span>(sources):</span><br><span class="line">        <span class="keyword">if</span> roles[source[<span class="number">0</span>][<span class="string">"from"</span>]] != conv.roles[<span class="number">0</span>]:</span><br><span class="line">            <span class="comment"># Skip the first one if it is not from human</span></span><br><span class="line">            source = source[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">        conv.messages = []</span><br><span class="line">        <span class="keyword">for</span> j, sentence <span class="keyword">in</span> <span class="built_in">enumerate</span>(source):</span><br><span class="line">            role = roles[sentence[<span class="string">"from"</span>]]</span><br><span class="line">            <span class="keyword">assert</span> role == conv.roles[j % <span class="number">2</span>], <span class="string">f"<span class="subst">{i}</span>"</span></span><br><span class="line">            conv.append_message(role, sentence[<span class="string">"value"</span>])</span><br><span class="line">        conversations.append(conv.get_prompt())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Tokenize conversations</span></span><br><span class="line">    input_ids = tokenizer(</span><br><span class="line">        conversations,</span><br><span class="line">        return_tensors=<span class="string">"pt"</span>,</span><br><span class="line">        padding=<span class="string">"max_length"</span>,</span><br><span class="line">        max_length=tokenizer.model_max_length,</span><br><span class="line">        truncation=<span class="literal">True</span>,</span><br><span class="line">    ).input_ids</span><br><span class="line">    targets = input_ids.clone()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> conv.sep_style == SeparatorStyle.ADD_COLON_TWO</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Mask targets. Only compute loss on the assistant outputs.</span></span><br><span class="line">    sep = conv.sep + conv.roles[<span class="number">1</span>] + <span class="string">": "</span></span><br><span class="line">    <span class="keyword">for</span> conversation, target <span class="keyword">in</span> <span class="built_in">zip</span>(conversations, targets):</span><br><span class="line">        total_len = <span class="built_in">int</span>(target.ne(tokenizer.pad_token_id).<span class="built_in">sum</span>())</span><br><span class="line"></span><br><span class="line">        turns = conversation.split(conv.sep2)</span><br><span class="line">        cur_len = <span class="number">1</span></span><br><span class="line">        target[:cur_len] = IGNORE_TOKEN_ID</span><br><span class="line">        <span class="keyword">for</span> i, turn <span class="keyword">in</span> <span class="built_in">enumerate</span>(turns):</span><br><span class="line">            <span class="keyword">if</span> turn == <span class="string">""</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            turn_len = <span class="built_in">len</span>(tokenizer(turn).input_ids)</span><br><span class="line"></span><br><span class="line">            parts = turn.split(sep)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(parts) != <span class="number">2</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            parts[<span class="number">0</span>] += sep</span><br><span class="line">            <span class="comment"># "-2" is hardcoded for the Llama tokenizer to make the offset correct.</span></span><br><span class="line">            instruction_len = <span class="built_in">len</span>(tokenizer(parts[<span class="number">0</span>]).input_ids) - <span class="number">2</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i != <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> tokenizer.legacy:</span><br><span class="line">                <span class="comment"># The legacy and non-legacy modes handle special tokens differently</span></span><br><span class="line">                instruction_len -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Ignore the user instructions</span></span><br><span class="line">            target[cur_len : cur_len + instruction_len] = IGNORE_TOKEN_ID</span><br><span class="line">            cur_len += turn_len</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i != <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> tokenizer.legacy:</span><br><span class="line">                <span class="comment"># The legacy and non-legacy modes handle special tokens differently</span></span><br><span class="line">                cur_len -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        target[cur_len:] = IGNORE_TOKEN_ID</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="literal">False</span>:  <span class="comment"># Inspect and check the correctness of masking</span></span><br><span class="line">            z = target.clone()</span><br><span class="line">            z = torch.where(z == IGNORE_TOKEN_ID, tokenizer.unk_token_id, z)</span><br><span class="line">            rank0_print(tokenizer.decode(z))</span><br><span class="line">            exit()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> cur_len &lt; tokenizer.model_max_length:</span><br><span class="line">           </span><br><span class="line"></span><br><span class="line"> <span class="keyword">if</span> cur_len != total_len:</span><br><span class="line">                target[:] = IGNORE_TOKEN_ID</span><br><span class="line">                rank0_print(</span><br><span class="line">                    <span class="string">f"WARNING: tokenization mismatch: <span class="subst">{cur_len}</span> vs. <span class="subst">{total_len}</span>."</span></span><br><span class="line">                    <span class="string">f" #turn = <span class="subst">{<span class="built_in">len</span>(turns) - <span class="number">1</span>}</span>. (ignored)"</span></span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">dict</span>(</span><br><span class="line">        input_ids=input_ids,</span><br><span class="line">        labels=targets,</span><br><span class="line">        attention_mask=input_ids.ne(tokenizer.pad_token_id),</span><br><span class="line">    )</span><br></pre></td></tr></tbody></table></figure><p>The function <code>preprocess(sources, tokenizer: transformers.PreTrainedTokenizer) -&gt; Dict</code> is intended for preprocessing dialogue data to be suitable for training machine learning models. This function can be broken down into several main parts for a more detailed explanation:</p><h4 id="1-Obtaining-Conversation-Templates-and-Role-Definitions"><a href="#1-Obtaining-Conversation-Templates-and-Role-Definitions" class="headerlink" title="1. Obtaining Conversation Templates and Role Definitions"></a>1. Obtaining Conversation Templates and Role Definitions</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conv = get_conversation_template(<span class="string">"vicuna"</span>)</span><br><span class="line">roles = {<span class="string">"human"</span>: conv.roles[<span class="number">0</span>], <span class="string">"gpt"</span>: conv.roles[<span class="number">1</span>]}</span><br></pre></td></tr></tbody></table></figure><ul><li><strong>Functionality</strong>: Initializes conversation templates and defines the roles of dialogue participants.</li><li><strong>Implementation</strong>:<ul><li><code>conv = get_conversation_template("vicuna")</code> obtains the conversation template for a specified model (e.g., “vicuna”).</li><li>The <code>roles</code> dictionary maps “human” and “gpt” to the roles defined in the conversation template.</li></ul></li><li><strong>Example</strong>:<ul><li>If the conversation template is for “vicuna”, then <code>roles</code> might map “human” to “user” and “gpt” to “assistant”. For example, <code>{'human': 'USER', 'gpt': 'ASSISTANT'}</code>.</li></ul></li></ul><h4 id="2-Applying-Prompt-Templates"><a href="#2-Applying-Prompt-Templates" class="headerlink" title="2. Applying Prompt Templates"></a>2. Applying Prompt Templates</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Apply prompt templates</span></span><br><span class="line">conversations = []</span><br><span class="line"><span class="keyword">for</span> i, source <span class="keyword">in</span> <span class="built_in">enumerate</span>(sources):</span><br><span class="line">    <span class="keyword">if</span> roles[source[<span class="number">0</span>][<span class="string">"from"</span>]] != conv.roles[<span class="number">0</span>]:</span><br><span class="line">        <span class="comment"># Skip the first one if it is not from human</span></span><br><span class="line">        source = source[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">    conv.messages = []</span><br><span class="line">    <span class="keyword">for</span> j, sentence <span class="keyword">in</span> <span class="built_in">enumerate</span>(source):</span><br><span class="line">        role = roles[sentence[<span class="string">"from"</span>]]</span><br><span class="line">        <span class="keyword">assert</span> role == conv.roles[j % <span class="number">2</span>], <span class="string">f"<span class="subst">{i}</span>"</span></span><br><span class="line">        conv.append_message(role, sentence[<span class="string">"value"</span>])</span><br><span class="line">    conversations.append(conv.get_prompt())</span><br></pre></td></tr></tbody></table></figure><ul><li><strong>Functionality</strong>: Applies prompt templates to source data to construct dialogues.</li><li><strong>Implementation</strong>:<ul><li>Iterates through <code>sources</code> (original dialogue data), transforming each dialogue source into a conversation in template format.</li><li>If the first part of a dialogue is not initiated by the “human” role, it skips that part.</li><li>Assigns a role to each sentence and adds it to the conversation template.</li><li>Ultimately, each processed dialogue is added to the <code>conversations</code> list.</li></ul></li><li><strong>Example</strong>:<ul><li>Suppose we have a source which is the first item in dummy input: <code>python source = [{'from': 'human', 'value': 'Who are you?'}, {'from': 'gpt', 'value': 'I am Vicuna, a language model trained by researchers from Large Model Systems Organization (LMSYS).'}, {'from': 'human', 'value': 'Have a nice day!'}, {'from': 'gpt', 'value': 'You too!'}]</code></li><li><code>conversations</code> under the Vicuna template, using <code>SeparatorStyle.ADD_COLON_TWO</code> as the separator style, might look like [“A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user’s questions. USER: Who are you? ASSISTANT: I am Vicuna, a language model trained by researchers from Large Model Systems Organization (LMSYS).USER: Have a nice day! ASSISTANT: You too!“]</li><li><details><summary> Implementation of get_prompt </summary>The `get_prompt` method implementation varies depending on the `SeparatorStyle`. Below is a table detailing the `get_prompt` method for various styles, along with English examples:<table><thead><tr><th>Separator Style (<code>SeparatorStyle</code>)</th><th>Description</th><th>Example</th></tr></thead><tbody><tr><td><code>ADD_COLON_SINGLE</code></td><td>Adds a colon and separator after each message.</td><td>USER: Hello there!\nASSISTANT: Hi, how can I help?\n</td></tr><tr><td><code>ADD_COLON_TWO</code></td><td>Uses two alternating separators, usually between different roles.</td><td>USER: What’s the weather?\nASSISTANT: It’s sunny today.\n\n</td></tr><tr><td><code>ADD_COLON_SPACE_SINGLE</code></td><td>Adds a colon, space, and separator after each message.</td><td>USER: Can you book a flight?\nASSISTANT: Sure, where to?\n</td></tr><tr><td><code>NO_COLON_SINGLE</code></td><td>Messages directly follow roles without a colon, followed by a separator.</td><td>USERWhat are you doing?\nASSISTANTI’m here to assist you.\n</td></tr><tr><td><code>NO_COLON_TWO</code></td><td>No colons, with two alternating separators.</td><td>USERHow’s the project going?\nASSISTANTIt’s on track.\n\n</td></tr><tr><td><code>ADD_NEW_LINE_SINGLE</code></td><td>Each message is preceded by a newline, followed by a separator.</td><td>USER\nHow can I reset my password?\nASSISTANT\nYou can reset it via email.\n</td></tr><tr><td><code>RWKV</code></td><td>Special format, usually for specific models.</td><td>USER: What is AI?\n\nASSISTANT: AI stands for Artificial Intelligence.\n\n</td></tr><tr><td><code>LLAMA2</code></td><td>Special label format for specific models.</td><td>[INST] USER How does blockchain work?\nASSISTANT It is a distributed ledger.\n\n</td></tr><tr><td><code>CHATGLM</code></td><td>Specific format for <code>CHATGLM</code> model.</td><td>[Round 1]\nUSER: Tell me a joke.\nASSISTANT: Why did the chicken cross the road?\n</td></tr><tr><td><code>CHATML</code></td><td>Similar to <code>CHATGLM</code>, but with newlines before and after each message.</td><td>USER\nDo you like music?\n\nASSISTANT\nYes, I enjoy many genres.\n\n</td></tr><tr><td><code>CHATGLM3</code></td><td>Format for <code>CHATGLM3</code> model.</td><td>USER\nCan you play chess?\nASSISTANTYes, I can play.\n</td></tr><tr><td><code>CHATINTERN</code></td><td>Format for <code>CHATINTERN</code> model, using special markers.</td><td><s>USER:Where is the nearest ATM?<s>\nASSISTANT:It’s next to the post office.\n</s></s></td></tr><tr><td><code>DOLLY</code></td><td>Specific format for <code>DOLLY</code> model.</td><td>USER:\nWhat is quantum computing?\nASSISTANT:\nIt involves computation using quantum-mechanical phenomena.\n\n</td></tr><tr><td><code>PHOENIX</code></td><td>For <code>PHOENIX</code> model, messages are wrapped in special markers.</td><td>USER: <s>How to bake a cake?</s>\nASSISTANT: <s>You need flour, sugar, and eggs.</s>\n</td></tr><tr><td><code>ROBIN</code></td><td>Similar to <code>ADD_NEW_LINE_SINGLE</code>, but with a newline after roles.</td><td>USER:\nIs AI dangerous?\nASSISTANT:\nIt depends on how it’s used.\n</td></tr><tr><td></td><td></td><td></td></tr></tbody></table></details></li></ul></li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP Insights </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
            <tag> FastChat </tag>
            
            <tag> Train </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FastChat 训练脚本代码逐行解析-Train.py 【FastChat 系列第 1 篇】</title>
      <link href="/2024/02/27/NLP%20Insights/FastChat%20%E8%AE%AD%E7%BB%83%E8%84%9A%E6%9C%AC%E4%BB%A3%E7%A0%81%E9%80%90%E8%A1%8C%E8%A7%A3%E6%9E%90-Train.py%20%E3%80%90FastChat%20%E7%B3%BB%E5%88%97%E7%AC%AC%201%20%E7%AF%87%E3%80%91/"/>
      <url>/2024/02/27/NLP%20Insights/FastChat%20%E8%AE%AD%E7%BB%83%E8%84%9A%E6%9C%AC%E4%BB%A3%E7%A0%81%E9%80%90%E8%A1%8C%E8%A7%A3%E6%9E%90-Train.py%20%E3%80%90FastChat%20%E7%B3%BB%E5%88%97%E7%AC%AC%201%20%E7%AF%87%E3%80%91/</url>
      
        <content type="html"><![CDATA[<h1 id="FastChat-训练脚本代码逐行解析-Train-py-【FastChat-系列第-1-篇】"><a href="#FastChat-训练脚本代码逐行解析-Train-py-【FastChat-系列第-1-篇】" class="headerlink" title="FastChat 训练脚本代码逐行解析-Train.py 【FastChat 系列第 1 篇】"></a>FastChat 训练脚本代码逐行解析-Train.py 【FastChat 系列第 1 篇】</h1><p>在本文中，我们将深入探讨 <a href="https://github.com/lm-sys/FastChat">FastChat</a> 的 <code>train.py</code> 脚本，这是一个用于训练和优化大型语言模型的关键组件。FastChat 是一个先进的开源平台，专注于开发、部署和评估基于大型语言模型（LLM）的聊天机器人。该平台不仅提供对顶尖模型如 Vicuna 和 MT-Bench 的支持，还包括一个分布式的多模型服务系统，配备了 Web UI 和与 OpenAI 兼容的 RESTful API，使用户能够高效地训练和评估他们的模型。</p><p>本文的深入分析将聚焦于 <code>train.py</code> 脚本的源代码。这个脚本是基于 transformers 库的自然语言处理模型训练脚本，涵盖了数据预处理、模型训练和保存等关键步骤。我们旨在提供对 <code>train.py</code> 中每个类和函数的详细解释，包括它们的功能和在整个训练过程中的作用。</p><h2 id="1-导入模块"><a href="#1-导入模块" class="headerlink" title="1. 导入模块"></a>1. 导入模块</h2><h3 id="1-内置模块"><a href="#1-内置模块" class="headerlink" title="1. 内置模块"></a>1. 内置模块</h3><p>这些是 Python 自带的标准库模块，无需额外安装。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> dataclasses <span class="keyword">import</span> dataclass, field</span><br></pre></td></tr></tbody></table></figure><p>导入 Python 的<code>dataclasses</code>模块，用于创建带有默认值的类。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br></pre></td></tr></tbody></table></figure><p>导入<code>json</code>模块，用于处理 JSON 格式的数据。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br></pre></td></tr></tbody></table></figure><p>导入<code>math</code>模块，用于数学运算。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pathlib</span><br></pre></td></tr></tbody></table></figure><p>导入<code>pathlib</code>模块，用于处理文件路径。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Dict</span>, <span class="type">Optional</span>, <span class="type">Sequence</span></span><br></pre></td></tr></tbody></table></figure><p>导入<code>typing</code>模块，用于类型注解。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></tbody></table></figure><h3 id="2-依赖库"><a href="#2-依赖库" class="headerlink" title="2. 依赖库"></a>2. 依赖库</h3><p>这些是外部安装的依赖库，通常通过包管理器如 pip 安装。<br>导入<code>numpy</code>库，一个常用的科学计算库。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></tbody></table></figure><p>导入<code>PyTorch</code>，一个流行的深度学习框架。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br></pre></td></tr></tbody></table></figure><p>从<code>torch</code>中导入<code>Dataset</code>，用于创建自定义数据集。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> transformers</span><br></pre></td></tr></tbody></table></figure><p>导入<code>transformers</code>库，一个流行的自然语言处理库。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Trainer</span><br></pre></td></tr></tbody></table></figure><p>从<code>transformers</code>中导入<code>Trainer</code>，用于训练模型。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers.trainer_pt_utils <span class="keyword">import</span> LabelSmoother</span><br></pre></td></tr></tbody></table></figure><p>从<code>transformers</code>中导入<code>LabelSmoother</code>，用于标签平滑。</p><h3 id="3-项目特定函数"><a href="#3-项目特定函数" class="headerlink" title="3. 项目特定函数"></a>3. 项目特定函数</h3><p>这些是在 Fast Chat 项目中自定义实现的函数或类。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fastchat.conversation <span class="keyword">import</span> SeparatorStyle</span><br><span class="line"></span><br><span class="line"><span class="comment">## code from FastChat/fastchat/conversation.py</span></span><br><span class="line">    <span class="string">"""Separator styles."""</span></span><br><span class="line"></span><br><span class="line">    ADD_COLON_SINGLE = auto()</span><br><span class="line">    ADD_COLON_TWO = auto()</span><br><span class="line">    ADD_COLON_SPACE_SINGLE = auto()</span><br><span class="line">    NO_COLON_SINGLE = auto()</span><br><span class="line">    NO_COLON_TWO = auto()</span><br><span class="line">    ADD_NEW_LINE_SINGLE = auto()</span><br><span class="line">    LLAMA2 = auto()</span><br><span class="line">    CHATGLM = auto()</span><br><span class="line">    CHATML = auto()</span><br><span class="line">    CHATINTERN = auto()</span><br><span class="line">    DOLLY = auto()</span><br><span class="line">    RWKV = auto()</span><br><span class="line">    PHOENIX = auto()</span><br><span class="line">    ROBIN = auto()</span><br><span class="line">    FALCON_CHAT = auto()</span><br><span class="line">    CHATGLM3 = auto()</span><br><span class="line">    DEEPSEEK_CHAT = auto()</span><br><span class="line">    METAMATH = auto()</span><br><span class="line">    YUAN2 = auto()</span><br></pre></td></tr></tbody></table></figure><p>从<code>fastchat</code>包导入<code>SeparatorStyle</code>，用于定义对话分隔符风格。<code>SeparatorStyle</code> 类是一个使用 Python 的 <code>enum</code> 模块创建的枚举类，用于定义一系列的分隔符样式。枚举（Enumeration）是一种编程概念，用于定义一组命名的常数，使代码更加清晰和易于维护。</p><p>在 <code>SeparatorStyle</code> 类中，每个成员代表一种特定的分隔符样式。这些样式通常用于文本处理中，特别是在需要区分不同部分或元素的情况下。例如，在处理对话或文本数据时，可能需要不同的方式来区分用户输入和机器回复。</p><p>关于 <code>auto()</code> 函数的使用：</p><ul><li><code>auto()</code> 是 Python <code>enum</code> 模块提供的一个特殊函数。它在枚举类中自动分配一个唯一的值给每个成员。</li><li>在不使用 <code>auto()</code> 的情况下，你需要手动为每个枚举成员指定一个唯一的值。使用 <code>auto()</code> 可以简化这个过程，让 Python 自动处理这些值的分配。</li><li><code>auto()</code> 分配的值通常是整数，从 1 开始依次递增。</li></ul><p>具体到 <code>SeparatorStyle</code> 类，<code>auto()</code> 被用来为每种分隔符样式自动分配一个唯一的整数值。例如，<code>ADD_COLON_SINGLE</code>、<code>ADD_COLON_TWO</code> 等将分别被赋予不同的整数值。</p><p>每个枚举成员的名称（如 <code>ADD_COLON_SINGLE</code>、<code>NO_COLON_SINGLE</code> 等）通常描述了该分隔符样式的特点。例如，<code>ADD_COLON_SINGLE</code> 可能表示在某个元素后添加一个冒号作为分隔符，而 <code>NO_COLON_SINGLE</code> 则表示不添加冒号。</p><p>这种方式使得在代码中引用和处理这些分隔符样式变得更加方便和清晰。例如，可以根据不同的场景或需求选择使用不同的分隔符样式，而无需记住它们对应的具体值。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fastchat.model.model_adapter <span class="keyword">import</span> get_conversation_template</span><br><span class="line"></span><br><span class="line"><span class="comment">## code from FastChat/fastchat/model/model_adapter.py</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_conversation_template</span>(<span class="params">model_path: <span class="built_in">str</span></span>) -&gt; Conversation:</span><br><span class="line">    <span class="string">"""Get the default conversation template."""</span></span><br><span class="line">    adapter = get_model_adapter(model_path)</span><br><span class="line">    <span class="keyword">return</span> adapter.get_default_conv_template(model_path)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## code from FastChat/fastchat/model/model_adapter.py</span></span><br><span class="line"><span class="meta">@cache</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_model_adapter</span>(<span class="params">model_path: <span class="built_in">str</span></span>) -&gt; BaseModelAdapter:</span><br><span class="line">    <span class="string">"""Get a model adapter for a model_path."""</span></span><br><span class="line">    model_path_basename = os.path.basename(os.path.normpath(model_path))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Try the basename of model_path at first</span></span><br><span class="line">    <span class="keyword">for</span> adapter <span class="keyword">in</span> model_adapters:</span><br><span class="line">        <span class="keyword">if</span> adapter.<span class="keyword">match</span>(model_path_basename) <span class="keyword">and</span> <span class="built_in">type</span>(adapter) != BaseModelAdapter:</span><br><span class="line">            <span class="keyword">return</span> adapter</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Then try the full path</span></span><br><span class="line">    <span class="keyword">for</span> adapter <span class="keyword">in</span> model_adapters:</span><br><span class="line">        <span class="keyword">if</span> adapter.<span class="keyword">match</span>(model_path):</span><br><span class="line">            <span class="keyword">return</span> adapter</span><br><span class="line"></span><br><span class="line">    <span class="keyword">raise</span> ValueError(<span class="string">f"No valid model adapter for <span class="subst">{model_path}</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## code from FastChat/fastchat/model/model_adapter.py</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_default_conv_template</span>(<span class="params">self, model_path: <span class="built_in">str</span></span>) -&gt; Conversation:</span><br><span class="line">        <span class="keyword">return</span> get_conv_template(<span class="string">"one_shot"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## code from FastChat/fastchat/conversation.py</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_conv_template</span>(<span class="params">name: <span class="built_in">str</span></span>) -&gt; Conversation:</span><br><span class="line">    <span class="string">"""Get a conversation template."""</span></span><br><span class="line">    <span class="keyword">return</span> conv_templates[name].copy()</span><br><span class="line"></span><br><span class="line"><span class="comment">## code from FastChat/fastchat/conversation.py</span></span><br><span class="line"><span class="comment"># A global registry for all conversation templates</span></span><br><span class="line">conv_templates: <span class="type">Dict</span>[<span class="built_in">str</span>, Conversation] = {}</span><br></pre></td></tr></tbody></table></figure><p>从<code>fastchat</code>包导入<code>get_conversation_template</code>，用于获取对话模板。<br>在这段代码中，调用逻辑主要涉及到获取特定模型的默认对话模板。调用链路如下：</p><ol><li><p><strong>起始调用 - <code>get_conversation_template(model_path: str)</code></strong></p><ul><li>这个函数是调用链的起点。它接收一个参数 <code>model_path</code>，用于指定模型的路径。</li><li>这个函数的目的是获取给定模型路径的默认对话模板。</li></ul></li><li><p><strong>调用 <code>get_model_adapter(model_path: str)</code></strong></p><ul><li><code>get_conversation_template</code> 函数首先调用 <code>get_model_adapter</code>，传入模型路径。</li><li><code>get_model_adapter</code> 的目的是根据提供的模型路径，找到并返回一个适合该模型的 <code>BaseModelAdapter</code> 对象。</li><li>这个函数首先尝试匹配 <code>model_path</code> 的基本名称（basename），如果没有找到匹配项，它会尝试匹配完整的路径。</li><li>如果找到合适的适配器，则返回该适配器；如果没有找到，则抛出一个 <code>ValueError</code>。</li></ul></li><li><p><strong>执行 <code>BaseModelAdapter.get_default_conv_template(model_path: str)</code></strong></p><ul><li>在获取到适当的模型适配器后，<code>get_conversation_template</code> 通过调用该适配器的 <code>get_default_conv_template</code> 方法来获取默认的对话模板。</li><li>注意这个方法在 <code>BaseModelAdapter</code> 类中定义，但可能在子类中被重写。</li></ul></li><li><p><strong>调用 <code>get_conv_template(name: str)</code></strong></p><ul><li>在 <code>get_default_conv_template</code> 方法内部，它调用 <code>get_conv_template</code> 函数，通常传入一个预定义的模板名称，比如 <code>"one_shot"</code>。</li><li><code>get_conv_template</code> 的作用是从全局注册的对话模板字典 <code>conv_templates</code> 中获取指定名称的模板。</li></ul></li><li><p><strong>获取并返回 <code>Conversation</code> 对象</strong></p><ul><li><code>get_conv_template</code> 函数返回 <code>Conversation</code> 类的一个实例，这通常是从 <code>conv_templates</code> 字典中复制得到的。</li><li>最终，这个 <code>Conversation</code> 实例被返回到最初调用 <code>get_conversation_template</code> 的地方。</li></ul></li></ol><p>总结调用链路：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">get_conversation_template(model_path)</span><br><span class="line">  -&gt; get_model_adapter(model_path)</span><br><span class="line">  -&gt; [BaseModelAdapter].get_default_conv_template(model_path)</span><br><span class="line">    -&gt; get_conv_template(name)</span><br><span class="line">      -&gt; 返回 Conversation 对象</span><br></pre></td></tr></tbody></table></figure><p>在这个过程中，代码通过一系列函数调用，根据提供的模型路径，找到相应的模型适配器，并从中获取特定的对话模板。这种设计模式允许灵活地为不同的模型提供不同的对话模板，从而提高了代码的可重用性和可扩展性。</p><hr><h2 id="2-配置类"><a href="#2-配置类" class="headerlink" title="2. 配置类"></a>2. 配置类</h2><p>这些类是使用 Python 的 <code>dataclass</code> 装饰器定义的，主要用于存储配置和参数。这些类通常不包含复杂的方法或逻辑，而是用于定义和存储数据结构。这些类包括：</p><ul><li><code>ModelArguments</code>: 存储与模型相关的参数，如模型路径、远程代码信任等。</li><li><code>DataArguments</code>: 存储与数据相关的参数，如数据路径、评估数据路径以及是否使用懒加载预处理。</li><li><code>TrainingArguments</code>: 存储与训练相关的参数，如缓存目录、优化器类型、模型最大长度等。这个类继承自 <code>transformers.TrainingArguments</code>，增加了一些自定义参数。</li></ul><p>这些类主要用于简化和组织代码中的参数管理，使得参数的修改和访问更加方便。</p><h3 id="1-ModelArguments-类"><a href="#1-ModelArguments-类" class="headerlink" title="1. ModelArguments 类"></a>1. ModelArguments 类</h3><h4 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ModelArguments</span>:</span><br><span class="line">    model_name_or_path: <span class="type">Optional</span>[<span class="built_in">str</span>] = field(default=<span class="string">"facebook/opt-125m"</span>)</span><br><span class="line">    trust_remote_code: <span class="built_in">bool</span> = field(</span><br><span class="line">        default=<span class="literal">False</span>,</span><br><span class="line">        metadata={</span><br><span class="line">            <span class="string">"help"</span>: <span class="string">"Whether or not to allow for custom models defined on the Hub in their own modeling files"</span></span><br><span class="line">        },</span><br><span class="line">    )</span><br><span class="line">    padding_side: <span class="built_in">str</span> = field(</span><br><span class="line">        default=<span class="string">"right"</span>, metadata={<span class="string">"help"</span>: <span class="string">"The padding side in tokenizer"</span>}</span><br><span class="line">    )</span><br></pre></td></tr></tbody></table></figure><h4 id="Explanation"><a href="#Explanation" class="headerlink" title="Explanation"></a>Explanation</h4><p><code>ModelArguments</code> 是一个数据类（<code>dataclass</code>），用于存储与模型相关的配置参数。<br><strong>属性：</strong></p><ol><li><code>model_name_or_path</code>: 指定预训练模型的名称或路径。</li><li><code>trust_remote_code</code>: 是否允许使用自定义模型，这些模型在 Hub 上有自己的模型文件。</li><li><code>padding_side</code>: 指定在分词器（<code>tokenizer</code>）中使用的填充方式，通常是左填充或右填充。</li></ol><details><summary> `@dataclass`装饰器的介绍，点击展开</summary>`@dataclass` 是一个装饰器，用于自动化生成特殊方法，如 `__init__()`、`__repr__()`、`__eq__()` 等，从而简化数据类的编写。这个装饰器是 Python 3.7 中引入的一部分，属于 `dataclasses` 模块。<p>当你在一个类定义前使用 <code>@dataclass</code> 装饰器时，Python 会自动为这个类添加一些由属性定义的特殊方法。这对于创建存储少量数据但不需要复杂方法的类非常有用。</p><p>具体来说，使用 <code>@dataclass</code> 时：</p><ol><li><p><strong>自动生成构造函数（<code>__init__</code> 方法）</strong>：Python 会根据类中定义的字段自动创建一个 <code>__init__</code> 方法，这样你就不需要手动编写这个方法来初始化类的实例了。</p></li><li><p><strong>自动生成 <code>__repr__</code> 方法</strong>：这使得打印类的实例时能够得到更具可读性的字符串表示，通常包含类名和其中的字段及其值。</p></li><li><p><strong>自动生成 <code>__eq__</code> 方法</strong>：这使得可以使用 <code>==</code> 操作符来比较两个类的实例，比较的是实例中字段的值。</p></li><li><p><strong>支持类型注解</strong>：在定义字段时，你可以使用类型注解，这不仅有助于代码清晰性，还可以通过一些工具进行类型检查。</p></li></ol><p>在<code>ModelArguments</code>类的例子中，<code>@dataclass</code>装饰器会为这个类生成上述的方法。这意味着你可以很方便地创建<code>ModelArguments</code>的实例，并在打印或比较这些实例时得到预期的行为。</p><p>例如，当你创建一个<code>ModelArguments</code>实例时：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">args = ModelArguments()</span><br></pre></td></tr></tbody></table></figure><p>这将调用自动生成的<code>__init__</code>方法，使用默认值”facebook/opt-125m”为<code>model_name_or_path</code>、<code>False</code>为<code>trust_remote_code</code>和”right”为<code>padding_side</code>。</p><p>当你打印这个实例：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(args)</span><br></pre></td></tr></tbody></table></figure><p>这将调用自动生成的<code>__repr__</code>方法，显示类实例的详细信息，如<code>ModelArguments(model_name_or_path="facebook/opt-125m", trust_remote_code=False, padding_side="right")</code>。</p><p>这样，<code>@dataclass</code>装饰器简化了类的创建过程，使得代码更加简洁和易于维护。</p><p>总的来说，<code>@dataclass</code> 装饰器是 Python 提供的一个便捷工具，用于快速创建主要用于存储数据的类。</p></details><h3 id="2-DataArguments-类"><a href="#2-DataArguments-类" class="headerlink" title="2. DataArguments 类"></a>2. DataArguments 类</h3><h4 id="Code-1"><a href="#Code-1" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DataArguments</span>:</span><br><span class="line">    data_path: <span class="built_in">str</span> = field(</span><br><span class="line">        default=<span class="literal">None</span>, metadata={<span class="string">"help"</span>: <span class="string">"Path to the training data."</span>}</span><br><span class="line">    )</span><br><span class="line">    eval_data_path: <span class="built_in">str</span> = field(</span><br><span class="line">        default=<span class="literal">None</span>, metadata={<span class="string">"help"</span>: <span class="string">"Path to the evaluation data."</span>}</span><br><span class="line">    )</span><br><span class="line">    lazy_preprocess: <span class="built_in">bool</span> = <span class="literal">False</span></span><br></pre></td></tr></tbody></table></figure><h4 id="Explanation-1"><a href="#Explanation-1" class="headerlink" title="Explanation"></a>Explanation</h4><p><strong>DataArguments 类</strong></p><ul><li><code>DataArguments</code> 也是一个数据类，用于存储数据相关的配置参数。</li><li>属性：<ul><li><code>data_path</code>: 训练数据的路径。</li><li><code>eval_data_path</code>: 评估数据的路径。</li><li><code>lazy_preprocess</code>: 是否在数据预处理时使用延迟加载，即在需要时才加载和处理数据。</li></ul></li></ul><h3 id="3-TrainingArguments-类"><a href="#3-TrainingArguments-类" class="headerlink" title="3. TrainingArguments 类"></a>3. TrainingArguments 类</h3><h4 id="Code-2"><a href="#Code-2" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TrainingArguments</span>(transformers.TrainingArguments):</span><br><span class="line">    cache_dir: <span class="type">Optional</span>[<span class="built_in">str</span>] = field(default=<span class="literal">None</span>)</span><br><span class="line">    optim: <span class="built_in">str</span> = field(default=<span class="string">"adamw_torch"</span>)</span><br><span class="line">    model_max_length: <span class="built_in">int</span> = field(</span><br><span class="line">        default=<span class="number">512</span>,</span><br><span class="line">        metadata={</span><br><span class="line">            <span class="string">"help"</span>: <span class="string">"Maximum sequence length. Sequences will be right padded (and possibly truncated)."</span></span><br><span class="line">        },</span><br><span class="line">    )</span><br></pre></td></tr></tbody></table></figure><h4 id="Explanation-2"><a href="#Explanation-2" class="headerlink" title="Explanation"></a>Explanation</h4><p><code>TrainingArguments</code> 类继承自 <code>transformers.TrainingArguments</code>。。</p><ol><li><p><strong>TrainingArguments 类</strong></p><ul><li><code>TrainingArguments</code> 是一个数据类，它通过继承 <code>transformers.TrainingArguments</code>，获得了处理训练参数的能力。</li><li>在 <code>TrainingArguments</code> 中定义的属性：<ul><li><code>cache_dir</code>: 用于指定模型和分词器缓存的目录路径。</li><li><code>optim</code>: 定义了要使用的优化器类型，例如 <code>'adamw_torch'</code>。</li><li><code>model_max_length</code>: 指定模型能处理的最大序列长度。</li></ul></li></ul></li><li><p><strong>transformers.TrainingArguments 类</strong></p><ul><li><code>transformers.TrainingArguments</code> 是 <code>transformers</code> 库中的一个类，用于配置模型训练过程中的各种参数。</li><li>这个类包含大量的属性，用于控制训练过程，例如：<ul><li><code>output_dir</code>: 指定保存模型和训练结果的目录。</li><li><code>num_train_epochs</code>: 训练的轮数（epochs）。</li><li><code>per_device_train_batch_size</code>: 每个设备上的训练批次大小。</li><li><code>save_steps</code>: 保存模型的步数间隔。</li><li><code>evaluation_strategy</code>: 评估模型的策略，如在每个 epoch 结束时进行评估。</li><li><code>learning_rate</code>: 学习率。</li><li><code>warmup_steps</code>: 在学习率调度中用于预热的步数。</li></ul></li><li><code>transformers.TrainingArguments</code> 还包含了许多其他参数，用于微调训练过程，包括日志记录、模型保存策略、学习率调度等。</li></ul></li></ol><p>通过继承 <code>transformers.TrainingArguments</code>，<code>TrainingArguments</code> 类不仅继承了所有这些训练参数的配置能力，而且还可以添加一些自定义的训练参数，如本例中的 <code>cache_dir</code>、<code>optim</code> 和 <code>model_max_length</code>。这种做法提高了代码的可复用性和灵活性，使得您可以根据项目的具体需求调整和扩展训练配置。</p><h2 id="3-功能型函数-Functional-Utility-Functions"><a href="#3-功能型函数-Functional-Utility-Functions" class="headerlink" title="3.功能型函数 (Functional Utility Functions)"></a>3.功能型函数 (Functional Utility Functions)</h2><h3 id="1-rank0-print-args"><a href="#1-rank0-print-args" class="headerlink" title="1. rank0_print(*args)"></a>1. rank0_print(*args)</h3><h4 id="Code-3"><a href="#Code-3" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">local_rank = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">rank0_print</span>(<span class="params">*args</span>):</span><br><span class="line">    <span class="keyword">if</span> local_rank == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(*args)</span><br></pre></td></tr></tbody></table></figure><h4 id="Explanation-3"><a href="#Explanation-3" class="headerlink" title="Explanation"></a>Explanation</h4><p>定义一个全局变量 local_rank，用于分布式训练。<br>定义一个函数 rank0_print，只在 local_rank 为 0 时打印信息，用于分布式训练中的信息输出控制。这样可以避免在多个节点上重复打印相同的信息，使得输出更加清晰和简洁。</p><ul><li>用于只在分布式训练环境中的主节点（rank 0）上打印信息。</li><li>参数：可变数量的参数，用于打印。</li></ul><h3 id="2-trainer-save-model-safe-trainer-transformers-Trainer"><a href="#2-trainer-save-model-safe-trainer-transformers-Trainer" class="headerlink" title="2. trainer_save_model_safe(trainer: transformers.Trainer)"></a>2. <code>trainer_save_model_safe(trainer: transformers.Trainer)</code></h3><h4 id="Code-4"><a href="#Code-4" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">trainer_save_model_safe</span>(<span class="params">trainer: transformers.Trainer</span>):</span><br><span class="line">    <span class="keyword">from</span> torch.distributed.fsdp <span class="keyword">import</span> FullyShardedDataParallel <span class="keyword">as</span> FSDP</span><br><span class="line">    <span class="keyword">from</span> torch.distributed.fsdp <span class="keyword">import</span> StateDictType, FullStateDictConfig</span><br><span class="line"></span><br><span class="line">    save_policy = FullStateDictConfig(offload_to_cpu=<span class="literal">True</span>, rank0_only=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">with</span> FSDP.state_dict_type(</span><br><span class="line">        trainer.model, StateDictType.FULL_STATE_DICT, save_policy</span><br><span class="line">    ):</span><br><span class="line">        trainer.save_model()</span><br></pre></td></tr></tbody></table></figure><p>函数 <code>trainer_save_model_safe(trainer: transformers.Trainer)</code> 旨在安全地保存使用 PyTorch 分布式框架训练的模型。让我们详细了解此函数及其涉及的关键组件。</p><h4 id="Explanation-4"><a href="#Explanation-4" class="headerlink" title="Explanation"></a>Explanation</h4><ol><li>参数：</li></ol><ul><li><code>trainer</code>: <code>transformers.Trainer</code> 的实例。这个类是 Hugging Face Transformers 库的核心组件之一，用于训练和评估模型。</li></ul><ol start="2"><li>功能：</li></ol><ul><li>此函数的主要目的是在分布式训练环境中安全地保存模型。它特别考虑了使用 <code>FullyShardedDataParallel</code> (FSDP) 进行训练时的模型保存策略。</li></ul><ol start="3"><li>FSDP</li></ol><ul><li><strong>FullyShardedDataParallel (FSDP)</strong><ul><li>这是 PyTorch 分布式训练框架的一个组件。FSDP 通过将模型参数分片到多个 GPU 上来减少每个 GPU 的内存占用，从而实现更大模型的训练。</li><li>在此场景中，FSDP 主要用于处理和保存分布式训练中的模型状态。</li></ul></li><li><strong>StateDictType</strong><ul><li>这是一个枚举类型，定义了如何保存模型的状态字典（state dict）。在 FSDP 环境中，保存和加载模型状态可能需要特殊的处理。</li></ul></li><li><strong>FullStateDictConfig</strong><ul><li>这个类用于配置保存完整状态字典时的参数。它是 FSDP 功能的一部分，用于控制如何保存模型状态。</li></ul></li></ul><ol start="4"><li>函数实现</li></ol><ul><li><strong>设置保存策略</strong><ul><li><code>save_policy = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)</code> 创建了一个保存策略。这里指定两个关键参数：<ul><li><code>offload_to_cpu</code>: 在保存状态字典之前，将模型参数卸载到 CPU，这有助于减少 GPU 内存的使用。</li><li><code>rank0_only</code>: 只在 rank 0（通常是主节点）上保存模型。在分布式训练中，这可以避免每个节点都保存相同的模型副本，节省存储空间。</li></ul></li></ul></li><li><strong>保存模型</strong><ul><li>使用 <code>with FSDP.state_dict_type(trainer.model, StateDictType.FULL_STATE_DICT, save_policy)</code> 上下文管理器设置模型保存的状态字典类型和策略。</li><li>在这个上下文内，调用 <code>trainer.save_model()</code> 来保存模型。由于使用了 <code>save_policy</code>，模型将根据上述配置安全地保存。</li></ul></li></ul><p>函数 <code>trainer_save_model_safe</code> 封装了一个安全的模型保存逻辑，特别是针对使用 PyTorch 的 FSDP 进行分布式训练的场景。它确保了只在一个节点上保存完整的模型状态，并且在保存之前将模型参数转移到 CPU，从而优化内存使用和存储效率。这对于训练大型模型和管理大规模分布式训练环境至关重要。</p><h3 id="3-preprocess-sources-tokenizer-transformers-PreTrainedTokenizer-Dict"><a href="#3-preprocess-sources-tokenizer-transformers-PreTrainedTokenizer-Dict" class="headerlink" title="3.preprocess(sources,tokenizer: transformers.PreTrainedTokenizer) -> Dict"></a>3.<code>preprocess(sources,tokenizer: transformers.PreTrainedTokenizer) -&gt; Dict</code></h3><h4 id="Code-5"><a href="#Code-5" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess</span>(<span class="params"></span></span><br><span class="line"><span class="params">    sources,</span></span><br><span class="line"><span class="params">    tokenizer: transformers.PreTrainedTokenizer,</span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="type">Dict</span>:</span><br><span class="line">    conv = get_conversation_template(<span class="string">"vicuna"</span>)</span><br><span class="line">    roles = {<span class="string">"human"</span>: conv.roles[<span class="number">0</span>], <span class="string">"gpt"</span>: conv.roles[<span class="number">1</span>]}</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Apply prompt templates</span></span><br><span class="line">    conversations = []</span><br><span class="line">    <span class="keyword">for</span> i, source <span class="keyword">in</span> <span class="built_in">enumerate</span>(sources):</span><br><span class="line">        <span class="keyword">if</span> roles[source[<span class="number">0</span>][<span class="string">"from"</span>]] != conv.roles[<span class="number">0</span>]:</span><br><span class="line">            <span class="comment"># Skip the first one if it is not from human</span></span><br><span class="line">            source = source[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">        conv.messages = []</span><br><span class="line">        <span class="keyword">for</span> j, sentence <span class="keyword">in</span> <span class="built_in">enumerate</span>(source):</span><br><span class="line">            role = roles[sentence[<span class="string">"from"</span>]]</span><br><span class="line">            <span class="keyword">assert</span> role == conv.roles[j % <span class="number">2</span>], <span class="string">f"<span class="subst">{i}</span>"</span></span><br><span class="line">            conv.append_message(role, sentence[<span class="string">"value"</span>])</span><br><span class="line">        conversations.append(conv.get_prompt())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Tokenize conversations</span></span><br><span class="line">    input_ids = tokenizer(</span><br><span class="line">        conversations,</span><br><span class="line">        return_tensors=<span class="string">"pt"</span>,</span><br><span class="line">        padding=<span class="string">"max_length"</span>,</span><br><span class="line">        max_length=tokenizer.model_max_length,</span><br><span class="line">        truncation=<span class="literal">True</span>,</span><br><span class="line">    ).input_ids</span><br><span class="line">    targets = input_ids.clone()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> conv.sep_style == SeparatorStyle.ADD_COLON_TWO</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Mask targets. Only compute loss on the assistant outputs.</span></span><br><span class="line">    sep = conv.sep + conv.roles[<span class="number">1</span>] + <span class="string">": "</span></span><br><span class="line">    <span class="keyword">for</span> conversation, target <span class="keyword">in</span> <span class="built_in">zip</span>(conversations, targets):</span><br><span class="line">        total_len = <span class="built_in">int</span>(target.ne(tokenizer.pad_token_id).<span class="built_in">sum</span>())</span><br><span class="line"></span><br><span class="line">        turns = conversation.split(conv.sep2)</span><br><span class="line">        cur_len = <span class="number">1</span></span><br><span class="line">        target[:cur_len] = IGNORE_TOKEN_ID</span><br><span class="line">        <span class="keyword">for</span> i, turn <span class="keyword">in</span> <span class="built_in">enumerate</span>(turns):</span><br><span class="line">            <span class="keyword">if</span> turn == <span class="string">""</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            turn_len = <span class="built_in">len</span>(tokenizer(turn).input_ids)</span><br><span class="line"></span><br><span class="line">            parts = turn.split(sep)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(parts) != <span class="number">2</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            parts[<span class="number">0</span>] += sep</span><br><span class="line">            <span class="comment"># "-2" is hardcoded for the Llama tokenizer to make the offset correct.</span></span><br><span class="line">            instruction_len = <span class="built_in">len</span>(tokenizer(parts[<span class="number">0</span>]).input_ids) - <span class="number">2</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i != <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> tokenizer.legacy:</span><br><span class="line">                <span class="comment"># The legacy and non-legacy modes handle special tokens differently</span></span><br><span class="line">                instruction_len -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Ignore the user instructions</span></span><br><span class="line">            target[cur_len : cur_len + instruction_len] = IGNORE_TOKEN_ID</span><br><span class="line">            cur_len += turn_len</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i != <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> tokenizer.legacy:</span><br><span class="line">                <span class="comment"># The legacy and non-legacy modes handle special tokens differently</span></span><br><span class="line">                cur_len -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        target[cur_len:] = IGNORE_TOKEN_ID</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="literal">False</span>:  <span class="comment"># Inspect and check the correctness of masking</span></span><br><span class="line">            z = target.clone()</span><br><span class="line">            z = torch.where(z == IGNORE_TOKEN_ID, tokenizer.unk_token_id, z)</span><br><span class="line">            rank0_print(tokenizer.decode(z))</span><br><span class="line">            exit()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> cur_len &lt; tokenizer.model_max_length:</span><br><span class="line">            <span class="keyword">if</span> cur_len != total_len:</span><br><span class="line">                target[:] = IGNORE_TOKEN_ID</span><br><span class="line">                rank0_print(</span><br><span class="line">                    <span class="string">f"WARNING: tokenization mismatch: <span class="subst">{cur_len}</span> vs. <span class="subst">{total_len}</span>."</span></span><br><span class="line">                    <span class="string">f" #turn = <span class="subst">{<span class="built_in">len</span>(turns) - <span class="number">1</span>}</span>. (ignored)"</span></span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">dict</span>(</span><br><span class="line">        input_ids=input_ids,</span><br><span class="line">        labels=targets,</span><br><span class="line">        attention_mask=input_ids.ne(tokenizer.pad_token_id),</span><br><span class="line">    )</span><br></pre></td></tr></tbody></table></figure><p>函数 <code>preprocess(sources, tokenizer: transformers.PreTrainedTokenizer) -&gt; Dict</code> 用于预处理对话数据，使其适用于机器学习模型的训练。这个函数可以分为几个主要部分进行详细介绍：</p><h4 id="1-获取对话模板和角色定义"><a href="#1-获取对话模板和角色定义" class="headerlink" title="1. 获取对话模板和角色定义"></a>1. 获取对话模板和角色定义</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conv = get_conversation_template(<span class="string">"vicuna"</span>)</span><br><span class="line">roles = {<span class="string">"human"</span>: conv.roles[<span class="number">0</span>], <span class="string">"gpt"</span>: conv.roles[<span class="number">1</span>]}</span><br></pre></td></tr></tbody></table></figure><ul><li><strong>功能</strong>: 初始化对话模板和定义对话参与者的角色。</li><li><strong>实现</strong>:<ul><li><code>conv = get_conversation_template("vicuna")</code> 获取指定模型（如 “vicuna”）的对话模板。</li><li><code>roles</code> 字典将 “human” 和 “gpt” 分别映射到对话模板中定义的角色。</li></ul></li><li><strong>示例</strong>:<ul><li>如果对话模板是 “vicuna”，则 <code>roles</code> 可能是 <code>{"human": "user", "gpt": "assistant"}</code>。<ul><li><code>conv = get_conversation_template("vicuna")</code> 得到的模板如下：<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Conversation(name=<span class="string">'vicuna_v1.1'</span>, system_template=<span class="string">'{system_message}'</span>, system_message=<span class="string">"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."</span>, roles=(<span class="string">'USER'</span>, <span class="string">'ASSISTANT'</span>), messages=[], offset=<span class="number">0</span>, sep_style=&lt;SeparatorStyle.ADD_COLON_TWO: <span class="number">2</span>&gt;, sep=<span class="string">' '</span>, sep2=<span class="string">'&lt;/s&gt;'</span>, stop_str=<span class="literal">None</span>, stop_token_ids=<span class="literal">None</span>)</span><br></pre></td></tr></tbody></table></figure></li><li><code>roles</code> 将 “human” 映射到 “USER”，将 “gpt” 映射到 “ASSISTANT”。<code>{'human': 'USER', 'gpt': 'ASSISTANT'}</code></li></ul></li></ul></li></ul><h4 id="2-prompt-模板"><a href="#2-prompt-模板" class="headerlink" title="2. prompt 模板"></a>2. prompt 模板</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Apply prompt templates</span></span><br><span class="line">conversations = []</span><br><span class="line"><span class="keyword">for</span> i, source <span class="keyword">in</span> <span class="built_in">enumerate</span>(sources):</span><br><span class="line">    <span class="keyword">if</span> roles[source[<span class="number">0</span>][<span class="string">"from"</span>]] != conv.roles[<span class="number">0</span>]:</span><br><span class="line">        <span class="comment"># Skip the first one if it is not from human</span></span><br><span class="line">        source = source[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">    conv.messages = []</span><br><span class="line">    <span class="keyword">for</span> j, sentence <span class="keyword">in</span> <span class="built_in">enumerate</span>(source):</span><br><span class="line">        role = roles[sentence[<span class="string">"from"</span>]]</span><br><span class="line">        <span class="keyword">assert</span> role == conv.roles[j % <span class="number">2</span>], <span class="string">f"<span class="subst">{i}</span>"</span></span><br><span class="line">        conv.append_message(role, sentence[<span class="string">"value"</span>])</span><br><span class="line">    conversations.append(conv.get_prompt())</span><br></pre></td></tr></tbody></table></figure><ul><li><p><strong>功能</strong>: 为源数据应用提示模板，构建对话。</p></li><li><p><strong>实现</strong>:</p><ul><li>遍历 <code>sources</code>（原始对话数据），将每个对话源转换为模板格式的对话。</li><li>如果对话的第一部分不是 “human” 角色发起，则跳过该部分。</li><li>为每个句子指定角色，并将其添加到对话模板中。</li><li>最终，每个处理后的对话被添加到 <code>conversations</code> 列表中。</li></ul></li><li><p><strong>示例</strong>:</p><ul><li>假如我们的 source 是 dummy input 中的第一条数据:<br><code>python source = [{'from': 'human', 'value': 'Who are you?'}, {'from': 'gpt', 'value': 'I am Vicuna, a language model trained by researchers from Large Model Systems Organization (LMSYS).'}, {'from': 'human', 'value': 'Have a nice day!'}, {'from': 'gpt', 'value': 'You too!'}]</code></li><li><code>conversations</code> 在 Vicuna template 下,我们会使用<code>SeparatorStyle.ADD_COLON_TWO</code>作为分隔符风格，构成的数据可能是 [“A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user’s questions. USER: Who are you? ASSISTANT: I am Vicuna, a language model trained by researchers from Large Model Systems Organization (LMSYS).USER: Have a nice day! ASSISTANT: You too!“]</li><li><details><summary> get_prompt的实现 </summary>`get_prompt` 方法的实现根据不同的 `SeparatorStyle` 有着不同的行为。下面是一个表格，详细介绍了各种风格的 `get_prompt` 方法，以及对应的英文示例：<table><thead><tr><th>分隔符风格 (<code>SeparatorStyle</code>)</th><th>描述</th><th>示例</th></tr></thead><tbody><tr><td><code>ADD_COLON_SINGLE</code></td><td>在每个消息后加冒号和分隔符。</td><td>USER: Hello there!\nASSISTANT: Hi, how can I help?\n</td></tr><tr><td><code>ADD_COLON_TWO</code></td><td>使用两种分隔符交替，通常在不同角色之间切换。</td><td>USER: What’s the weather?\nASSISTANT: It’s sunny today.\n\n</td></tr><tr><td><code>ADD_COLON_SPACE_SINGLE</code></td><td>消息后加冒号、空格和分隔符。</td><td>USER: Can you book a flight?\nASSISTANT: Sure, where to?\n</td></tr><tr><td><code>NO_COLON_SINGLE</code></td><td>消息直接跟在角色后，不加冒号，后接分隔符。</td><td>USERWhat are you doing?\nASSISTANTI’m here to assist you.\n</td></tr><tr><td><code>NO_COLON_TWO</code></td><td>无冒号，使用两种分隔符交替。</td><td>USERHow’s the project going?\nASSISTANTIt’s on track.\n\n</td></tr><tr><td><code>ADD_NEW_LINE_SINGLE</code></td><td>每条消息前换行，消息后加分隔符。</td><td>USER\nHow can I reset my password?\nASSISTANT\nYou can reset it via email.\n</td></tr><tr><td><code>RWKV</code></td><td>特殊格式，通常用于特定模型。</td><td>USER: What is AI?\n\nASSISTANT: AI stands for Artificial Intelligence.\n\n</td></tr><tr><td><code>LLAMA2</code></td><td>特殊标签格式，针对特定模型。</td><td>[INST] USER How does blockchain work?\nASSISTANT It is a distributed ledger.\n\n</td></tr><tr><td><code>CHATGLM</code></td><td>特定于 <code>CHATGLM</code> 模型的格式。</td><td>[Round 1]\nUSER: Tell me a joke.\nASSISTANT: Why did the chicken cross the road?\n</td></tr><tr><td><code>CHATML</code></td><td>类似 <code>CHATGLM</code>，但每条消息前后都有换行。</td><td>USER\nDo you like music?\n\nASSISTANT\nYes, I enjoy many genres.\n\n</td></tr><tr><td><code>CHATGLM3</code></td><td>适用于 <code>CHATGLM3</code> 模型的格式。</td><td>USER\nCan you play chess?\nASSISTANTYes, I can play.\n</td></tr><tr><td><code>CHATINTERN</code></td><td>适用于 <code>CHATINTERN</code> 模型的格式，使用特殊标记。</td><td><s>USER:Where is the nearest ATM?<s>\nASSISTANT:It’s next to the post office.\n</s></s></td></tr><tr><td><code>DOLLY</code></td><td>特定于 <code>DOLLY</code> 模型的格式。</td><td>USER:\nWhat is quantum computing?\nASSISTANT:\nIt involves computation using quantum-mechanical phenomena.\n\n</td></tr><tr><td><code>PHOENIX</code></td><td>适用于 <code>PHOENIX</code> 模型，消息被特殊标记包裹。</td><td>USER: <s>How to bake a cake?</s>\nASSISTANT: <s>You need flour, sugar, and eggs.</s>\n</td></tr><tr><td><code>ROBIN</code></td><td>类似 <code>ADD_NEW_LINE_SINGLE</code>，但角色后有换行。</td><td>USER:\nIs AI dangerous?\nASSISTANT:\nIt depends on how it’s used.\n</td></tr><tr><td><code>FALCON_CHAT</code></td><td>类似 <code>ADD_COLON_SINGLE</code>，但可适用于 <code>FALCON</code> 模型。</td><td>USER: What is the capital of France?\nASSISTANT: It’s Paris.\n</td></tr><tr><td><code>METAMATH</code></td><td>对话中使用特殊前缀和后缀，适用于 <code>METAMATH</code> 模型。</td><td>USER:\nWhat is 2+2?\n: It’s 4\n</td></tr><tr><td><code>DEEPSEEK_CHAT</code></td><td>适用于 <code>DEEPSEEK</code> 模型的特定格式。</td><td>USER: What’s your favorite color?\nASSISTANT: I like blue.\n\n</td></tr><tr><td><code>YUAN2</code></td><td>适用于 <code>YUAN2</code> 模型，特殊的分隔符应用。</td><td>How are you today?<n>I’m fine, thank you!<n></n></n></td></tr></tbody></table>每种风格都有其特定的格式，这在处理与不同模型或任务相关的对话数据时非常重要。通过 <code>get_prompt</code> 方法的不同实现，可以灵活地适应各种需求，使对话生成或处理更加准确和高效。</details></li></ul></li></ul><h4 id="3-对话的分词"><a href="#3-对话的分词" class="headerlink" title="3. 对话的分词"></a>3. 对话的分词</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Tokenize conversations</span></span><br><span class="line">input_ids = tokenizer(</span><br><span class="line">    conversations,</span><br><span class="line">    return_tensors=<span class="string">"pt"</span>,</span><br><span class="line">    padding=<span class="string">"max_length"</span>,</span><br><span class="line">    max_length=tokenizer.model_max_length,</span><br><span class="line">    truncation=<span class="literal">True</span>,</span><br><span class="line">).input_ids</span><br><span class="line">targets = input_ids.clone()</span><br></pre></td></tr></tbody></table></figure><ul><li><p><strong>功能</strong>: 文本对话首先被分词处理，转换成模型能够处理的数值序列。然后，这些序列被克隆以形成初始的训练目标。这样做的目的是为了在训练过程中提供一个基准，指导模型学习生成正确的输出。在后续步骤中，这些目标可能会根据特定的训练目标进行调整。</p></li><li><p><strong>实现</strong>：</p><ul><li><code>tokenizer</code> 函数接收文本列表（这里是 <code>conversations</code>），并返回一个包含数值化表示的 <code>input_ids</code>。<ul><li><code>return_tensors="pt"</code> 指定返回的数据类型为 PyTorch 张量。</li><li><code>padding="max_length"</code> 和 <code>max_length=tokenizer.model_max_length</code> 确保所有输入长度统一，不足的部分使用填充。</li><li><code>truncation=True</code> 表示如果输入过长，将其截断到最大长度。</li></ul></li><li>在训练期间，模型需要知道期望的输出以计算损失和进行反向传播。这些期望的输出被称为 “targets”。<code>targets = input_ids.clone()</code> 表示创建 <code>input_ids</code> 的一个副本作为初始的目标。<ul><li>之所以需要克隆 <code>input_ids</code>，是因为在许多语言模型训练任务中（特别是像自回归模型这样的生成任务），模型的目标输出往往与输入非常相似，但在某些细节上存在差异。</li><li>在后续步骤中，这个 <code>targets</code> 可能会根据特定的训练需求进一步修改或掩码（例如，在对话任务中，可能只对模型生成的回复部分计算损失，而不是整个对话）。</li></ul></li></ul></li></ul><h4 id="4-目标掩码"><a href="#4-目标掩码" class="headerlink" title="4. 目标掩码"></a>4. 目标掩码</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Mask targets. Only compute loss on the assistant outputs.</span></span><br><span class="line">sep = conv.sep + conv.roles[<span class="number">1</span>] + <span class="string">": "</span></span><br><span class="line"><span class="keyword">for</span> conversation, target <span class="keyword">in</span> <span class="built_in">zip</span>(conversations, targets):</span><br><span class="line">    total_len = <span class="built_in">int</span>(target.ne(tokenizer.pad_token_id).<span class="built_in">sum</span>())</span><br><span class="line"></span><br><span class="line">    turns = conversation.split(conv.sep2)</span><br><span class="line">    cur_len = <span class="number">1</span></span><br><span class="line">    target[:cur_len] = IGNORE_TOKEN_ID</span><br><span class="line">    <span class="keyword">for</span> i, turn <span class="keyword">in</span> <span class="built_in">enumerate</span>(turns):</span><br><span class="line">        <span class="keyword">if</span> turn == <span class="string">""</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        turn_len = <span class="built_in">len</span>(tokenizer(turn).input_ids)</span><br><span class="line"></span><br><span class="line">        parts = turn.split(sep)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(parts) != <span class="number">2</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        parts[<span class="number">0</span>] += sep</span><br><span class="line">        <span class="comment"># "-2" is hardcoded for the Llama tokenizer to make the offset correct.</span></span><br><span class="line">        instruction_len = <span class="built_in">len</span>(tokenizer(parts[<span class="number">0</span>]).input_ids) - <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> i != <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> tokenizer.legacy:</span><br><span class="line">            <span class="comment"># The legacy and non-legacy modes handle special tokens differently</span></span><br><span class="line">            instruction_len -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Ignore the user instructions</span></span><br><span class="line">        target[cur_len : cur_len + instruction_len] = IGNORE_TOKEN_ID</span><br><span class="line">        cur_len += turn_len</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> i != <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> tokenizer.legacy:</span><br><span class="line">            <span class="comment"># The legacy and non-legacy modes handle special tokens differently</span></span><br><span class="line">            cur_len -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    target[cur_len:] = IGNORE_TOKEN_ID</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="literal">False</span>:  <span class="comment"># Inspect and check the correctness of masking</span></span><br><span class="line">        z = target.clone()</span><br><span class="line">        z = torch.where(z == IGNORE_TOKEN_ID, tokenizer.unk_token_id, z)</span><br><span class="line">        rank0_print(tokenizer.decode(z))</span><br><span class="line">        exit()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> cur_len &lt; tokenizer.model_max_length:</span><br><span class="line">        <span class="keyword">if</span> cur_len != total_len:</span><br><span class="line">            target[:] = IGNORE_TOKEN_ID</span><br><span class="line">            rank0_print(</span><br><span class="line">                <span class="string">f"WARNING: tokenization mismatch: <span class="subst">{cur_len}</span> vs. <span class="subst">{total_len}</span>."</span></span><br><span class="line">                <span class="string">f" #turn = <span class="subst">{<span class="built_in">len</span>(turns) - <span class="number">1</span>}</span>. (ignored)"</span></span><br><span class="line">            )</span><br></pre></td></tr></tbody></table></figure><ul><li><p><strong>功能</strong>: 对目标输出进行掩码处理，以便模型只对特定输出计算损失。目标是对生成的 targets（即模型的输出标签）进行掩码处理。这是为了确保在训练过程中只对助手（assistant）的输出计算损失，而不是整个对话。</p></li><li><p><strong>实现</strong>：</p><ul><li><code>sep = conv.sep + conv.roles[1] + ": "</code> 定义了用于识别助手回复的分隔符。在这个例子中，<code>sep</code> 可能是 “\n\nAssistant: “。</li><li>循环遍历每个处理后的对话 (<code>conversation</code>) 及其对应的目标 (<code>target</code>)。</li><li><code>total_len</code> 是当前目标序列中非填充（padding）部分的长度。</li><li><code>turns</code> 是将对话根据 <code>conv.sep2</code> 分隔成不同轮次的列表。</li></ul></li></ul><ol><li>对每个轮次进行处理</li></ol><ul><li>每个轮次（turn）包含用户和助手的消息。</li><li>使用 <code>tokenizer(turn)</code> 将每个轮次的文本转换为模型能理解的 ID 序列。</li><li>通过 <code>parts = turn.split(sep)</code> 分离用户和助手的消息。</li><li><code>instruction_len</code> 是用户消息部分的长度（在某些情况下需要调整，比如 <code>-2</code> 是为了适应特定的分词器）。</li></ul><ol start="2"><li>掩码目标</li></ol><ul><li><code>target[cur_len : cur_len + instruction_len] = IGNORE_TOKEN_ID</code> 将用户消息部分的目标 ID 替换为 <code>IGNORE_TOKEN_ID</code>，这意味着在计算损失时会忽略这部分。</li><li><code>cur_len</code> 用于跟踪当前处理到的位置。</li><li>每处理完一个轮次，更新 <code>cur_len</code>。</li></ul><ol start="3"><li>最终处理</li></ol><ul><li><code>target[cur_len:] = IGNORE_TOKEN_ID</code> 确保在最后一个轮次之后的所有内容都被忽略。</li><li>如果 <code>cur_len</code> 小于 <code>tokenizer.model_max_length</code>，但不等于 <code>total_len</code>，则表示有不一致性，此时会发出警告，并将整个目标序列设置为 <code>IGNORE_TOKEN_ID</code>。</li></ul><h4 id="5-返回处理后的数据"><a href="#5-返回处理后的数据" class="headerlink" title="5. 返回处理后的数据"></a>5. 返回处理后的数据</h4><ul><li><strong>功能</strong>: 返回预处理后的数据，包括输入 ID、目标标签和注意力掩码。</li><li><strong>实现</strong>:<ul><li>返回一个字典，包含 <code>input_ids</code>（模型输入）、<code>labels</code>（训练目标）和 <code>attention_mask</code>（指示哪些部分是有效输入的掩码）。</li></ul></li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>这个 <code>preprocess</code> 函数通过将原始文本数据转换为模型可以理解的格式，为训练准备数据。它涵盖了从文本处理到分词，再到目标掩码的整个预处理流程。这个过程对于任何基于对话的自然语言处理任务至关重要，特别是在需要模型专注于对特定部分的响应时。</p><h2 id="4-数据集类"><a href="#4-数据集类" class="headerlink" title="4. 数据集类"></a>4. 数据集类</h2><p>这些类继承自 PyTorch 的 <code>Dataset</code> 类，并且是为特定的数据处理任务定制的。这些类包含具体的方法来处理和准备数据，以便用于模型训练。这些类包括：</p><ul><li><code>SupervisedDataset</code>: 用于有监督学习的数据集。它处理原始数据，将其转换为适合模型训练的格式。</li><li><code>LazySupervisedDataset</code>: 类似于 <code>SupervisedDataset</code>，但使用懒加载方式处理数据。这意味着数据只在需要时才被加载和处理，这对于处理大型数据集特别有用。</li></ul><p>这些类通常包含 <code>__init__</code>, <code>__len__</code>, 和 <code>__getitem__</code> 方法，分别用于初始化数据集、获取数据集大小和检索特定索引的数据。这样的设计模式使得数据集可以轻松地与 PyTorch 的 DataLoader 配合使用，从而实现高效的数据加载和批处理。</p><h3 id="1-SupervisedDataset-类"><a href="#1-SupervisedDataset-类" class="headerlink" title="1. SupervisedDataset 类"></a>1. SupervisedDataset 类</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SupervisedDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="string">"""Dataset for supervised fine-tuning."""</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, raw_data, tokenizer: transformers.PreTrainedTokenizer</span>):</span><br><span class="line">        <span class="built_in">super</span>(SupervisedDataset, <span class="variable language_">self</span>).__init__()</span><br><span class="line"></span><br><span class="line">        rank0_print(<span class="string">"Formatting inputs..."</span>)</span><br><span class="line">        sources = [example[<span class="string">"conversations"</span>] <span class="keyword">for</span> example <span class="keyword">in</span> raw_data]</span><br><span class="line">        data_dict = preprocess(sources, tokenizer)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.input_ids = data_dict[<span class="string">"input_ids"</span>]</span><br><span class="line">        <span class="variable language_">self</span>.labels = data_dict[<span class="string">"labels"</span>]</span><br><span class="line">        <span class="variable language_">self</span>.attention_mask = data_dict[<span class="string">"attention_mask"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.input_ids)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, i</span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, torch.Tensor]:</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">dict</span>(</span><br><span class="line">            input_ids=<span class="variable language_">self</span>.input_ids[i],</span><br><span class="line">            labels=<span class="variable language_">self</span>.labels[i],</span><br><span class="line">            attention_mask=<span class="variable language_">self</span>.attention_mask[i],</span><br><span class="line">        )</span><br></pre></td></tr></tbody></table></figure><p><code>SupervisedDataset</code> 类是一个用于有监督学习的数据集类，特别是为了微调（fine-tuning）任务设计。这个类继承自 PyTorch 的 <code>Dataset</code> 类，并重写了其方法以适应特定的数据处理需求。下面是对这个类的详细介绍：<code>SupervisedDataset</code> 类提供了一种结构化和高效的方法来处理和加载用于有监督学习的对话数据。它遵循 PyTorch 数据集（<code>Dataset</code>）的标准结构，使得与 PyTorch 的数据加载器（<code>DataLoader</code>）等其他组件兼容，从而方便在训练循环中使用。通过预处理步骤，该类确保数据以适当的格式提供给模型，以便进行有效的训练。</p><ul><li><strong>类名</strong>: <code>SupervisedDataset</code></li><li><strong>继承</strong>: <code>Dataset</code>（来自 PyTorch）</li><li><strong>目的</strong>: 用于有监督的模型微调任务。</li></ul><h4 id="1-1-init-self-raw-data-tokenizer-transformers-PreTrainedTokenizer"><a href="#1-1-init-self-raw-data-tokenizer-transformers-PreTrainedTokenizer" class="headerlink" title="1.1 __init__(self, raw_data, tokenizer: transformers.PreTrainedTokenizer)"></a>1.1 <code>__init__(self, raw_data, tokenizer: transformers.PreTrainedTokenizer)</code></h4><ul><li><strong>调用时机</strong>：创建 <code>SupervisedDataset</code> 类的实例时。这通常发生在准备训练数据集的阶段，当你创建数据加载器（DataLoader）之前。</li><li><strong>功能</strong>：初始化数据集实例，处理原始对话数据，并将其转换为模型可以理解的格式。</li><li><strong>参数</strong>：<ul><li><code>raw_data</code>：包含对话数据的列表或类似结构。</li><li><code>tokenizer</code>：一个预训练的分词器实例，用于将文本转换为模型可以处理的格式。</li></ul></li><li><strong>返回值</strong>：无返回值，但此方法会设置 <code>input_ids</code>、<code>labels</code> 和 <code>attention_mask</code> 作为类的内部状态。</li><li><strong>实现细节</strong>:<ul><li>使用列表推导式从 <code>raw_data</code> 中提取每个样本的对话内容。</li><li>调用 <code>preprocess</code> 函数处理这些对话，将其转换为适合模型输入的格式。</li><li>从返回的 <code>data_dict</code> 中提取 <code>input_ids</code>（模型输入 ID）、<code>labels</code>（目标标签）和 <code>attention_mask</code>（注意力掩码）。</li></ul></li></ul><h4 id="1-2-len-self"><a href="#1-2-len-self" class="headerlink" title="1.2 __len__(self)"></a>1.2 <code>__len__(self)</code></h4><ul><li><strong>调用时机</strong>：当需要获取数据集大小时，例如在设置数据加载器时，或者在训练循环中迭代数据集时。</li><li><strong>功能</strong>：返回数据集中的样本数量。</li><li><strong>返回值</strong>：一个整数，表示数据集中的样本数量。</li><li><strong>实现</strong>: 直接返回 <code>input_ids</code> 的长度，即样本的数量。</li></ul><h4 id="1-3-getitem-self-i"><a href="#1-3-getitem-self-i" class="headerlink" title="1.3 __getitem__(self, i)"></a>1.3 <code>__getitem__(self, i)</code></h4><ul><li><strong>调用时机</strong>：在数据加载器请求数据集的特定样本时，这通常发生在训练或评估循环的每个迭代中。</li><li><strong>功能</strong>：获取指定索引 <code>i</code> 处的数据样本。</li><li><strong>参数</strong>：<ul><li><code>i</code>：所请求样本的索引。</li></ul></li><li><strong>返回值</strong>：一个字典，包含索引 <code>i</code> 处样本的 <code>input_ids</code>、<code>labels</code> 和 <code>attention_mask</code>。这些是 PyTorch 张量（<code>torch.Tensor</code>），适用于模型的训练或评估。</li></ul><p>在有监督学习的场景中，<code>SupervisedDataset</code> 类扮演着数据预处理和封装的角色，确保数据以正确的格式提供给模型。<code>__init__</code> 方法在数据集实例化时调用，负责数据的初始化和预处理。<code>__len__</code> 和 <code>__getitem__</code> 方法则在训练和评估过程中被频繁调用，分别用于获取数据集的大小和提取特定的数据样本。这些方法的设计和实现使得 <code>SupervisedDataset</code> 类可以无缝地与 PyTorch 的其他数据处理和训练工具集成。</p><h3 id="2-LazySupervisedDataset-类"><a href="#2-LazySupervisedDataset-类" class="headerlink" title="2. LazySupervisedDataset 类"></a>2. LazySupervisedDataset 类</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LazySupervisedDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="string">"""Dataset for supervised fine-tuning."""</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, raw_data, tokenizer: transformers.PreTrainedTokenizer</span>):</span><br><span class="line">        <span class="built_in">super</span>(LazySupervisedDataset, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.tokenizer = tokenizer</span><br><span class="line"></span><br><span class="line">        rank0_print(<span class="string">"Formatting inputs...Skip in lazy mode"</span>)</span><br><span class="line">        <span class="variable language_">self</span>.tokenizer = tokenizer</span><br><span class="line">        <span class="variable language_">self</span>.raw_data = raw_data</span><br><span class="line">        <span class="variable language_">self</span>.cached_data_dict = {}</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.raw_data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, i</span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, torch.Tensor]:</span><br><span class="line">        <span class="keyword">if</span> i <span class="keyword">in</span> <span class="variable language_">self</span>.cached_data_dict:</span><br><span class="line">            <span class="keyword">return</span> <span class="variable language_">self</span>.cached_data_dict[i]</span><br><span class="line"></span><br><span class="line">        ret = preprocess([<span class="variable language_">self</span>.raw_data[i][<span class="string">"conversations"</span>]], <span class="variable language_">self</span>.tokenizer)</span><br><span class="line">        ret = <span class="built_in">dict</span>(</span><br><span class="line">            input_ids=ret[<span class="string">"input_ids"</span>][<span class="number">0</span>],</span><br><span class="line">            labels=ret[<span class="string">"labels"</span>][<span class="number">0</span>],</span><br><span class="line">            attention_mask=ret[<span class="string">"attention_mask"</span>][<span class="number">0</span>],</span><br><span class="line">        )</span><br><span class="line">        <span class="variable language_">self</span>.cached_data_dict[i] = ret</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ret</span><br></pre></td></tr></tbody></table></figure><p><code>LazySupervisedDataset</code> 类是另一种数据集实现，用于有监督的模型微调。与 <code>SupervisedDataset</code> 相比，它采用了一种“懒加载”（lazy loading）的策略。以下是对该类的详细解释，以及它与非懒加载版本的比较。</p><h4 id="2-1-init-self-raw-data-tokenizer-transformers-PreTrainedTokenizer"><a href="#2-1-init-self-raw-data-tokenizer-transformers-PreTrainedTokenizer" class="headerlink" title="2.1 __init__(self, raw_data, tokenizer: transformers.PreTrainedTokenizer)"></a>2.1 <code>__init__(self, raw_data, tokenizer: transformers.PreTrainedTokenizer)</code></h4><ul><li><strong>作用</strong>：初始化 <code>LazySupervisedDataset</code> 实例。</li><li><strong>实现细节</strong>：<ul><li>将原始数据 (<code>raw_data</code>) 和分词器 (<code>tokenizer</code>) 保存为类的属性。</li><li>初始化一个空字典 <code>cached_data_dict</code>，用于缓存已处理的数据。</li></ul></li><li><strong>与 <code>SupervisedDataset</code> 的差异</strong>：<ul><li>在 <code>LazySupervisedDataset</code> 中，原始数据不是在初始化时立即处理，而是存储原始形式以便稍后处理。</li><li><code>cached_data_dict</code> 用于缓存按需处理的数据，以避免重复处理。</li></ul></li></ul><h4 id="2-2-len-self"><a href="#2-2-len-self" class="headerlink" title="2.2 __len__(self)"></a>2.2 <code>__len__(self)</code></h4><ul><li><p><strong>作用</strong>：返回数据集中的样本数量。</p></li><li><p><strong>实现</strong>：直接返回原始数据 (<code>raw_data</code>) 的长度。</p></li><li><p><strong>与 <code>SupervisedDataset</code> 的差异</strong>：</p><ul><li><p>在 <code>LazySupervisedDataset</code> 类中，<code>__len__</code> 方法确实返回的是数据集中样本的数量，但是这里的“样本数量”是指原始数据 (<code>raw_data</code>) 中的样本数量，而不是处理后的数据的数量。由于 <code>LazySupervisedDataset</code> 采用懒加载策略，数据在初始化时并未被处理，因此 <code>__len__</code> 方法基于原始数据计算长度是合理的。</p></li><li><p>这意味着即便数据尚未被转换为模型可用的格式，<code>__len__</code> 方法仍能准确反映数据集中待处理样本的数量。这与 <code>SupervisedDataset</code> 的主要区别在于后者在初始化时就对所有数据进行预处理，因此其 <code>__len__</code> 方法返回的是已处理数据的数量。而在 <code>LazySupervisedDataset</code> 中，数据处理是按需进行的，因此 <code>__len__</code> 返回的是原始数据中的样本数。</p></li></ul></li></ul><h4 id="getitem-self-i"><a href="#getitem-self-i" class="headerlink" title="__getitem__(self, i)"></a><code>__getitem__(self, i)</code></h4><ul><li><strong>作用</strong>：按需获取并处理指定索引 <code>i</code> 处的数据样本。</li><li><strong>实现细节</strong>：<ul><li>首先检查索引 <code>i</code> 是否在缓存 <code>cached_data_dict</code> 中。</li><li>如果是，则直接返回缓存的数据；如果不是，则处理原始数据中索引 <code>i</code> 处的样本，并将处理后的结果添加到缓存中。</li><li>返回一个包含 <code>input_ids</code>、<code>labels</code> 和 <code>attention_mask</code> 的字典。</li></ul></li><li><strong>与 <code>SupervisedDataset</code> 的差异</strong>：<ul><li><code>LazySupervisedDataset</code> 在 <code>__getitem__</code> 被调用时才处理数据，而 <code>SupervisedDataset</code> 在初始化时就处理所有数据。</li><li><code>LazySupervisedDataset</code> 使用缓存来避免重复处理同一样本，而 <code>SupervisedDataset</code> 不需要这种机制，因为所有数据在初始化时就已经被处理。</li></ul></li></ul><h4 id="懒加载-vs-非懒加载"><a href="#懒加载-vs-非懒加载" class="headerlink" title="懒加载 vs 非懒加载"></a>懒加载 vs 非懒加载</h4><ul><li><strong>懒加载（Lazy Loading）</strong>：<ul><li><strong>优点</strong>：减少内存占用，因为只有需要时才处理数据。对于大型数据集非常有用。</li><li><strong>缺点</strong>：可能增加训练时的数据加载时间，尤其是当缓存未命中时。</li></ul></li><li><strong>非懒加载（Eager Loading）</strong>：<ul><li><strong>优点</strong>：在训练开始前一次性处理所有数据，可以减少训练过程中的延迟。</li><li><strong>缺点</strong>：需要更多的初始内存来存储处理后的所有数据，对于非常大的数据集可能不实用。</li></ul></li></ul><h3 id="3-make-supervised-data-module-函数"><a href="#3-make-supervised-data-module-函数" class="headerlink" title="3. make_supervised_data_module 函数"></a>3. <code>make_supervised_data_module</code> 函数</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">make_supervised_data_module</span>(<span class="params"></span></span><br><span class="line"><span class="params">    tokenizer: transformers.PreTrainedTokenizer, data_args</span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="type">Dict</span>:</span><br><span class="line">    <span class="string">"""Make dataset and collator for supervised fine-tuning."""</span></span><br><span class="line">    dataset_cls = (</span><br><span class="line">        LazySupervisedDataset <span class="keyword">if</span> data_args.lazy_preprocess <span class="keyword">else</span> SupervisedDataset</span><br><span class="line">    )</span><br><span class="line">    rank0_print(<span class="string">"Loading data..."</span>)</span><br><span class="line"></span><br><span class="line">    train_json = json.load(<span class="built_in">open</span>(data_args.data_path, <span class="string">"r"</span>))</span><br><span class="line">    train_dataset = dataset_cls(train_json, tokenizer=tokenizer)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> data_args.eval_data_path:</span><br><span class="line">        eval_json = json.load(<span class="built_in">open</span>(data_args.eval_data_path, <span class="string">"r"</span>))</span><br><span class="line">        eval_dataset = dataset_cls(eval_json, tokenizer=tokenizer)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        eval_dataset = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">dict</span>(train_dataset=train_dataset, eval_dataset=eval_dataset)</span><br></pre></td></tr></tbody></table></figure><p>函数 <code>make_supervised_data_module</code> 的目的是为有监督的模型微调创建数据集和数据整理器（collator）。这个函数根据提供的参数构建适合训练和评估的数据集。下面是对这个函数的超级详细解释：</p><h3 id="函数签名"><a href="#函数签名" class="headerlink" title="函数签名"></a>函数签名</h3><ul><li><strong>函数名</strong>：<code>make_supervised_data_module</code></li><li><strong>参数</strong>：<ul><li><code>tokenizer</code>: <code>transformers.PreTrainedTokenizer</code> 的实例，用于文本的分词处理。</li><li><code>data_args</code>: 包含数据相关设置的对象，通常包括数据文件路径等信息。</li></ul></li><li><strong>返回值</strong>：一个字典，包含训练和评估数据集。</li></ul><h3 id="函数实现细节"><a href="#函数实现细节" class="headerlink" title="函数实现细节"></a>函数实现细节</h3><h4 id="1-选择数据集类"><a href="#1-选择数据集类" class="headerlink" title="1. 选择数据集类"></a>1. 选择数据集类</h4><ul><li>根据 <code>data_args.lazy_preprocess</code> 的值选择使用 <code>LazySupervisedDataset</code> 还是 <code>SupervisedDataset</code> 类。<ul><li>如果 <code>data_args.lazy_preprocess</code> 为 <code>True</code>，则使用 <code>LazySupervisedDataset</code> 实现懒加载。</li><li>否则，使用 <code>SupervisedDataset</code> 进行预加载。</li></ul></li><li>这一选择影响数据的加载方式，即数据是一次性全部加载并预处理，还是按需加载和处理。</li></ul><h4 id="2-加载训练数据"><a href="#2-加载训练数据" class="headerlink" title="2. 加载训练数据"></a>2. 加载训练数据</h4><ul><li>使用 <code>json.load(open(data_args.data_path, "r"))</code> 加载训练数据。<ul><li>这里假设训练数据以 JSON 格式存储，并且 <code>data_args.data_path</code> 包含了数据文件的路径。</li></ul></li><li>创建训练数据集 <code>train_dataset</code> 实例，传入加载的训练数据和分词器。</li></ul><h4 id="3-加载评估数据（如果提供）"><a href="#3-加载评估数据（如果提供）" class="headerlink" title="3. 加载评估数据（如果提供）"></a>3. 加载评估数据（如果提供）</h4><ul><li>检查是否提供了评估数据路径 <code>data_args.eval_data_path</code>。<ul><li>如果提供，同样使用 <code>json.load</code> 加载评估数据。</li><li>创建评估数据集 <code>eval_dataset</code> 实例。</li></ul></li><li>如果没有提供评估数据路径，将 <code>eval_dataset</code> 设置为 <code>None</code>。</li></ul><h4 id="4-返回结果"><a href="#4-返回结果" class="headerlink" title="4. 返回结果"></a>4. 返回结果</h4><ul><li>返回一个字典，包含两个键：<code>train_dataset</code> 和 <code>eval_dataset</code>。<ul><li><code>train_dataset</code> 对应训练数据集实例。</li><li><code>eval_dataset</code> 对应评估数据集实例（如果有的话）。</li></ul></li></ul><h2 id="5-训练流程"><a href="#5-训练流程" class="headerlink" title="5. 训练流程"></a>5. 训练流程</h2><p>了解了，我将为 <code>train()</code> 函数中每行代码提供更详细的解释：</p><h3 id="解析命令行参数"><a href="#解析命令行参数" class="headerlink" title="解析命令行参数"></a>解析命令行参数</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parser = transformers.HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))</span><br></pre></td></tr></tbody></table></figure><ul><li>创建一个 <code>HfArgumentParser</code> 实例，这是一个帮助解析命令行参数的工具，特别用于处理 Hugging Face transformers 库中的参数。</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model_args, data_args, training_args = parser.parse_args_into_dataclasses()</span><br></pre></td></tr></tbody></table></figure><ul><li>解析命令行参数并将它们映射到三个数据类 (<code>ModelArguments</code>, <code>DataArguments</code>, <code>TrainingArguments</code>) 的实例中。</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">local_rank = training_args.local_rank</span><br></pre></td></tr></tbody></table></figure><ul><li><code>local_rank</code> 用于标识分布式训练中的进程编号。<code>training_args.local_rank</code> 获取这个编号。</li></ul><h3 id="设置模型配置"><a href="#设置模型配置" class="headerlink" title="设置模型配置"></a>设置模型配置</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">config = transformers.AutoConfig.from_pretrained(</span><br><span class="line">    model_args.model_name_or_path,</span><br><span class="line">    cache_dir=training_args.cache_dir,</span><br><span class="line">    trust_remote_code=model_args.trust_remote_code,</span><br><span class="line">)</span><br></pre></td></tr></tbody></table></figure><ul><li>从预训练模型的配置创建 <code>AutoConfig</code> 实例。它自动加载与特定模型相关的配置。</li><li><code>model_args.model_name_or_path</code>: 指定模型的名称或路径。</li><li><code>cache_dir=training_args.cache_dir</code>: 指定缓存目录。</li><li><code>trust_remote_code=model_args.trust_remote_code</code>: 指定是否信任从远程下载的代码。</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">orig_ctx_len = <span class="built_in">getattr</span>(config, <span class="string">"max_position_embeddings"</span>, <span class="literal">None</span>)</span><br></pre></td></tr></tbody></table></figure><ul><li>使用 <code>getattr</code> 函数从配置中获取 <code>max_position_embeddings</code> 属性，该属性指示模型的最大位置嵌入数（即模型能处理的最大序列长度）。如果不存在该属性，则返回 <code>None</code>。<code>getattr</code> 是一个 Python 内置函数，用于获取对象的属性值。如果属性不存在，返回第三个参数指定的默认值（此处为 <code>None</code>）。</li><li><code>orig_ctx_len</code> 存储模型配置中的 <code>max_position_embeddings</code> 属性值，即模型可以处理的最大位置嵌入数（通常与最大序列长度相关）。</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> orig_ctx_len <span class="keyword">and</span> training_args.model_max_length &gt; orig_ctx_len:</span><br><span class="line">    scaling_factor = <span class="built_in">float</span>(math.ceil(training_args.model_max_length / orig_ctx_len))</span><br><span class="line">    config.rope_scaling = {<span class="string">"type"</span>: <span class="string">"linear"</span>, <span class="string">"factor"</span>: scaling_factor}</span><br></pre></td></tr></tbody></table></figure><ul><li>如果提供的模型最大长度 (<code>training_args.model_max_length</code>) 超过了原始模型的最大长度 (<code>orig_ctx_len</code>)，则计算一个缩放因子以进行位置编码的调整。这通常用于处理超出预训练模型原始设计的序列长度。</li><li><code>rope_scaling</code> 用于调整相对位置编码。</li><li><code>scaling_factor</code> 和 RoPE 缩放<ul><li>如果模型的最大长度超过原始配置的最大长度，<code>scaling_factor</code> 被用来计算缩放因子。</li><li>这涉及到 Rotary Positional Embedding（RoPE）的概念，即在位置嵌入中使用的技术，可以随序列长度线性缩放。</li><li>缩放因子用于调整位置嵌入，使其适应更长的序列。</li></ul></li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">config.use_cache = <span class="literal">False</span></span><br></pre></td></tr></tbody></table></figure><ul><li>禁用模型在前向传播时缓存中间计算结果的功能，这有助于减少内存消耗。这个设置告诉模型在前向传播时不使用或保存缓存。</li></ul><h3 id="加载模型和分词器"><a href="#加载模型和分词器" class="headerlink" title="加载模型和分词器"></a>加载模型和分词器</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = transformers.AutoModelForCausalLM.from_pretrained(</span><br><span class="line">    model_args.model_name_or_path,</span><br><span class="line">    config=config,</span><br><span class="line">    cache_dir=training_args.cache_dir,</span><br><span class="line">    trust_remote_code=model_args.trust_remote_code,</span><br><span class="line">)</span><br></pre></td></tr></tbody></table></figure><ul><li>加载预训练的因果语言模型（Causal Language Model）。这类模型通常用于生成任务。</li><li><code>trust_remote_code</code>这个参数用于确定是否信任从远程（如 Hugging Face Hub）加载的自定义模型代码。</li><li><code>cache_dir=training_args.cache_dir</code><ul><li>指定下载和缓存预训练模型和分词器的目录。</li><li>如果指定，模型和分词器将从这个目录加载，如果不存在，将从远程下载并缓存到此目录。</li></ul></li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tokenizer = transformers.AutoTokenizer.from_pretrained(</span><br><span class="line">    model_args.model_name_or_path,</span><br><span class="line">    cache_dir=training_args.cache_dir,</span><br><span class="line">    model_max_length=training_args.model_max_length,</span><br><span class="line">    padding_side=model_args.padding_side,</span><br><span class="line">    use_fast=<span class="literal">False</span>,</span><br><span class="line">    trust_remote_code=model_args.trust_remote_code,</span><br><span class="line">)</span><br></pre></td></tr></tbody></table></figure><ul><li>加载与模型对应的分词器。</li><li><code>use_fast=False</code>: 表示不使用快速分词器，快速分词器通常是基于 Rust 的分词器，提供更高效的分词处理。</li><li><code>padding_side=model_args.padding_side</code>: 指定填充（padding）应该发生在序列的哪一侧。</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> tokenizer.pad_token != tokenizer.unk_token:</span><br><span class="line">    tokenizer.pad_token = tokenizer.unk_token</span><br></pre></td></tr></tbody></table></figure><ul><li>将分词器的填充令牌设置为未知令牌（<code>unk_token</code>），如果它们不一致的话。这是因为某些模型需要在填充位置使用特定的令牌。</li></ul><h3 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)</span><br></pre></td></tr></tbody></table></figure><ul><li>调用 <code>make_supervised_data_module</code> 函数，为训练和评估准备数据集。这个函数会根据 <code>data_args</code> 中的设置，选择使用懒加载或预加载的方式处理数据。</li></ul><h3 id="初始化并启动训练器"><a href="#初始化并启动训练器" class="headerlink" title="初始化并启动训练器"></a>初始化并启动训练器</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">trainer = Trainer(</span><br><span class="line">    model=model, tokenizer=tokenizer, args=training_args, **data_module</span><br><span class="line">)</span><br></pre></td></tr></tbody></table></figure><ul><li>初始化 <code>Trainer</code> 对象，传入模型、分词器、训练参数以及通过 <code>make_supervised_data_module</code> 函数准备好的数据。</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="built_in">list</span>(pathlib.Path(training_args.output_dir).glob(<span class="string">"checkpoint-*"</span>)):</span><br><span class="line">    trainer.train(resume_from_checkpoint=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    trainer.train()</span><br></pre></td></tr></tbody></table></figure><ul><li>检查是否存在训练检查点，如果存在，则从检查点恢复训练；如果不存在，开始新的训练过程。</li></ul><h3 id="保存模型"><a href="#保存模型" class="headerlink" title="保存模型"></a>保存模型</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model.config.use_cache = <span class="literal">True</span></span><br><span class="line">trainer.save_state()</span><br></pre></td></tr></tbody></table></figure><ul><li>启用模型的缓存并保存训练器的状态。</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> trainer.is_deepspeed_enabled:</span><br><span class="line">    trainer.save_model()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    trainer_save_model_safe(trainer)</span><br></pre></td></tr></tbody></table></figure><ul><li>检查是否启用了 DeepSpeed。如果启用了，则使用 <code>trainer.save_model()</code> 保存模型。如果没有启用 DeepSpeed，则使用 <code>trainer_save_model_safe</code> 安全地保存模型，特别是在使用分布式训练时。</li></ul><h3 id="Trainer-类解释"><a href="#Trainer-类解释" class="headerlink" title="Trainer 类解释"></a>Trainer 类解释</h3><p><code>Trainer</code> 是 Hugging Face Transformers 库提供的一个类，用于封装模型的训练逻辑。以下是对 <code>Trainer</code> 类的功能的详细介绍：</p><ul><li><p><strong>模型训练与评估</strong>：<code>Trainer</code> 类负责设置和执行模型的训练和评估过程。它自动处理数据的批处理、梯度计算、优化器步骤和设备管理等任务。</p></li><li><p><strong>参数</strong>：在初始化时，<code>Trainer</code> 接受多种参数，包括模型（<code>model</code>）、分词器（<code>tokenizer</code>）、训练参数（如学习率、批大小等，通过 <code>training_args</code> 传入）和数据集。</p></li><li><p><strong>灵活性和高级功能</strong>：<code>Trainer</code> 支持多种训练设置，如多 GPU 训练、混合精度训练和 TPU 训练。它还支持自定义回调函数，用于在训练过程中执行特定操作。</p></li><li><p><strong>简化 API</strong>：<code>Trainer</code> 类提供了一个简化的 API，使得用户可以用几行代码配置和运行模型训练。它抽象了许多底层细节，使得用户可以专注于模型的构建和训练策略。</p></li><li><p><strong>检查点和恢复</strong>：<code>Trainer</code> 支持保存和加载检查点，这意味着训练过程可以在中断后从上次保存的状态恢复。</p></li></ul><p>总体来说，<code>Trainer</code> 类是一个功能强大且灵活的工具，为训练复杂的 Transformer 模型提供了便利和高效性。</p><h2 id="6-run-bash"><a href="#6-run-bash" class="headerlink" title="6. run bash"></a>6. run bash</h2><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">torchrun --nproc_per_node=8 --master_port=20001 fastchat/train/train.py \</span><br><span class="line">    --model_name_or_path ~/vicuna-7b-v1.5-16k  \</span><br><span class="line">    --data_path data/dummy_conversation.json \</span><br><span class="line">    --fp16 True \</span><br><span class="line">    --output_dir output_vicuna \</span><br><span class="line">    --num_train_epochs 3 \</span><br><span class="line">    --per_device_train_batch_size 8 \</span><br><span class="line">    --per_device_eval_batch_size 1 \</span><br><span class="line">    --gradient_accumulation_steps 1 \</span><br><span class="line">    --evaluation_strategy <span class="string">"no"</span> \</span><br><span class="line">    --save_strategy <span class="string">"steps"</span> \</span><br><span class="line">    --save_steps 1200 \</span><br><span class="line">    --save_total_limit 10 \</span><br><span class="line">    --learning_rate 2e-5 \</span><br><span class="line">    --weight_decay 0. \</span><br><span class="line">    --warmup_ratio 0.03 \</span><br><span class="line">    --lr_scheduler_type <span class="string">"cosine"</span> \</span><br><span class="line">    --logging_steps 1 \</span><br><span class="line">    --fsdp <span class="string">"full_shard auto_wrap"</span> \</span><br><span class="line">    --fsdp_transformer_layer_cls_to_wrap <span class="string">'LlamaDecoderLayer'</span> \</span><br><span class="line">    --model_max_length 2048 \</span><br><span class="line">    --gradient_checkpointing True \</span><br><span class="line">    --lazy_preprocess True</span><br></pre></td></tr></tbody></table></figure><h3 id="torchrun"><a href="#torchrun" class="headerlink" title="torchrun"></a>torchrun</h3><p><code>torchrun</code> 是 PyTorch 提供的一个命令行工具，用于启动分布式训练。它是 <code>torch.distributed.launch</code> 模块的一部分，旨在简化在多个进程上运行 PyTorch 程序的过程。以下是对 <code>torchrun</code> 中使用的参数的详细解释：</p><ol><li><p><code>--nproc_per_node=8</code></p><ul><li><code>--nproc_per_node</code> 指定每个节点（在这种情况下通常是一台机器）上要启动的进程数。这里设置为 8，意味着在当前节点上将启动 8 个训练进程。</li><li>作用：用于控制每个节点上的并行度。在多 GPU 系统中，这通常等于 GPU 的数量。</li></ul></li><li><p><code>--master_port=20001</code></p><ul><li><code>--master_port</code> 指定主节点用于通信的端口。这里设置为 20001。</li><li>作用：在分布式训练中，不同进程需要通过网络进行通信。这个参数指定了用于进程间通信的端口。</li></ul></li><li><p><code>fastchat/train/train.py</code></p><ul><li>这不是 <code>torchrun</code> 的参数，而是指定了要执行的 Python 脚本，即训练脚本的路径。</li></ul></li></ol><p>在分布式训练中，<code>torchrun</code> 负责在每个进程中正确地设置环境变量，如 <code>LOCAL_RANK</code>（当前进程在其节点上的排名）、<code>WORLD_SIZE</code>（总进程数）和 <code>RANK</code>（全局进程排名）。这些环境变量对于使用 PyTorch 分布式包（如 <code>torch.distributed</code>）进行有效通信至关重要。</p><h3 id="使用示例"><a href="#使用示例" class="headerlink" title="使用示例"></a>使用示例</h3><p>假设您有一台拥有 8 个 GPU 的机器，您想在所有 GPU 上并行运行训练。使用 <code>torchrun</code>，您的命令可能如下所示：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torchrun --nproc_per_node=8 --master_port=20001 fastchat/train/train.py --其他参数</span><br></pre></td></tr></tbody></table></figure><p>这个命令会在每个 GPU 上启动一个训练进程，每个进程运行 <code>train.py</code> 脚本，并且所有进程能够通过分布式通信有效协作。</p><p><code>torchrun</code> 是分布式训练的关键工具，它简化了在多个进程上启动 PyTorch 程序的流程，特别是在多 GPU 环境中。通过自动设置必要的环境变量，<code>torchrun</code> 使得实现和运行分布式训练变得更加容易和可靠。</p><h3 id="2-参数"><a href="#2-参数" class="headerlink" title="2. 参数"></a>2. 参数</h3><ol><li><p><code>--model_name_or_path</code></p><ul><li>可以是预训练模型的官方名称（如 “bert-base-uncased”）、自定义训练的模型路径或 Hugging Face Model Hub 上的模型。</li><li>作用：指定用于训练的模型。</li></ul></li><li><p><code>--data_path</code></p><ul><li>路径可以是本地文件系统上的路径。</li><li>作用：指定训练使用的数据文件。</li></ul></li><li><p><code>--fp16</code></p><ul><li>可取值为 True 或 False。</li><li>作用：启用或禁用混合精度训练，以提高训练速度和降低显存使用。</li></ul></li><li><p><code>--output_dir</code></p><ul><li>任何有效的文件路径。</li><li>作用：指定输出目录，用于保存训练过程中产生的文件。</li></ul></li><li><p><code>--num_train_epochs</code></p><ul><li>任何正整数。</li><li>作用：指定训练的轮次。</li></ul></li><li><p><code>--per_device_train_batch_size</code> 和 <code>--per_device_eval_batch_size</code></p><ul><li>任何正整数。</li><li>作用：分别指定每个设备上的训练和评估批次大小。</li></ul></li><li><p><code>--gradient_accumulation_steps</code></p><ul><li>任何正整数。</li><li>作用：指定梯度累积的步骤数，用于在有限的显存下增加有效的批次大小。</li></ul></li><li><p><code>--evaluation_strategy</code></p><ul><li>可取值包括 “no”、”steps”、”epoch”。</li><li>作用：指定评估的策略，如每个 epoch 或特定步数后进行评估，或不进行评估。</li></ul></li><li><p><code>--save_strategy</code></p><ul><li>可取值包括 “no”、”steps”、”epoch”。</li><li>作用：指定模型保存的策略。</li></ul></li><li><p><code>--save_steps</code> 和 <code>--save_total_limit</code></p><ul><li><code>--save_steps</code> 取任何正整数。</li><li><code>--save_total_limit</code> 取任何正整数或 None。</li><li>作用：分别指定保存模型的步数间隔和最大保存的检查点数量。</li></ul></li><li><p><code>--learning_rate</code></p><ul><li>任何正浮点数。</li><li>作用：指定优化器的学习率。</li></ul></li><li><p><code>--weight_decay</code></p><ul><li>任何非负浮点数。</li><li>作用：指定权重衰减，用于正则化。</li></ul></li><li><p><code>--warmup_ratio</code></p><ul><li>任何非负浮点数，通常在 0 到 1 之间。</li><li>作用：指定预热的比例，即学习率在初始阶段逐渐增加的过程。</li></ul></li><li><p><code>--lr_scheduler_type</code></p><ul><li>可取值如 “linear”、”cosine”、”cosine_with_restarts”、”polynomial” 等。</li><li>作用：指定学习率调度器的类型。</li></ul></li><li><p><code>--logging_steps</code></p><ul><li>任何正整数。</li><li>作用：指定记录日志的步数间隔。</li></ul></li><li><p><code>--fsdp</code></p><ul><li>可取值如 “full_shard”、”auto_wrap” 等，或它们的组合。</li><li>作用：指定使用全分片数据并行（Fully Sharded Data Parallel）的配置。</li></ul></li><li><p><code>--fsdp_transformer_layer_cls_to_wrap</code></p><ul><li>指定要在 FSDP 中包装的特定层的类名。</li><li>作用：针对大型模型的分布式训练进行优化。</li></ul></li><li><p><code>--model_max_length</code></p><ul><li>任何正整数。</li><li>作用：指定模型处理的最大序列长度。</li></ul></li><li><p><code>--gradient_checkpointing</code></p><ul><li>可取值为 True 或 False。</li><li>作用：启用或禁用梯度检查点，以减少显存使用。</li></ul></li><li><p><code>--lazy_preprocess</code></p><ul><li>可取值为 True 或 False。</li><li>作用：启用或禁用懒加载预处理，即按需加载和处理数据。</li></ul></li></ol><p>这些参数共同构成了一个复杂的训练配置，允许用户根据特定需求灵活调整模型训练过程。</p><h2 id="7-总结"><a href="#7-总结" class="headerlink" title="7. 总结"></a>7. 总结</h2><p>随着本文的结束，我们完成了对 FastChat 平台中 train.py 脚本的深入解析，这只是我们系列技术博客中的第一部分。在这一部分中，我们聚焦于 train.py 脚本的结构和功能，涵盖了从数据预处理到模型训练和保存等关键步骤。通过这次解析，读者不仅能够更好地理解 FastChat 平台的工作原理，还能获得如何有效利用这个工具进行大型语言模型训练的宝贵知识。</p><p>随着我们技术博客系列的不断展开，我们将继续深入探索 FastChat 的其他组件和功能。接下来的文章将进一步拓展我们的讨论范围，涉及到更多高级功能和实际应用场景。我们期望这些内容能够为对 AI 和机器学习感兴趣的读者提供更全面、深入的见解。</p><p>最后，我们鼓励读者持续关注我们的博客，以获取关于 FastChat 及其在大型语言模型训练领域应用的最新信息和分析。无论您是该领域的专家还是初学者，我们相信这个系列将为您提供价值和启发。敬请期待我们下一篇文章的发布，它将为您揭开 FastChat 更多令人兴奋的面纱。</p>]]></content>
      
      
      <categories>
          
          <category> NLP Insights </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
            <tag> FastChat </tag>
            
            <tag> Train </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>英语学习日记：De Facto</title>
      <link href="/2024/02/26/Life%20Reflections/%E8%8B%B1%E8%AF%AD%E5%AD%A6%E4%B9%A0%E6%97%A5%E8%AE%B0%EF%BC%9ADe%20Facto/"/>
      <url>/2024/02/26/Life%20Reflections/%E8%8B%B1%E8%AF%AD%E5%AD%A6%E4%B9%A0%E6%97%A5%E8%AE%B0%EF%BC%9ADe%20Facto/</url>
      
        <content type="html"><![CDATA[<p>大家好！🌐 今天我们要探讨一个在英语对话和写作中常见的短语：<strong>de facto</strong></p><h3 id="理解-“De-Facto”"><a href="#理解-“De-Facto”" class="headerlink" title="理解 “De Facto”"></a>理解 “De Facto”</h3><ol><li><p><strong>含义</strong>：’De facto’ 这个短语用来描述一些实际上存在的事物，即使它们没有被官方认可或法律确立。就像是在说“实际上”或“实践中”，而不是“理论上”或“官方上”。</p></li><li><p><strong>词源</strong>：这个短语有着非常有趣的历史。它源自拉丁语，其中 ‘de’ 意为 ‘来自’，’facto’ 意味着 ‘事实’。随着时间的推移，它被英语采纳，并保留了从拉丁语中原始的精髓。</p></li></ol><h3 id="例句"><a href="#例句" class="headerlink" title="例句"></a>例句</h3><ul><li>🌟 <em>In many organizations, there is a <strong>de facto</strong> leader who isn’t officially the boss but is respected and followed by the team.</em></li><li>🌟 <em>While English is the <strong>de facto</strong> language of international business, it’s not the official language in many countries where it’s widely spoken.</em></li><li>🌟 <em>The museum, though not formally recognized, acts as the <strong>de facto</strong> cultural center of the small town.</em></li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>‘De facto’ 这个术语是一种微妙但强大的方式，用来描述情况的现实，区别于其官方或法律地位。将这样的短语纳入你的语言库不仅丰富了你的词汇，还增强了你表达细微想法的能力。继续探索并拥抱语言的美妙吧！📚💬</p><p>记住，语言是一段旅程，不是终点。祝学习愉快，我们下次见！🚀🌟</p>]]></content>
      
      
      <categories>
          
          <category> Life Reflections </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Language Learning </tag>
            
            <tag> English Vocabulary </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>De Facto: Unveiling the Power of an Intriguing Phrase</title>
      <link href="/2024/02/26/Life%20Reflections/De%20Facto:%20Unveiling%20the%20Power%20of%20an%20Intriguing%20Phrase%20/"/>
      <url>/2024/02/26/Life%20Reflections/De%20Facto:%20Unveiling%20the%20Power%20of%20an%20Intriguing%20Phrase%20/</url>
      
        <content type="html"><![CDATA[<p>Hello, language enthusiasts! 🌐 Today, we’re diving into a fascinating phrase that often pops up in English conversations and writings: <strong>de facto</strong>. Let’s explore its meaning, origins, and how to use it effectively in sentences. Whether you’re a language learner or a word nerd, you’ll find this exploration both enlightening and fun! ✨</p><h3 id="Understanding-“De-Facto”"><a href="#Understanding-“De-Facto”" class="headerlink" title="Understanding “De Facto”"></a>Understanding “De Facto”</h3><ol><li><p><strong>Meaning</strong>: The term ‘de facto’ is used to describe something that exists in reality, even if it’s not officially recognized or legally established. It’s like saying “in practice” or “in actuality,” as opposed to “in theory” or “officially.”</p></li><li><p><strong>Origins</strong>: This phrase has an interesting journey. It comes from Latin, where ‘de’ means ‘from’ and ‘facto’ means ‘fact.’ Over time, it’s been adopted into English, retaining its original essence from Latin.</p></li></ol><h3 id="Examples-in-Sentences"><a href="#Examples-in-Sentences" class="headerlink" title="Examples in Sentences"></a>Examples in Sentences</h3><ul><li>🌟 <em>In many organizations, there is a <strong>de facto</strong> leader who isn’t officially the boss but is respected and followed by the team.</em></li><li>🌟 <em>While English is the <strong>de facto</strong> language of international business, it’s not the official language in many countries where it’s widely spoken.</em></li><li>🌟 <em>The museum, though not formally recognized, acts as the <strong>de facto</strong> cultural center of the small town.</em></li></ul><h3 id="Wrapping-Up"><a href="#Wrapping-Up" class="headerlink" title="Wrapping Up"></a>Wrapping Up</h3><p>The term ‘de facto’ is a subtle but powerful way to describe the reality of a situation, distinguishing it from its official or legal status. Incorporating such phrases into your language arsenal not only enriches your vocabulary but also enhances your ability to express nuanced ideas. Keep exploring and embracing the beauty of language! 📚💬</p><p>Remember, language is a journey, not a destination. Happy learning, and see you in the next post! 🚀🌟</p>]]></content>
      
      
      <categories>
          
          <category> Life Reflections </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Language Learning </tag>
            
            <tag> English Vocabulary </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>英语学习日记：探索金融术语 &#39;Giro Date&#39;</title>
      <link href="/2024/02/21/Life%20Reflections/%E8%8B%B1%E8%AF%AD%E5%AD%A6%E4%B9%A0%E6%97%A5%E8%AE%B0%EF%BC%9A%E6%8E%A2%E7%B4%A2%E9%87%91%E8%9E%8D%E6%9C%AF%E8%AF%AD%20&#39;Giro%20Date&#39;/"/>
      <url>/2024/02/21/Life%20Reflections/%E8%8B%B1%E8%AF%AD%E5%AD%A6%E4%B9%A0%E6%97%A5%E8%AE%B0%EF%BC%9A%E6%8E%A2%E7%B4%A2%E9%87%91%E8%9E%8D%E6%9C%AF%E8%AF%AD%20&#39;Giro%20Date&#39;/</url>
      
        <content type="html"><![CDATA[<p>今天的英语学习之旅中，我遇到了一个有趣的金融术语：“Giro date”。这个探索过程不仅丰富了我的词汇，还加深了我对英语中金融概念的理解。让我和你分享一下我是如何分析这个术语及其在金融世界中的重要性。</p><h3 id="发现词源-🌍"><a href="#发现词源-🌍" class="headerlink" title="发现词源 🌍"></a>发现词源 🌍</h3><ul><li><strong>意大利语根源</strong>: 我的研究发现 ‘Giro’ 来自意大利语单词 “girare”，意味着转账或支付。</li><li><strong>金融语境</strong>: 在金融领域中，’giro’ 通常指的是通过银行或其他金融机构的转账。</li></ul><h3 id="学习其用法-💡"><a href="#学习其用法-💡" class="headerlink" title="学习其用法 💡"></a>学习其用法 💡</h3><ul><li><strong>定义</strong>: 在金融交易中，’Giro date’ 特指支付或结算日期。</li><li><strong>银行业重要性</strong>: 这是一个预定的日期，资金预期在此日期被支付或结算。</li></ul><h3 id="实际应用-📘"><a href="#实际应用-📘" class="headerlink" title="实际应用 📘"></a>实际应用 📘</h3><ul><li><strong>发票支付日期</strong>: “Please ensure that the giro date for the invoice is set to the 25th of this month.”（请确保发票的支付日期设为本月25日。）</li><li><strong>贷款还款日期</strong>: “The giro date for the loan repayment is automatically set for the 1st of each month.”（贷款还款的支付日期自动设定为每月的第一天。）</li></ul><p>这些例子帮助我巩固了对这个术语的理解，展示了它在银行业务、财务管理和账务处理中的用途。</p><h3 id="反思与前行-🌟"><a href="#反思与前行-🌟" class="headerlink" title="反思与前行 🌟"></a>反思与前行 🌟</h3><p>随我继续在英语学习的迷人世界中旅行，每一个术语都解锁了新的知识和理解！📚✨</p>]]></content>
      
      
      <categories>
          
          <category> Life Reflections </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Language Learning </tag>
            
            <tag> English Vocabulary </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>English Learning Journey: Unraveling the Term &#39;Giro Date&#39;</title>
      <link href="/2024/02/21/Life%20Reflections/English%20Learning%20Journey:%20Unraveling%20the%20Term%20&#39;Giro%20Date&#39;/"/>
      <url>/2024/02/21/Life%20Reflections/English%20Learning%20Journey:%20Unraveling%20the%20Term%20&#39;Giro%20Date&#39;/</url>
      
        <content type="html"><![CDATA[<p>Today in my English learning journey, I encountered an intriguing financial term: “Giro date”. This exploration not only expanded my vocabulary but also deepened my understanding of financial concepts in English. Let me share with you how I dissected this term and its relevance in the financial world.</p><h3 id="Discovering-the-Origin-🌍"><a href="#Discovering-the-Origin-🌍" class="headerlink" title="Discovering the Origin 🌍"></a>Discovering the Origin 🌍</h3><ul><li><strong>Italian Roots</strong>: My research revealed that ‘Giro’ originates from the Italian word “girare,” meaning to transfer or pay. </li><li><strong>Financial Context</strong>: In the realm of finance, ‘giro’ typically refers to a bank or institutional transfer.</li></ul><h3 id="Learning-the-Usage-💡"><a href="#Learning-the-Usage-💡" class="headerlink" title="Learning the Usage 💡"></a>Learning the Usage 💡</h3><ul><li><strong>Definition</strong>: In financial transactions, ‘Giro date’ specifically denotes the payment or settlement date.</li><li><strong>Banking Significance</strong>: It’s a predetermined date when funds are expected to be paid or settled.</li></ul><h3 id="Applying-it-in-Context-📘"><a href="#Applying-it-in-Context-📘" class="headerlink" title="Applying it in Context 📘"></a>Applying it in Context 📘</h3><ul><li><strong>Invoice Payment Date</strong>: I practiced using the term in a sentence: “Please ensure that the giro date for the invoice is set to the 25th of this month.”</li><li><strong>Loan Repayment Date</strong>: Another example I came up with was, “The giro date for the loan repayment is automatically set for the 1st of each month.”</li></ul><p>These examples helped cement the term in my mind, illustrating its use in banking, financial management, and account processing.</p><h3 id="Reflections-and-Forward-Steps-🌟"><a href="#Reflections-and-Forward-Steps-🌟" class="headerlink" title="Reflections and Forward Steps 🌟"></a>Reflections and Forward Steps 🌟</h3><p>Understanding ‘Giro date’ was not just about adding a new word to my vocabulary; it was about comprehending a concept that plays a vital role in financial transactions. This learning experience has made me appreciate the nuances of financial English and motivated me to delve deeper into industry-specific terminology.</p><p>Join me as I continue my journey through the fascinating world of English language learning, where every term unlocks new knowledge and understanding! 📚✨</p>]]></content>
      
      
      <categories>
          
          <category> Life Reflections </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Language Learning </tag>
            
            <tag> English Vocabulary </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to Resolve SSH Key Issues with Multiple Git Services</title>
      <link href="/2024/02/19/Debugging%20Diaries/How%20to%20Resolve%20SSH%20Key%20Issues%20with%20Multiple%20Git%20Services/"/>
      <url>/2024/02/19/Debugging%20Diaries/How%20to%20Resolve%20SSH%20Key%20Issues%20with%20Multiple%20Git%20Services/</url>
      
        <content type="html"><![CDATA[<h1 id="How-to-Resolve-SSH-Key-Issues-with-Multiple-Git-Services-🗝️"><a href="#How-to-Resolve-SSH-Key-Issues-with-Multiple-Git-Services-🗝️" class="headerlink" title="How to Resolve SSH Key Issues with Multiple Git Services 🗝️"></a>How to Resolve SSH Key Issues with Multiple Git Services 🗝️</h1><p>When using Git with different Git services such as GitHub and GitLab, you may encounter SSH key issues. This article will guide you on how to set up and configure SSH keys so that you can work smoothly with multiple services simultaneously.</p><h2 id="1-Generate-SSH-Keys-🔑"><a href="#1-Generate-SSH-Keys-🔑" class="headerlink" title="1. Generate SSH Keys 🔑"></a>1. Generate SSH Keys 🔑</h2><p>First, generate a separate SSH key for each Git service.</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -b 4096 -C <span class="string">"your_email@example.com"</span></span><br></pre></td></tr></tbody></table></figure><p>When generating the keys, save each key with a different filename, for example, <code>id_rsa_github</code> and <code>id_rsa_gitlab</code>.</p><h2 id="2-Add-SSH-Keys-to-Git-Services-🌐"><a href="#2-Add-SSH-Keys-to-Git-Services-🌐" class="headerlink" title="2. Add SSH Keys to Git Services 🌐"></a>2. Add SSH Keys to Git Services 🌐</h2><p>Log in to your GitHub and GitLab accounts, then add the generated public keys (<code>.pub</code> files) to the SSH key sections of each respective account.</p><h2 id="3-Configure-SSH-⚙️"><a href="#3-Configure-SSH-⚙️" class="headerlink" title="3. Configure SSH ⚙️"></a>3. Configure SSH ⚙️</h2><p>Create or edit the <code>~/.ssh/config</code> file to configure different SSH keys for each service.</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GitHub</span></span><br><span class="line">Host github.com</span><br><span class="line">  HostName github.com</span><br><span class="line">  User git</span><br><span class="line">  IdentityFile ~/.ssh/id_rsa_github</span><br><span class="line"></span><br><span class="line"><span class="comment"># GitLab</span></span><br><span class="line">Host gitlab.com</span><br><span class="line">  HostName gitlab.com</span><br><span class="line">  User git</span><br><span class="line">  IdentityFile ~/.ssh/id_rsa_gitlab</span><br></pre></td></tr></tbody></table></figure><p>If your company uses a custom GitLab instance, add a separate configuration block for it.</p><h2 id="4-Test-SSH-Connections-🧪"><a href="#4-Test-SSH-Connections-🧪" class="headerlink" title="4. Test SSH Connections 🧪"></a>4. Test SSH Connections 🧪</h2><p>Test if you can successfully connect to each service via SSH.</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ssh -T git@github.com</span><br><span class="line">ssh -T git@gitlab.com</span><br></pre></td></tr></tbody></table></figure><h2 id="5-Handling-Common-Errors-❗"><a href="#5-Handling-Common-Errors-❗" class="headerlink" title="5. Handling Common Errors ❗"></a>5. Handling Common Errors ❗</h2><p>If you encounter errors such as “Permission denied (publickey)”, check the following:</p><ul><li>Ensure SSH keys are correctly added to the respective Git services.</li><li>Verify if the <code>~/.ssh/config</code> file is configured correctly.</li><li>Use the <code>ssh-add</code> command to ensure SSH keys are loaded into the SSH Agent.</li></ul><h2 id="6-Common-Issues-and-Solutions-💡"><a href="#6-Common-Issues-and-Solutions-💡" class="headerlink" title="6. Common Issues and Solutions 💡"></a>6. Common Issues and Solutions 💡</h2><ul><li><strong>Multiple GitLab Instances</strong>: If you use a custom GitLab instance along with GitLab.com, ensure they are separately configured in the SSH <code>config</code> file.</li><li><strong>Network Issues</strong>: Check if any network settings (like proxies, VPNs) might affect SSH connections.</li></ul><hr><p>By following the above steps, you should be able to resolve most SSH key-related issues, especially when dealing with multiple Git services. If you have further questions or specific scenarios, feel free to ask in the comments. 🚀✨</p>]]></content>
      
      
      <categories>
          
          <category> Debugging Diaries </category>
          
      </categories>
      
      
        <tags>
            
            <tag> IssueFix </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>如何解决多个 Git 服务的 SSH 密钥问题</title>
      <link href="/2024/02/19/Debugging%20Diaries/%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E5%A4%9A%E4%B8%AA%20Git%20%E6%9C%8D%E5%8A%A1%E7%9A%84%20SSH%20%E5%AF%86%E9%92%A5%E9%97%AE%E9%A2%98/"/>
      <url>/2024/02/19/Debugging%20Diaries/%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E5%A4%9A%E4%B8%AA%20Git%20%E6%9C%8D%E5%8A%A1%E7%9A%84%20SSH%20%E5%AF%86%E9%92%A5%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<h1 id="如何解决多个-Git-服务的-SSH-密钥问题-🗝️"><a href="#如何解决多个-Git-服务的-SSH-密钥问题-🗝️" class="headerlink" title="如何解决多个 Git 服务的 SSH 密钥问题 🗝️"></a>如何解决多个 Git 服务的 SSH 密钥问题 🗝️</h1><p>在使用 Git 和不同的 Git 服务（如 GitHub 和 GitLab）时，可能会遇到 SSH 密钥的问题。本文将指导你如何设置和配置 SSH 密钥，以便可以同时与多个服务顺利工作。</p><h2 id="1-生成-SSH-密钥-🔑"><a href="#1-生成-SSH-密钥-🔑" class="headerlink" title="1. 生成 SSH 密钥 🔑"></a>1. 生成 SSH 密钥 🔑</h2><p>首先，为每个 Git 服务生成一个独立的 SSH 密钥。</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -b 4096 -C <span class="string">"your_email@example.com"</span></span><br></pre></td></tr></tbody></table></figure><p>在生成密钥时，将每个密钥保存为不同的文件名，例如 <code>id_rsa_github</code> 和 <code>id_rsa_gitlab</code>。</p><h2 id="2-将-SSH-密钥添加到-Git-服务-🌐"><a href="#2-将-SSH-密钥添加到-Git-服务-🌐" class="headerlink" title="2. 将 SSH 密钥添加到 Git 服务 🌐"></a>2. 将 SSH 密钥添加到 Git 服务 🌐</h2><p>登录到你的 GitHub 和 GitLab 账户，然后将生成的公钥（<code>.pub</code> 文件）添加到各自账户的 SSH 密钥部分。</p><h2 id="3-配置-SSH-⚙️"><a href="#3-配置-SSH-⚙️" class="headerlink" title="3. 配置 SSH ⚙️"></a>3. 配置 SSH ⚙️</h2><p>创建或编辑 <code>~/.ssh/config</code> 文件，为每个服务配置不同的 SSH 密钥。</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GitHub</span></span><br><span class="line">Host github.com</span><br><span class="line">  HostName github.com</span><br><span class="line">  User git</span><br><span class="line">  IdentityFile ~/.ssh/id_rsa_github</span><br><span class="line"></span><br><span class="line"><span class="comment"># GitLab</span></span><br><span class="line">Host gitlab.com</span><br><span class="line">  HostName gitlab.com</span><br><span class="line">  User git</span><br><span class="line">  IdentityFile ~/.ssh/id_rsa_gitlab</span><br></pre></td></tr></tbody></table></figure><p>如果你的公司使用自定义的 GitLab 实例，请为其添加一个单独的配置块。</p><h2 id="4-测试-SSH-连接-🧪"><a href="#4-测试-SSH-连接-🧪" class="headerlink" title="4. 测试 SSH 连接 🧪"></a>4. 测试 SSH 连接 🧪</h2><p>测试是否能成功通过 SSH 连接到每个服务。</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ssh -T git@github.com</span><br><span class="line">ssh -T git@gitlab.com</span><br></pre></td></tr></tbody></table></figure><h2 id="5-处理常见错误-❗"><a href="#5-处理常见错误-❗" class="headerlink" title="5. 处理常见错误 ❗"></a>5. 处理常见错误 ❗</h2><p>如果遇到错误，如 “Permission denied (publickey)”，请检查以下几点：</p><ul><li>确认 SSH 密钥是否已正确添加到相应的 Git 服务。</li><li>检查 <code>~/.ssh/config</code> 文件是否正确配置。</li><li>使用 <code>ssh-add</code> 命令确保 SSH 密钥已加载到 SSH Agent。</li></ul><h2 id="6-常见问题和解决方案-💡"><a href="#6-常见问题和解决方案-💡" class="headerlink" title="6. 常见问题和解决方案 💡"></a>6. 常见问题和解决方案 💡</h2><ul><li><strong>多个 GitLab 实例</strong>：如果你使用了公司的自定义 GitLab 实例和 GitLab.com，请确保 SSH <code>config</code> 文件中它们的配置是分开的。</li><li><strong>网络问题</strong>：检查是否有网络设置（如代理、VPN）可能影响 SSH 连接。</li></ul><hr><p>通过遵循以上步骤，你应该能够解决大部分与 SSH 密钥相关的问题，特别是在处理多个 Git 服务时。如果有进一步的问题或者特殊情况，欢迎在评论中提出。 🚀✨</p>]]></content>
      
      
      <categories>
          
          <category> Debugging Diaries </category>
          
      </categories>
      
      
        <tags>
            
            <tag> IssueFix </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>理解大型语言模型中Fine-tuning和Further Pretraining的区别</title>
      <link href="/2024/02/19/NLP%20Insights/%E7%90%86%E8%A7%A3%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%ADFine-tuning%E5%92%8CFurther%20Pretraining%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
      <url>/2024/02/19/NLP%20Insights/%E7%90%86%E8%A7%A3%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%ADFine-tuning%E5%92%8CFurther%20Pretraining%E7%9A%84%E5%8C%BA%E5%88%AB/</url>
      
        <content type="html"><![CDATA[<h1 id="理解大型语言模型中-Fine-tuning-和-Further-Pretraining-的区别"><a href="#理解大型语言模型中-Fine-tuning-和-Further-Pretraining-的区别" class="headerlink" title="理解大型语言模型中 Fine-tuning 和 Further Pretraining 的区别"></a>理解大型语言模型中 Fine-tuning 和 Further Pretraining 的区别</h1><p>在自然语言处理（NLP）领域，大型语言模型，如 GPT 和 BERT 的出现，彻底改变了我们处理文本分类、情感分析和问答等任务的方式。在这些模型的应用中，Fine-tuning（微调）和 Further Pretraining（进一步预训练）是两种关键技术。虽然它们看起来相似，但实际上服务于 NLP 流程中的不同需求和场景。</p><h2 id="什么是-Fine-tuning？"><a href="#什么是-Fine-tuning？" class="headerlink" title="什么是 Fine-tuning？"></a>什么是 Fine-tuning？</h2><p>Fine-tuning 是指在特定任务的数据集上进一步训练（或“微调”）一个预训练好的模型的过程。这种方法在数据集相对较小但标注良好的情况下特别有效。</p><h3 id="示例场景：情感分析"><a href="#示例场景：情感分析" class="headerlink" title="示例场景：情感分析"></a>示例场景：情感分析</h3><p>假设你有一组电影评论数据，每条评论都标记了正面或负面情感。你想创建一个模型来预测评论的情感。</p><h4 id="Python-代码示例（使用-PyTorch-和-HuggingFace-的-Transformers）"><a href="#Python-代码示例（使用-PyTorch-和-HuggingFace-的-Transformers）" class="headerlink" title="Python 代码示例（使用 PyTorch 和 HuggingFace 的 Transformers）"></a>Python 代码示例（使用 PyTorch 和 HuggingFace 的 Transformers）</h4><p>This notebook demonstrates the fine-tuning of a BERT model on the IMDB dataset for sentiment analysis. For detailed code implementation, please refer to the following link:<a href="https://colab.research.google.com/drive/15naxP8pNMoCCBMgMSOv4ETDRGL46YR38?usp=sharing">link</a>.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> (</span><br><span class="line">    BertTokenizer,</span><br><span class="line">    BertForSequenceClassification,</span><br><span class="line">    Trainer,</span><br><span class="line">    TrainingArguments,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset, DatasetDict</span><br><span class="line"></span><br><span class="line"><span class="string">"""## 1.加载和准备IMDB数据集样本</span></span><br><span class="line"><span class="string">选取一部分数据用于Fine-tuning。</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载IMDB数据集</span></span><br><span class="line">dataset = load_dataset(<span class="string">'imdb'</span>, split=<span class="string">'train'</span>)</span><br><span class="line">small_dataset = dataset.shuffle(seed=<span class="number">42</span>).select(<span class="built_in">range</span>(<span class="number">10000</span>))  <span class="comment"># 选取前10000个样本</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化tokenizer</span></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">"bert-base-uncased"</span>)</span><br><span class="line"></span><br><span class="line">device = <span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 编码数据集</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">examples</span>):</span><br><span class="line">    <span class="keyword">return</span> tokenizer(examples[<span class="string">"text"</span>], padding=<span class="string">"max_length"</span>, truncation=<span class="literal">True</span>, max_length=<span class="number">512</span>)</span><br><span class="line"></span><br><span class="line">encoded_small_dataset = small_dataset.<span class="built_in">map</span>(encode, batched=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertModel</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">visualize_attention</span>(<span class="params">sentence, model, tokenizer</span>):</span><br><span class="line">    model.to(device)</span><br><span class="line">    <span class="comment"># 将模型设置为评估模式</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将输入文本转换为模型可以理解的形式</span></span><br><span class="line">    inputs = tokenizer(sentence, return_tensors=<span class="string">"pt"</span>).to(device) <span class="comment"># 确保输入也在相同设备</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用模型获取注意力权重</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        outputs = model(**inputs)</span><br><span class="line">    attentions = outputs.attentions</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 选择要可视化的层和头</span></span><br><span class="line">    layer = <span class="number">5</span></span><br><span class="line">    head = <span class="number">1</span></span><br><span class="line">    attention = attentions[layer][<span class="number">0</span>, head].cpu().numpy() <span class="comment"># 将注意力权重移回CPU进行可视化</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设置可视化的tokens</span></span><br><span class="line">    tokens = tokenizer.convert_ids_to_tokens(inputs[<span class="string">"input_ids"</span>][<span class="number">0</span>].cpu()) <span class="comment"># 同样确保tokens在CPU上</span></span><br><span class="line">    <span class="comment"># 绘制注意力矩阵</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">10</span>, <span class="number">10</span>))</span><br><span class="line">    plt.matshow(attention, cmap=<span class="string">'viridis'</span>)</span><br><span class="line">    plt.xticks(<span class="built_in">range</span>(<span class="built_in">len</span>(tokens)), tokens, rotation=<span class="number">90</span>)</span><br><span class="line">    plt.yticks(<span class="built_in">range</span>(<span class="built_in">len</span>(tokens)), tokens)</span><br><span class="line">    plt.colorbar()</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="string">"""## 2. 可视化一个样本句子的注意力权重（未经Fine-tuning）</span></span><br><span class="line"><span class="string">选择数据集中的一个句子并展示其原始BERT模型的注意力权重。</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用未经Fine-tuning的模型</span></span><br><span class="line">model = BertModel.from_pretrained(<span class="string">"bert-base-uncased"</span>, output_attentions=<span class="literal">True</span>)</span><br><span class="line">sample_sentence = <span class="string">"I love this movie, it's fantastic!"</span></span><br><span class="line">visualize_attention(sample_sentence, model, tokenizer)</span><br><span class="line"></span><br><span class="line"><span class="string">"""## 3. Fine-tuning BERT模型</span></span><br><span class="line"><span class="string">在选取的IMDB样本上进行Fine-tuning。</span></span><br><span class="line"><span class="string">### 3.1 准备数据加载器</span></span><br><span class="line"><span class="string">为了训练模型，我们需要创建PyTorch的DataLoader。这将使我们能够在训练过程中有效地加载数据。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">### 3.2 设置Fine-tuning环境</span></span><br><span class="line"><span class="string">初始化模型、优化器以及损失函数。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">### 3.3 Fine-tuning模型</span></span><br><span class="line"><span class="string">执行Fine-tuning的训练循环。执行以上代码将在IMDB数据集的小样本上对BERT模型进行Fine-tuning。这可能需要一些时间，具体取决于您的硬件配置。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据集转换为PyTorch Tensor</span></span><br><span class="line">encoded_small_dataset.set_format(<span class="string">'torch'</span>, columns=[<span class="string">'input_ids'</span>, <span class="string">'attention_mask'</span>, <span class="string">'label'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建数据加载器</span></span><br><span class="line">train_loader = DataLoader(encoded_small_dataset, batch_size=<span class="number">8</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertConfig, BertForSequenceClassification</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载配置并设置输出注意力权重</span></span><br><span class="line">config = BertConfig.from_pretrained(<span class="string">"bert-base-uncased"</span>, output_attentions=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化用于序列分类的BERT模型</span></span><br><span class="line"><span class="comment"># 使用更新后的配置加载模型</span></span><br><span class="line">model = BertForSequenceClassification.from_pretrained(<span class="string">"bert-base-uncased"</span>, config=config)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置优化器</span></span><br><span class="line">optimizer = optim.AdamW(model.parameters(), lr=<span class="number">5e-5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用交叉熵损失函数</span></span><br><span class="line">criterion = torch.nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置训练的轮次</span></span><br><span class="line">epochs = <span class="number">8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    model.train()</span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> train_loader:</span><br><span class="line">        <span class="comment"># 将数据移至GPU</span></span><br><span class="line">        input_ids = batch[<span class="string">'input_ids'</span>].to(device)</span><br><span class="line">        attention_mask = batch[<span class="string">'attention_mask'</span>].to(device)</span><br><span class="line">        labels = batch[<span class="string">'label'</span>].to(device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 模型前向传播</span></span><br><span class="line">        outputs = model(input_ids, attention_mask=attention_mask)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算损失</span></span><br><span class="line">        loss = criterion(outputs.logits, labels)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 反向传播和优化</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        total_loss += loss.item()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"Epoch: <span class="subst">{epoch+<span class="number">1</span>}</span>, Loss: <span class="subst">{total_loss/<span class="built_in">len</span>(train_loader)}</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">"""### 4. 可视化同一句子的注意力权重（经过Fine-tuning）</span></span><br><span class="line"><span class="string">使用Fine-tuning后的模型再次可视化同一句子的注意力权重。您可以重用之前提供的visualize_attention函数：</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">visualize_attention(sample_sentence, model, tokenizer)</span><br></pre></td></tr></tbody></table></figure><p>在这个例子中，BERT 模型在电影评论数据集上进行了 fine-tuning，用于情感分析。</p><h2 id="什么是-Further-Pretraining？"><a href="#什么是-Further-Pretraining？" class="headerlink" title="什么是 Further Pretraining？"></a>什么是 Further Pretraining？</h2><p>Further Pretraining（也称为 Domain-adaptive Pretraining，领域适应性预训练）是在一个新的数据集上继续训练一个预训练模型的过程，这个新的数据集与特定的领域更相关，但不一定为特定任务标注。</p><h3 id="示例场景：法律文档分析"><a href="#示例场景：法律文档分析" class="headerlink" title="示例场景：法律文档分析"></a>示例场景：法律文档分析</h3><p>假设你正在处理法律文档，并希望利用一个在通用文本上训练的语言模型。</p><h4 id="Further-Pretraining-的代码示例"><a href="#Further-Pretraining-的代码示例" class="headerlink" title="Further Pretraining 的代码示例"></a>Further Pretraining 的代码示例</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertModel, BertTokenizer</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载预训练的BERT模型和分词器</span></span><br><span class="line">model = BertModel.from_pretrained(<span class="string">'bert-base-uncased'</span>)</span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">'bert-base-uncased'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备法律文档数据集</span></span><br><span class="line"><span class="comment"># 假设'legal_documents'是法律文档的文本列表</span></span><br><span class="line">encoded_input = tokenizer(legal_documents, padding=<span class="literal">True</span>, truncation=<span class="literal">True</span>, max_length=<span class="number">512</span>, return_tensors=<span class="string">'pt'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 继续预训练模型</span></span><br><span class="line"><span class="comment"># 这一步通常包括掩码语言建模或其他预训练目标</span></span><br><span class="line"><span class="comment"># 这里提供一个概念性示例</span></span><br><span class="line">model.train()</span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> encoded_input:</span><br><span class="line">    outputs = model(**batch)</span><br><span class="line">    <span class="comment"># ... 执行进一步训练步骤</span></span><br></pre></td></tr></tbody></table></figure><p>在这个例子中，BERT 模型在法律文档数据集上进行了进一步的预训练，使其在进行特定法律 NLP 任务的 fine-tuning 之前，更擅长理解法律术语和概念。</p><h2 id="关键区别"><a href="#关键区别" class="headerlink" title="关键区别"></a>关键区别</h2><ul><li><strong>目的</strong>：Fine-tuning 针对具有标签数据的特定任务进行模型调整，而 Further Pretraining 则是使模型更好地适应特定领域或语言风格。</li><li><strong>数据集</strong>：Fine-tuning 使用特定任务的标注数据集。Further Pretraining 使用更大的、特定领域的数据集，这些数据集可能不是为特定任务标注的。</li><li><strong>训练目标</strong>：Fine-tuning 涉及调整模型进行特定预测，而 Further Pretraining 侧重于在新领域中的通用语言理解</li></ul><p>。</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>Fine-tuning 和 Further Pretraining 都是 NLP 领域的强大技术。通过理解它们的区别和应用，我们可以更好地利用大型语言模型来解决各种领域中的多样化和复杂任务。无论你是在构建社交媒体帖子的情感分析模型，还是调整模型以理解法律文档，这些技术都为 NLP 领域的不断发展提供了稳健的解决方案。</p><hr><p><strong>注意</strong>：提供的代码示例是概念性的，需要适当的环境设置，包括必要的库和数据集，才能执行。</p>]]></content>
      
      
      <categories>
          
          <category> NLP Insights </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Understanding the Differences Between Fine-tuning and Further Pretraining in Large Language Models</title>
      <link href="/2024/02/19/NLP%20Insights/Understanding%20the%20Differences%20Between%20Fine-tuning%20and%20Further%20Pretraining%20in%20Large%20Language%20Models/"/>
      <url>/2024/02/19/NLP%20Insights/Understanding%20the%20Differences%20Between%20Fine-tuning%20and%20Further%20Pretraining%20in%20Large%20Language%20Models/</url>
      
        <content type="html"><![CDATA[<h1 id="Understanding-the-Differences-Between-Fine-tuning-and-Further-Pretraining-in-Large-Language-Models"><a href="#Understanding-the-Differences-Between-Fine-tuning-and-Further-Pretraining-in-Large-Language-Models" class="headerlink" title="Understanding the Differences Between Fine-tuning and Further Pretraining in Large Language Models"></a>Understanding the Differences Between Fine-tuning and Further Pretraining in Large Language Models</h1><p>In the world of Natural Language Processing (NLP), the advent of large language models like GPT and BERT has revolutionized how we approach tasks such as text classification, sentiment analysis, and question-answering. Two pivotal techniques in leveraging these models are Fine-tuning and Further Pretraining. While they may seem similar at a glance, they cater to different needs and scenarios in the NLP pipeline.</p><h2 id="What-is-Fine-tuning"><a href="#What-is-Fine-tuning" class="headerlink" title="What is Fine-tuning?"></a>What is Fine-tuning?</h2><p>Fine-tuning is a process where a pretrained model is further trained (or ‘fine-tuned’) on a specific task with a dataset corresponding to that task. This approach is particularly effective when the dataset is relatively small but well-labeled.</p><h3 id="Example-Scenario-Sentiment-Analysis"><a href="#Example-Scenario-Sentiment-Analysis" class="headerlink" title="Example Scenario: Sentiment Analysis"></a>Example Scenario: Sentiment Analysis</h3><p>Imagine you have a dataset of movie reviews, each labeled as positive or negative. You want to create a model that can predict the sentiment of a review.</p><h4 id="Code-Snippet-in-Python-using-PyTorch-and-HuggingFace’s-Transformers"><a href="#Code-Snippet-in-Python-using-PyTorch-and-HuggingFace’s-Transformers" class="headerlink" title="Code Snippet in Python (using PyTorch and HuggingFace’s Transformers)"></a>Code Snippet in Python (using PyTorch and HuggingFace’s Transformers)</h4><p>This notebook demonstrates the fine-tuning of a BERT model on the IMDB dataset for sentiment analysis. For detailed code implementation, please refer to the following link:<a href="https://colab.research.google.com/drive/15naxP8pNMoCCBMgMSOv4ETDRGL46YR38?usp=sharing">link</a>.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br></pre></td><td class="code"><pre><span class="line">ls -al ~/.ssh</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> (</span><br><span class="line">    BertTokenizer,</span><br><span class="line">    BertForSequenceClassification,</span><br><span class="line">    Trainer,</span><br><span class="line">    TrainingArguments,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset, DatasetDict</span><br><span class="line"></span><br><span class="line"><span class="string">"""## 1. Load and Prepare IMDB Dataset Samples</span></span><br><span class="line"><span class="string">Select a portion of the data for Fine-tuning.</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Load IMDB dataset</span></span><br><span class="line">dataset = load_dataset(<span class="string">'imdb'</span>, split=<span class="string">'train'</span>)</span><br><span class="line">small_dataset = dataset.shuffle(seed=<span class="number">42</span>).select(<span class="built_in">range</span>(<span class="number">10000</span>))  <span class="comment"># Selecting the first 10,000 samples</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize tokenizer</span></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">"bert-base-uncased"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Encode the dataset</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">examples</span>):</span><br><span class="line">    <span class="keyword">return</span> tokenizer(examples[<span class="string">"text"</span>], padding=<span class="string">"max_length"</span>, truncation=<span class="literal">True</span>, max_length=<span class="number">512</span>)</span><br><span class="line"></span><br><span class="line">encoded_small_dataset = small_dataset.<span class="built_in">map</span>(encode, batched=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertModel</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">visualize_attention</span>(<span class="params">sentence, model, tokenizer</span>):</span><br><span class="line">    <span class="comment"># Set the model to evaluation mode</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Convert the input text into a format understandable by the model</span></span><br><span class="line">    inputs = tokenizer(sentence, return_tensors=<span class="string">"pt"</span>).to(device) <span class="comment"># Making sure inputs are on the same device</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Get attention weights using the model</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        outputs = model(**inputs)</span><br><span class="line">    attentions = outputs.attentions</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Choose the layer and head to visualize</span></span><br><span class="line">    layer = <span class="number">5</span></span><br><span class="line">    head = <span class="number">1</span></span><br><span class="line">    attention = attentions[layer][<span class="number">0</span>, head].cpu().numpy() <span class="comment"># Move attention weights back to CPU for visualization</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Set tokens for visualization</span></span><br><span class="line">    tokens = tokenizer.convert_ids_to_tokens(inputs[<span class="string">"input_ids"</span>][<span class="number">0</span>].cpu()) <span class="comment"># Also make sure tokens are on CPU</span></span><br><span class="line">    <span class="comment"># Plot attention matrix</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">10</span>, <span class="number">10</span>))</span><br><span class="line">    plt.matshow(attention, cmap=<span class="string">'viridis'</span>)</span><br><span class="line">    plt.xticks(<span class="built_in">range</span>(<span class="built_in">len</span>(tokens)), tokens, rotation=<span class="number">90</span>)</span><br><span class="line">    plt.yticks(<span class="built_in">range</span>(<span class="built_in">len</span>(tokens)), tokens)</span><br><span class="line">    plt.colorbar()</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="string">"""## 2. Visualize Attention Weights of a Sample Sentence (Before Fine-tuning)</span></span><br><span class="line"><span class="string">Select a sentence from the dataset and visualize the attention weights of the original BERT model.</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Use the model without Fine-tuning</span></span><br><span class="line">model = BertModel.from_pretrained(<span class="string">"bert-base-uncased"</span>, output_attentions=<span class="literal">True</span>)</span><br><span class="line">sample_sentence = <span class="string">"I love this movie, it's fantastic!"</span></span><br><span class="line">visualize_attention(sample_sentence, model, tokenizer)</span><br><span class="line"></span><br><span class="line"><span class="string">"""## 3. Fine-tuning the BERT Model</span></span><br><span class="line"><span class="string">Perform Fine-tuning on the selected IMDB samples.</span></span><br><span class="line"><span class="string">### 3.1 Prepare Data Loaders</span></span><br><span class="line"><span class="string">To train the model, we need to create PyTorch's DataLoader. This will allow us to efficiently load data during training.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">### 3.2 Set up Fine-tuning Environment</span></span><br><span class="line"><span class="string">Initialize the model, optimizer, and loss function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">### 3.3 Fine-tuning the Model</span></span><br><span class="line"><span class="string">Execute the training loop for Fine-tuning. Running the above code will Fine-tune the BERT model on a small sample of the IMDB dataset. This may take some time depending on your hardware configuration.</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert the dataset to PyTorch Tensor</span></span><br><span class="line">encoded_small_dataset.set_format(<span class="string">'torch'</span>, columns=[<span class="string">'input_ids'</span>, <span class="string">'attention_mask'</span>, <span class="string">'label'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create data loader</span></span><br><span class="line">train_loader = DataLoader(encoded_small_dataset, batch_size=<span class="number">8</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertConfig, BertForSequenceClassification</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load the configuration and set output attention weights</span></span><br><span class="line">config = BertConfig.from_pretrained(<span class="string">"bert-base-uncased"</span>, output_attentions=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize the BERT model for sequence classification</span></span><br><span class="line"><span class="comment"># Load the model with updated configuration</span></span><br><span class="line">model = BertForSequenceClassification.from_pretrained(<span class="string">"bert-base-uncased"</span>, config=config)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Set up the optimizer</span></span><br><span class="line">optimizer = optim.AdamW(model.parameters(), lr=<span class="number">5e-5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use the cross-entropy loss function</span></span><br><span class="line">criterion = torch.nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">device = <span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span></span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the number of epochs for training</span></span><br><span class="line">epochs = <span class="number">8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    model.train()</span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> train_loader:</span><br><span class="line">        <span class="comment"># Move the data to GPU</span></span><br><span class="line">        input_ids = batch[<span class="string">'input_ids'</span>].to(device)</span><br><span class="line">        attention_mask = batch[<span class="string">'attention_mask'</span>].to(device)</span><br><span class="line">        labels = batch[<span class="string">'label'</span>].to(device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Model forward pass</span></span><br><span class="line">        outputs = model(input_ids, attention_mask=attention_mask)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute the loss</span></span><br><span class="line">        loss = criterion(outputs.logits, labels)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Backpropagation and optimization</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        total_loss += loss.item()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"Epoch: <span class="subst">{epoch+<span class="number">1</span>}</span>, Loss: <span class="subst">{total_loss/<span class="built_in">len</span>(train_loader)}</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">"""### 4. Visualize Attention Weights of the Same Sentence (After Fine-tuning)</span></span><br><span class="line"><span class="string">Visualize the attention weights of the same sentence using the model after Fine-tuning. You can reuse the visualize_attention function provided earlier:</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">visualize_attention(sample_sentence, model, tokenizer)</span><br></pre></td></tr></tbody></table></figure><p>In this example, the BERT model is fine-tuned on the movie reviews dataset for sentiment analysis.</p><h2 id="What-is-Further-Pretraining"><a href="#What-is-Further-Pretraining" class="headerlink" title="What is Further Pretraining?"></a>What is Further Pretraining?</h2><p>Further Pretraining, also known as Domain-adaptive Pretraining, is where a pretrained model is further trained on a new dataset that is more closely related to the specific domain of interest but not necessarily labeled for a specific task.</p><h3 id="Example-Scenario-Legal-Document-Analysis"><a href="#Example-Scenario-Legal-Document-Analysis" class="headerlink" title="Example Scenario: Legal Document Analysis"></a>Example Scenario: Legal Document Analysis</h3><p>Suppose you’re working on legal documents and wish to leverage a language model trained on general texts.</p><h4 id="Code-Snippet-for-Further-Pretraining"><a href="#Code-Snippet-for-Further-Pretraining" class="headerlink" title="Code Snippet for Further Pretraining"></a>Code Snippet for Further Pretraining</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertModel, BertTokenizer</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load a pre-trained BERT model and tokenizer</span></span><br><span class="line">model = BertModel.from_pretrained(<span class="string">'bert-base-uncased'</span>)</span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">'bert-base-uncased'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Prepare the legal documents dataset</span></span><br><span class="line"><span class="comment"># Assume 'legal_documents' is a list of text from legal documents</span></span><br><span class="line">encoded_input = tokenizer(legal_documents, padding=<span class="literal">True</span>, truncation=<span class="literal">True</span>, max_length=<span class="number">512</span>, return_tensors=<span class="string">'pt'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Further pretrain the model</span></span><br><span class="line"><span class="comment"># This step typically involves masked language modeling or other pretraining objectives</span></span><br><span class="line"><span class="comment"># Here we provide a conceptual example</span></span><br><span class="line">model.train()</span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> encoded_input:</span><br><span class="line">    outputs = model(**batch)</span><br><span class="line">    <span class="comment"># ... Perform further training steps</span></span><br></pre></td></tr></tbody></table></figure><p>In this case, the BERT model is further pretrained on a legal document dataset, making it more adept at understanding legal jargon and concepts before being fine-tuned on a specific legal NLP task.</p><h2 id="Key-Differences"><a href="#Key-Differences" class="headerlink" title="Key Differences"></a>Key Differences</h2><ul><li><strong>Purpose</strong>: Fine-tuning is tailored for a specific task with labeled data, while Further Pretraining is about adapting the model to a specific domain or style of language.</li><li><strong>Dataset</strong>: Fine-tuning uses task-specific, labeled datasets. Further Pretraining uses larger, domain-specific datasets, which may not be labeled for a specific task.</li><li><strong>Training Objective</strong>: Fine-tuning involves adjusting the model to make specific predictions, while Further Pretraining focuses on general language understanding in a new domain.</li></ul><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Both Fine-tuning and Further Pretraining are powerful techniques in NLP. By understanding their differences and applications, we can better leverage large language models to solve diverse and complex tasks in various domains. Whether you’re building a sentiment analysis model for social media posts or adapting a model to understand legal documents, these techniques offer robust solutions in the ever-evolving field of NLP.</p><hr><p><strong>Note</strong>: The code examples provided are conceptual and require a suitable environment setup, including necessary libraries and datasets, for execution.</p>]]></content>
      
      
      <categories>
          
          <category> NLP Insights </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>🤖 Create a Telegram Bot with Python and OpenAI in 10 Minutes! 🚀</title>
      <link href="/2023/09/24/NLP%20Insights/Create%20a%20Telegram%20Bot%20with%20Python%20and%20OpenAI%20in%2010%20Minutes/"/>
      <url>/2023/09/24/NLP%20Insights/Create%20a%20Telegram%20Bot%20with%20Python%20and%20OpenAI%20in%2010%20Minutes/</url>
      
        <content type="html"><![CDATA[<h1 id="🤖-Create-a-Telegram-Bot-with-Python-and-OpenAI-in-10-Minutes-🚀"><a href="#🤖-Create-a-Telegram-Bot-with-Python-and-OpenAI-in-10-Minutes-🚀" class="headerlink" title="🤖 Create a Telegram Bot with Python and OpenAI in 10 Minutes! 🚀"></a>🤖 Create a Telegram Bot with Python and OpenAI in 10 Minutes! 🚀</h1><p>In this fun tutorial, we’ll show you how to create a Telegram bot that can chat with users and generate witty responses. We’ll explain each step in detail, making it easy for you to get started!</p><h2 id="Step-1-Create-a-Telegram-Bot-🤖"><a href="#Step-1-Create-a-Telegram-Bot-🤖" class="headerlink" title="Step 1: Create a Telegram Bot 🤖"></a>Step 1: Create a Telegram Bot 🤖</h2><p>First, let’s create your very own Telegram bot. Here’s how:</p><ol><li>Open the Telegram app and search for “BotFather.”</li><li>In the BotFather chat, use the <code>/newbot</code> command to create a new bot. You’ll need to give your bot a name, like “PunshineBot.”</li><li>BotFather will generate a unique API token for you. Be sure to save this token; we’ll use it in the code later.</li></ol><img src="/2023/09/24/NLP%20Insights/Create%20a%20Telegram%20Bot%20with%20Python%20and%20OpenAI%20in%2010%20Minutes/step1.jpg" class=""><h2 id="Step-2-Import-Necessary-Libraries-📚"><a href="#Step-2-Import-Necessary-Libraries-📚" class="headerlink" title="Step 2: Import Necessary Libraries 📚"></a>Step 2: Import Necessary Libraries 📚</h2><p>We’ll first need to import some Python libraries to create the Telegram bot and perform natural language processing with OpenAI.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> Final, Deque, <span class="type">Dict</span>, <span class="type">Union</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> deque</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> openai</span><br><span class="line"><span class="keyword">from</span> telegram <span class="keyword">import</span> Update</span><br><span class="line"><span class="keyword">from</span> telegram.ext <span class="keyword">import</span> (</span><br><span class="line">    Application,</span><br><span class="line">    CommandHandler,</span><br><span class="line">    ContextTypes,</span><br><span class="line">    MessageHandler,</span><br><span class="line">    filters,</span><br><span class="line">)</span><br></pre></td></tr></tbody></table></figure><p>Make sure you have the Telegram Bot API and OpenAI Python library installed.</p><h2 id="Step-3-Set-API-Keys-and-Telegram-Token-🔑"><a href="#Step-3-Set-API-Keys-and-Telegram-Token-🔑" class="headerlink" title="Step 3: Set API Keys and Telegram Token 🔑"></a>Step 3: Set API Keys and Telegram Token 🔑</h2><p>In this step, we need to set the OpenAI API key and Telegram bot token. Make sure you’ve signed up for OpenAI and obtained your API key. Then, replace the example API key and Telegram token in the following code with your own:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">openai.api_key = <span class="string">"Your OpenAI API Key"</span></span><br><span class="line">TOKEN: Final = <span class="string">"Your Telegram Bot Token"</span></span><br></pre></td></tr></tbody></table></figure><h2 id="Step-4-Create-Start-and-Help-Commands-🚀"><a href="#Step-4-Create-Start-and-Help-Commands-🚀" class="headerlink" title="Step 4: Create Start and Help Commands 🚀"></a>Step 4: Create Start and Help Commands 🚀</h2><p>Next, we’ll create two command handling functions for the start and help commands. These commands allow users to interact with the bot.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">start_command</span>(<span class="params">update: Update, context: ContextTypes.DEFAULT_TYPE</span>):</span><br><span class="line">    <span class="keyword">await</span> update.message.reply_text(<span class="string">"Hello, world! 😄"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">help_command</span>(<span class="params">update: Update, context: ContextTypes.DEFAULT_TYPE</span>):</span><br><span class="line">    <span class="keyword">await</span> update.message.reply_text(<span class="string">"I'm PunShine Bot, and I can help you find the funniest puns! 😂"</span>)</span><br></pre></td></tr></tbody></table></figure><p>The <code>start_command</code> function handles the start command, responding with “Hello, world! 😄” when users send the <code>/start</code> command. The <code>help_command</code> function handles the help command, providing assistance when users send the <code>/help</code> command.</p><h2 id="Step-5-Handle-User-Messages-📨"><a href="#Step-5-Handle-User-Messages-📨" class="headerlink" title="Step 5: Handle User Messages 📨"></a>Step 5: Handle User Messages 📨</h2><p>In this step, we define the <code>handle_response</code> function to process user messages and create the <code>message_handler</code>. The function adds user messages to the conversation history and uses the OpenAI API to generate responses.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">handle_response</span>(<span class="params">chat_id: <span class="type">Union</span>[<span class="built_in">int</span>, <span class="built_in">str</span>], text: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">    <span class="comment"># Get the chat history</span></span><br><span class="line">    chat_history = get_chat_history(chat_id)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create a user message</span></span><br><span class="line">    user_message = {<span class="string">"role"</span>: <span class="string">"user"</span>, <span class="string">"content"</span>: text}</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Add the user message to the chat history</span></span><br><span class="line">    chat_history.append(user_message)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Build a message list, including a system message and user messages</span></span><br><span class="line">    messages = [</span><br><span class="line">        {</span><br><span class="line">            <span class="string">"role"</span>: <span class="string">"system"</span>,</span><br><span class="line">            <span class="string">"content"</span>: <span class="string">"You are PunshineBot, reply in English, keep it casual, use emojis, and keep responses short. 😄🚀"</span>,</span><br><span class="line">        }</span><br><span class="line">    ]</span><br><span class="line">    messages.extend(chat_history)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Use the OpenAI API to generate the bot's response</span></span><br><span class="line">    response = openai.ChatCompletion.create(</span><br><span class="line">        model=<span class="string">"gpt-3.5-turbo"</span>,</span><br><span class="line">        messages=messages,</span><br><span class="line">        temperature=<span class="number">1</span>,</span><br><span class="line">        max_tokens=<span class="number">256</span>,</span><br><span class="line">        top_p=<span class="number">1</span>,</span><br><span class="line">        frequency_penalty=<span class="number">0</span>,</span><br><span class="line">        presence_penalty=<span class="number">0</span>,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    bot_response = response[<span class="string">"choices"</span>][<span class="number">0</span>][<span class="string">"message"</span>][<span class="string">"content"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Add the bot's response to the conversation history</span></span><br><span class="line">    chat_history.append({<span class="string">"role"</span>: <span class="string">"assistant"</span>, <span class="string">"content"</span>: bot_response})</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> bot_response</span><br></pre></td></tr></tbody></table></figure><p>The <code>handle_response</code> function adds user messages to the conversation history and generates the bot’s response using the OpenAI API. The <code>message_handler</code> function processes messages sent by users, checking their type (group message or private chat) and content, and then calls the <code>handle_response</code> function to generate the bot’s response.</p><h2 id="Step-6-Run-the-Telegram-Bot-🚀"><a href="#Step-6-Run-the-Telegram-Bot-🚀" class="headerlink" title="Step 6: Run the Telegram Bot 🚀"></a>Step 6: Run the Telegram Bot 🚀</h2><p>Finally, in the main function, we create a Telegram bot application, add command handlers and message handlers, and start the Telegram bot to receive and process messages.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    app = Application.builder().token(TOKEN).build()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Add command handlers</span></span><br><span class="line">    app.add_handler(CommandHandler(<span class="string">"start"</span>, start_command))</span><br><span class="line">    app.add_handler(CommandHandler(<span class="string">"help"</span>, help_command))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Add message handler</span></span><br><span class="line">    app.add_handler(MessageHandler(filters.TEXT, callback=message_handler))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Start the Telegram bot</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"Bot started! 😎🤖"</span>)</span><br><span class="line">    app.run_polling(poll_interval=<span class="number">1</span>)</span><br></pre></td></tr></tbody></table></figure><p><code>app.run_polling(poll_interval=1)</code> starts the Telegram bot, polling for new messages every second. This keeps the bot running and triggers the respective handlers when it receives messages.</p><img src="/2023/09/24/NLP%20Insights/Create%20a%20Telegram%20Bot%20with%20Python%20and%20OpenAI%20in%2010%20Minutes/step6_en.jpg" class=""><h2 id="Step-7-Host-Your-Telegram-Bot-Online-for-Free-24-7-🚀"><a href="#Step-7-Host-Your-Telegram-Bot-Online-for-Free-24-7-🚀" class="headerlink" title="Step 7: Host Your Telegram Bot Online for Free 24/7 🚀"></a>Step 7: Host Your Telegram Bot Online for Free 24/7 🚀</h2><p>In the previous tutorial, we showed you how to create a Telegram bot. However, to keep your bot online 24/7, you would need to leave your computer running, which can be inconvenient. In this blog post, we’ll show you how to host your bot for free on a cloud server so that it can run around the clock.</p><p>PythonAnywhere is a website that allows you to host Python code for free. It comes with some usage limitations, but it’s perfect for simple use cases. Here’s how to use PythonAnywhere to host your Telegram bot.</p><ol><li>Visit the <a href="https://www.pythonanywhere.com/">PythonAnywhere website</a> and create an account.</li></ol><img src="/2023/09/24/NLP%20Insights/Create%20a%20Telegram%20Bot%20with%20Python%20and%20OpenAI%20in%2010%20Minutes/step7.jpg" class=""><h3 id="Step-7-1-Create-a-Python-Script"><a href="#Step-7-1-Create-a-Python-Script" class="headerlink" title="Step 7.1: Create a Python Script"></a>Step 7.1: Create a Python Script</h3><p>Once you’re logged into the PythonAnywhere dashboard, you’ll see an option called “Files.” Click on it and create a new file. Name it “telegram_bot.py.” In this file, paste the Telegram bot code you created earlier.</p><h3 id="Step-7-2-Install-Required-Packages"><a href="#Step-7-2-Install-Required-Packages" class="headerlink" title="Step 7.2: Install Required Packages"></a>Step 7.2: Install Required Packages</h3><p>Before running the Telegram bot, we need to install the necessary Python packages. PythonAnywhere provides a command-line interface where you can execute the following commands:</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install python-telegram-bot</span><br><span class="line">pip install openai</span><br></pre></td></tr></tbody></table></figure><p>This will install the Telegram bot library, allowing your code to communicate with the Telegram servers.</p><h3 id="Step-7-3-Run-the-Bot"><a href="#Step-7-3-Run-the-Bot" class="headerlink" title="Step 7.3: Run the Bot"></a>Step 7.3: Run the Bot</h3><p>Now, go back to the PythonAnywhere dashboard and find your “telegram_bot.py” file under “Files.” Click the “Run” button, and your bot will start running. The bot will stay online even if you close your computer.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>You’ve now learned how to create a Python Telegram bot, integrate it with OpenAI to respond to user messages, and host it for free on a cloud server, ensuring it remains online 24/7. This way, you can interact with your bot without worrying about whether your computer is on. If you have other free hosting services or questions, feel free to share in the comments! Happy bot building! 🤖🚀😄</p><p>Additionally, if you want to learn more about hosting bots on Discord, you can watch this YouTube video: <a href="https://www.youtube.com/watch?v=2TI-tCVhe9k">How To Host Your Bot Online 24/7 For FREE With Python (Telegram, Discord, Etc)</a>. The video provides more detailed tutorials and examples.</p><p>If you have any questions or need further assistance, please don’t hesitate to leave a comment. Happy coding! 🚀🤖😄</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://www.youtube.com/watch?v=vZtm1wuA2yc">How To Create A Telegram Bot In Python For Beginners (2023 Tutorial)</a></li><li><a href="https://www.youtube.com/watch?v=2TI-tCVhe9k">How To Host Your Bot Online 24/7 For FREE With Python (Telegram, Discord, Etc)</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Code Chronicles </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Chatbot </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>🤖 十分钟用 Python 和 OpenAI 创建 Telegram 机器人！ 🚀</title>
      <link href="/2023/09/24/NLP%20Insights/%E5%8D%81%E5%88%86%E9%92%9F%E7%94%A8%20Python%20%E5%92%8C%20OpenAI%20%E5%88%9B%E5%BB%BA%20Telegram%20%E6%9C%BA%E5%99%A8%E4%BA%BA/"/>
      <url>/2023/09/24/NLP%20Insights/%E5%8D%81%E5%88%86%E9%92%9F%E7%94%A8%20Python%20%E5%92%8C%20OpenAI%20%E5%88%9B%E5%BB%BA%20Telegram%20%E6%9C%BA%E5%99%A8%E4%BA%BA/</url>
      
        <content type="html"><![CDATA[<h1 id="🤖-十分钟用-Python-和-OpenAI-创建-Telegram-机器人！-🚀"><a href="#🤖-十分钟用-Python-和-OpenAI-创建-Telegram-机器人！-🚀" class="headerlink" title="🤖 十分钟用 Python 和 OpenAI 创建 Telegram 机器人！ 🚀"></a>🤖 十分钟用 Python 和 OpenAI 创建 Telegram 机器人！ 🚀</h1><p>在这个有趣的教程中，我们将向您展示如何创建一个具有的 Telegram 机器人，该机器人能够与用户聊天并生成幽默回复。我们将详细解释每一步，让您轻松入门！</p><h2 id="步骤-1：创建-Telegram-机器人-🤖"><a href="#步骤-1：创建-Telegram-机器人-🤖" class="headerlink" title="步骤 1：创建 Telegram 机器人 🤖"></a>步骤 1：创建 Telegram 机器人 🤖</h2><p>首先，让我们来创建您自己的 Telegram 机器人。这是如何做的：</p><ol><li>打开 Telegram 应用并搜索 “BotFather”。</li><li>在 BotFather 聊天中，使用 <code>/newbot</code> 命令创建一个新机器人。您需要为机器人取个名字，比如 “PunshineBot”。</li><li>BotFather 会为您生成一个独一无二的 API 令牌（Token）。一定要妥善保存这个令牌，稍后我们会在代码中用到它。</li></ol><img src="/2023/09/24/NLP%20Insights/%E5%8D%81%E5%88%86%E9%92%9F%E7%94%A8%20Python%20%E5%92%8C%20OpenAI%20%E5%88%9B%E5%BB%BA%20Telegram%20%E6%9C%BA%E5%99%A8%E4%BA%BA/step1.jpg" class=""><h2 id="步骤-2：导入必要的库-📚"><a href="#步骤-2：导入必要的库-📚" class="headerlink" title="步骤 2：导入必要的库 📚"></a>步骤 2：导入必要的库 📚</h2><p>我们首先需要导入一些 Python 库，以便创建 Telegram 机器人并与 OpenAI 进行自然语言处理。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> Final, Deque, <span class="type">Dict</span>, <span class="type">Union</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> deque</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> openai</span><br><span class="line"><span class="keyword">from</span> telegram <span class="keyword">import</span> Update</span><br><span class="line"><span class="keyword">from</span> telegram.ext <span class="keyword">import</span> (</span><br><span class="line">    Application,</span><br><span class="line">    CommandHandler,</span><br><span class="line">    ContextTypes,</span><br><span class="line">    MessageHandler,</span><br><span class="line">    filters,</span><br><span class="line">)</span><br></pre></td></tr></tbody></table></figure><p>请确保您已经安装了 Telegram Bot API 和 OpenAI Python 库。</p><h2 id="步骤-3：设置-API-密钥和-Telegram-令牌-🔑"><a href="#步骤-3：设置-API-密钥和-Telegram-令牌-🔑" class="headerlink" title="步骤 3：设置 API 密钥和 Telegram 令牌 🔑"></a>步骤 3：设置 API 密钥和 Telegram 令牌 🔑</h2><p>在这一步，我们需要设置 OpenAI API 密钥和 Telegram 机器人令牌。确保您已经注册了 OpenAI 并获取了 API 密钥。然后，将下面的示例代码中的 API 密钥和 Telegram 令牌替换为您自己的：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">openai.api_key = <span class="string">"您的 OpenAI API 密钥"</span></span><br><span class="line">TOKEN: Final = <span class="string">"您的 Telegram 机器人令牌"</span></span><br></pre></td></tr></tbody></table></figure><h2 id="步骤-4：创建启动和帮助命令-🚀"><a href="#步骤-4：创建启动和帮助命令-🚀" class="headerlink" title="步骤 4：创建启动和帮助命令 🚀"></a>步骤 4：创建启动和帮助命令 🚀</h2><p>接下来，我们将创建两个命令处理函数，用于处理启动和帮助命令。这些命令允许用户与机器人进行互动。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">start_command</span>(<span class="params">update: Update, context: ContextTypes.DEFAULT_TYPE</span>):</span><br><span class="line">    <span class="keyword">await</span> update.message.reply_text(<span class="string">"你好世界! 😄"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">help_command</span>(<span class="params">update: Update, context: ContextTypes.DEFAULT_TYPE</span>):</span><br><span class="line">    <span class="keyword">await</span> update.message.reply_text(<span class="string">"我是 PunShine Bot，我可以帮你找到最搞笑的双关语！ 😂"</span>)</span><br></pre></td></tr></tbody></table></figure><p><code>start_command</code> 函数用于处理启动命令，当用户发送 <code>/start</code> 命令时，机器人将回复 “你好世界! 😄”。<code>help_command</code> 函数用于处理帮助命令，当用户发送 <code>/help</code> 命令时，机器人将回复帮助信息。</p><h2 id="步骤-5：处理用户消息-📨"><a href="#步骤-5：处理用户消息-📨" class="headerlink" title="步骤 5：处理用户消息 📨"></a>步骤 5：处理用户消息 📨</h2><p>在这一步，我们定义了处理用户消息的 <code>handle_response</code> 函数，并创建了消息处理程序 <code>message_handler</code>。这个函数将用户的消息添加到对话历史记录中，并使用 OpenAI API 生成机器人的回复。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">handle_response</span>(<span class="params">chat_id: <span class="type">Union</span>[<span class="built_in">int</span>, <span class="built_in">str</span>], text: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">    <span class="comment"># 获取聊天历史记录</span></span><br><span class="line">    chat_history = get_chat_history(chat_id)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 创建用户消息</span></span><br><span class="line">    user_message = {<span class="string">"role"</span>: <span class="string">"user"</span>, <span class="string">"content"</span>: text}</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将用户消息添加到聊天历史记录</span></span><br><span class="line">    chat_history.append(user_message)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建消息列表，包括系统消息和用户消息</span></span><br><span class="line">    messages = [</span><br><span class="line">        {</span><br><span class="line">            <span class="string">"role"</span>: <span class="string">"system"</span>,</span><br><span class="line">            <span class="string">"content"</span>: <span class="string">"你是 PunshineBot，用中文回答，语气随意，别太正式，多用 emoji，回复要简短一点。 😄🚀"</span>,</span><br><span class="line">        }</span><br><span class="line">    ]</span><br><span class="line">    messages.extend(chat_history)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 使用 OpenAI API 生成机器人的回复</span></span><br><span class="line">    response = openai.ChatCompletion.create(</span><br><span class="line">        model=<span class="string">"gpt-3.5-turbo"</span>,</span><br><span class="line">        messages=messages,</span><br><span class="line">        temperature=<span class="number">1</span>,</span><br><span class="line">        max_tokens=<span class="number">256</span>,</span><br><span class="line">        top_p=<span class="number">1</span>,</span><br><span class="line">        frequency_penalty=<span class="number">0</span>,</span><br><span class="line">        presence_penalty=<span class="number">0</span>,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    bot_response = response[<span class="string">"choices"</span>][<span class="number">0</span>][<span class="string">"message"</span>][<span class="string">"content"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将机器人的回复添加到对话历史记录</span></span><br><span class="line">    chat_history.append({<span class="string">"role"</span>: <span class="string">"assistant"</span>, <span class="string">"content"</span>: bot_response})</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> bot_response</span><br></pre></td></tr></tbody></table></figure><p><code>handle_response</code> 函数将用户消息添加到对话历史记录中，然后使用 OpenAI API 生成机器人的回复，并将机器人的回复添加到对话历史记录中，以保持上下文。</p><p><code>message_handler</code> 函数用于处理用户发送的消息。它检查消息的类型（群组消息或私聊消息）以及消息的内容，然后调用 <code>handle_response</code> 函数生成机器人的回复。</p><h2 id="步骤-6：运行-Telegram-机器人-🚀"><a href="#步骤-6：运行-Telegram-机器人-🚀" class="headerlink" title="步骤 6：运行 Telegram 机器人 🚀"></a>步骤 6：运行 Telegram 机器人 🚀</h2><p>最后，在主函数中，我们创建了一个 Telegram 机器人应用程序，并添加了命令处理程序和消息处理程序。然后，我们启动了 Telegram 机器人，使其可以接收和处理消息。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    app = Application.builder().token(TOKEN).build()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 添加命令处理程序</span></span><br><span class="line">    app.add_handler(CommandHandler(<span class="string">"start"</span>, start_command))</span><br><span class="line">    app.add_handler(CommandHandler(<span class="string">"help"</span>, help_command))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 添加消息处理程序</span></span><br><span class="line">    app.add_handler(MessageHandler(filters.TEXT, callback=message_handler))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 启动 Telegram 机器人</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"机器人已启动！ 😎🤖"</span>)</span><br><span class="line">    app.run_polling(poll_interval=<span class="number">1</span>)</span><br></pre></td></tr></tbody></table></figure><img src="/2023/09/24/NLP%20Insights/%E5%8D%81%E5%88%86%E9%92%9F%E7%94%A8%20Python%20%E5%92%8C%20OpenAI%20%E5%88%9B%E5%BB%BA%20Telegram%20%E6%9C%BA%E5%99%A8%E4%BA%BA/step6_zh.jpg" class=""><p><code>app.run_polling(poll_interval=1)</code> 启动了 Telegram 机器人，以每秒的间隔轮询新消息。这意味着机器人会一直运行，并在接收到消息时触发相应的处理程序。</p><h2 id="步骤-7：免费在线托管您的-Telegram-机器人-24-7-🚀"><a href="#步骤-7：免费在线托管您的-Telegram-机器人-24-7-🚀" class="headerlink" title="步骤 7：免费在线托管您的 Telegram 机器人 24/7 🚀"></a>步骤 7：免费在线托管您的 Telegram 机器人 24/7 🚀</h2><p>在之前的教程中，我们教您如何创建一个 Telegram 机器人。但是，要使您的机器人一直在线运行，您需要让您的计算机保持开启，这可能不太方便。在这篇博客中，我们将向您展示如何将您的机器人免费托管在云服务器上，以便您的机器人可以全天候在线运行。</p><p>PythonAnywhere 是一个允许您免费托管 Python 代码的网站，它具有一定的使用限制，但对于简单的用途来说，非常合适。在这里，我们将展示如何使用 PythonAnywhere 来托管您的 Telegram 机器人。</p><p>访问 <a href="https://www.pythonanywhere.com/">PythonAnywhere 网站</a> 并创建一个帐户。</p><img src="/2023/09/24/NLP%20Insights/%E5%8D%81%E5%88%86%E9%92%9F%E7%94%A8%20Python%20%E5%92%8C%20OpenAI%20%E5%88%9B%E5%BB%BA%20Telegram%20%E6%9C%BA%E5%99%A8%E4%BA%BA/step7.jpg" class=""><h3 id="7-1：创建一个-Python-脚本"><a href="#7-1：创建一个-Python-脚本" class="headerlink" title="7.1：创建一个 Python 脚本"></a>7.1：创建一个 Python 脚本</h3><p>一旦您登录到 PythonAnywhere 的仪表板，您将看到一个名为 “Files” 的选项。点击它，然后创建一个新文件，我们将命名它为 “telegram_bot.py”。在这个文件中，我们将粘贴之前创建的 Telegram 机器人代码。</p><h3 id="7-2：安装所需的包"><a href="#7-2：安装所需的包" class="headerlink" title="7.2：安装所需的包"></a>7.2：安装所需的包</h3><p>在运行 Telegram 机器人之前，我们需要安装所需的 Python 包。PythonAnywhere 提供了一个命令行界面，您可以在其中执行以下命令：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install python-telegram-bot</span><br><span class="line">pip install openai</span><br></pre></td></tr></tbody></table></figure><p>这将安装 Telegram 机器人库，使您的代码能够与 Telegram 服务器通信。</p><h3 id="7-3：运行机器人"><a href="#7-3：运行机器人" class="headerlink" title="7.3：运行机器人"></a>7.3：运行机器人</h3><p>现在，您可以返回到 PythonAnywhere 的仪表板，并在 “Files” 下找到您的 “telegram_bot.py” 文件。然后，点击 “Run” 按钮，您的机器人将开始运行。机器人将一直在线，即使您关闭了计算机也是如此。</p><h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>现在，您已经掌握了如何创建一个 Python Telegram 机器人，并与 OpenAI 集成，使其能够回应用户的消息并生成回复。通过这个简单的步骤，您可以将您的 Telegram 机器人免费托管在云服务器上，以便它可以全天候在线运行。这样，您就可以与您的机器人互动，而不必担心计算机是否开启。如果您有其他免费托管服务或其他问题，欢迎在评论中分享！祝您的机器人运行愉快！🤖🚀😄</p><p>另外，如果您想了解更多关于如何在 Discord 上托管机器人的信息，可以观看此 YouTube 视频：<a href="https://www.youtube.com/watch?v=2TI-tCVhe9k">How To Host Your Bot Online 24/7 For FREE With Python (Telegram, Discord, Etc)</a>。视频中有更详细的教程和示例。</p><p>如果您有任何问题或需要进一步的帮助，请随时留下评论。祝您编程愉快！ 🚀🤖😄</p><h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><ul><li><a href="https://www.youtube.com/watch?v=vZtm1wuA2yc">How To Create A Telegram Bot In Python For Beginners (2023 Tutorial)</a></li><li><a href="https://www.youtube.com/watch?v=2TI-tCVhe9k">How To Host Your Bot Online 24/7 For FREE With Python (Telegram, Discord, Etc)</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Code Chronicles </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Chatbot </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Resolving Port Conflicts: Identifying and Terminating Processes</title>
      <link href="/2023/09/13/Debugging%20Diaries/Resolving%20Port%20Conflicts:%20Identifying%20and%20Terminating%20Processes/"/>
      <url>/2023/09/13/Debugging%20Diaries/Resolving%20Port%20Conflicts:%20Identifying%20and%20Terminating%20Processes/</url>
      
        <content type="html"><![CDATA[<h1 id="Resolving-Port-Conflicts-Identifying-and-Terminating-Processes"><a href="#Resolving-Port-Conflicts-Identifying-and-Terminating-Processes" class="headerlink" title="Resolving Port Conflicts: Identifying and Terminating Processes"></a>Resolving Port Conflicts: Identifying and Terminating Processes</h1><h2 id="Table-of-Contents"><a href="#Table-of-Contents" class="headerlink" title="Table of Contents"></a>Table of Contents</h2><ul><li>Introduction 📝</li><li>Finding Processes on Windows 🕵️‍♂️</li><li>Finding Processes on Linux/macOS 🐧</li><li>Viewing Process Details 📊</li><li>Terminating Processes ⛔️</li></ul><h3 id="Introduction-📝"><a href="#Introduction-📝" class="headerlink" title="Introduction 📝"></a>Introduction 📝</h3><p>Sometimes, when you try to start an application or service, you may encounter an “Address already in use” error. This means that the specified port is already in use by another process. To resolve this issue, you need to identify which process is using that port and decide whether to terminate it or change the port configuration of your application.</p><h3 id="Finding-Processes-on-Windows-🕵️‍♂️"><a href="#Finding-Processes-on-Windows-🕵️‍♂️" class="headerlink" title="Finding Processes on Windows 🕵️‍♂️"></a>Finding Processes on Windows 🕵️‍♂️</h3><p>On Windows, you can use the Command Prompt to find the process that is using a specific port. Open the Command Prompt and run the following command:</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">netstat -ano | findstr :8080</span><br></pre></td></tr></tbody></table></figure><p>This command will list all processes using port 8080 and display their Process ID (PID).</p><h3 id="Finding-Processes-on-Linux-macOS-🐧"><a href="#Finding-Processes-on-Linux-macOS-🐧" class="headerlink" title="Finding Processes on Linux/macOS 🐧"></a>Finding Processes on Linux/macOS 🐧</h3><p>On Linux and macOS systems, you can use the terminal to find the process using a specific port. Open the terminal and run the following command:</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo lsof -i :8080</span><br></pre></td></tr></tbody></table></figure><p>This command will list all processes using port 8080 and display detailed information, including the PID and process name.</p><h3 id="Viewing-Process-Details-📊"><a href="#Viewing-Process-Details-📊" class="headerlink" title="Viewing Process Details 📊"></a>Viewing Process Details 📊</h3><p>Once you have the PID of the process, you can further view details about the process. On Linux and macOS, use the following command:</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ps -p &lt;PID&gt;</span><br></pre></td></tr></tbody></table></figure><p>On Windows, you can use the Task Manager or run the following command (replace <code>&lt;PID&gt;</code> with the correct PID):</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tasklist | findstr &lt;PID&gt;</span><br></pre></td></tr></tbody></table></figure><p>This will display detailed information about the process associated with the specified PID, including the process name and other relevant details.</p><h3 id="Terminating-Processes-⛔️"><a href="#Terminating-Processes-⛔️" class="headerlink" title="Terminating Processes ⛔️"></a>Terminating Processes ⛔️</h3><p>If you decide to terminate the process that is using a specific port, follow these steps:</p><p>On Windows, you can use the Task Manager or run the following command (replace <code>&lt;PID&gt;</code> with the correct PID):</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">taskkill /F /PID &lt;PID&gt;</span><br></pre></td></tr></tbody></table></figure><p>On Linux and macOS, you can run the following command (replace <code>&lt;PID&gt;</code> with the correct PID):</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo kill &lt;PID&gt;</span><br></pre></td></tr></tbody></table></figure><p>Please note that terminating a process may interrupt running applications or services, so use this operation carefully. Ensure that you know which process to terminate and avoid impacting critical system processes.</p>]]></content>
      
      
      <categories>
          
          <category> Debugging Diaries </category>
          
      </categories>
      
      
        <tags>
            
            <tag> IssueFix </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>解决端口冲突：查找并终止进程</title>
      <link href="/2023/09/13/Debugging%20Diaries/%E8%A7%A3%E5%86%B3%E7%AB%AF%E5%8F%A3%E5%86%B2%E7%AA%81%EF%BC%9A%E6%9F%A5%E6%89%BE%E5%B9%B6%E7%BB%88%E6%AD%A2%E8%BF%9B%E7%A8%8B/"/>
      <url>/2023/09/13/Debugging%20Diaries/%E8%A7%A3%E5%86%B3%E7%AB%AF%E5%8F%A3%E5%86%B2%E7%AA%81%EF%BC%9A%E6%9F%A5%E6%89%BE%E5%B9%B6%E7%BB%88%E6%AD%A2%E8%BF%9B%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="解决端口冲突：查找并终止进程"><a href="#解决端口冲突：查找并终止进程" class="headerlink" title="解决端口冲突：查找并终止进程"></a>解决端口冲突：查找并终止进程</h1><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>简介</li><li>在Windows上查找进程 🕵️‍♂️</li><li>在Linux/macOS上查找进程 🐧</li><li>查看进程详细信息 📊</li><li>终止进程 ⛔️</li></ul><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>有时候，当你尝试启动一个应用程序或服务时，可能会遇到”Address already in use”（地址已经在使用）的错误，这意味着指定的端口已经被另一个进程占用。为了解决这个问题，你需要确定哪个进程正在使用该端口，并可以选择终止该进程或更改应用程序的端口配置。</p><h3 id="在Windows上查找进程-🕵️‍♂️"><a href="#在Windows上查找进程-🕵️‍♂️" class="headerlink" title="在Windows上查找进程 🕵️‍♂️"></a>在Windows上查找进程 🕵️‍♂️</h3><p>在Windows上，你可以使用命令提示符来查找正在使用特定端口的进程。打开命令提示符，并执行以下命令：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">netstat -ano | findstr :8080</span><br></pre></td></tr></tbody></table></figure><p>这个命令会列出所有正在使用端口8080的进程，并显示它们的进程ID（PID）。</p><h3 id="在Linux-macOS上查找进程-🐧"><a href="#在Linux-macOS上查找进程-🐧" class="headerlink" title="在Linux/macOS上查找进程 🐧"></a>在Linux/macOS上查找进程 🐧</h3><p>在Linux和macOS系统上，你可以使用终端来查找正在使用特定端口的进程。打开终端，并执行以下命令：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo lsof -i :8080</span><br></pre></td></tr></tbody></table></figure><p>这个命令会列出所有使用端口8080的进程，并显示它们的详细信息，包括PID和进程名。</p><h3 id="查看进程详细信息-📊"><a href="#查看进程详细信息-📊" class="headerlink" title="查看进程详细信息 📊"></a>查看进程详细信息 📊</h3><p>一旦你获得了进程的PID，你可以进一步查看有关进程的详细信息。在Linux和macOS上，使用以下命令：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ps -p &lt;PID&gt;</span><br></pre></td></tr></tbody></table></figure><p>在Windows上，你可以使用任务管理器或执行以下命令：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tasklist | findstr &lt;PID&gt;</span><br></pre></td></tr></tbody></table></figure><p>这将显示与特定PID相关的进程的详细信息，包括进程名称和其他详细信息。</p><h3 id="终止进程-⛔️"><a href="#终止进程-⛔️" class="headerlink" title="终止进程 ⛔️"></a>终止进程 ⛔️</h3><p>如果你确定要终止正在使用特定端口的进程，可以执行以下步骤：</p><p>在Windows上，你可以使用任务管理器或执行以下命令（使用正确的PID替换<pid>）：</pid></p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">taskkill /F /PID &lt;PID&gt;</span><br></pre></td></tr></tbody></table></figure><p>在Linux和macOS上，你可以执行以下命令（使用正确的PID替换<pid>）：</pid></p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo kill &lt;PID&gt;</span><br></pre></td></tr></tbody></table></figure><p>请注意，终止进程可能会导致正在运行的应用程序或服务中断，因此请谨慎使用此操作。确保你知道终止哪个进程，并确保不会影响到重要的系统进程。</p>]]></content>
      
      
      <categories>
          
          <category> Debugging Diaries </category>
          
      </categories>
      
      
        <tags>
            
            <tag> IssueFix </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>无断开烦恼！远程服务器后台运行程序的3种方法：`nohup`、`tmux`和`screen`</title>
      <link href="/2023/08/23/Code%20Chronicles/Python-Uninterrupted%20Remote%20Program%20Execution:%203%20Methods/"/>
      <url>/2023/08/23/Code%20Chronicles/Python-Uninterrupted%20Remote%20Program%20Execution:%203%20Methods/</url>
      
        <content type="html"><![CDATA[<h1 id="无断开烦恼！远程服务器后台运行程序的3种方法：nohup、tmux和screen"><a href="#无断开烦恼！远程服务器后台运行程序的3种方法：nohup、tmux和screen" class="headerlink" title="无断开烦恼！远程服务器后台运行程序的3种方法：nohup、tmux和screen"></a>无断开烦恼！远程服务器后台运行程序的3种方法：<code>nohup</code>、<code>tmux</code>和<code>screen</code></h1><h1 id="Uninterrupted-Remote-Program-Execution-3-Methods"><a href="#Uninterrupted-Remote-Program-Execution-3-Methods" class="headerlink" title="Uninterrupted Remote Program Execution: 3 Methods"></a>Uninterrupted Remote Program Execution: 3 Methods</h1><p>在数据分析或机器学习项目中，经常需要在远程服务器上运行耗时长、计算密集型的任务。通过SSH连接到远程服务器是常见的操作方式。但是，如何确保在断开SSH连接之后，远程服务器上的程序能够继续运行呢？本文详细介绍了三种方法：<code>nohup</code>、<code>tmux</code>和<code>screen</code>。</p><h2 id="使用nohup命令"><a href="#使用nohup命令" class="headerlink" title="使用nohup命令"></a>使用<code>nohup</code>命令</h2><h3 id="开始"><a href="#开始" class="headerlink" title="开始"></a>开始</h3><ol><li><p><strong>连接到远程机器</strong>：在本地终端中执行以下命令。</p> <figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh username@remote-server-address</span><br></pre></td></tr></tbody></table></figure></li><li><p><strong>启动后台程序</strong>：在远程机器上执行。</p> <figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">nohup</span> your-command-to-run-the-program &amp;</span><br></pre></td></tr></tbody></table></figure></li></ol><p>例如：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">nohup</span> python train_model.py &amp;</span><br></pre></td></tr></tbody></table></figure><p>此方法会将程序的输出重定向到一个名为<code>nohup.out</code>的文件中。</p><h3 id="结束"><a href="#结束" class="headerlink" title="结束"></a>结束</h3><p>在远程机器上执行以下命令。</p><ol><li><p>**查找程序的进程ID (PID)**：</p> <figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ps aux | grep your-command-to-run-the-program</span><br></pre></td></tr></tbody></table></figure></li><li><p><strong>结束进程</strong>：</p> <figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">kill</span> -9 PID</span><br></pre></td></tr></tbody></table></figure></li><li><p><strong>输出含义</strong>:<br>当你执行如下命令：</p></li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">nohup</span> python train_model.py &amp;</span><br></pre></td></tr></tbody></table></figure><p>你可能会看到这样的输出：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[1] 337082</span><br><span class="line"><span class="built_in">nohup</span>: ignoring input and appending output to <span class="string">'nohup.out'</span></span><br></pre></td></tr></tbody></table></figure><p>这里，</p><ul><li><strong><a href="%E8%BF%99%E6%98%AF%E8%AF%A5%E5%90%8E%E5%8F%B0%E4%BD%9C%E4%B8%9A%E7%9A%84%E4%BD%9C%E4%B8%9A%E7%BC%96%E5%8F%B7%E3%80%82%E5%A6%82%E6%9E%9C%E4%BD%A0%E5%9C%A8%E5%90%8E%E5%8F%B0%E8%BF%90%E8%A1%8C%E5%A4%9A%E4%B8%AA%E4%BD%9C%E4%B8%9A%EF%BC%8C%E6%AF%8F%E4%B8%AA%E4%BD%9C%E4%B8%9A%E9%83%BD%E4%BC%9A%E6%9C%89%E4%B8%80%E4%B8%AA%E5%94%AF%E4%B8%80%E7%9A%84%E4%BD%9C%E4%B8%9A%E7%BC%96%E5%8F%B7%E3%80%82">1</a> 337082</strong> 表示该任务现在在后台运行，PID（进程ID）为337082。<ul><li></li><li>337082: 这是该后台作业对应的进程ID (PID)。你可以使用这个PID来监视或结束该进程。</li></ul></li><li><strong>nohup: ignoring input and appending output to ‘nohup.out’</strong> 表示该进程现在将忽略任何输入，并将输出追加到 <code>nohup.out</code> 文件。<ul><li>这是 <code>nohup</code> 命令的标准消息，含义如下：<ul><li><code>ignoring input</code>: 当你使用 <code>nohup</code> 命令，它将不会接收任何从终端输入的数据。这意味着，一旦你使用 <code>nohup</code> 启动了一个程序，你不能再向它提供任何交互式输入（除非程序有其他的输入方法）。</li><li><code>appending output to 'nohup.out'</code>: 默认情况下，<code>nohup</code> 会将程序的输出重定向到一个名为 <code>nohup.out</code> 的文件中。所以，如果你的程序在执行过程中产生了任何输出（如打印语句），这些输出都会被写入到 <code>nohup.out</code> 文件中。如果该文件之前不存在，<code>nohup</code> 会自动创建它；如果文件已存在，<code>nohup</code> 会将新的输出追加到文件的末尾。</li><li>如果你想查看程序的输出，你可以使用 <code>cat</code> 或 <code>tail</code> 命令来查看 <code>nohup.out</code> 文件的内容。例如，使用 <code>tail -f nohup.out</code> 可以实时查看该文件的末尾内容，这对于监视程序的运行状态很有用。</li></ul></li></ul></li></ul><h2 id="使用tmux"><a href="#使用tmux" class="headerlink" title="使用tmux"></a>使用<code>tmux</code></h2><h3 id="开始-1"><a href="#开始-1" class="headerlink" title="开始"></a>开始</h3><ol><li>**安装<code>tmux</code>**：在远程机器上执行以下命令。</li></ol><ul><li>Ubuntu/Debian  <figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> apt-get install tmux <span class="comment"># Ubuntu/Debian</span></span><br></pre></td></tr></tbody></table></figure></li><li>MacOS  <figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install tmux</span><br></pre></td></tr></tbody></table></figure></li></ul><ol start="2"><li><p><strong>连接到远程机器</strong>：在本地终端中执行。</p> <figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh username@remote-server-address</span><br></pre></td></tr></tbody></table></figure></li><li><p><strong>开始新的tmux会话</strong>：在远程机器上执行。</p> <figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tmux</span><br></pre></td></tr></tbody></table></figure></li></ol><p>然后在tmux会话内运行您的程序。</p><ol start="4"><li><strong>断开会话</strong>：在远程机器上按 <code>Ctrl+b d</code>。</li></ol><h3 id="结束-1"><a href="#结束-1" class="headerlink" title="结束"></a>结束</h3><p>在远程机器上执行以下命令。</p><ol><li><p><strong>重新连接到tmux会话</strong>：</p> <figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tmux attach</span><br></pre></td></tr></tbody></table></figure></li><li><p><strong>结束程序和会话</strong>：使用<code>Ctrl+c</code>结束程序，然后按<code>Ctrl+b</code>再按<code>x</code>结束tmux会话。</p></li></ol><h2 id="使用screen"><a href="#使用screen" class="headerlink" title="使用screen"></a>使用<code>screen</code></h2><h3 id="开始-2"><a href="#开始-2" class="headerlink" title="开始"></a>开始</h3><ol><li>**安装<code>screen</code>**：在本地终端中执行。</li></ol><ul><li>Ubuntu/Debian  <figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> apt-get install screen <span class="comment"># Ubuntu/Debian</span></span><br></pre></td></tr></tbody></table></figure></li><li>MacOS  <figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install screen</span><br></pre></td></tr></tbody></table></figure></li></ul><ol><li><p><strong>连接到远程机器</strong>：在本地终端中执行。</p> <figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh username@remote-server-address</span><br></pre></td></tr></tbody></table></figure></li><li><p><strong>开始新的screen会话</strong>：在远程机器上执行。</p> <figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">screen</span><br></pre></td></tr></tbody></table></figure></li></ol><p>然后在screen会话内运行您的程序。</p><ol start="4"><li><strong>断开会话</strong>：在远程机器上按 <code>Ctrl+a d</code>。</li></ol><h3 id="结束-2"><a href="#结束-2" class="headerlink" title="结束"></a>结束</h3><p>在远程机器上执行以下命令。</p><ol><li><p><strong>重新连接到screen会话</strong>：</p> <figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">screen -r</span><br></pre></td></tr></tbody></table></figure></li><li><p><strong>结束程序和会话</strong>：使用<code>Ctrl+c</code>结束程序，然后按<code>Ctrl+a</code>再按<code>k</code>结束screen会话。</p></li></ol><h2 id="特殊情况：macOS用户"><a href="#特殊情况：macOS用户" class="headerlink" title="特殊情况：macOS用户"></a>特殊情况：macOS用户</h2><p>macOS用户可以使用Homebrew来安装<code>tmux</code>或<code>screen</code>。</p><ul><li><p>安装<code>tmux</code>：</p>  <figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install tmux</span><br></pre></td></tr></tbody></table></figure></li><li><p>安装<code>screen</code>：</p>  <figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install screen</span><br></pre></td></tr></tbody></table></figure></li></ul>]]></content>
      
      
      <categories>
          
          <category> Code Chronicles </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python Basic </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>超越Python的边界：`subprocess` 助你一键执行外部命令</title>
      <link href="/2023/08/22/Code%20Chronicles/Python-subprocess/"/>
      <url>/2023/08/22/Code%20Chronicles/Python-subprocess/</url>
      
        <content type="html"><![CDATA[<h1 id="超越Python的边界：subprocess-助你一键执行外部命令"><a href="#超越Python的边界：subprocess-助你一键执行外部命令" class="headerlink" title="超越Python的边界：subprocess 助你一键执行外部命令"></a>超越Python的边界：<code>subprocess</code> 助你一键执行外部命令</h1><p>在日常开发中，有时候我们希望能够从 Python 脚本中执行系统命令或者其他程序。Python 提供了 <code>subprocess</code> 模块，使得这一操作变得既简单又安全。</p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p><code>subprocess</code> 模块是 Python 标准库的一部分，它提供了一种简单统一的方法来执行外部命令，与进程交互，读取它的输出，并获取它的返回码。无论你是在自动化某个系统任务，还是简单地想要从另一个程序中获取数据，<code>subprocess</code> 都能助你一臂之力。</p><h2 id="功能与用途"><a href="#功能与用途" class="headerlink" title="功能与用途"></a>功能与用途</h2><ol><li><p><strong>执行外部命令</strong>：你可以轻易地从 Python 脚本中运行任何外部命令，就像在命令行中输入命令一样。这种能力使得你能够在你的 Python 程序中调用并集成其他命令行工具，扩展你的应用的功能。</p></li><li><p><strong>捕获命令的输出</strong>：如果你想获取命令的输出并在 Python 脚本中处理，<code>subprocess</code> 也能满足你。你可以将命令的输出作为字符串捕获，然后进一步分析和处理，这在需要对命令输出进行解析或者提取时非常有用。</p></li><li><p><strong>错误处理</strong>：通过捕获返回码，你可以知道命令是否成功执行，或者是否发生了错误。这使得你能够根据命令的执行结果采取不同的操作，从而提高程序的健壮性。</p></li><li><p><strong>与进程交互</strong>：<code>subprocess</code> 不仅可以启动和停止进程，还可以与它们进行双向通信。这使得你能够在运行的外部进程中发送输入，并从其输出中获取数据，从而实现更高级的交互和控制。</p></li></ol><h2 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h2><h3 id="基本的命令执行"><a href="#基本的命令执行" class="headerlink" title="基本的命令执行"></a>基本的命令执行</h3><p>执行命令最简单的方法是使用 <code>subprocess.run()</code> 函数，它接受一个命令及其参数的列表，并返回一个 <code>CompletedProcess</code> 对象，其中包含了命令执行的结果。这使得你能够轻松地在你的脚本中运行外部命令，例如：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> subprocess</span><br><span class="line"></span><br><span class="line">result = subprocess.run([<span class="string">'ls'</span>, <span class="string">'-l'</span>])</span><br></pre></td></tr></tbody></table></figure><h3 id="捕获命令输出"><a href="#捕获命令输出" class="headerlink" title="捕获命令输出"></a>捕获命令输出</h3><p>想要捕获命令的输出到 Python 脚本中，可以设置 <code>capture_output=True</code> 参数。此外，通过设置 <code>text=True</code> 参数，你可以以文本形式获取命令的输出：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">result = subprocess.run([<span class="string">'ls'</span>, <span class="string">'-l'</span>], capture_output=<span class="literal">True</span>, text=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(result.stdout)</span><br></pre></td></tr></tbody></table></figure><h3 id="使用-shell"><a href="#使用-shell" class="headerlink" title="使用 shell"></a>使用 shell</h3><p>当你需要执行包含 shell 功能（例如管道或通配符）的命令时，可以设置 <code>shell=True</code> 参数，并将命令作为字符串传递给 <code>subprocess.run()</code>。这样，你可以执行更复杂的命令，如以下示例所示：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">result = subprocess.run(<span class="string">'ls -l | grep "my_file"'</span>, shell=<span class="literal">True</span>, capture_output=<span class="literal">True</span>, text=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(result.stdout)</span><br></pre></td></tr></tbody></table></figure><p><strong>警告</strong>：虽然 <code>shell=True</code> 参数很有用，但使用时必须小心，因为它可能会让你的代码暴露于命令注入攻击。确保永远不要执行包含不受信任的输入的命令。</p><h3 id="错误处理"><a href="#错误处理" class="headerlink" title="错误处理"></a>错误处理</h3><p>命令如果返回一个非零的退出码，可以通过 <code>check=True</code> 参数来抛出异常。这使得你能够捕获命令执行过程中的错误，从而进行适当的处理：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    result = subprocess.run([<span class="string">'ls'</span>, <span class="string">'non_existent_file'</span>], check=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">except</span> subprocess.CalledProcessError:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"命令执行出错!"</span>)</span><br></pre></td></tr></tbody></table></figure><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p><code>subprocess</code> 模块是 Python 中执行外部命令的强大工具。通过它，开发者可以轻松地与操作系统及其他应用程序交互，扩展程序的功能。然而，在使用 <code>subprocess</code> 时需要注意命令的安全性，以避免潜在的安全风险。通过合理地利用 <code>subprocess</code> 模块，你可以更好地管理外部命令的执行，并将其融入到你的 Python 应用中，提升开发效率。希望本文能够帮助你更深入地理解和使用 <code>subprocess</code> 模块的种种功能。</p>]]></content>
      
      
      <categories>
          
          <category> Code Chronicles </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python Basic </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Conver Pytorch Model to ONNX Format</title>
      <link href="/2023/08/21/NLP%20Insights/Conver%20Pytorch%20Model%20to%20ONNX%20Format/"/>
      <url>/2023/08/21/NLP%20Insights/Conver%20Pytorch%20Model%20to%20ONNX%20Format/</url>
      
        <content type="html"><![CDATA[<h1 id="使用-PyTorch-和-ONNX-检查模型一致性"><a href="#使用-PyTorch-和-ONNX-检查模型一致性" class="headerlink" title="使用 PyTorch 和 ONNX 检查模型一致性"></a>使用 PyTorch 和 ONNX 检查模型一致性</h1><p>在机器学习和深度学习的开发过程中，模型的互操作性变得越来越重要。ONNX (Open Neural Network Exchange) 是一种开放格式，用于表示机器学习和深度学习模型。它允许开发者在各种深度学习框架之间轻松地共享模型，从而提高了模型的可移植性和互操作性。</p><p>本教程将指导您完成以下步骤：</p><ol><li>将 PyTorch 模型转换为 ONNX 格式。</li><li>验证转换后的 ONNX 模型与原始 PyTorch 模型的输出是否一致。</li></ol><h2 id="1-导入必要的库"><a href="#1-导入必要的库" class="headerlink" title="1. 导入必要的库"></a>1. 导入必要的库</h2><p>首先，我们导入为模型转换和验证所需的所有库。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> onnx</span><br><span class="line"><span class="keyword">import</span> onnxruntime</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></tbody></table></figure><h2 id="2-定义模型转换函数"><a href="#2-定义模型转换函数" class="headerlink" title="2. 定义模型转换函数"></a>2. 定义模型转换函数</h2><p>为了将 PyTorch 模型转换为 ONNX 格式，我们定义了一个名为 <code>convert_onnx</code> 的函数。此函数使用 PyTorch 的内置函数 <code>torch.onnx.export</code> 将模型转换为 ONNX 格式。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">convert_onnx</span>(<span class="params">model, dummy_input, onnx_path</span>):</span><br><span class="line">    input_names = [<span class="string">'modelInput'</span>]</span><br><span class="line">    output_names = [<span class="string">"modelOutput"</span>]</span><br><span class="line">    torch.onnx.export(model=model,</span><br><span class="line">                      args=dummy_input,</span><br><span class="line">                      f=onnx_path,</span><br><span class="line">                      opset_version=<span class="number">10</span>,</span><br><span class="line">                      input_names=input_names,</span><br><span class="line">                      output_names=output_names)</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><p>此函数接收三个参数：PyTorch 模型、模拟输入数据以及要保存 ONNX 模型的路径。<code>torch.onnx.export</code> 函数需要模型、输入和保存路径作为参数，以及其他一些可选参数来指定输入和输出的名称。</p><h2 id="3-定义一致性检查函数"><a href="#3-定义一致性检查函数" class="headerlink" title="3. 定义一致性检查函数"></a>3. 定义一致性检查函数</h2><p>一旦我们有了 ONNX 格式的模型，就可以使用 <code>check_consistency</code> 函数来验证 PyTorch 模型和 ONNX 模型的输出是否一致。这是确保转换过程没有引入任何差异的关键步骤。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">check_consistency</span>(<span class="params">pytorch_model, onnx_model_path, input_tensor, tolerance=<span class="number">1e-6</span></span>):</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        pytorch_output_dict = pytorch_model(input_tensor)</span><br><span class="line">        pytorch_output = pytorch_output_dict[<span class="string">'y_pred'</span>].cpu().numpy()</span><br><span class="line"></span><br><span class="line">    ort_session = onnxruntime.InferenceSession(onnx_model_path)</span><br><span class="line">    ort_inputs = {ort_session.get_inputs()[<span class="number">0</span>].name: input_tensor.cpu().numpy()}</span><br><span class="line">    ort_output = ort_session.run(<span class="literal">None</span>, ort_inputs)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    difference = np.<span class="built_in">abs</span>(pytorch_output - ort_output)</span><br><span class="line">    consistent = np.<span class="built_in">all</span>(difference &lt;= tolerance)</span><br><span class="line">    <span class="keyword">return</span> consistent</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><p>此函数首先使用 PyTorch 模型计算输出，然后使用 ONNX 运行时计算 ONNX 模型的输出。最后，它比较两个输出，检查它们之间的差异是否在预定义的容忍范围内。</p><h2 id="4-示例调用"><a href="#4-示例调用" class="headerlink" title="4. 示例调用"></a>4. 示例调用</h2><p>为了确保上述函数的正确性，我们提供了一个简单的示例，展示了如何使用上述函数来转换模型并检查一致性。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载 PyTorch 模型 (此处只是一个示例，需要根据实际情况进行修改)</span></span><br><span class="line">model = YOUR_PYTORCH_MODEL</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转换为 ONNX 格式</span></span><br><span class="line">dummy_input = YOUR_INPUT_TENSOR</span><br><span class="line">onnx_path = <span class="string">"path_to_save_onnx_model.onnx"</span></span><br><span class="line">convert_onnx(model, dummy_input, onnx_path)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查一致性</span></span><br><span class="line">is_consistent = check_consistency(model, onnx_path, dummy_input)</span><br><span class="line"><span class="keyword">if</span> is_consistent:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"The outputs of the PyTorch model and the ONNX model are consistent!"</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"There is a discrepancy between the outputs of the PyTorch model and the ONNX model."</span>)</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><p>在实际应用中，确保根据您的实际模型和数据替换 <code>YOUR_PYTORCH_MODEL</code> 和 <code>YOUR_INPUT_TENSOR</code>。</p><hr><p>以上就是关于如何使用 PyTorch 和 ONNX 来检查模型一致性的教程。希望这篇文章对你有所帮助，如果有任何问题，欢迎在下方留言。</p>]]></content>
      
      
      <categories>
          
          <category> NLP Insights </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Onnx </tag>
            
            <tag> Deployment </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Training Llama 2 Model on Single GPU with int8 Quantization and LoRA</title>
      <link href="/2023/08/02/NLP%20Insights/LLAMA2/"/>
      <url>/2023/08/02/NLP%20Insights/LLAMA2/</url>
      
        <content type="html"><![CDATA[<h1 id="Training-Llama-2-Model-on-Single-GPU-with-int8-Quantization-and-LoRA"><a href="#Training-Llama-2-Model-on-Single-GPU-with-int8-Quantization-and-LoRA" class="headerlink" title="Training Llama 2 Model on Single GPU with int8 Quantization and LoRA"></a>Training Llama 2 Model on Single GPU with int8 Quantization and LoRA</h1><h1 id="Llama-2"><a href="#Llama-2" class="headerlink" title="Llama 2"></a>Llama 2</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p><em>Llama 2</em> 是一个包含预训练和微调的生成式文本模型的集合，其规模从 70 亿到 700 亿个参数不等。Llama2模型是由Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert等人在<a href="https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/">Llama 2: Open Foundation and Fine-Tuned Chat Models</a>中提出的。</p><p>该论文的摘要如下：</p><p>在这项工作中，我们开发并发布了Llama 2，这是一组从70亿到700亿参数的预训练和微调的大型语言模型（LLMs）。我们的微调LLMs，称为Llama 2-Chat，针对对话用例进行了优化。我们的模型在我们测试的大多数基准上胜过开源聊天模型，并且基于我们对有用性和安全性的人类评估，可能是闭源模型的合适替代品。我们提供了关于微调和改进Llama 2-Chat安全性的方法的详细描述，以便社区能够在我们的工作基础上构建，并有助于LLMs的负责任发展。</p><p><a href="https://huggingface.co/models?search=llama2">在此处查看所有Llama2模型</a></p><h3 id="提示："><a href="#提示：" class="headerlink" title="提示："></a>提示：</h3><ul><li>通过填写<a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/">此表格</a>可以获得Llama2模型的权重</li><li>该架构与第一个Llama非常相似，增加了Groupe Query Attention（GQA）<a href="https://arxiv.org/pdf/2305.13245.pdf">此论文</a>之后</li><li>将<code>config.pretraining_tp</code>设置为不同于1的值将激活线性层的更准确但更慢的计算，这应更好地匹配原始logits。</li><li>原始模型使用<code>pad_id = -1</code>，这意味着没有填充令牌。我们不能使用相同的逻辑，请确保使用<code>tokenizer.add_special_tokens({"pad_token":"&lt;pad&gt;"})</code>添加填充令牌，并相应地调整令牌嵌入大小。您还应设置<code>model.config.pad_token_id</code>。模型的embed_tokens层用<code>self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.config.padding_idx)</code>初始化，确保编码填充令牌将输出零，因此在初始化时传递它是推荐的。</li><li>填写表格并获得模型检查点的访问权限后，您应该能够使用已转换的检查点。否则，如果您正在转换自己的模型，请随时使用转换脚本。可以使用以下（示例）命令调用脚本：<figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python src/transformers/models/llama/convert_llama_weights_to_hf.py \</span><br><span class="line">    --input_dir /path/to/downloaded/llama/weights --model_size 7B --output_dir /output/path</span><br></pre></td></tr></tbody></table></figure></li></ul><h2 id="模型详情"><a href="#模型详情" class="headerlink" title="模型详情"></a>模型详情</h2><p>注意：使用该模型受 Meta 许可证的约束。为了下载模型权重和分词器，请访问网站并在请求访问之前接受许可证。</p><p>Meta 开发并公开发布了 Llama 2 系列大型语言模型（LLMs），这是一系列规模从 70 亿到 700 亿参数的预训练和微调的生成式文本模型。我们的微调 LLMs，称为 Llama-2-Chat，经过优化用于对话应用场景。Llama-2-Chat 模型在我们测试的大多数基准测试中优于开源聊天模型，并在我们的人工评估中在有用性和安全性方面与一些流行的闭源模型（如ChatGPT和PaLM）持平。</p><h2 id="模型开发者"><a href="#模型开发者" class="headerlink" title="模型开发者"></a>模型开发者</h2><p>Model Developers Meta</p><h2 id="不同版本"><a href="#不同版本" class="headerlink" title="不同版本"></a>不同版本</h2><p>Llama 2 有不同规模的参数版本，包括 7B、13B 和 70B，以及预训练和微调的变体。</p><h2 id="输入输出"><a href="#输入输出" class="headerlink" title="输入输出"></a>输入输出</h2><p>输入模型仅支持文本输入。</p><p>输出模型仅生成文本。</p><h2 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h2><p>Llama 2 是一种自回归语言模型，采用了优化的 Transformer 架构。微调版本使用有监督的微调（SFT）和基于人类反馈的强化学习（RLHF）来与人类对 helpfulness 和 safety 的偏好保持一致。</p><h2 id="训练数据"><a href="#训练数据" class="headerlink" title="训练数据"></a>训练数据</h2><table><thead><tr><th>模型名称</th><th>训练数据</th><th>参数规模</th><th>内容长度</th><th>GQA</th><th>Tokens</th><th>LR</th></tr></thead><tbody><tr><td>Llama 2</td><td>一种新的公开可用的在线数据混合</td><td>7B</td><td>4k</td><td>✗</td><td>2.0T</td><td>3.0 x 10-4</td></tr><tr><td>Llama 2</td><td>一种新的公开可用的在线数据混合</td><td>13B</td><td>4k</td><td>✗</td><td>2.0T</td><td>3.0 x 10-4</td></tr><tr><td>Llama 2</td><td>一种新的公开可用的在线数据混合</td><td>70B</td><td>4k</td><td>✔</td><td>2.0T</td><td>1.5 x 10-4</td></tr></tbody></table><p>注：Token counts 仅指预训练数据。所有模型都使用全局 batch-size 为 4M tokens 进行训练。规模更大的模型（70B）使用 Grouped-Query Attention（GQA）来提高推理可伸缩性。</p><h2 id="模型训练日期"><a href="#模型训练日期" class="headerlink" title="模型训练日期"></a>模型训练日期</h2><p>Llama 2 在2023年1月至2023年7月之间进行训练。</p><h2 id="状态"><a href="#状态" class="headerlink" title="状态"></a>状态</h2><p>这是一个在离线数据集上训练的静态模型。随着我们根据社区反馈改进模型的安全性，将发布微调版本的未来版本。</p><h2 id="许可证"><a href="#许可证" class="headerlink" title="许可证"></a>许可证</h2><p>定制的商业许可证可在以下网址获取：<a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/">https://ai.meta.com/resources/models-and-libraries/llama-downloads/</a></p><h2 id="研究论文"><a href="#研究论文" class="headerlink" title="研究论文"></a>研究论文</h2><p>《Llama-2: Open Foundation and Fine-tuned Chat Models》</p><h2 id="使用目的"><a href="#使用目的" class="headerlink" title="使用目的"></a>使用目的</h2><h3 id="预期用途"><a href="#预期用途" class="headerlink" title="预期用途"></a>预期用途</h3><p>Llama 2 旨在用于英语商业和研究用途。微调模型适用于类似助理的聊天应用，而预训练模型可适应多种自然语言生成任务。</p><p>要获得聊天版本的预期功能和性能，需要遵循特定的格式，包括 INST 和 &lt;<sys>&gt; 标签、BOS 和 EOS tokens，以及它们之间的空格和换行符（我们建议对输入调用 strip() 方法，以避免双空格）。有关详情，请参阅我们在 GitHub 上的参考代码：chat_completion。</sys></p><h3 id="不在范围内的用途"><a href="#不在范围内的用途" class="headerlink" title="不在范围内的用途"></a>不在范围内的用途</h3><ul><li>用于违反适用法律法规（包括贸易合规法）的任何方式。</li><li>用于除英语以外的其他语言。</li><li>用于 Llama 2 可接受使用政策和许可协议所禁止的任何其他方式。</li></ul><h2 id="硬件和软件"><a href="#硬件和软件" class="headerlink" title="硬件和软件"></a>硬件和软件</h2><h3 id="训练因素"><a href="#训练因素" class="headerlink" title="训练因素"></a>训练因素</h3><p>我们使用自定义训练库、Meta 的 Research Super Cluster 以及生产集群进行预训练。微调、标注和评估也是在第三方云计算上执行的。</p><h3 id="碳足迹"><a href="#碳足迹" class="headerlink" title="碳足迹"></a>碳足迹</h3><p>预训练过程中使用了累计 330 万 GPU 小时的计算，使用的硬件类型为 A100-80GB（TDP 为 350-400W）。预计总排放量为 539 tCO2eq，其中 100% 由 Meta 的可持续性计划抵消。</p><table><thead><tr><th>模型</th><th>时间（GPU 小时）</th><th>功耗（瓦）</th><th>排放碳量（tCO2eq）</th></tr></thead><tbody><tr><td>Llama 2 7B</td><td>184,320</td><td>400</td><td>31.22</td></tr><tr><td>Llama 2 13B</td><td>368,640</td><td>400</td><td>62.44</td></tr><tr><td>Llama 2 70B</td><td>1,720,320</td><td>400</td><td>291.42</td></tr><tr><td>总计</td><td>3,311,616</td><td></td><td>539.00</td></tr></tbody></table><p>预训练期间的二氧化碳排放量。时间：每个模型训练所需的总 GPU 时间。功耗：用于所使用的 GPU 设备的每个 GPU 的峰值功率容量，调整后的</p><p>功耗使用效率。100% 的排放直接由 Meta 的可持续性计划抵消，因为我们正在公开发布这些模型，预训练成本不需要由他人承担。</p><h2 id="训练数据-1"><a href="#训练数据-1" class="headerlink" title="训练数据"></a>训练数据</h2><h3 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h3><p>Llama 2 在来自公开来源的数据中预训练了 2 万亿个 tokens。微调数据包括公开可用的指导数据集，以及一百万个新的人工标注示例。预训练和微调数据集均不包含 Meta 用户数据。</p><h3 id="数据新鲜度"><a href="#数据新鲜度" class="headerlink" title="数据新鲜度"></a>数据新鲜度</h3><p>预训练数据截止日期为 2022 年 9 月，但一些微调数据更近，最多至 2023 年 7 月。</p><h2 id="评估结果"><a href="#评估结果" class="headerlink" title="评估结果"></a>评估结果</h2><p>在此部分，我们报告了 Llama 1 和 Llama 2 模型在标准学术基准测试上的结果。对于所有评估，我们使用我们的内部评估库。</p><table><thead><tr><th>模型</th><th>规模</th><th>Code</th><th>常识推理</th><th>世界知识</th><th>阅读理解</th><th>数学</th><th>MMLU</th><th>BBH</th><th>AGI Eval</th></tr></thead><tbody><tr><td>Llama 1</td><td>7B</td><td>14.1</td><td>60.8</td><td>46.2</td><td>58.5</td><td>6.95</td><td>35.1</td><td>30.3</td><td>23.9</td></tr><tr><td>Llama 1</td><td>13B</td><td>18.9</td><td>66.1</td><td>52.6</td><td>62.3</td><td>10.9</td><td>46.9</td><td>37.0</td><td>33.9</td></tr><tr><td>Llama 1</td><td>33B</td><td>26.0</td><td>70.0</td><td>58.4</td><td>67.6</td><td>21.4</td><td>57.8</td><td>39.8</td><td>41.7</td></tr><tr><td>Llama 1</td><td>65B</td><td>30.7</td><td>70.7</td><td>60.5</td><td>68.6</td><td>30.8</td><td>63.4</td><td>43.5</td><td>47.6</td></tr><tr><td>Llama 2</td><td>7B</td><td>16.8</td><td>63.9</td><td>48.9</td><td>61.3</td><td>14.6</td><td>45.3</td><td>32.6</td><td>29.3</td></tr><tr><td>Llama 2</td><td>13B</td><td>24.5</td><td>66.9</td><td>55.4</td><td>65.8</td><td>28.7</td><td>54.8</td><td>39.4</td><td>39.1</td></tr><tr><td>Llama 2</td><td>70B</td><td>37.5</td><td>71.9</td><td>63.6</td><td>69.4</td><td>35.2</td><td>68.9</td><td>51.2</td><td>54.2</td></tr></tbody></table><p>模型在 grouped academic benchmarks 上的整体表现。Code：我们报告模型在 HumanEval 和 MBPP 上的平均 pass@1 分数。常识推理：我们报告 PIQA、SIQA、HellaSwag、WinoGrande、ARC easy 和 challenge、OpenBookQA 和 CommonsenseQA 的平均分数。我们对 CommonSenseQA 进行了 7-shot 结果评估，对其他所有基准测试进行了 0-shot 结果评估。世界知识：我们在 NaturalQuestions 和 TriviaQA 上进行 5-shot 性能评估并报告平均分数。阅读理解：对于阅读理解，我们报告 SQuAD、QuAC 和 BoolQ 的 0-shot 平均分数。数学：我们报告 GSM8K（8-shot）和 MATH（4-shot）基准测试的平均分数。</p><h3 id="TruthfulQA-和-Toxigen"><a href="#TruthfulQA-和-Toxigen" class="headerlink" title="TruthfulQA 和 Toxigen"></a>TruthfulQA 和 Toxigen</h3><table><thead><tr><th>模型</th><th>规模</th><th>TruthfulQA</th><th>Toxigen</th></tr></thead><tbody><tr><td>Llama 1</td><td>7B</td><td>27.42</td><td>23.00</td></tr><tr><td>Llama 1</td><td>13B</td><td>41.74</td><td>23.08</td></tr><tr><td>Llama 1</td><td>33B</td><td>44.19</td><td>22.57</td></tr><tr><td>Llama 1</td><td>65B</td><td>48.71</td><td>21.77</td></tr><tr><td>Llama 2</td><td>7B</td><td>33.29</td><td>21.25</td></tr><tr><td>Llama 2</td><td>13B</td><td>41.86</td><td>26.10</td></tr><tr><td>Llama 2</td><td>70B</td><td>50.18</td><td>24.60</td></tr></tbody></table><p>预训练 LLMs 在自动安全基准测试上的评估结果。对于 TruthfulQA，我们呈现同时具有真实性和信息量的生成百分比（百分比越高越好）。对于 ToxiGen，我们呈现有害生成的百分比（百分比越小越好）。</p><h3 id="TruthfulQA-和-Toxigen（微调版本-LLMs）"><a href="#TruthfulQA-和-Toxigen（微调版本-LLMs）" class="headerlink" title="TruthfulQA 和 Toxigen（微调版本 LLMs）"></a>TruthfulQA 和 Toxigen（微调版本 LLMs）</h3><table><thead><tr><th>模型</th><th>规模</th><th>TruthfulQA</th><th>Toxigen</th></tr></thead><tbody><tr><td>Llama-2-Chat</td><td>7B</td><td>57.04</td><td>0.00</td></tr><tr><td>Llama-2-Chat</td><td>13B</td><td>62.18</td><td>0.00</td></tr><tr><td>Llama-2-Chat</td><td>70B</td><td>64.14</td><td>0.01</td></tr></tbody></table><p>不同安全数据集上微调 LLMs 的评估结果。度量标准定义同上。</p><h2 id="道德考虑和局限性"><a href="#道德考虑和局限性" class="headerlink" title="道德考虑和局限性"></a>道德考虑和局限性</h2><p>Llama 2 是一项具有风险的新技术。迄今为止的测试仅涵盖了英语，并且无法覆盖所有场景。因此，与所有 LLMs 一样，Llama 2 的潜在输出无法事先预测，并且在某些情况下可能会产生不准确、带偏见或其他不可取的响应。因此，在部署任何 Llama 2 应用程序之前，开发人员应根据其特定的模型应用进行安全测试和调整。</p><p>请参阅“负责任使用指南”，网址为：<a href="https://ai.meta.com/llama/responsible-use-guide/">https://ai.meta.com/llama/responsible-use-guide/</a></p><h2 id="报告问题"><a href="#报告问题" class="headerlink" title="报告问题"></a>报告问题</h2><p>请通过以下方式之一报告任何软件“bug”或模型的其他问题：</p><ul><li>报告模型问题：[github.com/facebookresearch/llama](<a href="https://github/">https://github</a></li></ul><p>.com/facebookresearch/llama)</p><ul><li>报告模型生成的有问题内容：<a href="https://developers.facebook.com/llama_output_feedback">developers.facebook.com/llama_output_feedback</a></li><li>报告 bug 和安全问题：<a href="https://facebook.com/whitehat/info">facebook.com/whitehat/info</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP Insights </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LONGNET - Scaling Transformers to 1,000,000,000 Tokens</title>
      <link href="/2023/07/28/NLP%20Insights/LONGNET/"/>
      <url>/2023/07/28/NLP%20Insights/LONGNET/</url>
      
        <content type="html"><![CDATA[<h1 id="LONGNET：将Transformer扩展到10亿个标记"><a href="#LONGNET：将Transformer扩展到10亿个标记" class="headerlink" title="LONGNET：将Transformer扩展到10亿个标记"></a>LONGNET：将Transformer扩展到10亿个标记</h1><p>在本篇文章中，我们将详细讨论一个近期发布的先进模型——“LongNet”。该模型由微软亚洲研究院研发，于大约两周前正式公布。LongNet基于Transformer模型构建，其核心理念在于拓展Transformer的应用规模。值得一提的是，研究团队成功地将其扩展至处理10亿个令牌的规模。对于熟悉语言模型的人来说，会明白序列长度对模型性能的影响，因为序列长度决定了在执行注意力机制时，能够关联的令牌数量，从而影响模型可以获取的上下文信息长度。例如，我们希望像GPT这样的模型能拥有更长的上下文，使得模型可以参考更久之前的单词来预测下一个令牌。而LongNet就成功地将这个能力扩展到了10亿个令牌。以下图为例，可以清晰看出，GPT的序列长度仅为512，而Power Transformer的序列长度可扩展至12、000、64、262、000、甚至1000万，然而LongNet将序列长度扩展至惊人的10亿个令牌。试想一下，我们可以将所有维基百科的文本信息输入到模型中，模型可以利用所有这些令牌进行注意力计算。接下来，让我们首先来了解一下LongNet的工作原理。</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>在大型语言模型时代，扩展序列长度已成为一个重要需求。然而，现有方法在处理计算复杂性或模型表达能力时遇到困难，导致最大序列长度受限。为了解决这个问题，我们引入了LONGNET，这是一种Transformer变体，可以将序列长度扩展到10亿个标记以上，同时不损害对较短序列的性能。具体而言，我们提出了扩张注意力（dilated attention），随着距离增加，它以指数级扩展注意力范围。LONGNET有显著的优势：1）它具有线性计算复杂性，并且序列中任意两个标记之间存在对数依赖关系；2）它可以作为分布式训练器用于非常长的序列；3）它的扩张注意力可以无缝替换标准注意力，并且可以与现有基于Transformer的优化方法无缝集成。实验结果表明，LONGNET在长序列建模和一般语言任务上表现出强大的性能。我们的工作为建模非常长的序列打开了新的可能性，例如将整个语料库甚至整个互联网视为一个序列。</p><img src="/2023/07/28/NLP%20Insights/LONGNET/Figure_1.png" class="" title="Figure 1: Trend of Transformer sequence lengths over time."><h3 id="LongNet的优点"><a href="#LongNet的优点" class="headerlink" title="LongNet的优点"></a>LongNet的优点</h3><p>LongNet具有多种优点。首先，其计算复杂度与序列长度呈线性关系，稍后将具体解释原因。其次，令牌之间存在对数依赖，也就是说，两个距离较远的令牌之间的依赖性较弱，而距离较近的令牌之间的依赖性较强。此外，它可在分布式网络中进行训练，这意味着我们可以利用分布式系统计算该注意力机制，如使用多个GPU或多台计算机。同时，LongNet可以作为标准注意力的替代品，这意味着如果我们已经有一个使用注意力机制的模型，我们只需将注意力机制替换为LongNet的机制，无需改变模型的其他部分，模型仍然能够像以前一样运行，但通过使用这种改进的注意力机制，可以处理更长的序列长度。</p><h3 id="关于Transformer模型的梳理"><a href="#关于Transformer模型的梳理" class="headerlink" title="关于Transformer模型的梳理"></a>关于Transformer模型的梳理</h3><p>自注意力机制，我们使用了被称为“Q、K、V”的矩阵。其中，“Q”矩阵代表查询，其规模为“序列长度乘以模型大小”，模型大小指的是每个词嵌入的向量表示。当我们计算查询与键（K）的乘积，或者查询与K的转置的乘积来产生此矩阵时，所需的操作次数是“序列长度的平方乘以模型大小”，因为我们需要为矩阵中的每个元素计算点积。这就是为什么自注意力的复杂度是“序列长度的平方乘以模型大小”。这个比较在相关论文中也有详细描述，常规的注意力复杂度是“序列长度的平方乘以模型大小”，然而LongNet这种新模型，其注意力机制复杂度仅为“序列长度乘以模型大小”，下文我将说明如何实现这种线性复杂度。</p><img src="/2023/07/28/NLP%20Insights/LONGNET/image-5.png" class="" title="Figure 2"><h3 id="LongNet的注意力分配原理"><a href="#LongNet的注意力分配原理" class="headerlink" title="LongNet的注意力分配原理"></a>LongNet的注意力分配原理</h3><p>LongNet的核心原理是，令牌间的注意力分配会随着它们之间距离的增加而呈指数级地减小。让我们参照图表来理解它的运作方式。在传统方式中，我们计算所有令牌与其他所有令牌之间的注意力，但LongNet并未如此操作。它采用了一种将序列切分为不同大小窗口的方法。首先，以4为窗口大小为例，这里的“N”是序列的令牌数，我们将其分成四个大小为4的段，并计算这个小窗口内的所有词与其他词之间的注意力。然后，我们对所有这些小段中的词执行同样的操作，接着使用更大的窗口，这次窗口大小为8。如此类推，直到覆盖整个序列长度，然后我们再以增加窗口大小的方式进行操作，同时我们也增加了跳过的令牌数，即“R”。例如，我们可以先计算大小为8的窗口，然后将“R”设为2，这意味着我们会跳过一个令牌，然后计算注意力，再跳过一个令牌，继续计算注意力。以这种方式，随着窗口大小和跳过的令牌数的增加，计算的复杂度变得更小，因为我们并不是计算每个令牌与所有其他令牌之间的注意力，而是只计算在有限范围内的注意力。这样，LongNet的注意力分配就遵循了对数依赖的原则，即，距离较远的令牌之间的依赖性较弱，而距离较近的令牌之间的依赖性较强。这是LongNet能在更大序列长度上进行工作的关键。<br>扩展注意力由一系列用于建模短程和长程依赖关系的注意力模式组成，注意力模式的数量可以根据序列长度进行扩展。在每个注意力模式中，查询向量和键向量之间的点积被分解为多个子点积，每个子点积仅涉及到一小部分的键向量。这种分解方式可以减少计算复杂度，同时也可以使模型更好地处理长序列。具体如下图所示：</p><img src="/2023/07/28/NLP%20Insights/LONGNET/image.png" class="" title="Figure 3"><img src="/2023/07/28/NLP%20Insights/LONGNET/image-4.png" class="" title="Figure 4"><p>扩张注意力还引入了“多头”机制，可以在不同的头之间分别计算注意力。每个头都有自己的偏移量，这样就可以在不同的位置上计算注意力，从而更好地捕捉序列中的信息。通过这种方式，扩张注意力可以更好地处理长序列，同时保持较短序列的性能。具体如下图所示：</p><img src="/2023/07/28/NLP%20Insights/LONGNET/image-4.png" class="" title="Figure 5"><h3 id="计算复杂度的优化"><a href="#计算复杂度的优化" class="headerlink" title="计算复杂度的优化"></a>计算复杂度的优化</h3><p>现在我们来看看为什么LongNet的计算复杂度是线性的。在传统的自注意力机制中，我们需要执行序列长度平方次数的点积操作，而LongNet通过使用窗口和跳过的方式，将计算的复杂度降低到了线性。如果我们假设窗口大小是固定的，例如为4，然后“R”也是固定的，例如为2，那么计算复杂度将是“O(N)”。当然，在实际操作中，窗口大小和跳过的令牌数可能会根据实际情况进行调整，但是它们是常数，不随序列长度增加而增加。这就是为什么LongNet的计算复杂度是线性的。</p><img src="/2023/07/28/NLP%20Insights/LONGNET/image-3.png" class="" title="Figure 6"><h3 id="Token扩展10亿"><a href="#Token扩展10亿" class="headerlink" title="Token扩展10亿+"></a>Token扩展10亿+</h3><p>分布式训练方法，利用LONGNET的线性计算复杂度，将序列维度分布式地进行训练。具体而言，算法首先将输入序列沿着序列维度进行切分，每个序列片段被分配到不同的设备上进行计算。然后，每个设备将序列片段投影为查询、键和值，并使用本地计算得到局部的注意力权重。对于超出本地设备序列长度的部分，键和值将被发送到其他设备上进行计算。最后，所有设备将局部的注意力权重进行汇总，得到全局的注意力权重，并使用全局的注意力权重计算每个标记的表示。具体如下图所示：</p><img src="/2023/07/28/NLP%20Insights/LONGNET/image-2.png" class="" title="Figure 7"><p>该算法可以在任意数量的设备上进行扩展，并且可以通过并行计算来加速训练过程。由于LONGNET具有线性计算复杂度，因此该算法可以有效地处理超长序列，而不会牺牲训练速度和模型性能。此外，该算法还支持标准Transformer的优化技术，例如内核融合、量化和分布式训练，从而使得LONGNET可以无缝地与现有的深度学习框架进行集成。</p><h3 id="LongNet的应用前景"><a href="#LongNet的应用前景" class="headerlink" title="LongNet的应用前景"></a>LongNet的应用前景</h3><p>LongNet的发布为自然语言处理领域带来了诸多潜在的应用前景。首先，它可以应用于更长文本的生成任务，如生成长篇小说或长篇新闻报道。其次，它可以应用于更复杂的对话任务，因为在对话中，我们往往需要处理大量的历史信息和上下文。另外，它还可能在翻译任务中发挥更大的作用，因为翻译往往涉及到处理长句子或长段落的情况。总的来说，LongNet的发布为我们提供了处理更长文本的新工具和可能性。</p><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>总的来说，LongNet是一个基于Transformer的模型，它成功地将自注意力机制扩展到了10亿个令牌，实现了处理更长文本的能力。它的优势包括计算复杂度是线性的、遵循对数依赖原则，以及可以在分布式系统上进行训练。通过LongNet，我们可以探索更多自然语言处理任务，并处理那些过去由于序列长度限制而难以处理的任务。这个新模型的发布为我们带来了更多可能性。</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><a href="https://arxiv.org/pdf/2307.02486.pdf">LONGNET: Scaling Transformers to 1,000,000,000 Tokens</a></li><li><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ">Youtube Tutorial by Umar Jamil</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP Insights </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Prompt Engineering</title>
      <link href="/2023/07/27/NLP%20Insights/Prompt%20Engineering/"/>
      <url>/2023/07/27/NLP%20Insights/Prompt%20Engineering/</url>
      
        <content type="html"><![CDATA[<h1 id="Prompt-Engineering"><a href="#Prompt-Engineering" class="headerlink" title="Prompt Engineering"></a>Prompt Engineering</h1><p>Prompt Engineering, 也被称为上下文提示，是指在不更新模型权重的情况下，与LLM（语言模型）进行交互以引导其产生期望输出的方法。它是一门实证科学，提示工程方法的效果在不同模型之间可能会有很大的差异，因此需要进行大量的实验和试探。</p><p>本文仅关注自回归语言模型的提示工程，不涉及填空测试、图像生成或多模态模型。在本质上，提示工程的目标是实现模型的对齐和可操控性。您可以查阅我之前关于可控文本生成的帖子。</p><h2 id="基本提示方法"><a href="#基本提示方法" class="headerlink" title="基本提示方法"></a>基本提示方法</h2><p>zero-shot学习和few-shot学习是两种最基本的提示模型方法，这些方法由许多LLM论文首创，并且通常用于评估LLM性能。</p><h3 id="zero-shot学习"><a href="#zero-shot学习" class="headerlink" title="zero-shot学习"></a>zero-shot学习</h3><p>zero-shot学习是将任务文本直接输入模型并要求获得结果。</p><p>（所有情感分析示例来自于SST-2数据集）</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Text: i'll bet the video game is a lot more fun than the film.</span><br><span class="line">Sentiment:</span><br></pre></td></tr></tbody></table></figure><h3 id="few-shot学习"><a href="#few-shot学习" class="headerlink" title="few-shot学习"></a>few-shot学习</h3><p>few-shot学习通过提供一组高质量的示例演示，每个示例都包含目标任务的输入和期望输出。当模型首先看到好的示例时，它可以更好地理解人类的意图和期望的答案类型。因此，few-shot学习通常比zero-shot学习表现更好。然而，这样做的代价是更多的记号消耗，并且在输入和输出文本较长时可能会达到上下文长度限制。</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Text: (lawrence bounces) all over the stage, dancing, running, sweating, mopping his face and generally displaying the wacky talent that brought him fame in the first place.</span><br><span class="line">Sentiment: positive</span><br><span class="line"></span><br><span class="line">Text: despite all evidence to the contrary, this clunker has somehow managed to pose as an actual feature movie, the kind that charges full admission and gets hyped on tv and purports to amuse small children and ostensible adults.</span><br><span class="line">Sentiment: negative</span><br><span class="line"></span><br><span class="line">Text: for the first time in years, de niro digs deep emotionally, perhaps because he's been stirred by the powerful work of his co-stars.</span><br><span class="line">Sentiment: positive</span><br><span class="line"></span><br><span class="line">Text: i'll bet the video game is a lot more fun than the film.</span><br><span class="line">Sentiment:</span><br></pre></td></tr></tbody></table></figure><p>许多研究都探讨了如何构建上下文示例以最大化性能，并观察到提示格式、训练示例和示例的顺序选择可能会导致截然不同的性能，从几乎随机猜测到接近SOTA（State-of-the-Art）。</p><p>赵等人（2021年）研究了few-shot分类的情况，并提出了一些与LLM（他们在实验中使用了GPT-3）相关的偏差，这些偏差导致了高方差的情况：</p><ul><li>（1）多数类别偏差存在于示例的标签分布不平衡的情况下；</li><li>（2）最近偏差是指模型可能在结尾重复标签；</li><li>（3）常见记号偏差表明LLM倾向于更频繁地生成常见的记号而不是罕见的记号。为了克服这些偏差，他们提出了一种方法，通过对模型输出的标签概率进行校准，使其在输入字符串为N/A时保持均匀。</li></ul><h2 id="提示工程技巧"><a href="#提示工程技巧" class="headerlink" title="提示工程技巧"></a>提示工程技巧</h2><h3 id="示例选择的建议"><a href="#示例选择的建议" class="headerlink" title="示例选择的建议"></a>示例选择的建议</h3><ul><li><p>使用嵌入空间中的NN聚类（Liu等人，2021年）来选择与测试示例在语义上相似的示例。</p></li><li><p>Su等人（2022年）提出了一种基于图的方法来选择多样且代表性的示例：</p><ol><li>首先，根据样本之间的嵌入（例如SBERT或其他嵌入模型）余弦相似性构建一个有向图，其中每个节点指向其最近的邻居；</li><li>开始时有一组已选择的示例和一组剩余示例。每个示例都通过得分函数进行评分，其中得分函数的目标是保持低值，以鼓励选择多样化的示例。具体得分函数的计算公式未提供。</li></ol></li><li><p>Rubin等人（2022年）提出了针对上下文学习示例选择的对比学习方法。对于每个训练对（格式化的输入-输出对），可以通过LM分配的条件概率来衡量一个示例的质量。然后，可以根据得分对训练对进行排名，选择得分较高和得分较低的示例作为对比学习的正样本和负样本集。</p></li><li><p>有些研究人员尝试使用Q-Learning进行示例选择（Zhang等人，2022年）。</p></li><li><p>受不确定性主导的主动学习的启发，Diao等人（2023年）建议确定具有多次采样试验中高度不一致或熵值较高的示例，并注释这些示例以在few-shot提示中使用。</p></li></ul><h3 id="示例排序的建议"><a href="#示例排序的建议" class="headerlink" title="示例排序的建议"></a>示例排序的建议</h3><ul><li><p>一般建议保持示例选择的多样性，与测试示例相关，并以随机顺序进行排列，以避免多数类别偏差和最近偏差。</p></li><li><p>增加模型大小或包含更多训练示例并不能减少上下文示例不同排列之间的方差。同一顺序对一个模型可能有效，但对另一个模型可能无效。当验证集有限时，可以考虑选择顺序，以使模型不会产生极端不平衡的预测或对其预测过于自信（Lu等人，2022年）。</p></li></ul><h2 id="指令提示"><a href="#指令提示" class="headerlink" title="指令提示"></a>指令提示</h2><ul><li><p>在提示中展示few-shot示例的目的是向模型解释我们的意图；换句话说，用示例来描述任务指令，以便模型能够理解用户意图并遵循指令。然而，few-shot的使用可能会消耗较多的记号，并限制输入长度，因为上下文长度有限。所以，为什么不直接给出指令呢？</p></li><li><p>Instructed LM（例如InstructGPT，自然语言指令）使用高质量的（任务指令，输入，真实输出）元组对预训练模型进行微调，以使LM更好地理解用户意图并遵循指令。RLHF（人类反馈的强化学习）是一种常见的方法。采用指令遵循风格的微调使得模型更加符合人类意图，并极大地降低了通信成本。</p></li><li><p>在与指令模型进行交互时，我们应该详细描述任务要求，尽量具体和准确，并避免使用”不做某事”的表述，而是要明确指定要做什么。</p></li></ul><h1 id="Chain-of-Thought-CoT-Prompting"><a href="#Chain-of-Thought-CoT-Prompting" class="headerlink" title="Chain-of-Thought (CoT) Prompting"></a>Chain-of-Thought (CoT) Prompting</h1><p>Chain-of-Thought (CoT) Prompting（Wei等人，2022年）通过生成一系列简短的句子，逐步描述推理逻辑，即所谓的推理链或理由链，最终引导出最终答案。CoT在复杂的推理任务中效果更显著，特别是在使用大型模型（例如超过50亿参数的模型）时。对于简单的任务，CoT提示的受益较小。</p><p>CoT提示的两种主要类型：</p><h2 id="few-shot-CoT"><a href="#few-shot-CoT" class="headerlink" title="few-shot CoT"></a>few-shot CoT</h2><p>few-shot CoT是使用少量演示来引导模型，每个演示包含人工编写（或模型生成）的高质量推理链。</p><p>（以下所有数学推理示例来自GSM8k数据集）</p><p>问题：Tom和Elizabeth比赛爬山。Elizabeth花了30分钟爬上山。Tom花费的时间是Elizabeth的四倍。Tom爬上山需要多少小时？</p><p>答案：Tom需要30 * 4 = 120分钟爬上山。<br>Tom需要120/60 = 2小时爬上山。<br>所以答案是2。</p><p>===</p><p>问题：Jack是个足球运动员。他需要买两双袜子和一双足球鞋。每双袜子的价格是9.50美元，鞋子的价格是92美元。Jack有40美元。Jack还需要多少钱？</p><p>答案：两双袜子的总费用是9.50美元 x 2 = 19美元。<br>袜子和鞋子的总费用是19美元 + 92美元 = 111美元。<br>Jack还需要111美元 - 40美元 = 71美元。<br>所以答案是71。</p><p>===</p><p>问题：Marty有100厘米的缎带，他必须将其分成4等份。每个切割部分必须再分成5等份。每个最终切割部分将有多长？</p><p>答案：（待填写）</p><h2 id="zero-shot-CoT"><a href="#zero-shot-CoT" class="headerlink" title="zero-shot CoT"></a>zero-shot CoT</h2><p>zero-shot CoT是使用自然语言陈述，例如“让我们逐步思考”，明确地鼓励模型首先生成推理链，然后再通过“因此，答案是”等提示来产生答案（Kojima等人，2022年）。或者使用类似的语句“让我们一步一步来计算，确保我们得到正确的答案”（Zhou等人，2022年）。</p><p>问题：Marty有100厘米的缎带，他必须将其分成4等份。每个切割部分必须再分成5等份。每个最终切割部分将有多长？</p><p>答案：让我们逐步思考。</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">@article{weng2023prompt,</span><br><span class="line">  title   = "Prompt Engineering",</span><br><span class="line">  author  = "Weng, Lilian",</span><br><span class="line">  journal = "lilianweng.github.io",</span><br><span class="line">  year    = "2023",</span><br><span class="line">  month   = "Mar",</span><br><span class="line">  url     = "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/"</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>]]></content>
      
      
      <categories>
          
          <category> NLP Insights </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
            <tag> Prompt </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ColoredLogger-彩色打印日志到控制台并记录到文件</title>
      <link href="/2023/07/10/Code%20Chronicles/ColoredLogger/"/>
      <url>/2023/07/10/Code%20Chronicles/ColoredLogger/</url>
      
        <content type="html"><![CDATA[<h1 id="彩色打印日志到控制台并记录到文件"><a href="#彩色打印日志到控制台并记录到文件" class="headerlink" title="彩色打印日志到控制台并记录到文件"></a>彩色打印日志到控制台并记录到文件</h1><p>本文档介绍了一个名为 ColoredLogger 的日志记录器类，它可以根据不同的消息类型以不同的颜色打印日志，并将日志记录到文件中。该类使用了 colorama 库来实现在控制台中显示带颜色的文本。为了使控制台输出的日志更加易于阅读和理解，我们通常会使用彩色的输出。同时，将日志记录到文件中可以方便我们后续的调试和分析。在Python中，我们可以使用<code>logging</code>和<code>colorama</code>库来实现这样的功能。</p><img src="/2023/07/10/Code%20Chronicles/ColoredLogger/image.png" class=""><p>以下是一个如何使用这两个库的详细介绍。</p><h2 id="功能"><a href="#功能" class="headerlink" title="功能"></a>功能</h2><ul><li>可以根据不同的消息类型以不同的颜色打印日志消息。</li><li>将日志消息记录到文件中，使用标准的 <code>logging</code> 模块进行记录。</li><li>在控制台中显示带颜色的日志消息。</li></ul><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p><code>logging</code>库提供了强大的日志记录功能，允许我们将日志记录到控制台、文件或者其他输出设备，并提供了详细的配置选项。</p><p><code>colorama</code>库可以使我们在控制台输出彩色的文本。它提供了对ANSI颜色编码的支持，可以在几乎所有的平台和终端中使用。</p><p>我们先初始化<code>colorama</code>，然后定义了一个<code>ColoredLogger</code>类，它包含了各种彩色的输出样式和对应的日志级别。然后，我们设置了<code>logging</code>的配置，定义了日志的输出级别、输出文件和文件模式。最后，我们使用<code>ColoredLogger</code>的<code>log</code>方法来记录日志，它会同时将彩色的文本输出到控制台，并去除颜色控制字符后写入文件。<br><code>ColoredLogger</code> 类使用了 <code>colorama</code> 库来设置控制台中的颜色输出。它通过定义不同类型消息的颜色，并使用 <code>Fore</code> 和 <code>Style</code> 类来应用相应的颜色。</p><p><code>ColoredLogger</code> 类的 <code>log</code> 方法接受两个参数：<code>msg_type</code> 和 <code>msg</code>。根据 <code>msg_type</code> 参数的值，方法将选择适当的颜色，并将带有颜色的消息打印到控制台上。然后，它使用正则表达式去除颜色控制字符，并使用 <code>logging.info</code> 方法将不带颜色的消息记录到文件中。</p><h2 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h2><ol><li><p>安装 <code>colorama</code> 库，使用以下命令：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install colorama</span><br></pre></td></tr></tbody></table></figure></li><li><p>导入所需的模块和类：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">from</span> colorama <span class="keyword">import</span> init, Fore, Style</span><br></pre></td></tr></tbody></table></figure></li><li><p>初始化 <code>colorama</code>：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">init(autoreset=<span class="literal">True</span>)</span><br></pre></td></tr></tbody></table></figure></li><li><p>定义 <code>ColoredLogger</code> 类，并设置不同类型消息的颜色。例如：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ColoredLogger</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">         <span class="variable language_">self</span>.type_A = Fore.CYAN + Style.BRIGHT</span><br><span class="line">         <span class="variable language_">self</span>.type_B = Fore.GREEN + Style.BRIGHT</span><br><span class="line">         <span class="variable language_">self</span>.type_C = Fore.YELLOW + Style.BRIGHT</span><br><span class="line">         <span class="variable language_">self</span>.type_D = Fore.MAGENTA + Style.BRIGHT</span><br><span class="line">         <span class="variable language_">self</span>.type_E = Fore.BLUE + Style.BRIGHT</span><br><span class="line">         <span class="variable language_">self</span>.RESET = Style.RESET_ALL</span><br></pre></td></tr></tbody></table></figure></li><li><p>实例化 <code>ColoredLogger</code> 类，并使用 <code>log</code> 方法打印和记录日志消息。例如：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">logger = ColoredLogger()</span><br><span class="line">logger.log(<span class="string">'type_A'</span>, <span class="string">'This is a test message for type A.'</span>)</span><br><span class="line">logger.log(<span class="string">'type_B'</span>, <span class="string">'This is a test message for type B.'</span>)</span><br><span class="line"><span class="comment"># ...</span></span><br></pre></td></tr></tbody></table></figure></li><li><p>通过以下命令配置日志记录器，将日志消息写入文件：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">logging.basicConfig(level=logging.INFO, filename=<span class="string">'example.log'</span>, filemode=<span class="string">'w'</span>)</span><br></pre></td></tr></tbody></table></figure></li><li><p>运行代码，查看在控制台和文件中显示的带颜色的日志消息。</p></li></ol><h2 id="完整代码实现"><a href="#完整代码实现" class="headerlink" title="完整代码实现"></a>完整代码实现</h2><pre><code class="python">import reimport loggingfrom colorama import init, Fore, Style# Initialize coloramainit(autoreset=True)class ColoredLogger:    def __init__(self):        self.type_A = Fore.CYAN + Style.BRIGHT        self.type_B = Fore.GREEN + Style.BRIGHT        self.type_C = Fore.YELLOW + Style.BRIGHT        self.type_D = Fore.MAGENTA + Style.BRIGHT        self.type_E = Fore.BLUE + Style.BRIGHT        self.RESET = Style.RESET_ALL    def log(self, msg_type, msg):        if msg_type == "type_A":            self.print_and_log(self.type_A + f"type_A: {msg}" + self.RESET)        elif msg_type == "type_B":            self.print_and_log(self.type_B + f"type_B: {msg}" + self.RESET)        elif msg_type == "type_C":            self.print_and_log(self.type_C + f"type_C: {msg}" + self.RESET)        elif msg_type == "type_D":            self.print_and_log(self.type_D + f"type_D: {msg}" + self.RESET)        elif msg_type == "type_E":            self.print_and_log(self.type_E + f"type_E: {msg}" + self.RESET)    def print_and_log(self, msg):        # Print to console with color        print(msg)        # Remove color control chars for logging to file        msg_without_color = re.sub('\x1b\[[0-9;]*m', '', msg)        logging.info(msg_without_color)# Setup logging configurationlogging.basicConfig(level=logging.INFO, filename='example.log', filemode='w')# Use the ColoredLoggerlogger = ColoredLogger()logger.log('type_A', 'This is a test message for type A.')logger.log('type_B', 'This is a test message for type B.')logger.log('type_C', 'This is a test message for type C.')logger.log('type_D', 'This is a test message for type D.')logger.log('type_E', 'This is a test message for type E.')```![Alt text](image.png)## 注意事项1. `colorama`库的颜色和样式可能在不同的平台和终端上有不同的效果，所以需要在目标环境上进行测试。2. 在写入文件时，需要去除颜色控制字符，否则会在文本中留下一些无法识别的字符。3. 在设置`logging`的配置时，需要注意文件模式的设置。'w'模式会在每次运行时覆盖之前的日志，而'a'模式则会在之前的日志后追加新的日志。4. 使用`logging`库记录日志时，需要注意日志的级别。不同级别的日志会有不同的输出效果和记录方式。5. 需要注意线程安全。如果在多线程环境下使用`logging`库，可能需要额外的配置来保证线程安全。</code></pre>]]></content>
      
      
      <categories>
          
          <category> Code Chronicles </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>详解梯度下降算法</title>
      <link href="/2023/07/09/NLP%20Insights/%E8%AF%A6%E8%A7%A3%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95/"/>
      <url>/2023/07/09/NLP%20Insights/%E8%AF%A6%E8%A7%A3%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h1 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h1><p>梯度是一个在微积分中使用的重要概念，它用于衡量函数在给定点上的方向导数沿各个方向最大时的最大值。对于一个标量函数，梯度的方向是函数增长最快的方向，而梯度的反方向则是函数减小最快的方向。</p><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>对于在点$x \in \mathbb{R}^n$可微的函数$f: \mathbb{R}^n \rightarrow \mathbb{R}$，其梯度被定义为一个向量，其各个分量为函数在该点上的偏导数。对于函数$f(x_1, x_2, …, x_n)$，它的梯度可以表示为：</p><p>$$<br>\nabla f(x) = \left[ \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, …, \frac{\partial f}{\partial x_n} \right]^T<br>$$</p><p>这里，$\nabla f(x)$表示$f(x)$的梯度，$\frac{\partial f}{\partial x_i}$表示$f$关于$x_i$的偏导数，$T$表示矩阵转置。</p><h2 id="物理含义"><a href="#物理含义" class="headerlink" title="物理含义"></a>物理含义</h2><p>梯度有一个重要的物理含义。在二维空间中，可以把函数$f(x, y)$看作地形的高度，那么梯度就是指向最陡峭上升方向的向量。而梯度的大小，则对应了最陡峭上升方向的斜率。因此，在优化问题中，我们通常沿着梯度的反方向更新参数，以最快地降低函数值。</p><h2 id="梯度在机器学习中的应用"><a href="#梯度在机器学习中的应用" class="headerlink" title="梯度在机器学习中的应用"></a>梯度在机器学习中的应用</h2><p>在机器学习中，我们的目标通常是找到一组参数，使得损失函数达到最小。为了实现这个目标，我们可以使用梯度下降算法，不断地沿着损失函数梯度的反方向更新参数。</p><p>在深度学习中，由于模型通常有大量的参数，我们需要使用反向传播算法来高效地计算梯度。这种算法基于链式法则，可以在计算图中从输出端到输入端，一层一层地计算各个参数的梯度。</p><h2 id="计算梯度"><a href="#计算梯度" class="headerlink" title="计算梯度"></a>计算梯度</h2><p>在实践中，我们通常使用自动微分（如PyTorch和TensorFlow提供的自动微分功能）来计算梯度。这使得我们无需手动推导和实现复杂的梯度公式，大大提高了编程的效率。以下是一个PyTorch中计算梯度的例子：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个张量，并设置requires_grad=True使得我们可以计算关于它的梯度</span></span><br><span class="line">x = torch.tensor([<span class="number">1.0</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义函数</span></span><br><span class="line">y = <span class="number">2</span> * x * x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过反向传播计算梯度</span></span><br><span class="line">y.backward()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出梯度</span></span><br><span class="line"><span class="built_in">print</span>(x.grad)  <span class="comment"># 输出：tensor([4.])，因为y = 2 * x^2, dy/dx = 4 * x = 4 * 1 = 4</span></span><br></pre></td></tr></tbody></table></figure><p>在上述例子中，<code>y.backward()</code>表示计算关于<code>y</code>的梯度，然后将这个梯度反向传播回其输入<code>x</code>。因此，<code>y</code>本身的梯度被认为是1（因为对于任何变量<code>x</code>，<code>dx/dx</code>都等于1），然后这个梯度被传递到<code>x</code>，得到的是<code>y</code>关于<code>x</code>的梯度，即<code>dy/dx</code>。</p><p>注意，<code>y</code>本身没有<code>.grad</code>属性，因为它不是通过<code>requires_grad=True</code>创建的。只有那些通过<code>requires_grad=True</code>创建，并且参与了运算的张量才有<code>.grad</code>属性，这个属性存储了梯度信息。</p><p>在PyTorch中，<code>backward()</code>函数的作用是计算梯度，并将梯度信息存储在<code>.grad</code>属性中。<code>backward()</code>函数的调用者（即<code>y</code>）自身的梯度被认为是1，然后这个梯度会被反向传播回所有参与了运算并需要计算梯度的张量。</p><h1 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a>梯度下降算法</h1><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>梯度下降算法是一种用于优化目标函数的迭代方法。具体来说，它是一个寻找函数最小值的算法。对于最大化问题，我们可以通过最小化目标函数的相反数来求解。</p><p>在梯度下降中，我们首先选择一个初始点（即初始参数值），然后我们迭代地将参数向负梯度方向移动，这样在每一步，我们都能够减小目标函数的值，直到找到函数的局部最小值。</p><p>在这个过程中，梯度（函数的一阶导数）给出了函数值下降最快的方向。我们使用一个叫做学习率的参数来控制每一步移动的大小。学习率决定了每次迭代时参数更新的步长，过大的学习率可能会使算法在最小值处震荡，过小则可能会导致算法收敛速度过慢。</p><h2 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h2><p>基本的梯度下降更新公式为：</p><p>$$<br>\theta_{new} = \theta_{old} - \alpha \nabla J(\theta_{old})<br>$$</p><p>其中，$\theta$ 表示我们试图优化的参数，$J(\theta)$ 是我们试图最小化的目标函数，$\nabla J(\theta_{old})$ 是在当前参数值 $\theta_{old}$ 处的目标函数的梯度，$\alpha$ 是学习率。</p><h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><p>考虑一个简单的线性回归问题。我们有一个目标函数（损失函数）为均方误差：</p><p>$$<br>J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)})^2<br>$$</p><p>其中，$h_{\theta}(x^{(i)}) = \theta^T x^{(i)}$ 是预测函数，$m$ 是训练样本数量。</p><p>对于这个问题，我们可以使用梯度下降算法来找到最小化损失函数的参数 $\theta$。在每次迭代中，我们首先计算损失函数的梯度，然后根据前面的公式更新参数。</p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>下面是使用 Python 实现梯度下降的示例代码：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradient_descent</span>(<span class="params">X, y, theta, alpha, num_iters</span>):</span><br><span class="line">    m = y.size</span><br><span class="line">    J_history = np.zeros(num_iters)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_iters):</span><br><span class="line">        theta = theta - alpha * (<span class="number">1</span>/m) * (X.T @ (X @ theta - y))</span><br><span class="line">        J_history[i] = compute_cost(X, y, theta)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> theta, J_history</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_cost</span>(<span class="params">X, y, theta</span>):</span><br><span class="line">    m = y.size</span><br><span class="line">    J = <span class="number">1</span>/(<span class="number">2</span>*m) * np.<span class="built_in">sum</span>(np.square(X @ theta - y))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> J</span><br></pre></td></tr></tbody></table></figure><p>在这个代码中，<code>gradient_descent</code> 函数实现了梯度下降算法，<code>compute_cost</code> 函数用于计算目标函数（损失函数）的值。</p><h1 id="梯度下降算法的分类"><a href="#梯度下降算法的分类" class="headerlink" title="梯度下降算法的分类"></a>梯度下降算法的分类</h1><p>梯度下降算法是一种优化算法，用于寻找损失函数的最小值。在机器学习和深度学习中，我们通常使用梯度下降算法来优化我们的模型，即调整模型参数以最小化损失函数。以下将介绍梯度下降的几种主要变体。</p><h2 id="批量梯度下降（Batch-Gradient-Descent）"><a href="#批量梯度下降（Batch-Gradient-Descent）" class="headerlink" title="批量梯度下降（Batch Gradient Descent）"></a>批量梯度下降（Batch Gradient Descent）</h2><p>批量梯度下降是最基本的形式，它在每一次迭代中都使用全量的训练数据来计算损失函数的梯度。因此，批量梯度下降的每一步都朝着全局最优的方向。然而，这也使得批量梯度下降在大规模数据集上非常慢，且无法在线（实时）更新模型。</p><p>更新公式：</p><p>$$<br>\theta = \theta - \alpha \nabla J(\theta)<br>$$</p><p>其中，$\theta$ 是参数，$\alpha$ 是学习率，$\nabla J(\theta)$ 是损失函数 $J$ 关于参数 $\theta$ 的梯度。</p><h2 id="随机梯度下降（Stochastic-Gradient-Descent，SGD）"><a href="#随机梯度下降（Stochastic-Gradient-Descent，SGD）" class="headerlink" title="随机梯度下降（Stochastic Gradient Descent，SGD）"></a>随机梯度下降（Stochastic Gradient Descent，SGD）</h2><p>随机梯度下降在每一次迭代中随机选择一个样本来计算梯度。因此，每一步的更新方向并不一定是全局最优的方向，结果会有一些噪音。然而，这使得随机梯度下降在大规模数据集上比批量梯度下降快很多，且能够在线更新模型。</p><p>更新公式与批量梯度下降一致，只是每次只对一个随机样本进行计算。</p><h2 id="小批量梯度下降（Mini-Batch-Gradient-Descent）"><a href="#小批量梯度下降（Mini-Batch-Gradient-Descent）" class="headerlink" title="小批量梯度下降（Mini-Batch Gradient Descent）"></a>小批量梯度下降（Mini-Batch Gradient Descent）</h2><p>小批量梯度下降是批量梯度下降和随机梯度下降的折中，它在每一次迭代中使用一部分（小批量）样本来计算梯度。小批量梯度下降比随机梯度下降更稳定，同时仍然具有相对较高的计算速度。</p><p>更新公式与前面两者一致，只是每次对一个小批量的样本进行计算。</p><h1 id="梯度下降算法的进阶变体"><a href="#梯度下降算法的进阶变体" class="headerlink" title="梯度下降算法的进阶变体"></a>梯度下降算法的进阶变体</h1><p>在梯度下降的基础上，研究者们引入了一些额外的概念以改善算法的性能。以下是一些广为使用的梯度下降算法的变体。</p><h2 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h2><p>动量（Momentum）是一种帮助优化器在相关方向上保持速度，从而抑制振荡并加快收敛的策略。其核心思想是引入一个新的变量（通常称为速度），它在每一步中都会增加当前梯度，然后参数更新会按照这个速度进行。</p><p>Momentum的更新公式如下：</p><p>$$<br>v = \beta v - \alpha \nabla J(\theta)<br>$$<br>$$<br>\theta = \theta + v<br>$$</p><p>其中，$\theta$ 是参数，$\nabla J(\theta)$ 是损失函数 $J$ 关于参数 $\theta$ 的梯度，$\alpha$ 是学习率，$v$ 是速度，$\beta$ 是动量因子，通常设为0.9。</p><h2 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h2><p>AdaGrad（Adaptive Gradient Algorithm）的主要思想是为每个参数分配一个自适应的学习率，这对于稀疏数据和处理非平稳目标函数非常有用。具体来说，对于经常出现且有大梯度的参数，其学习率会被降低；反之，对于稀疏或小梯度的参数，其学习率会被提高。</p><p>AdaGrad的更新公式如下：</p><p>$$<br>G_{t} = G_{t-1} + (\nabla J(\theta))^2<br>$$<br>$$<br>\theta = \theta - \frac{\alpha}{\sqrt{G_{t} + \epsilon}} \cdot \nabla J(\theta)<br>$$</p><p>其中，$G_{t}$ 是到目前为止所有梯度的平方和，$\epsilon$ 是一个很小的数，通常设为1e-8，用于防止除零错误。</p><h2 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h2><p>RMSProp（Root Mean Square Propagation）是AdaGrad的一个改进版本，主要解决了AdaGrad在非凸设置下学习率快速下降的问题。与AdaGrad一样，RMSProp也是为每个参数分配一个自适应的学习率，但是它使用了一个滑动平均的梯度平方来更新 $G_{t}$。</p><p>RMSProp的更新公式如下：</p><p>$$<br>G_{t} = \beta G_{t-1} + (1 - \beta) (\nabla J(\theta))^2<br>$$<br>$$<br>\theta = \theta - \frac{\alpha}{\sqrt{G_{t} + \epsilon}} \cdot \nabla J(\theta)<br>$$</p><p>其中，$\beta$ 是平方梯度的滑动平均因子，通常设为0.9。</p><h2 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h2><p>Adam（Adaptive Moment Estimation）结合了Momentum和RMSProp的思想。它计算了梯度的指数滑动平均（第一矩）和平方梯度的指数滑动平均（第二矩），并使用这两个量来更新参数。</p><p>Adam的更新公式如下：</p><p>$$<br>m = \beta_{1} m + (1 - \beta_{1}) \nabla J(\theta)<br>$$<br>$$<br>v = \beta_{2} v + (1 - \beta_{2}) (\nabla J(\theta))^2<br>$$<br>$$<br>\hat{m} = \frac{m}{1 - \beta_{1}^{t}}<br>$$<br>$$<br>\hat{v} = \frac{v}{1 - \beta_{2}^{t}}<br>$$<br>$$<br>\theta = \theta - \frac{\alpha \hat{m}}{\sqrt{\hat{v}} + \epsilon}<br>$$</p><p>其中，$m$ 和 $v$ 分别是第一矩和第二矩的估计，$\beta_{1}$ 和 $\beta_{2}$ 分别是第一矩和第二矩的滑动平均因子，通常设为0.9和0.999，$t$ 是当前的迭代步数。</p><h1 id="PyTorch优化器详细介绍"><a href="#PyTorch优化器详细介绍" class="headerlink" title="PyTorch优化器详细介绍"></a>PyTorch优化器详细介绍</h1><p>PyTorch提供了一些已经实现的优化器，这些优化器都在torch.optim模块中。优化器的主要作用是更新模型的参数以最小化目标函数（通常为损失函数）。</p><h2 id="常见优化器介绍及使用"><a href="#常见优化器介绍及使用" class="headerlink" title="常见优化器介绍及使用"></a>常见优化器介绍及使用</h2><ol><li><strong>随机梯度下降（SGD）</strong></li></ol><p>SGD是最基本的优化器，它对每一个参数使用相同的学习率进行更新。这是其更新公式：</p><p>$$<br>\theta_{new} = \theta_{old} - \alpha \nabla J(\theta_{old})<br>$$</p><p>在PyTorch中，可以这样使用SGD优化器：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.1</span>)</span><br></pre></td></tr></tbody></table></figure><ol start="2"><li><strong>带动量的随机梯度下降（Momentum SGD）</strong></li></ol><p>Momentum SGD是SGD的一种改进，它在更新参数时会考虑过去的梯度，从而达到平滑更新的效果。这是其更新公式：</p><p>$$<br>v = \beta v - \alpha \nabla J(\theta)<br>$$<br>$$<br>\theta = \theta + v<br>$$</p><p>其中，$v$是动量，$\beta$是动量衰减因子。在PyTorch中，可以这样使用Momentum SGD优化器：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.1</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></tbody></table></figure><ol start="3"><li><strong>自适应梯度算法（Adagrad）</strong></li></ol><p>Adagrad是一种自适应学习率的优化器，它会对每一个参数使用不同的学习率进行更新。这使得它在处理稀疏数据时有很好的表现。这是其更新公式：</p><p>$$<br>\theta_{new} = \theta_{old} - \frac{\alpha}{\sqrt{G_{t} + \epsilon}} \cdot \nabla J(\theta_{old})<br>$$</p><p>其中，$G_{t}$是到目前为止所有梯度的平方和，$\epsilon$是一个很小的数（如1e-8）用于防止除零错误。在PyTorch中，可以这样使用Adagrad优化器：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.Adagrad(model.parameters(), lr=<span class="number">0.1</span>)</span><br></pre></td></tr></tbody></table></figure><ol start="4"><li><strong>自适应动量估计（Adam）</strong></li></ol><p>Adam结合了Momentum SGD和Adagrad的思想，它对每个参数都有一个自适应的学习率，并且会考虑过去的梯度。这使得它在许多任务上都有很好的表现。这是其更新公式：</p><p>$$<br>m = \beta_{1} m + (1 - \beta_{1}) \nabla J(\theta)<br>$$<br>$$<br>v = \beta_{2} v + (1 - \beta_{2}) (\nabla J(\theta))^2<br>$$<br>$$<br>\hat{m} = \frac{m}{1 - \beta_{1}^{t}}<br>$$<br>$$<br>\hat{v} = \frac{v}{1 - \beta_{2}^{t}}<br>$$<br>$$<br>\theta = \theta - \frac{\alpha \hat{m}}{\sqrt{\hat{v}} + \epsilon}<br>$$</p><p>其中，$m$和$v$分别是第一和第二矩估计，$\beta_{1}$和$\beta_{2}$是衰减因子（一般设为0.9和0.999），$t$是迭代次数。在PyTorch中，可以这样使用Adam优化器：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br></pre></td></tr></tbody></table></figure><h2 id="优化器的使用方法"><a href="#优化器的使用方法" class="headerlink" title="优化器的使用方法"></a>优化器的使用方法</h2><p>优化器的基本使用流程如下：</p><ol><li>定义模型（model）。</li><li>定义损失函数（loss function）。</li><li>选择优化器（optimizer），并将模型的参数和学习率传入优化器。</li><li>在训练循环中，首先清空优化器的梯度（optimizer.zero_grad()），然后计算损失（loss.backward()），最后更新模型的参数（optimizer.step()）。</li></ol><h2 id="优化器的选择场景"><a href="#优化器的选择场景" class="headerlink" title="优化器的选择场景"></a>优化器的选择场景</h2><ol><li><strong>SGD</strong>：适用于大规模线性模型，或者在训练初期快速降低损失。</li><li><strong>Momentum SGD</strong>：适用于深度学习任务，比SGD有更快的收敛速度。</li><li><strong>Adagrad</strong>：适用于处理稀疏数据的任务，如自然语言处理中的词向量训练。</li><li><strong>Adam</strong>：适用于大多数深度学习任务，是一个比较通用的优化器。</li></ol><p>选择哪种优化器并没有固定的规则，具体要根据任务的特点和数据的特性来决定。一般来说，可以先试用Adam，如果效果不佳，再考虑其他优化器。</p><h1 id="反向传播算法"><a href="#反向传播算法" class="headerlink" title="反向传播算法"></a>反向传播算法</h1><p>反向传播（Backpropagation）是神经网络中用于训练模型的主要算法之一，它是许多现代深度学习框架（如TensorFlow和PyTorch）的核心部分。以下将详细介绍反向传播的工作原理。它的主要任务是通过计算损失函数（一个衡量模型预测与真实值差异的函数）对模型参数的梯度来有效地更新网络的权重和偏置，以最小化损失函数。</p><h2 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h2><p>反向传播的目标是计算损失函数关于神经网络参数的梯度，以便使用梯度下降或其他优化算法来更新参数。为此，反向传播从输出层开始，沿着神经网络反向传递梯度。</p><p>反向传播的关键思想是链式法则（chain rule），这是微积分的一个基本定理，用于计算复合函数的导数。在神经网络的上下文中，链式法则用于计算损失函数关于参数的梯度，通过将这个复合函数拆分成一系列更简单的函数，并将它们的导数相乘。</p><h2 id="算法过程"><a href="#算法过程" class="headerlink" title="算法过程"></a>算法过程</h2><p>以下是反向传播算法的一般步骤：</p><ol><li><p><strong>前向传播</strong>：从输入层开始，通过网络向前传播数据，计算每一层的输出，直到得到最终的预测结果。</p></li><li><p><strong>计算损失</strong>：使用损失函数计算预测结果和实际目标之间的误差。</p></li><li><p><strong>反向传播损失</strong>：从输出层开始，计算损失函数关于每一层输出的梯度，并反向传播这些梯度。具体来说，对于每一层，我们首先计算损失函数关于这一层输出的梯度，然后使用链式法则，计算这个梯度关于这一层输入的梯度，以及关于这一层参数的梯度。</p></li><li><p><strong>更新参数</strong>：使用梯度下降或其他优化算法，利用在步骤3中计算出的梯度来更新网络参数。</p></li></ol><p>这个过程在每个训练迭代中重复，直到网络参数收敛，或者达到预设的最大迭代次数。</p><h2 id="计算方法"><a href="#计算方法" class="headerlink" title="计算方法"></a>计算方法</h2><p>假设我们有一个损失函数 $L$，我们想要知道它关于权重 $w_{ij}$ （表示从第 $i$ 个神经元到第 $j$ 个神经元的权重）的梯度，我们可以使用以下的公式：</p><p>$$<br>\frac{\partial L}{\partial w_{ij}} = \frac{\partial L}{\partial z_j} \cdot \frac{\partial z_j}{\partial w_{ij}}<br>$$</p><p>其中，$z_j$ 是 $j$ 个神经元的输入，它等于 $\sum_i w_{ij} x_i + b_j$。</p><p>现在我们需要找到每个部分的导数。首先，$\frac{\partial L}{\partial z_j}$ 通常直接由网络的后面部分提供。这是因为我们一般会按照从后向前的顺序计算导数，也就是说，我们首先计算出损失函数对于最后一层的输出的导数，然后用这个导数来计算损失函数对于倒数第二层的输出的导数，以此类推。</p><p>然后，我们需要找到 $\frac{\partial z_j}{\partial w_{ij}}$。由于 $z_j = \sum_i w_{ij} x_i + b_j$，我们可以看到，如果我们改变 $w_{ij}$，$z_j$ 就会按照 $x_i$ 的大小改变。所以，$\frac{\partial z_j}{\partial w_{ij}} = x_i$。</p><p>这样，我们就得到了：</p><p>$$<br>\frac{\partial L}{\partial w_{ij}} = \frac{\partial L}{\partial z_j} \cdot x_i<br>$$</p><p>让我们来看一个具体的例子。假设我们有一个简单的神经网络，只有输入层和输出层，没有隐藏层。输入层有一个神经元，其值为 $x$，输出层也有一个神经元，其值为 $y$。他们之间的权重是 $w$，偏置为 $b$。那么，$y$ 的计算公式就是 $y = wx + b$。我们用均方误差作为损失函数，也就是说，如果真实值是 $t$，那么损失函数就是 $L = (t - y)^2$。</p><p>现在，我们想要计算 $\frac{\partial L}{\partial w}$。首先，我们需要找到 $\frac{\partial L}{\partial y}$。由于 $L = (t - y)^2$，我们可以得到 $\frac{\partial L}{\partial y} = -2(t - y)$。</p><p>然后，我们需要找到 $\frac{\partial y}{\partial w}$。由于 $y = wx + b$，我们可以得到 $\frac{\partial y}{\partial w} = x$。</p><p>所以，最后我们得到：</p><p>$$<br>\frac{\partial L}{\partial w} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial w} = -2(t - y) \cdot x<br>$$</p><p>这就是我们要找的梯度，我们可以用它来更新权重 $w$，以减小损失函数。</p><h2 id="代码实现-1"><a href="#代码实现-1" class="headerlink" title="代码实现"></a>代码实现</h2><h3 id="PyyTorch中的反向传播"><a href="#PyyTorch中的反向传播" class="headerlink" title="PyyTorch中的反向传播"></a>PyyTorch中的反向传播</h3><p>以下是一个简单的反向传播算法PyTorch代码示例进行详细分步解释：</p><ol><li><strong>创建一个简单的神经网络</strong>：</li></ol><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model = nn.Sequential(</span><br><span class="line">    nn.Linear(<span class="number">10</span>, <span class="number">20</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">20</span>, <span class="number">1</span>),</span><br><span class="line">)</span><br></pre></td></tr></tbody></table></figure><p>首先，我们使用PyTorch的<code>nn.Sequential</code>来定义一个简单的神经网络。这个网络由两个线性层（<code>nn.Linear</code>）和一个ReLU激活函数（<code>nn.ReLU</code>）组成。第一个线性层将输入的大小从10变为20，ReLU激活函数增加了模型的非线性，第二个线性层将大小为20的隐藏状态映射为大小为1的输出。</p><ol start="2"><li><strong>选择损失函数和优化器</strong>：</li></ol><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">criterion = nn.MSELoss()</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br></pre></td></tr></tbody></table></figure><p>接着，我们选择一个损失函数和一个优化器。损失函数用于度量模型的预测与实际目标之间的差距，优化器用于根据损失的梯度来更新模型的参数。在这个例子中，我们选择均方误差损失（<code>nn.MSELoss</code>）作为损失函数，选择随机梯度下降（<code>torch.optim.SGD</code>）作为优化器。</p><ol start="3"><li><strong>模拟输入和目标数据</strong>：</li></ol><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">inputs = torch.randn(<span class="number">5</span>, <span class="number">10</span>)</span><br><span class="line">targets = torch.randn(<span class="number">5</span>, <span class="number">1</span>)</span><br></pre></td></tr></tbody></table></figure><p>然后，我们生成一些模拟的输入和目标数据。在这个例子中，我们生成一个形状为[5, 10]的输入张量和一个形状为[5, 1]的目标张量。</p><ol start="4"><li><strong>前向传播</strong>：</li></ol><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">outputs = model(inputs)</span><br></pre></td></tr></tbody></table></figure><p>在前向传播阶段，我们将输入数据传入模型，得到模型的预测输出。</p><ol start="5"><li><strong>计算损失</strong>：</li></ol><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = criterion(outputs, targets)</span><br></pre></td></tr></tbody></table></figure><p>接着，我们使用损失函数来计算模型的预测输出与实际目标之间的差距。</p><ol start="6"><li><strong>反向传播</strong>：</li></ol><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss.backward()</span><br></pre></td></tr></tbody></table></figure><p>在反向传播阶段，我们调用损失张量的<code>backward</code>方法，计算损失关于模型参数的梯度。这些梯度将存储在对应参数的<code>.grad</code>属性中。</p><ol start="7"><li><strong>更新参数</strong>：</li></ol><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer.step()</span><br></pre></td></tr></tbody></table></figure><p>然后，我们调用优化器的<code>step</code>方法，根据存储在模型参数的<code>.grad</code>属性中的梯度来更新参数。</p><ol start="8"><li><strong>清零梯度</strong>：</li></ol><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer.zero_grad()</span><br></pre></td></tr></tbody></table></figure><p>最后，我们调用优化器的<code>zero_grad</code>方法，将模型参数的<code>.grad</code>属性中的梯度清零。这一步是必要的，因为PyTorch默认会累加梯度，即每次调用<code>.backward</code>方法时，梯度都会累加到<code>.grad</code>属性中，而不是替换。如果不清零梯度，下一次迭代时计算的梯度将会与这一次迭代的梯度叠加，导致错误。</p><p>注意事项：以上的代码是一个完整的训练步骤，实际使用时通常需要将这个过程放入一个循环中，对整个训练数据集进行多次迭代，直到模型性能满足要求或达到预设的最大迭代次数。在每次迭代中，还可以添加一些代码来记录训练进度，如打印当前的损失值，或者在验证数据集上评估模型的性能。</p><p>完整代码如下：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个简单的神经网络</span></span><br><span class="line">model = nn.Sequential(</span><br><span class="line">    nn.Linear(<span class="number">10</span>, <span class="number">20</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">20</span>, <span class="number">1</span>),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 选择损失函数和优化器</span></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟输入和目标数据</span></span><br><span class="line">inputs = torch.randn(<span class="number">5</span>, <span class="number">10</span>)</span><br><span class="line">targets = torch.randn(<span class="number">5</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 前向传播</span></span><br><span class="line">outputs = model(inputs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算损失</span></span><br><span class="line">loss = criterion(outputs, targets)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 反向传播</span></span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 更新参数</span></span><br><span class="line">optimizer.step()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 清零梯度，为下一次迭代做准备</span></span><br><span class="line">optimizer.zero_grad()</span><br></pre></td></tr></tbody></table></figure><p>以上就是反向传播算法的基本介绍，其在深度学习训练中扮演着非常重要的角色，是理解和实现神经网络的基础知识。</p><h3 id="Python手写反向传播"><a href="#Python手写反向传播" class="headerlink" title="Python手写反向传播"></a>Python手写反向传播</h3><p>以下是一个简单的反向传播算法Python代码示例进行详细分步解释：</p><ol><li><p><strong>设定随机数种子和数据</strong>：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">X = np.array([[<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>],[<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]])</span><br><span class="line">y = np.array([[<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>]]).T</span><br></pre></td></tr></tbody></table></figure><p>这段代码首先设定了随机数种子，以确保每次运行程序时，初始化的权重值都是一样的。接着，我们设定了输入数据<code>X</code>和输出数据<code>y</code>。</p></li><li><p><strong>定义Sigmoid函数</strong>：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">x, deriv=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="keyword">if</span> deriv:</span><br><span class="line">        <span class="keyword">return</span> x*(<span class="number">1</span>-x)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+np.exp(-x))</span><br></pre></td></tr></tbody></table></figure><p>这里我们定义了Sigmoid函数，该函数用于激活神经元的输出。在参数<code>deriv</code>为True时，该函数返回Sigmoid函数的导数，这对于反向传播计算梯度非常重要。</p></li><li><p><strong>初始化权重</strong>：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">w0 = <span class="number">2</span>*np.random.random((<span class="number">3</span>,<span class="number">4</span>)) - <span class="number">1</span></span><br><span class="line">w1 = <span class="number">2</span>*np.random.random((<span class="number">4</span>,<span class="number">1</span>)) - <span class="number">1</span></span><br></pre></td></tr></tbody></table></figure><p>在这里，我们初始化了权重<code>w0</code>和<code>w1</code>。我们的网络有两层，所以需要两组权重。这些权重的初始化值是在-1到1之间随机选择的。</p></li><li><p><strong>迭代训练</strong>：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">60000</span>):</span><br></pre></td></tr></tbody></table></figure><p>这是我们的训练循环，我们训练网络60000次。</p></li><li><p><strong>前向传播</strong>：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">l0 = X</span><br><span class="line">l1 = sigmoid(np.dot(l0, w0))</span><br><span class="line">l2 = sigmoid(np.dot(l1, w1))</span><br></pre></td></tr></tbody></table></figure><p>这是我们的前向传播步骤。首先，我们的输入<code>l0</code>就是数据<code>X</code>。然后，我们计算<code>l1</code>层，这是<code>l0</code>和<code>w0</code>的点积通过Sigmoid函数的结果。同样，我们计算<code>l2</code>，这是<code>l1</code>和<code>w1</code>的点积通过Sigmoid函数的结果。</p></li><li><p><strong>计算误差</strong>：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">l2_loss = y - l2</span><br></pre></td></tr></tbody></table></figure><p>在这里，我们计算了网络预测的误差，这就是实际值<code>y</code>减去预测值<code>l2</code>。</p></li><li><p><strong>打印误差</strong>：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> j % <span class="number">10000</span> == <span class="number">0</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f'Loss: <span class="subst">{np.mean(np.<span class="built_in">abs</span>(l2_loss))}</span>'</span>)</span><br></pre></td></tr></tbody></table></figure><p>每10000次迭代，我们计算并打印一次平均误差。</p></li><li><p><strong>反向传播</strong>：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">l2_delta = l2_loss * sigmoid(l2, deriv=<span class="literal">True</span>)</span><br><span class="line">l1_loss = l2_delta.dot(w1.T)</span><br><span class="line">l1_delta = l1_loss * sigmoid(l1, deriv=<span class="literal">True</span>)</span><br></pre></td></tr></tbody></table></figure><p>这是反向传播的步骤。我们首先计算<code>l2_delta</code>，这是<code>l2_loss</code>和<code>l2</code>通过Sigmoid导数函数的结果。然后，我们计算<code>l1_loss</code>，这是<code>l2_delta</code>和<code>w1</code>的转置的点积。最后，我们计算<code>l1_delta</code>，这是<code>l1_loss</code>和<code>l1</code>通过Sigmoid导数函数的结果。</p></li><li><p><strong>更新权重</strong>：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">w1 += l1.T.dot(l2_delta)</span><br><span class="line">w0 += l0.T.dot(l1_delta)</span><br></pre></td></tr></tbody></table></figure><p>在这里，我们根据反向传播的结果更新权重。<code>w1</code>增加的是<code>l1</code>的转置和<code>l2_delta</code>的点积，<code>w0</code>增加的是<code>l0</code>的转置和<code>l1_delta</code>的点积。</p></li></ol><p>这就是整个网络的训练过程，包括前向传播、计算误差、反向传播和更新权重。</p>]]></content>
      
      
      <categories>
          
          <category> NLP Insights </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Gradient Descent </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo博客添加可交互式足迹地图</title>
      <link href="/2023/07/08/Tech%20Toolbox/GeoMapToBlog/"/>
      <url>/2023/07/08/Tech%20Toolbox/GeoMapToBlog/</url>
      
        <content type="html"><![CDATA[<h1 id="在Blog中添加可交互式足迹地图"><a href="#在Blog中添加可交互式足迹地图" class="headerlink" title="在Blog中添加可交互式足迹地图"></a>在Blog中添加可交互式足迹地图</h1><p>这篇文章将向你展示如何在基于Hexo和Next的GitHub Pages博客中创建一个交互式的世界地图页面，这个地图将展示你曾经访问过的城市，你可以根据你对每个城市访问的频率在地图上显示不同颜色的标记，你还可以点击这些标记来显示更多关于这个城市的信息。</p><h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><p>你需要安装以下工具：</p><ul><li>Node.js 和 NPM</li><li>Hexo</li></ul><p>确保你的博客已经被部署到GitHub Pages，并且你在本地的开发环境已经正确设置。</p><h2 id="步骤一：创建新页面"><a href="#步骤一：创建新页面" class="headerlink" title="步骤一：创建新页面"></a>步骤一：创建新页面</h2><p>在你的Hexo项目的根目录下，运行以下命令：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new page <span class="string">"travel"</span></span><br></pre></td></tr></tbody></table></figure><p>这个命令将在<code>source</code>目录下创建一个名为”travel”的文件夹，并在该文件夹下创建一个<code>index.md</code>文件。</p><h2 id="步骤二：安装-Leaflet"><a href="#步骤二：安装-Leaflet" class="headerlink" title="步骤二：安装 Leaflet"></a>步骤二：安装 Leaflet</h2><p>在你的Hexo项目的根目录下，运行以下命令：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install leaflet</span><br></pre></td></tr></tbody></table></figure><p>然后在<code>index.md</code>文件的最顶部引入Leaflet的CSS和JS：</p><figure class="highlight markdown"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line"><span class="section">title: Travel</span></span><br><span class="line"><span class="section">---</span></span><br><span class="line"></span><br><span class="line">&lt;!-- 引入 Leaflet 的 CSS --&gt;</span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">link</span> <span class="attr">rel</span>=<span class="string">"stylesheet"</span> <span class="attr">href</span>=<span class="string">"/node_modules/leaflet/dist/leaflet.css"</span> /&gt;</span></span></span><br><span class="line"></span><br><span class="line">&lt;!-- 引入 Leaflet 的 JavaScript 文件 --&gt;</span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">"/node_modules/leaflet/dist/leaflet.js"</span>&gt;</span></span><span class="language-xml"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span></span><br></pre></td></tr></tbody></table></figure><h2 id="步骤三：创建地图容器"><a href="#步骤三：创建地图容器" class="headerlink" title="步骤三：创建地图容器"></a>步骤三：创建地图容器</h2><p>在<code>index.md</code>中，创建一个用来承载地图的<code>&lt;div&gt;</code>元素，然后通过CSS给它设置一个明确的高度：</p><figure class="highlight markdown"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">style</span>&gt;</span></span></span><br><span class="line"><span class="code">    #map {</span></span><br><span class="line"><span class="code">        height: 500px;</span></span><br><span class="line"><span class="code">        width: 100%;</span></span><br><span class="line"><span class="code">        position: relative;</span></span><br><span class="line"><span class="code">    }</span></span><br><span class="line"><span class="code">&lt;/style&gt;</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="language-xml"><span class="language-xml"><span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">"map"</span>&gt;</span></span></span><span class="language-xml"><span class="language-css">&lt;/<span class="selector-tag">div</span>&gt;</span></span></span><br></pre></td></tr></tbody></table></figure><h2 id="步骤四：初始化地图"><a href="#步骤四：初始化地图" class="headerlink" title="步骤四：初始化地图"></a>步骤四：初始化地图</h2><p>使用Leaflet的API初始化地图，并将其中心设置为经度0,纬度0（大西洋中部），并设置初始的缩放级别为2。</p><figure class="highlight markdown"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">script</span>&gt;</span></span></span><br><span class="line"><span class="code">    document.addEventListener('DOMContentLoaded', function() {</span></span><br><span class="line"><span class="code">        var map = L.map('map').setView([0, 0], 2);</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code">        L.tileLayer('https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png', {</span></span><br><span class="line"><span class="code">            maxZoom: 19,</span></span><br><span class="line"><span class="code">        }).addTo(map);</span></span><br><span class="line"><span class="code">    });</span></span><br><span class="line"><span class="code">&lt;/script&gt;</span></span><br></pre></td></tr></tbody></table></figure><h2 id="步骤五：添加城市数据"><a href="#步骤五：添加城市数据" class="headerlink" title="步骤五：添加城市数据"></a>步骤五：添加城市数据</h2><p>创建一个GeoJSON文件来存储城市的数据，包括城市的名称，经纬度，你访问的次数，以及一个图片的URL。文件的格式应该如下：</p><figure class="highlight json"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">{</span></span><br><span class="line">    <span class="attr">"type"</span><span class="punctuation">:</span> <span class="string">"FeatureCollection"</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">"features"</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">        <span class="punctuation">{</span></span><br><span class="line">            <span class="attr">"type"</span><span class="punctuation">:</span> <span class="string">"Feature"</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">"properties"</span><span class="punctuation">:</span> <span class="punctuation">{</span></span><br><span class="line">                <span class="attr">"name"</span><span class="punctuation">:</span> <span class="string">"北京"</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">"visits"</span><span class="punctuation">:</span> <span class="number">3</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">"image"</span><span class="punctuation">:</span> <span class="string">"https://your-image-url.com"</span></span><br><span class="line">            <span class="punctuation">}</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">"geometry"</span><span class="punctuation">:</span> <span class="punctuation">{</span></span><br><span class="line">                <span class="attr">"type"</span><span class="punctuation">:</span> <span class="string">"Point"</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">"coordinates"</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="number">116.4074</span><span class="punctuation">,</span> <span class="number">39.9042</span><span class="punctuation">]</span></span><br><span class="line">            <span class="punctuation">}</span></span><br><span class="line">        <span class="punctuation">}</span><span class="punctuation">,</span></span><br><span class="line">        ...</span><br><span class="line">    <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">}</span></span><br></pre></td></tr></tbody></table></figure><p>将此文件保存在你的Hexo项目的<code>source</code>目录下，例如，你可以命名为<code>cities.json</code>。</p><h2 id="步骤六：加载数据并显示在地图上"><a href="#步骤六：加载数据并显示在地图上" class="headerlink" title="步骤六：加载数据并显示在地图上"></a>步骤六：加载数据并显示在地图上</h2><p>在<code>index.md</code>文件中，添加以下代码来加载GeoJSON文件，然后在地图上显示数据：</p><figure class="highlight markdown"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">script</span>&gt;</span></span></span><br><span class="line"><span class="code">    document.addEventListener('DOMContentLoaded', function() {</span></span><br><span class="line"><span class="code">        var map = L.map('map').setView([0, 0], 2);</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code">        L.tileLayer('https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png', {</span></span><br><span class="line"><span class="code">            maxZoom: 19,</span></span><br><span class="line"><span class="code">        }).addTo(map);</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code">        fetch('/travel/cities.json')</span></span><br><span class="line"><span class="code">            .then(response =&gt; response.json())</span></span><br><span class="line"><span class="code">            .then(data =&gt; {</span></span><br><span class="line"><span class="code">                L.geoJSON(data, {</span></span><br><span class="line"><span class="code">                    pointToLayer: function (feature, latlng) {</span></span><br><span class="line"><span class="code">                        var color = getColor(feature.properties.visits);</span></span><br><span class="line"><span class="code">                        return L.circleMarker(latlng, { fillColor: color, fillOpacity: 0.5 });</span></span><br><span class="line"><span class="code">                    },</span></span><br><span class="line"><span class="code">                    onEachFeature: function (feature, layer) {</span></span><br><span class="line"><span class="code">                        layer.bindPopup(`&lt;h2&gt;${feature.properties.name}&lt;/h2&gt;&lt;img src="${feature.properties.image}" width="200"&gt;`);</span></span><br><span class="line"><span class="code">                    }</span></span><br><span class="line"><span class="code">                }).addTo(map);</span></span><br><span class="line"><span class="code">            });</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code">        function getColor(visits) {</span></span><br><span class="line"><span class="code">            // 根据你访问的次数返回不同的颜色</span></span><br><span class="line"><span class="code">            return visits &gt; 10 ? '#800026' :</span></span><br><span class="line"><span class="code">                   visits &gt; 5  ? '#BD0026' :</span></span><br><span class="line"><span class="code">                   visits &gt; 2  ? '#E31A1C' :</span></span><br><span class="line"><span class="code">                   visits &gt; 1  ? '#FC4E2A' :</span></span><br><span class="line"><span class="code">                                 '#FFEDA0';</span></span><br><span class="line"><span class="code">        }</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code">        // 强制Leaflet重新计算地图的尺寸</span></span><br><span class="line"><span class="code">        setTimeout(function() {</span></span><br><span class="line"><span class="code">            map.invalidateSize();</span></span><br><span class="line"><span class="code">        }, 100);</span></span><br><span class="line"><span class="code">    });</span></span><br><span class="line"><span class="code">&lt;/script&gt;</span></span><br></pre></td></tr></tbody></table></figure><p>以上的JavaScript代码在页面加载完成后会运行，首先初始化地图，然后加载并解析GeoJSON文件，对于每一个城市，创建一个圆形的标记，颜色根据访问次数决定。当点击标记时，弹出一个包含城市名称和图片的窗口。</p><p>至此，你的博客就成功地添加了一个交互式的足迹地图页面。</p>]]></content>
      
      
      <categories>
          
          <category> Tech Toolbox </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Blog </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SUP Experience</title>
      <link href="/2023/07/07/Life%20Reflections/SUP%E4%BD%93%E9%AA%8C.en/"/>
      <url>/2023/07/07/Life%20Reflections/SUP%E4%BD%93%E9%AA%8C.en/</url>
      
        <content type="html"><![CDATA[<h1 id="Stand-Up-Paddleboarding-SUP"><a href="#Stand-Up-Paddleboarding-SUP" class="headerlink" title="Stand-Up Paddleboarding (SUP)"></a>Stand-Up Paddleboarding (SUP)</h1><p>Stand-Up Paddleboarding (SUP) is a water activity that originated from modern surfing in Hawaii. Surfers stand on a floating board and use a single paddle to propel themselves forward in the water. This sport has rapidly gained popularity worldwide in recent years and has become a beloved water activity for many people.</p><h2 id="Experience-🌊"><a href="#Experience-🌊" class="headerlink" title="Experience 🌊"></a>Experience 🌊</h2><p>If you are interested in stand-up paddleboarding, consider taking a dedicated course to learn and experience this activity. I recently participated in a stand-up paddleboarding course located on the East Coast of Singapore, and here is my experience sharing.</p><p>The class lasted for two hours, and the coach provided a detailed introduction to the basic knowledge and skills of SUP. They explained how to stand on the board and maintain balance, how to use the paddle correctly to propel oneself forward in the water, and how to turn and control the direction of the board. Although I felt a bit unstable at first, I quickly adapted to this standing posture and was able to easily master the skill of using the paddle.</p>  <style type="text/css"><pre><code>.fancybox &#123;    display: inline-block;&#125;</code></pre><p></style><p></p><p>The curriculum arrangement is very reasonable and suitable for beginners. With the guidance of the instructor, I was able to quickly get the hang of it, and only fell into the water once 😅, which made me feel a little embarrassed. However, SUP is a very fun and challenging activity, and falling into the water is also part of the learning process. I believe that with practice, I will become more skilled and stable.</p><p>The courses take place in the open waters of the East Coast, which means we can freely surf in the vast ocean. This open environment makes people feel very comfortable and free. However, we also need to pay attention to sun protection ☀️, as prolonged exposure to sunlight can damage the skin.</p><h2 id="Registration-Information-📝"><a href="#Registration-Information-📝" class="headerlink" title="Registration Information 📝"></a>Registration Information 📝</h2><p>If you are interested in the Stand-Up Paddleboarding (SUP) Starter Course, you can register at the following website: <a href="https://www.onepa.gov.sg/courses/sup-starter-course-open-water-c026977237">https://www.onepa.gov.sg/courses/sup-starter-course-open-water-c026977237</a>. This course is offered by PAssion WaVe @ East Coast, an organization in Singapore. The registration fee is SGD 61 💰.</p><p>Before registration, you need to meet certain course requirements.</p><p>You need to have basic swimming skills 🏊‍♂️ and be able to swim at least 50 meters in the ocean while wearing a life jacket. This is to ensure your safety during water activities.</p><p>The course venue is located at PAssion WaVe @ East Coast in Singapore, specifically at the address 1390 ECP, Singapore 468961 📍. This location is perfect for stand-up paddleboarding as it offers an open sea area and suitable conditions.</p><p>Vertical single-blade surfing is an exciting and exhilarating water activity that is a great choice for both surfing enthusiasts and beginners. By taking part in a course, you will be able to learn the correct techniques and safety knowledge, and enjoy the thrill of surfing while creating wonderful memories in the ocean. If you are interested in this activity, why not consider signing up for a course and start your journey of vertical single-blade surfing! 🏄‍♂️🌊</p>]]></content>
      
      
      <categories>
          
          <category> Life Reflections </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Guide to Living on Singapore Island </tag>
            
            <tag> Sports </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SUP体验</title>
      <link href="/2023/07/07/Life%20Reflections/SUP%E4%BD%93%E9%AA%8C/"/>
      <url>/2023/07/07/Life%20Reflections/SUP%E4%BD%93%E9%AA%8C/</url>
      
        <content type="html"><![CDATA[<h1 id="立式单桨冲浪-SUP"><a href="#立式单桨冲浪-SUP" class="headerlink" title="立式单桨冲浪 (SUP)"></a>立式单桨冲浪 (SUP)</h1><p>立式单桨冲浪 (Stand-Up Paddleboarding, 简称SUP) 是一项起源于夏威夷现代冲浪运动的水上活动。冲浪者站在漂浮在水面上的木板上，通过使用单桨来推动自己在水中前进。这项运动近年来在世界各地迅速流行起来，成为许多人喜爱的水上活动之一。</p><h2 id="体验-🌊"><a href="#体验-🌊" class="headerlink" title="体验 🌊"></a>体验 🌊</h2><p>如果你对立式单桨冲浪感兴趣，不妨考虑参加一个专门的课程来了解和体验这项活动。我最近参加了一个位于新加坡东海岸的立式单桨冲浪课程，以下是我的体验分享。</p><p>课程持续了两个小时，教练非常详细地介绍了SUP的基本知识和技巧。他们解释了如何站在板上保持平衡，正确使用桨来推动自己在水中前进，以及如何转向和控制板的方向。虽然一开始我感到有些不稳定，但很快就适应了这种站立的姿势，并且能够轻松地掌握桨的使用技巧。</p><style type="text/css">    .fancybox {        display: inline-block;    }</style><img src="/2023/07/07/Life%20Reflections/SUP%E4%BD%93%E9%AA%8C/Me.jpg" class=""><img src="/2023/07/07/Life%20Reflections/SUP%E4%BD%93%E9%AA%8C/certification.jpg" class=""><p>课程的安排非常合理，适合初学者。在教练的指导下，我能够很快上手，并且只掉进海里一次 😅，这让我感到有些尴尬。不过，SUP是一项非常有趣和挑战性的活动，掉进水里也是学习过程中的一部分。我相信随着练习的积累，我能够变得更加熟练和稳定。</p><p>课程在东海岸的开放海域进行，这意味着我们可以在广阔的海洋中自由冲浪。这种开放的环境让人感到非常舒适和自由。然而，我们也需要注意防晒 ☀️，因为长时间暴露在阳光下可能会对皮肤造成伤害。</p><h2 id="报名信息-📝"><a href="#报名信息-📝" class="headerlink" title="报名信息 📝"></a>报名信息 📝</h2><p>如果你对立式单桨冲浪课程感兴趣，你可以在以下网址报名：<a href="https://www.onepa.gov.sg/courses/sup-starter-course-open-water-c026977237">https://www.onepa.gov.sg/courses/sup-starter-course-open-water-c026977237</a>。这是新加坡一家名为PAssion WaVe @ East Coast的机构提供的课程。报名费用为61新币 💰。</p><p>在报名之前，需要满足一些课程要求</p><p>。你需要具备基本的游泳能力 🏊‍♂️，并且可以在穿着救生衣的情况下在海里游泳至少50米。这是为了确保你在水上活动中的安全。</p><p>课程地点位于新加坡东海岸的PAssion WaVe @ East Coast，具体地址是1390 ECP, Singapore 468961 📍。这个地点非常适合进行立式单桨冲浪，因为它提供了开放的海域和适宜的条件。</p><p>立式单桨冲浪是一项令人兴奋和刺激的水上活动，无论是对于冲浪爱好者还是初学者来说，都是一个很好的选择。通过参加课程，你将能够学习到正确的技巧和安全知识，享受冲浪的乐趣，并在海洋中留下美好的回忆。如果你对这项活动感兴趣，不妨考虑报名参加一个课程，开始你的立式单桨冲浪之旅吧！🏄‍♂️🌊</p>]]></content>
      
      
      <categories>
          
          <category> Life Reflections </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Sports </tag>
            
            <tag> 坡岛生活指北 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SUP体验</title>
      <link href="/2023/07/07/Life%20Reflections/SUP%E4%BD%93%E9%AA%8C.zh-CN/"/>
      <url>/2023/07/07/Life%20Reflections/SUP%E4%BD%93%E9%AA%8C.zh-CN/</url>
      
        <content type="html"><![CDATA[<h1 id="立式单桨冲浪-SUP"><a href="#立式单桨冲浪-SUP" class="headerlink" title="立式单桨冲浪 (SUP)"></a>立式单桨冲浪 (SUP)</h1><p>立式单桨冲浪 (Stand-Up Paddleboarding, 简称SUP) 是一项起源于夏威夷现代冲浪运动的水上活动。冲浪者站在漂浮在水面上的木板上，通过使用单桨来推动自己在水中前进。这项运动近年来在世界各地迅速流行起来，成为许多人喜爱的水上活动之一。</p><h2 id="体验-🌊"><a href="#体验-🌊" class="headerlink" title="体验 🌊"></a>体验 🌊</h2><p>如果你对立式单桨冲浪感兴趣，不妨考虑参加一个专门的课程来了解和体验这项活动。我最近参加了一个位于新加坡东海岸的立式单桨冲浪课程，以下是我的体验分享。</p><p>课程持续了两个小时，教练非常详细地介绍了SUP的基本知识和技巧。他们解释了如何站在板上保持平衡，正确使用桨来推动自己在水中前进，以及如何转向和控制板的方向。虽然一开始我感到有些不稳定，但很快就适应了这种站立的姿势，并且能够轻松地掌握桨的使用技巧。</p><style type="text/css">    .fancybox {        display: inline-block;    }</style><p>课程的安排非常合理，适合初学者。在教练的指导下，我能够很快上手，并且只掉进海里一次 😅，这让我感到有些尴尬。不过，SUP是一项非常有趣和挑战性的活动，掉进水里也是学习过程中的一部分。我相信随着练习的积累，我能够变得更加熟练和稳定。</p><p>课程在东海岸的开放海域进行，这意味着我们可以在广阔的海洋中自由冲浪。这种开放的环境让人感到非常舒适和自由。然而，我们也需要注意防晒 ☀️，因为长时间暴露在阳光下可能会对皮肤造成伤害。</p><h2 id="报名信息-📝"><a href="#报名信息-📝" class="headerlink" title="报名信息 📝"></a>报名信息 📝</h2><p>如果你对立式单桨冲浪课程感兴趣，你可以在以下网址报名：<a href="https://www.onepa.gov.sg/courses/sup-starter-course-open-water-c026977237">https://www.onepa.gov.sg/courses/sup-starter-course-open-water-c026977237</a>。这是新加坡一家名为PAssion WaVe @ East Coast的机构提供的课程。报名费用为61新币 💰。</p><p>在报名之前，需要满足一些课程要求</p><p>。你需要具备基本的游泳能力 🏊‍♂️，并且可以在穿着救生衣的情况下在海里游泳至少50米。这是为了确保你在水上活动中的安全。</p><p>课程地点位于新加坡东海岸的PAssion WaVe @ East Coast，具体地址是1390 ECP, Singapore 468961 📍。这个地点非常适合进行立式单桨冲浪，因为它提供了开放的海域和适宜的条件。</p><p>立式单桨冲浪是一项令人兴奋和刺激的水上活动，无论是对于冲浪爱好者还是初学者来说，都是一个很好的选择。通过参加课程，你将能够学习到正确的技巧和安全知识，享受冲浪的乐趣，并在海洋中留下美好的回忆。如果你对这项活动感兴趣，不妨考虑报名参加一个课程，开始你的立式单桨冲浪之旅吧！🏄‍♂️🌊</p>]]></content>
      
      
      <categories>
          
          <category> Life Reflections </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Sports </tag>
            
            <tag> 坡岛生活指北 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DSSM 模型详解</title>
      <link href="/2023/07/07/NLP%20Insights/DSSM%20%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/"/>
      <url>/2023/07/07/NLP%20Insights/DSSM%20%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<h1 id="DSSM-Deep-Structured-Semantic-Models-模型详解"><a href="#DSSM-Deep-Structured-Semantic-Models-模型详解" class="headerlink" title="DSSM (Deep Structured Semantic Models) 模型详解"></a>DSSM (Deep Structured Semantic Models) 模型详解</h1><h2 id="一、模型简介"><a href="#一、模型简介" class="headerlink" title="一、模型简介"></a>一、模型简介</h2><p>DSSM（Deep Structured Semantic Models）是微软提出的一个深度学习模型，用于学习文本的语义表达。DSSM首次在信息检索领域中被提出，用于处理查询-文档匹配任务，但很快被应用到了各种其他场景，如广告点击率预测，推荐系统等。</p><h2 id="二、模型结构"><a href="#二、模型结构" class="headerlink" title="二、模型结构"></a>二、模型结构</h2><p>DSSM模型主要由三个部分构成：</p><ol><li><strong>输入层</strong>：在这一层，将输入的文本（查询或者文档）转化为词向量。</li><li><strong>深度神经网络</strong>：输入层之后是一系列全连接层，它们学习输入的语义表示。</li><li><strong>输出层</strong>：最后，输出层将深度神经网络的输出转化为概率分布，用于表示查询与文档之间的语义匹配度。</li></ol><h2 id="三、在推荐系统中的应用"><a href="#三、在推荐系统中的应用" class="headerlink" title="三、在推荐系统中的应用"></a>三、在推荐系统中的应用</h2><p>DSSM可应用在推荐系统中，它可以学习用户的行为特征与物品特征的语义匹配度，用于评估用户对物品的兴趣。在实际应用中，通常将用户行为序列作为查询，将候选物品的特征作为文档，通过DSSM学习用户的实时兴趣，并将兴趣与物品的匹配度用于排序。</p><h2 id="四、模型实现"><a href="#四、模型实现" class="headerlink" title="四、模型实现"></a>四、模型实现</h2><p>以下是一个使用PyTorch实现的DSSM模型的示例：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DSSM</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, hidden_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(DSSM, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.vocab_size = vocab_size</span><br><span class="line">        <span class="variable language_">self</span>.hidden_size = hidden_size</span><br><span class="line">        <span class="variable language_">self</span>.linear1 = nn.Linear(vocab_size, hidden_size)</span><br><span class="line">        <span class="variable language_">self</span>.linear2 = nn.Linear(hidden_size, hidden_size)</span><br><span class="line">        <span class="variable language_">self</span>.linear3 = nn.Linear(hidden_size, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, query, doc</span>):</span><br><span class="line">        query = <span class="variable language_">self</span>.linear1(query)</span><br><span class="line">        query = torch.relu(query)</span><br><span class="line">        query = <span class="variable language_">self</span>.linear2(query)</span><br><span class="line">        query = torch.relu(query)</span><br><span class="line"></span><br><span class="line">        doc = <span class="variable language_">self</span>.linear1(doc)</span><br><span class="line">        doc = torch.relu(doc)</span><br><span class="line">        doc = <span class="variable language_">self</span>.linear2(doc)</span><br><span class="line">        doc = torch.relu(doc)</span><br><span class="line"></span><br><span class="line">        out = <span class="variable language_">self</span>.linear3(query * doc)</span><br><span class="line">        out = torch.sigmoid(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></tbody></table></figure><h2 id="五、训练过程"><a href="#五、训练过程" class="headerlink" title="五、训练过程"></a>五、训练过程</h2><p>在训练DSSM模型时，我们通常会采用pairwise的训练方式，也就是利用正样本（正匹配对）和负样本（负匹配对）进行训练。在信息检索任务中，一个正样本可能是一个查询和一个相关的文档，负样本可能是同一个查询和一个不相关的文档。</p><p>对于每个训练样本，都将经过DSSM模型，得到查询和文档的向量表示，然后计算两者的余弦相似度作为预测的匹配分数。接下来，使用一个合适的损失函数，例如交叉熵损失，来优化模型参数。</p><p>以下是一个使用PyTorch训练DSSM模型的示例：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化模型</span></span><br><span class="line">model = DSSM(vocab_size=<span class="number">10000</span>, hidden_size=<span class="number">128</span>)</span><br><span class="line"><span class="comment"># 使用Adam优化器</span></span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line"><span class="comment"># 使用二元交叉熵作为损失函数</span></span><br><span class="line">criterion = torch.nn.BCELoss()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader, <span class="number">0</span>):</span><br><span class="line">        query, doc, label = data</span><br><span class="line">        <span class="comment"># 清零梯度</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        <span class="comment"># 前向传播</span></span><br><span class="line">        output = model(query, doc)</span><br><span class="line">        <span class="comment"># 计算损失</span></span><br><span class="line">        loss = criterion(output, label)</span><br><span class="line">        <span class="comment"># 反向传播</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        <span class="comment"># 更新权重</span></span><br><span class="line">        optimizer.step()</span><br></pre></td></tr></tbody></table></figure><p>以上代码中，首先初始化了一个DSSM模型，并定义了一个优化器和损失函数。在每个训练步骤中，我们对数据进行前向传播，计算损失，然后进行反向传播和优化。</p><p>在训练过程中，可以在验证集上评估模型的效果，并根据需要调整学习率和其他训练参数。训练结束后，通常会在测试集上对模型进行最后的评估，以确保模型能够很好地泛化到未见过的数据。</p><p>训练DSSM模型的主要挑战是选择合适的正样本和负样本，以及设置合理的训练参数。在实际应用中，可能需要使用一些策略来平衡正负样本的比例，或者使用更复杂的损失函数来处理不均衡的数据。</p><h2 id="六、线上推理方法"><a href="#六、线上推理方法" class="headerlink" title="六、线上推理方法"></a>六、线上推理方法</h2><p>DSSM模型在线上推理主要有以下两个步骤：</p><ol><li><strong>向量化</strong>：在模型训练完毕后，可以先对所有的物品进行向量化处理，保存为物品的向量表示。当新的用户请求到来时，将用户的行为序列转化为向量表示。</li><li><strong>计算相似度</strong>：然后，计算用户向量与各个物品向量的相似度，一般使用余弦相似度作为计算方法。相似度越高，表示用户对物品的兴趣越大。</li></ol><p>具体实施时，需要注意的是，为了提高在线推理的效率，通常会采用一些近似最近邻搜索的技术（如Faiss等）来快速找到与用户最相似的物品，而不是遍历所有的物品。</p><p>总的来说，DSSM是一个非常有效的深度学习模型，用于学习文本或其他类型数据的语义表示，广泛应用于信息检索、推荐系统等多种场景中。</p>]]></content>
      
      
      <categories>
          
          <category> NLP Insights </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DSSM </tag>
            
            <tag> Recommendation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>推荐系统离线评估指标详解</title>
      <link href="/2023/07/07/NLP%20Insights/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%A6%BB%E7%BA%BF%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87%E8%AF%A6%E8%A7%A3/"/>
      <url>/2023/07/07/NLP%20Insights/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%A6%BB%E7%BA%BF%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87%E8%AF%A6%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<h1 id="推荐系统离线评估指标详解"><a href="#推荐系统离线评估指标详解" class="headerlink" title="推荐系统离线评估指标详解"></a>推荐系统离线评估指标详解</h1><p>本文将详细介绍推荐系统中常用的离线评估指标，包括精确率（Precision）、召回率（Recall）、准确率（Accuracy）、F1-Score、NDCG、命中率（Hit Rate）、AUC、GAUC和对数损失（Log Loss）。这些指标对于评估推荐系统的性能和效果至关重要。这些指标对于评估推荐系统的性能和效果至关重要。</p><p>在推荐系统中，精确率衡量了推荐列表中真正符合用户兴趣的物品比例，召回率衡量了所有符合用户兴趣的物品中被成功推荐出的比例。准确率用于衡量模型预测结果与实际结果一致的比例。F1-Score综合考虑了精确率和召回率，对模型进行综合评价。NDCG则用于评价推荐系统排序质量，特别适用于考虑元素相关性排序的推荐系统。</p><p>我们将为每个指标提供详细的解释和计算公式，并给出Python实现的示例代码。这些指标的适用性将根据推荐系统的需求进行评估，以帮助您选择适合自己系统评估的指标。</p><p>通过深入了解这些离线评估指标，您将能够更好地评估和改进您的推荐系统，提供更准确和个性化的推荐服务。</p><h2 id="1-精确率-Precision"><a href="#1-精确率-Precision" class="headerlink" title="1. 精确率 (Precision)"></a>1. 精确率 (Precision)</h2><h3 id="功能"><a href="#功能" class="headerlink" title="功能"></a>功能</h3><p>精确率用于度量分类模型的准确程度，即模型预测为正类别的样本中实际为正类别的比例。在推荐系统中，精确率可以理解为在用户接收的推荐物品中，真正符合用户兴趣的物品比例。</p><h3 id="计算公式"><a href="#计算公式" class="headerlink" title="计算公式"></a>计算公式</h3><p>在二分类问题中，精确率的计算公式为：</p><p>$$ Precision = \frac{TP}{TP+FP} $$</p><p>其中，TP表示真正例（True Positives），FP表示假正例（False Positives）。</p><h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><p>假设一个推荐系统推荐出了10个商品，其中5个商品是用户真正感兴趣的。那么精确率为：5 / 10 = 0.5。</p><h3 id="Python-实现"><a href="#Python-实现" class="headerlink" title="Python 实现"></a>Python 实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># y_true 表示真实标签，y_pred 表示预测标签</span></span><br><span class="line">y_true = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">y_pred = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">precision = precision_score(y_true, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f'Precision: <span class="subst">{precision}</span>'</span>)</span><br></pre></td></tr></tbody></table></figure><h3 id="适用性"><a href="#适用性" class="headerlink" title="适用性"></a>适用性</h3><p>⭐️⭐️⭐️⭐️⭐️</p><p>精确率非常适用于推荐系统的评估。推荐系统的主要目标就是准确地推荐用户可能感兴趣的商品或服务。如果推荐的商品中大部分都是用户感兴趣的，那么精确率就高。</p><hr><h2 id="2-召回率-Recall"><a href="#2-召回率-Recall" class="headerlink" title="2. 召回率 (Recall)"></a>2. 召回率 (Recall)</h2><h3 id="功能-1"><a href="#功能-1" class="headerlink" title="功能"></a>功能</h3><p>召回率用于度量分类模型覆盖正类样本的能力，即在所有实际为正类别的样本中，模型预测为正类别的比例。在推荐系统中，召回率可以理解为所有符合用户兴趣的物品中，被模型成功推荐出的物品比例。</p><h3 id="计算公式-1"><a href="#计算公式-1" class="headerlink" title="计算公式"></a>计算公式</h3><p>在二分类问题中，召回率的计算公式为：</p><p>$$ Recall = \frac{TP}{TP+FN} $$</p><p>其中，TP表示真正例（True Positives），FN表示假反例（False Negatives）。</p><h3 id="示例-1"><a href="#示例-1" class="headerlink" title="示例"></a>示例</h3><p>假设一个用户真正感兴趣的商品有20个，推荐系统成功推荐出了10个。那么召回率为：10 / 20 = 0.5。</p><h3 id="Python-实现-1"><a href="#Python-实现-1" class="headerlink" title="Python 实现"></a>Python 实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> recall_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># y_true 表示真实标签，y_pred 表示预测标签</span></span><br><span class="line">y_true = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">y_pred = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">recall = recall_score(y_true, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f'Recall: <span class="subst">{recall}</span>'</span>)</span><br></pre></td></tr></tbody></table></figure><h3 id="适用性-1"><a href="#适用性-1" class="headerlink" title="适用性"></a>适用性</h3><p>⭐️⭐️⭐️⭐️⭐️</p><p>召回率也非常适用于推荐系统的评估。推荐系统的目标之一是覆盖尽可能多的用户感兴趣的商品或服务。如果能将用户感兴趣的商品大部分都推荐出来，那么召回率就高。</p><hr><h2 id="3-准确率-Accuracy"><a href="#3-准确率-Accuracy" class="headerlink" title="3. 准确率 (Accuracy)"></a>3. 准确率 (Accuracy)</h2><h3 id="功能-2"><a href="#功能-2" class="headerlink" title="功能"></a>功能</h3><p>准确率用于度量分类模型的预测结果与实际结果一致的比例，即在所有样本中，模型预测正确的比例。</p><h3 id="计算公式-2"><a href="#计算公式-2" class="headerlink" title="计算公式"></a>计算公式</h3><p>在二分类问题中，准确率的计算公式为：</p><p>$$ Accuracy = \frac{TP+TN}{TP+FP+TN+FN} $$</p><p>其中，TP表示真正例（True Positives），TN表示真反例（True Negatives），FP表示假正例（False Positives），FN表示假反例（False Negatives）。</p><h3 id="示例-2"><a href="#示例-2" class="headerlink" title="示例"></a>示例</h3><p>假设一个推荐系统对100个商品做出了预测，其中70个预测正确，那么准确率为：70 / 100 = 0.7。</p><h3 id="Python-实现-2"><a href="#Python-实现-2" class="headerlink" title="Python 实现"></a>Python 实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># y_true 表示真实标签，y_pred 表示预测标签</span></span><br><span class="line">y_true = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">y_pred = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">accuracy = accuracy_score(y_true, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f'Accuracy: <span class="subst">{accuracy}</span>'</span>)</span><br></pre></td></tr></tbody></table></figure><h3 id="适用性-2"><a href="#适用性-2" class="headerlink" title="适用性"></a>适用性</h3><p>⭐️⭐️⭐️</p><p>准确率在推荐系统中的适用性较低，因为推荐系统往往面临着不平衡的标签问题。比如，在一个商品推荐系统中，用户可能对大部分商品都没有兴趣，这样准确率就不能很好地反映出模型的性能。</p><hr><h2 id="4-F1-Score"><a href="#4-F1-Score" class="headerlink" title="4. F1-Score"></a>4. F1-Score</h2><h3 id="功能-3"><a href="#功能-3" class="headerlink" title="功能"></a>功能</h3><p>F1-Score 是精确率和召回率的调和平均值，用于同时考虑精确率和召回率，对模型进行综合评价。</p><h3 id="计算公式-3"><a href="#计算公式-3" class="headerlink" title="计算公式"></a>计算公式</h3><p>F1-Score 的计算公式为：</p><p>$$ F1-Score = 2 \times \frac{Precision \times Recall}{Precision + Recall} $$</p><h3 id="示例-3"><a href="#示例-3" class="headerlink" title="示例"></a>示例</h3><p>假设一个推荐系统的精确率为0.5，召回率为0.7，那么 F1-Score 为：2 * (0.5 * 0.7) / (0.5 + 0.7) = 0.583。</p><h3 id="Python-实现-3"><a href="#Python-实现-3" class="headerlink" title="Python 实现"></a>Python 实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># y_true 表示真实标签，y_pred 表示预测标签</span></span><br><span class="line">y_true = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">y_pred = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">f1 = f1_score(y_true, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f'F1-Score: <span class="subst">{f1}</span>'</span>)</span><br></pre></td></tr></tbody></table></figure><h3 id=""><a href="#" class="headerlink" title=""></a></h3><p>适用性</p><p>⭐️⭐️⭐️⭐️⭐️</p><p>F1-Score 非常适用于推荐系统的评估。推荐系统需要平衡精确率和召回率，精确率高说明推荐的准确，召回率高说明推荐的全面，而 F1-Score 正是一个兼顾两者的评价指标。</p><hr><h2 id="5-NDCG-Normalized-Discounted-Cumulative-Gain"><a href="#5-NDCG-Normalized-Discounted-Cumulative-Gain" class="headerlink" title="5. NDCG (Normalized Discounted Cumulative Gain)"></a>5. NDCG (Normalized Discounted Cumulative Gain)</h2><h3 id="功能-4"><a href="#功能-4" class="headerlink" title="功能"></a>功能</h3><p>NDCG 是一个用于评价推荐系统排序质量的指标，特别是对于那些考虑元素相关性排序的推荐系统。它可以衡量模型预测的排序列表与真实的排序列表的相似程度。</p><h3 id="计算公式-4"><a href="#计算公式-4" class="headerlink" title="计算公式"></a>计算公式</h3><p>NDCG 的计算公式为：</p><p>$$ NDCG = \frac{DCG}{IDCG} $$</p><p>其中，DCG 表示推荐列表的 Discounted Cumulative Gain，计算公式为：</p><p>$$ DCG = \sum_{i=1}^{N} \frac{2^{rel_i} - 1}{log_2(i + 1)} $$</p><p>IDCG 表示理想情况下的最大 DCG，即所有相关性商品都排在前面，计算公式与 DCG 相同，只是商品的排序按照相关性从大到小。</p><h3 id="示例-4"><a href="#示例-4" class="headerlink" title="示例"></a>示例</h3><p>假设一个推荐系统对5个商品的相关性预测为 [3, 2, 3, 0, 1]，那么 DCG = (2^3 - 1)/log2(1+1) + (2^2 - 1)/log2(2+1) + (2^3 - 1)/log2(3+1) + (2^0 - 1)/log2(4+1) + (2^1 - 1)/log2(5+1)。如果理想的排序为 [3, 3, 2, 1, 0]，那么 IDCG 可以按照相同的方式计算，NDCG = DCG / IDCG。</p><h3 id="Python-实现-4"><a href="#Python-实现-4" class="headerlink" title="Python 实现"></a>Python 实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> ndcg_score</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># y_true 表示真实的相关性，y_score 表示预测的相关性</span></span><br><span class="line">y_true = np.array([[<span class="number">3</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">1</span>]])</span><br><span class="line">y_score = np.array([[<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">3</span>]])</span><br><span class="line"></span><br><span class="line">ndcg = ndcg_score(y_true, y_score)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f'NDCG: <span class="subst">{ndcg}</span>'</span>)</span><br></pre></td></tr></tbody></table></figure><h3 id="适用性-3"><a href="#适用性-3" class="headerlink" title="适用性"></a>适用性</h3><p>⭐️⭐️⭐️⭐️⭐️</p><p>NDCG 非常适用于推荐系统的评估，特别是对于需要考虑推荐排序的系统。NDCG 能够衡量推荐系统预测的排序列表与真实的排序列表的相似程度，从而评价模型的性能。</p><hr><p>以下几个指标（Hit Rate, AUC, GAUC, LogLoss）由于篇幅限制，我将在下一次</p><p>回复中继续介绍。</p><h2 id="6-命中率-Hit-Rate"><a href="#6-命中率-Hit-Rate" class="headerlink" title="6. 命中率 (Hit Rate)"></a>6. 命中率 (Hit Rate)</h2><h3 id="功能-5"><a href="#功能-5" class="headerlink" title="功能"></a>功能</h3><p>命中率是评价推荐系统是否能够推荐出用户感兴趣的物品的指标，可以理解为系统是否“命中”用户感兴趣的商品。</p><h3 id="计算公式-5"><a href="#计算公式-5" class="headerlink" title="计算公式"></a>计算公式</h3><p>命中率的计算公式为：</p><p>$$ HitRate = \frac{Number\ of\ Hits}{Total\ Number\ of\ Tests} $$</p><h3 id="示例-5"><a href="#示例-5" class="headerlink" title="示例"></a>示例</h3><p>假设一个推荐系统对10个商品做出了预测，其中3个预测命中，那么命中率为：3 / 10 = 0.3。</p><h3 id="Python-实现-5"><a href="#Python-实现-5" class="headerlink" title="Python 实现"></a>Python 实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">hit_rate</span>(<span class="params">recommended_items, true_items</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">int</span>(<span class="built_in">any</span>(item <span class="keyword">in</span> true_items <span class="keyword">for</span> item <span class="keyword">in</span> recommended_items))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 推荐的商品和真正感兴趣的商品</span></span><br><span class="line">recommended_items = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">true_items = [<span class="number">2</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]</span><br><span class="line"></span><br><span class="line">hit_rate = hit_rate(recommended_items, true_items)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f'Hit Rate: <span class="subst">{hit_rate}</span>'</span>)</span><br></pre></td></tr></tbody></table></figure><h3 id="适用性-4"><a href="#适用性-4" class="headerlink" title="适用性"></a>适用性</h3><p>⭐️⭐️⭐️⭐️⭐️</p><p>命中率对于推荐系统的评估非常重要。因为推荐系统的主要目标就是推荐出用户感兴趣的物品，如果推荐的物品中包含了用户真正感兴趣的物品，那么命中率就高。</p><hr><p>非常感谢您的建议，下面是根据您的建议修改后的 AUC 和 GAUC 的介绍。</p><h2 id="7-AUC-Area-Under-Curve"><a href="#7-AUC-Area-Under-Curve" class="headerlink" title="7. AUC (Area Under Curve)"></a>7. AUC (Area Under Curve)</h2><h3 id="功能-6"><a href="#功能-6" class="headerlink" title="功能"></a>功能</h3><p>AUC 是一种常用的分类问题的性能评估指标，特别是对于推荐系统，它是一个衡量模型对正负样本区分度的指标。对于每一个正样本，计算其预测分数高于多少比例的负样本，即正样本的“正样本率”。AUC 就是所有正样本的平均正样本率。</p><h3 id="计算公式-6"><a href="#计算公式-6" class="headerlink" title="计算公式"></a>计算公式</h3><p>AUC 的计算可以描述为：</p><ol><li>对于每个用户，计算推荐系统对于正样本和负样本的预测分数。</li><li>对于每个正样本，计算其预测分数高于多少比例的负样本，即正样本的“正样本率”。</li><li>计算所有正样本的平均正样本率，即 AUC。</li></ol><p>具体的公式可以表示为：</p><p>$$ AUC = \frac{1}{M}\sum_{i=1}^{M} \frac{1}{P_iN_i} \sum_{j=1}^{P_i} \sum_{k=1}^{N_i} I(s_{ij} &gt; s_{ik}) $$</p><p>其中，$M$ 是用户数，$P_i$ 和 $N_i$ 分别是用户 $i$ 的正样本数和负样本数，$s_{ij}$ 和 $s_{ik}$ 分别是用户 $i$ 的正样本 $j$ 和负样本 $k$ 的预测分数，$I(\cdot)$ 是指示函数。</p><h3 id="Python-实现-6"><a href="#Python-实现-6" class="headerlink" title="Python 实现"></a>Python 实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对每个用户计算 AUC，然后取平均</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calculate_auc</span>(<span class="params">users, y_true, y_score</span>):</span><br><span class="line">    auc_list = []</span><br><span class="line">    <span class="keyword">for</span> user <span class="keyword">in</span> users:</span><br><span class="line">        auc = roc_auc_score(y_true[user], y_score[user])</span><br><span class="line">        auc_list.append(auc)</span><br><span class="line">    <span class="keyword">return</span> np.mean(auc_list)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用户列表、真实标签、预测得分</span></span><br><span class="line">users = [<span class="string">'user1'</span>, <span class="string">'user2'</span>]</span><br><span class="line">y_true = {<span class="string">'user1'</span>: [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>], <span class="string">'user2'</span>: [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]}</span><br><span class="line">y_score = {<span class="string">'user1'</span>: [<span class="number">0.9</span>, <span class="number">0.8</span>, <span class="number">0.5</span>, <span class="number">0.4</span>], <span class="string">'user2'</span>: [<span class="number">0.7</span>, <span class="number">0.6</span>, <span class="number">0.3</span>, <span class="number">0.2</span>]}</span><br><span class="line"></span><br><span class="line">auc = calculate_auc(users, y_true, y_score)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f'AUC: <span class="subst">{auc}</span>'</span>)</span><br></pre></td></tr></tbody></table></figure><p>特别需要注意的是，在面试中经常遇到要求使用 Python 手写 AUC 计算面试题，下面展示一个实例，该示例主要考虑二分类问题。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">calculate_auc_manual</span>(<span class="params">y_true, y_score</span>):</span><br><span class="line">    <span class="comment"># 首先，获取所有正样本和负样本的索引</span></span><br><span class="line">    pos_indices = [i <span class="keyword">for</span> i, x <span class="keyword">in</span> <span class="built_in">enumerate</span>(y_true) <span class="keyword">if</span> x == <span class="number">1</span>]</span><br><span class="line">    neg_indices = [i <span class="keyword">for</span> i, x <span class="keyword">in</span> <span class="built_in">enumerate</span>(y_true) <span class="keyword">if</span> x == <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算所有正样本的分数高于多少比例的负样本</span></span><br><span class="line">    pos_count = <span class="built_in">len</span>(pos_indices)</span><br><span class="line">    neg_count = <span class="built_in">len</span>(neg_indices)</span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> pos_indices:</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> neg_indices:</span><br><span class="line">            <span class="keyword">if</span> y_score[i] &gt; y_score[j]:</span><br><span class="line">                count += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 正样本率就是所有正样本的分数高于负样本的数量比上总的正负样本对的数量</span></span><br><span class="line">    pos_rate = count / (pos_count * neg_count)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> pos_rate</span><br><span class="line"></span><br><span class="line"><span class="comment"># 真实标签</span></span><br><span class="line">y_true = [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line"><span class="comment"># 预测得分</span></span><br><span class="line">y_score = [<span class="number">0.9</span>, <span class="number">0.8</span>, <span class="number">0.1</span>, <span class="number">0.4</span>, <span class="number">0.95</span>, <span class="number">0.6</span>]</span><br><span class="line"></span><br><span class="line">auc = calculate_auc_manual(y_true, y_score)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f'AUC: <span class="subst">{auc}</span>'</span>)</span><br><span class="line">``````</span><br><span class="line"></span><br><span class="line">在上述代码中，我们首先获取所有正样本和负样本的索引。然后，对于每一个正样本，我们计算其预测分数高于多少比例的负样本。最后，所有正样本的平均正样本率即为 AUC。</span><br><span class="line"></span><br><span class="line">注意：在真实环境中，上述实现可能会非常慢，因为它需要对所有正负样本对进行比较。在实际应用中，我们通常会使用更有效的算法来计算 AUC，例如使用排序和计数方法。</span><br><span class="line"></span><br><span class="line"><span class="comment">### 适用性</span></span><br><span class="line"></span><br><span class="line">⭐️⭐️⭐️⭐️⭐️</span><br><span class="line"></span><br><span class="line">AUC 非常适用于推荐系统的评估。AUC 可以衡量模型对正负样本的区分能力，对于推荐系统来说，就是能否准确地找出用户感兴趣的物品。因此，AUC 是一个很好的评价指标。</span><br><span class="line"></span><br><span class="line">------</span><br><span class="line"></span><br><span class="line"><span class="comment">## 8. GAUC (Group Area Under Curve)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line"></span><br><span class="line">功能</span><br><span class="line"></span><br><span class="line">GAUC 是 AUC 的扩展，主要应用于推荐系统等场景。它的计算方法和 AUC 类似，只是在计算正样本率时，需要按照用户组进行计算，而不是按照单个样本进行计算。</span><br><span class="line"></span><br><span class="line"><span class="comment">### 计算公式</span></span><br><span class="line"></span><br><span class="line">GAUC 的计算步骤如下：</span><br><span class="line"></span><br><span class="line"><span class="number">1.</span> 对于每个用户，计算推荐系统对于正样本和负样本的预测分数。</span><br><span class="line"><span class="number">2.</span> 对于每个用户，计算其正样本的预测分数高于多少比例的负样本，即用户的“正样本率”。</span><br><span class="line"><span class="number">3.</span> 计算所有用户的平均正样本率，即 GAUC。</span><br><span class="line"></span><br><span class="line">具体的公式可以表示为：</span><br><span class="line"></span><br><span class="line">$$ GAUC = \frac{<span class="number">1</span>}{M}\sum_{i=<span class="number">1</span>}^{M} \left( \frac{<span class="number">1</span>}{P_iN_i} \sum_{j=<span class="number">1</span>}^{P_i} \sum_{k=<span class="number">1</span>}^{N_i} I(s_{ij} &gt; s_{ik}) \right) $$</span><br><span class="line"></span><br><span class="line">其中，$M$ 是用户数，$P_i$ 和 $N_i$ 分别是用户 $i$ 的正样本数和负样本数，$s_{ij}$ 和 $s_{ik}$ 分别是用户 $i$ 的正样本 $j$ 和负样本 $k$ 的预测分数，$I(\cdot)$ 是指示函数。</span><br><span class="line"></span><br><span class="line"><span class="comment">### Python 实现</span></span><br><span class="line"></span><br><span class="line">Python 实现需要根据具体的数据情况进行，以下是一个基本的示例。</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对每个用户计算 AUC，然后取平均</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calculate_gauc</span>(<span class="params">users, y_true, y_score</span>):</span><br><span class="line">    auc_list = []</span><br><span class="line">    <span class="keyword">for</span> user <span class="keyword">in</span> users:</span><br><span class="line">        auc = roc_auc_score(y_true[user], y_score[user])</span><br><span class="line">        auc_list.append(auc)</span><br><span class="line">    <span class="keyword">return</span> np.mean(auc_list)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用户列表、真实标签、预测得分</span></span><br><span class="line">users = [<span class="string">'user1'</span>, <span class="string">'user2'</span>]</span><br><span class="line">y_true = {<span class="string">'user1'</span>: [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>], <span class="string">'user2'</span>: [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]}</span><br><span class="line">y_score = {<span class="string">'user1'</span>: [<span class="number">0.9</span>, <span class="number">0.8</span>, <span class="number">0.5</span>, <span class="number">0.4</span>], <span class="string">'user2'</span>: [<span class="number">0.7</span>, <span class="number">0.6</span>, <span class="number">0.3</span>, <span class="number">0.2</span>]}</span><br><span class="line"></span><br><span class="line">gauc = calculate_gauc(users, y_true, y_score)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f'GAUC: <span class="subst">{gauc}</span>'</span>)</span><br></pre></td></tr></tbody></table></figure><h3 id="适用性-5"><a href="#适用性-5" class="headerlink" title="适用性"></a>适用性</h3><p>⭐️⭐️⭐️⭐️⭐️</p><p>GAUC 也非常适用于推荐系统的评估，尤其是对于那些需要考虑个体差异的系统。通过计算每个用户的 AUC 并取平均，GAUC 能够更全面地反映推荐系统的性能。</p><h3 id="适用性-6"><a href="#适用性-6" class="headerlink" title="适用性"></a>适用性</h3><p>⭐️⭐️⭐️⭐️⭐️</p><p>GAUC 也非常适用于推荐系统的评估，尤其是对于那些需要考虑个体差异的系统。通过计算每个用户的 AUC 并取平均，GAUC 能够更全面地反映推荐系统的性能。</p><hr><h2 id="9-LogLoss-Logarithmic-Loss"><a href="#9-LogLoss-Logarithmic-Loss" class="headerlink" title="9. LogLoss (Logarithmic Loss)"></a>9. LogLoss (Logarithmic Loss)</h2><h3 id="功能-7"><a href="#功能-7" class="headerlink" title="功能"></a>功能</h3><p>LogLoss 是一种衡量分类模型的损失函数，它考虑了模型预测的概率值。对于二分类问题，其值越小，表示模型的性能越好。</p><h3 id="计算公式-7"><a href="#计算公式-7" class="headerlink" title="计算公式"></a>计算公式</h3><p>LogLoss 的计算公式为：</p><p>$$ LogLoss = -\frac{1}{n}\sum_{i=1}^{n}[y_i\log(p_i) + (1 - y_i)\log(1 - p_i)] $$</p><h3 id="示例-6"><a href="#示例-6" class="headerlink" title="示例"></a>示例</h3><p>假设一个推荐系统对3个样本的预测概率分别为 [0.8, 0.6, 0.3]，而这3个样本的真实标签分别为 [1, 1, 0]，那么 LogLoss 可以通过代入公式进行计算。</p><h3 id="Python-实现-7"><a href="#Python-实现-7" class="headerlink" title="Python 实现"></a>Python 实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> log_loss</span><br><span class="line"></span><br><span class="line"><span class="comment"># y_true 表示真实标签，y_pred 表示预测的概率</span></span><br><span class="line">y_true = [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line">y_pred = [<span class="number">0.8</span>, <span class="number">0.6</span>, <span class="number">0.3</span>]</span><br><span class="line"></span><br><span class="line">logloss = log_loss(y_true, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f'LogLoss: <span class="subst">{logloss}</span>'</span>)</span><br></pre></td></tr></tbody></table></figure><h3 id="适用性-7"><a href="#适用性-7" class="headerlink" title="适用性"></a>适用性</h3><p>⭐️⭐️⭐️⭐️</p><p>LogLoss 在推荐系统的评估中适用，但可能并不是最主要的指标。LogLoss 更注重模型预测的概率值是否准确，而推荐系统除了预测的准确性外，还需要考虑其他因素，如覆盖率、新颖性等。因此，LogLoss 可以作为衡量推荐系统的一个辅助指标。</p>]]></content>
      
      
      <categories>
          
          <category> NLP Insights </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Recommendation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>VS Code Remote SSH连接失败问题</title>
      <link href="/2023/07/06/Debugging%20Diaries/VS%20Code%20Remote%20SSH%E8%BF%9E%E6%8E%A5%E5%A4%B1%E8%B4%A5%E9%97%AE%E9%A2%98/"/>
      <url>/2023/07/06/Debugging%20Diaries/VS%20Code%20Remote%20SSH%E8%BF%9E%E6%8E%A5%E5%A4%B1%E8%B4%A5%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<h1 id="VS-Code-Remote-SSH连接失败的问题解决"><a href="#VS-Code-Remote-SSH连接失败的问题解决" class="headerlink" title="VS Code Remote SSH连接失败的问题解决"></a>VS Code Remote SSH连接失败的问题解决</h1><p>本文档针对VS Code中的Remote SSH插件在尝试连接远程服务器时出现”Failed to parse remote port from server output”错误的情况提供解决方案。作者在经过一系列的排查和尝试后，最终找到了解决的方法。</p><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>在使用VS Code的Remote SSH插件尝试连接远程服务器时，遇到了错误提示”Failed to parse remote port from server output”。此错误提示可能是由于VS Code不能正确地从SSH服务器的输出中解析出远程端口。</p><h2 id="尝试的解决方法"><a href="#尝试的解决方法" class="headerlink" title="尝试的解决方法"></a>尝试的解决方法</h2><ol><li>检查SSH配置文件</li><li>更新VS Code和Remote SSH扩展</li><li>手动SSH连接</li><li>检查远程服务器的状态</li><li>重启VS Code</li></ol><p>以上常见的解决方法都未能解决问题。</p><h2 id="成功的解决方案"><a href="#成功的解决方案" class="headerlink" title="成功的解决方案"></a>成功的解决方案</h2><p>最终，作者尝试了取消勾选VS Code设置中的<code>Remote.SSH: Use Local Server</code>选项，成功连接到了远程服务器。当该选项被选中（默认）时，VS Code会在本地机器上启动一个服务器，然后通过该本地服务器连接到远程SSH服务器。当取消勾选此选项时，VS Code会直接连接到远程SSH服务器，而不通过本地服务器。</p><h3 id="解决步骤"><a href="#解决步骤" class="headerlink" title="解决步骤"></a>解决步骤</h3><ol><li>打开VS Code。</li><li>在左侧的活动栏点击齿轮图标打开设置。</li><li>在设置搜索框中输入<code>Remote.SSH: Use Local Server</code>。</li><li>取消选中出现的<code>Remote.SSH: Use Local Server</code>复选框。</li></ol><h2 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h2><p>虽然直接连接到远程SSH服务器可以解决某些连接问题，但由于没有利用到本地服务器的优势，可能会导致VS Code的性能稍有下降。但只要没有遇到性能问题，这个设置就不需要过于担心。</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>如果你在使用VS Code的Remote SSH插件连接远程服务器时遇到了类似的问题，你也可以试试这个方法，希望这个解决方案能帮助到你。</p>]]></content>
      
      
      <categories>
          
          <category> Debugging Diaries </category>
          
      </categories>
      
      
        <tags>
            
            <tag> IssueFix </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kubernetes基础</title>
      <link href="/2023/06/06/Tech%20Toolbox/Kubernetes%E5%9F%BA%E7%A1%80/"/>
      <url>/2023/06/06/Tech%20Toolbox/Kubernetes%E5%9F%BA%E7%A1%80/</url>
      
        <content type="html"><![CDATA[<blockquote><p>前言：本文将介绍Kubernetes基础中的重要概念——Pod，以及它在Kubernetes中的应用和功能。首先，我们将详细解释Pod的基本概念和构成，包括它作为最小可部署单元的特点和包含的资源。然后，我们将探讨Pod在Kubernetes中的作用与功能，包括调度单位、网络单元、存储单元、生命周期管理和水平扩展。通过深入了解Pod，您将对Kubernetes中的核心概念有更全面的理解。</p></blockquote><h2 id="I-Kubernetes基础：Pod理解与应用"><a href="#I-Kubernetes基础：Pod理解与应用" class="headerlink" title="I. Kubernetes基础：Pod理解与应用"></a><strong>I. Kubernetes基础：Pod理解与应用</strong></h2><h3 id="1-1-Pod的基本概念与构成"><a href="#1-1-Pod的基本概念与构成" class="headerlink" title="1.1 Pod的基本概念与构成"></a><strong>1.1 Pod的基本概念与构成</strong></h3><p>在Kubernetes（简称K8s）中，Pod（容器组）是最小的可部署单元。它是Kubernetes集群中可以运行的一组一个或多个容器的逻辑主机。Pod提供了一个独立的环境，其中包含运行应用程序所需的所有资源，如存储、网络和其他依赖项。</p><p>Pod通常由一个或多个紧密相关的容器组成，这些容器共享相同的命名空间、网络和存储卷。它们可以通过本地主机上的localhost进行通信，并且可以共享文件系统的一部分或全部内容。</p><h3 id="1-2-Pod在Kubernetes中的作用与功能"><a href="#1-2-Pod在Kubernetes中的作用与功能" class="headerlink" title="1.2 Pod在Kubernetes中的作用与功能"></a><strong>1.2 Pod在Kubernetes中的作用与功能</strong></h3><p>Pod在Kubernetes中的作用是以下几个方面：</p><ol><li>调度单位：Kubernetes将Pod作为调度的基本单位，决定在哪个节点上运行Pod。</li><li>网络单元：每个Pod都有自己的IP地址，并且可以通过Kubernetes集群内部和外部的服务发现机制与其他Pod或外部服务通信。</li><li>存储单元：Pod可以共享存储卷，容器之间可以共享文件系统中的数据。</li><li>生命周期管理：Pod可以创建、启动、停止和销毁，它们的生命周期由Kubernetes控制器管理。</li><li>水平扩展：可以通过复制Pod的方式水平扩展应用程序的实例。</li></ol><h2 id="II-Kubernetes应用实践：在Kubernetes中安装和配置Miniconda"><a href="#II-Kubernetes应用实践：在Kubernetes中安装和配置Miniconda" class="headerlink" title="II. Kubernetes应用实践：在Kubernetes中安装和配置Miniconda"></a><strong><strong>II. Kubernetes应用实践：在Kubernetes中安装和配置Miniconda</strong></strong></h2><h3 id="2-1-安装和配置Miniconda的步骤"><a href="#2-1-安装和配置Miniconda的步骤" class="headerlink" title="2.1 安装和配置Miniconda的步骤"></a><strong><strong>2.1 安装和配置Miniconda的步骤</strong></strong></h3><p>本文介绍了如何在Kubernetes集群中安装和配置Miniconda。Miniconda是一个轻量级的Python环境管理工具，可用于创建和管理Python环境及其相关包。</p><h3 id="步骤-1：登录到Kubernetes-Pod的终端"><a href="#步骤-1：登录到Kubernetes-Pod的终端" class="headerlink" title="步骤 1：登录到Kubernetes Pod的终端"></a>步骤 1：登录到Kubernetes Pod的终端</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl <span class="built_in">exec</span> -it &lt;pod-name&gt; -- /bin/bash</span><br></pre></td></tr></tbody></table></figure><p>将 <code>&lt;pod-name&gt;</code> 替换为要登录的Pod的名称。</p><h3 id="步骤-2：下载和安装Miniconda"><a href="#步骤-2：下载和安装Miniconda" class="headerlink" title="步骤 2：下载和安装Miniconda"></a>步骤 2：下载和安装Miniconda</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh</span><br><span class="line">bash Miniconda3-latest-Linux-x86_64.sh</span><br></pre></td></tr></tbody></table></figure><p>这将下载Miniconda的安装脚本并启动安装过程。</p><h3 id="步骤-3：完成安装向导"><a href="#步骤-3：完成安装向导" class="headerlink" title="步骤 3：完成安装向导"></a>步骤 3：完成安装向导</h3><p>根据安装向导的提示，选择安装路径、环境变量配置等选项完成Miniconda的安装。</p><h3 id="步骤-4：激活Miniconda环境"><a href="#步骤-4：激活Miniconda环境" class="headerlink" title="步骤 4：激活Miniconda环境"></a>步骤 4：激活Miniconda环境</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> ~/.bashrc</span><br></pre></td></tr></tbody></table></figure><p>重新加载终端或执行上述命令来激活Miniconda环境。</p><h3 id="步骤-5：使用Miniconda"><a href="#步骤-5：使用Miniconda" class="headerlink" title="步骤 5：使用Miniconda"></a>步骤 5：使用Miniconda</h3><p>在激活的Miniconda环境中，您可以使用以下命令来管理环境和安装Python包：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda create --name myenv python=3.9</span><br><span class="line">conda activate myenv</span><br><span class="line">conda install package_name</span><br></pre></td></tr></tbody></table></figure><h3 id="2-2-安装和使用Miniconda的注意事项"><a href="#2-2-安装和使用Miniconda的注意事项" class="headerlink" title="2.2 安装和使用Miniconda的注意事项"></a><strong><strong>2.2 安装和使用Miniconda的注意事项</strong></strong></h3><ul><li>请根据Pod的操作系统和架构调整Miniconda的下载链接和安装命令。</li><li>为了自动化安装和配置Miniconda，请将相关步骤和环境配置包含在Pod的初始化脚本或容器镜像构建过程中。</li><li>请在安装和使用Miniconda时遵循适当的最佳实践和安全性措施，并根据具体需求进行配置和管理。</li></ul><h3 id="2-3-安装和配置Miniconda的总结"><a href="#2-3-安装和配置Miniconda的总结" class="headerlink" title="2.3 安装和配置Miniconda的总结"></a><strong><strong>2.3 安装和配置Miniconda的总结</strong></strong></h3><p>通过按照本文中的步骤，在Kubernetes中安装和配置Miniconda，您可以轻松管理Python环境和包，并为您的应用程序提供所需的依赖项。Miniconda的灵活性和可扩展性使其成为在Kubernetes环境中开发和部署Python应用程序的理想选择。</p><h2 id="III-Kubernetes数据操作：在本地Mac电脑将文件传输到Kubernetes集群的流程"><a href="#III-Kubernetes数据操作：在本地Mac电脑将文件传输到Kubernetes集群的流程" class="headerlink" title="III. Kubernetes数据操作：在本地Mac电脑将文件传输到Kubernetes集群的流程"></a><strong><strong>III. Kubernetes数据操作：在本地Mac电脑将文件传输到Kubernetes集群的流程</strong></strong></h2><p>本文介绍了如何在本地Mac电脑上将文件传输到Kubernetes集群中的Pod。我们使用lrzsz工具来实现文件的上传和下载操作。</p><h3 id="3-1-文件传输前的准备工作"><a href="#3-1-文件传输前的准备工作" class="headerlink" title="3.1 文件传输前的准备工作"></a><strong><strong>3.1 文件传输前的准备工作</strong></strong></h3><ul><li>本地Mac电脑已经安装了Homebrew。</li><li>Kubernetes集群已经安装了lrzsz工具。</li></ul><p><strong>[在本地执行]</strong></p><ol><li><p>打开终端应用程序。</p></li><li><p>安装lrzsz工具。在终端中执行以下命令：</p> <figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install lrzsz</span><br></pre></td></tr></tbody></table></figure></li><li><p>确保Kubernetes集群中已经安装了lrzsz工具。在Kubernetes集群中的终端中执行以下命令：</p> <figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">apt-get update</span><br><span class="line">apt-get install lrzsz</span><br></pre></td></tr></tbody></table></figure></li></ol><p>[<strong>在Kubernetes集群中执行]</strong></p><ol><li><p>登录到Pod的终端。在Kubernetes集群中的终端中执行以下命令：</p> <figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl <span class="built_in">exec</span> -it &lt;pod-name&gt; -- /bin/bash</span><br></pre></td></tr></tbody></table></figure><p> 将 <code>&lt;pod-name&gt;</code> 替换为目标Pod的名称。</p></li><li><p>在Pod的终端中，使用以下命令来接收文件：</p> <figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rz</span><br></pre></td></tr></tbody></table></figure><p> 执行该命令后，将弹出一个文件选择窗口。</p></li></ol><h3 id="3-2-文件传输的具体步骤"><a href="#3-2-文件传输的具体步骤" class="headerlink" title="3.2 文件传输的具体步骤"></a><strong><strong>3.2 文件传输的具体步骤</strong></strong></h3><p><strong>[在本地执行]</strong></p><ol><li><p>在本地终端中，使用以下命令将文件发送到Kubernetes集群的Pod：<br>将 <code>/path/to/environ.yaml</code> 替换为 <code>environ.yaml</code> 文件在本地计算机上的路径。</p> <figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sz /path/to/environ.yaml</span><br></pre></td></tr></tbody></table></figure></li></ol><p><strong>[在Kubernetes集群中执行]</strong></p><ol><li>在Kubernetes集群中的终端的文件选择窗口中，选择要上传的文件 <code>environ.yaml</code>。</li></ol><p>文件传输将在本地和Kubernetes集群之间进行，使用了lrzsz工具的上传和下载命令。请确保在本地和Kubernetes集群中都按照相应的步骤进行安装和操作，并使用正确的命令进行文件传输。</p><p>请在进行文件传输操作时，遵循适当的安全性和最佳实践，以保护数据和系统的安全性。</p><h2 id="IV-如何检查在终端断开连接后-Linux-命令是否继续执行"><a href="#IV-如何检查在终端断开连接后-Linux-命令是否继续执行" class="headerlink" title="IV. 如何检查在终端断开连接后 Linux 命令是否继续执行"></a><strong>IV. 如何检查在终端断开连接后 Linux 命令是否继续执行</strong></h2><p>在 Linux 终端中运行命令或脚本时，如果终端连接断开，您可能会想知道命令或脚本是否仍在后台执行。以下是几种方法来检查 Linux 命令在终端断开连接后是否继续执行。</p><h3 id="4-1-方法一：使用-ps-命令"><a href="#4-1-方法一：使用-ps-命令" class="headerlink" title="4.1 方法一：使用 ps 命令"></a>4.1 <strong>方法一：使用 ps 命令</strong></h3><ol><li>打开新的终端窗口。</li><li>运行以下命令：<code>ps aux | grep &lt;命令或脚本关键词&gt;</code></li><li>检查输出结果中是否存在与命令或脚本相关的进程。如果存在，表示命令或脚本仍在后台执行。</li></ol><h3 id="4-2-方法二：使用-pgrep-命令"><a href="#4-2-方法二：使用-pgrep-命令" class="headerlink" title="4.2 方法二：使用 pgrep 命令"></a>4.2 <strong>方法二：使用 pgrep 命令</strong></h3><ol><li>打开新的终端窗口。</li><li>运行以下命令：<code>pgrep -f &lt;命令或脚本关键词&gt;</code></li><li>检查输出结果中是否存在与命令或脚本相关的进程 ID。如果存在，表示命令或脚本仍在后台执行。</li></ol><h3 id="4-3-方法三：使用日志文件或输出文件"><a href="#4-3-方法三：使用日志文件或输出文件" class="headerlink" title="4.3 方法三：使用日志文件或输出文件"></a>4.3 <strong>方法三：使用日志文件或输出文件</strong></h3><ol><li>如果在命令或脚本中使用了输出重定向（如 <code>tee</code>），请检查日志文件或输出文件。</li><li>打开新的终端窗口。</li><li>使用 <code>tail</code> 命令查看日志文件或输出文件的最后几行：<code>tail -n &lt;行数&gt; &lt;文件路径&gt;</code></li><li>检查最后几行是否包含与命令或脚本的输出相关的内容。如果有新的输出，表示命令或脚本仍在执行。</li></ol><p>需要注意的是，即使命令或脚本在终端断开连接后仍在后台执行，如果发生错误或问题，它们可能会终止或停止运行。因此，还应检查命令或脚本本身是否存在问题。</p><p>总结：<br>通过使用 ps 命令、pgrep 命令或查看日志文件或输出文件，您可以检查在终端断开连接后 Linux 命令是否继续执行。这些方法提供了一种了解命令或脚本是否在后台持续执行的方式，以确保任务能够正常进行。</p><h2 id="V-使用-Bash-脚本执行-Python-脚本"><a href="#V-使用-Bash-脚本执行-Python-脚本" class="headerlink" title="V. 使用 Bash 脚本执行 Python 脚本"></a><strong>V.</strong> 使用 Bash 脚本执行 Python 脚本</h2><p>本文档介绍了如何使用 Bash 脚本来执行指定的 Python 脚本，并提供了一个示例脚本。该脚本还涉及使用 conda 环境来运行 Python。</p><h3 id="5-1-简介"><a href="#5-1-简介" class="headerlink" title="5.1 简介"></a>5.1 简介</h3><p>在某些情况下，您可能需要在终端中执行长时间运行的 Python 脚本。为了确保持久性并方便管理，可以编写一个 Bash 脚本来运行 Python 脚本。本文档提供了一个示例脚本，演示如何使用 Bash 脚本来执行 Python 脚本。</p><h3 id="5-2-脚本示例"><a href="#5-2-脚本示例" class="headerlink" title="5. 2 脚本示例"></a>5. 2 脚本示例</h3><p>以下是一个示例 Bash 脚本，用于执行特定的 Python 脚本，并使用 conda 环境：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 conda 环境名称</span></span><br><span class="line">conda_env=<span class="string">"python3.10"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 激活 conda 环境</span></span><br><span class="line"><span class="built_in">source</span> activate <span class="variable">$conda_env</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行 Python 脚本，并使用 tee 将输出同时重定向到文件和控制台</span></span><br><span class="line">python data_gen_updated.py conversations_0607_v1_500 500 | <span class="built_in">tee</span> output.log</span><br><span class="line"></span><br><span class="line"><span class="comment"># 停用 conda 环境</span></span><br><span class="line">conda deactivate</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><p>上述脚本包含以下步骤：</p><ol><li>定义 conda 环境的名称（根据需要进行修改）。</li><li>使用 <code>source activate</code> 命令激活指定的 conda 环境。</li><li>使用 <code>python</code> 命令执行特定的 Python 脚本。同时，使用 <code>tee</code> 命令将输出同时重定向到文件和控制台。</li><li>使用 <code>conda deactivate</code> 命令停用 conda 环境。</li></ol><h3 id="5-3-使用脚本"><a href="#5-3-使用脚本" class="headerlink" title="5.3 使用脚本"></a>5.3 使用脚本</h3><p>按照以下步骤在终端中使用脚本：</p><ol><li><p>使用文本编辑器创建一个新文件，并将上述示例脚本粘贴进去。</p></li><li><p>保存文件并关闭文本编辑器。</p></li><li><p>在终端中，赋予脚本执行权限：</p> <figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">chmod</span> +x script.sh</span><br></pre></td></tr></tbody></table></figure></li><li><p>运行脚本：</p> <figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./script.sh</span><br></pre></td></tr></tbody></table></figure></li></ol><p>确保在运行脚本之前，已正确安装并配置所需的 conda 环境，并将脚本中的 <code>python data_gen_updated.py conversations_0607_v1_500 500</code> 替换为您要执行的实际命令。</p><h3 id="5-4-结论"><a href="#5-4-结论" class="headerlink" title="5.4 结论"></a>5.4 结论</h3><p>使用 Bash 脚本可以在终端中执行 Python 脚本，并提供持久性和管理灵活性。本文档提供了一个示例脚本，帮助您开始使用 Bash 脚本来执行 Python 脚本，并演示了使用 conda 环境的方法。根据您的实际需求，可以修改和调整脚</p>]]></content>
      
      
      <categories>
          
          <category> Tech Toolbox </category>
          
      </categories>
      
      
        <tags>
            
            <tag> K8s </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GitLog</title>
      <link href="/2023/05/23/Tech%20Toolbox/GitLog/"/>
      <url>/2023/05/23/Tech%20Toolbox/GitLog/</url>
      
        <content type="html"><![CDATA[<h1 id="Git-Log"><a href="#Git-Log" class="headerlink" title="Git Log"></a>Git Log</h1><p>当使用Git进行版本控制时，**<code>git log</code>**命令是一个有用的工具，它可以显示提交历史记录和分支之间的关系。</p><h3 id="基本用法"><a href="#基本用法" class="headerlink" title="基本用法"></a>基本用法</h3><p>在终端或命令行中使用以下命令格式来调用<code>git log</code>：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">log</span></span><br></pre></td></tr></tbody></table></figure><p>这将显示包含所有提交历史记录的列表，最新的提交显示在最上面。</p><h3 id="限制输出"><a href="#限制输出" class="headerlink" title="限制输出"></a>限制输出</h3><p><code>git log</code>提供了一些选项来限制输出，以满足不同的需求。</p><ul><li><code>-oneline</code>：以紧凑的一行摘要形式显示提交历史记录。</li></ul><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">log</span> --oneline</span><br></pre></td></tr></tbody></table></figure><ul><li><code>-decorate</code>：在输出中显示分支和标签的引用名称。</li></ul><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git log --decorate</span><br></pre></td></tr></tbody></table></figure><ul><li><code>-graph</code>：使用图形表示法展示分支和合并的关系。</li></ul><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">log</span> --graph</span><br></pre></td></tr></tbody></table></figure><p>可以将这些选项组合在一起使用：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">log</span> --oneline --decorate --graph</span><br></pre></td></tr></tbody></table></figure><p>当您行 <strong><code>git log --oneline --decorate --graph</code></strong> 命令时，输出结果会以一种紧凑、图形化的方式显示提交历史记录和分支之间的关系。下面是对输出结果中每个部分的解释：</p><ol><li>Commit Hash（提交哈希值）：每个提交的唯一标识符，通常使用短的哈希值。这些哈希值是提交的独特标识，可以用来引用和检索特定的提交。</li><li>Commit Message（提交消息）：提交时输入的描述性消息，用于说明该提交所做的更改和目的。</li><li>Branches and Tags（分支和标签）：显示当前提交所在的分支和相关标签的引用名称。这些引用名称显示在提交哈希值后的括号内，以及在分支和标签之前的装饰符 **<code>decorate</code>**。</li><li>Graphical Representation（图形表示）：使用字符（如斜线、反斜线、竖线和星号）表示分支和合并的关系。这部分使用图形表示法展示了提交历史记录中不同分支的发展和合并情况。斜线（/）和反斜线（\）表示分支的发展，竖线（|）表示分支的分叉，星号（*）表示合并点。</li></ol><p>命令的输出结果可以通过以下方式进行阅读：</p><ol><li>每行表示一个提交，包含简短的提交哈希值和提交消息。例如：**<code>579ac2d Resolved merge conflicts with master branch</code>**。</li><li>分支和标签的引用名称显示在每个提交的后面。它们用括号括起来，并在引用名称前加上 <strong><code>tag:</code></strong> 或 <strong><code>HEAD -&gt;</code></strong> 的标识符。例如：**<code>(HEAD -&gt; huiyu/product_search_similarity_test, origin/huiyu/product_search_similarity_test)</code>** 表示当前所在的分支和远程分支。</li><li>图形表示法展示了分支和合并的关系。合并提交显示为一个或多个分支合并在一起的线条。例如，**<code>\</code>** 和 <strong><code>/</code></strong> 字符表示不同的分支合并。**<code>|</code>** 字符表示分支的分叉。这种图形表示法可以帮助您理解提交历史中不同分支之间的关系。</li></ol><p>通过阅读这些输出结果，您可以了解每个提交的信息，包括提交哈希值、提交消息、分支和标签的引用名称，以及分支和合并的关系。这有助于您跟踪代码的发展历程、分支的合并情况以及不同分支之间的关系。</p><h3 id="过滤和排序提交"><a href="#过滤和排序提交" class="headerlink" title="过滤和排序提交"></a>过滤和排序提交</h3><p>您可以使用一些选项来过滤和排序提交历史记录。</p><ul><li><code>-author=&lt;author&gt;</code>：仅显示特定作者的提交历史记录。</li></ul><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">log</span> --author=John</span><br></pre></td></tr></tbody></table></figure><ul><li><code>-since=&lt;date&gt;</code>：仅显示指定日期之后的提交历史记录。</li></ul><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">log</span> --since=<span class="string">"2023-01-01"</span></span><br></pre></td></tr></tbody></table></figure><ul><li><code>-until=&lt;date&gt;</code>：仅显示指定日期之前的提交历史记录。</li></ul><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">log</span> --<span class="keyword">until</span>=<span class="string">"2023-02-01"</span></span><br></pre></td></tr></tbody></table></figure><ul><li><code>-grep=&lt;pattern&gt;</code>：仅显示包含指定模式的提交消息。</li></ul><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">log</span> --grep=<span class="string">"bug fix"</span></span><br></pre></td></tr></tbody></table></figure><ul><li><code>-follow &lt;file&gt;</code>：跟踪指定文件的改动历史记录。</li></ul><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">log</span> --follow file.txt</span><br></pre></td></tr></tbody></table></figure><ul><li><code>-reverse</code>：按照提交时间的逆序显示提交历史记录。</li></ul><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">log</span> --reverse</span><br></pre></td></tr></tbody></table></figure><h3 id="分支和标签"><a href="#分支和标签" class="headerlink" title="分支和标签"></a>分支和标签</h3><p>默认情况下，<code>git log</code>显示所有分支的提交历史记录。您还可以指定特定的分支或标签。</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">log</span> &lt;branch-name&gt;</span><br></pre></td></tr></tbody></table></figure><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">log</span> &lt;tag-name&gt;</span><br></pre></td></tr></tbody></table></figure><h3 id="其他选项"><a href="#其他选项" class="headerlink" title="其他选项"></a>其他选项</h3><p><code>git log</code>命令还提供其他一些有用的选项，例如：</p><ul><li><code>-stat</code>：显示每个提交的简要统计信息，包括改动的文件和插入/删除的行数。</li></ul><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">log</span> --<span class="built_in">stat</span></span><br></pre></td></tr></tbody></table></figure><ul><li><code>-pretty=&lt;format&gt;</code>：使用自定义的输出格式显示提交历史记录。</li></ul><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">log</span> --pretty=format:<span class="string">"%h - %an, %ar : %s"</span></span><br></pre></td></tr></tbody></table></figure><ul><li><code>-graph</code>和<code>-oneline</code>可以与其他选项组合使用。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Tech Toolbox </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FAISS向量查询简介</title>
      <link href="/2023/02/23/Code%20Chronicles/FAISS%E5%90%91%E9%87%8F%E6%9F%A5%E8%AF%A2%E7%AE%80%E4%BB%8B/"/>
      <url>/2023/02/23/Code%20Chronicles/FAISS%E5%90%91%E9%87%8F%E6%9F%A5%E8%AF%A2%E7%AE%80%E4%BB%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="Faiss"><a href="#Faiss" class="headerlink" title="Faiss"></a>Faiss</h1><h2 id="Install"><a href="#Install" class="headerlink" title="Install"></a>Install</h2><ul><li>支持 CUDA 的 Linux ：<br><code>conda install -c pytorch faiss-gpu</code></li><li>其他：<br><code>conda install -c pytorch faiss-cpu</code></li></ul><h2 id="IndexFlatL2"><a href="#IndexFlatL2" class="headerlink" title="IndexFlatL2"></a><strong><strong>IndexFlatL2</strong></strong></h2><p><strong>IndexFlatL2</strong><br>测量查询向量与加载到索引中的向量之间所有给定点之间的 L2（或欧几里得）距离。它很简单，非常准确，但也不会太快。</p><p>给定一组维度为$d$的向量${ x_1,…, x_n }$，Faiss在Ram中构架一个数据结构——<code>index</code> ，构造完结构后，当给定一个新的维度为$d$向量$x$时，可以高效的执行以下操作：</p><p>$$<br>i = \mathrm{argmin}_i || x - x_i ||<br>$$</p><p>其中$||.||$表示欧氏（Euclidean distance）距离（L2）</p><p>用 Faiss 术语来说，数据结构是一个*<code>index</code><em>，<code>index</code> 是一个具有</em><code>add方法</code>的对象。*add可以用于添加 <code>x_i</code>向量。请注意，假定 <code>x_i</code>是固定的。</p><img src="/2023/02/23/Code%20Chronicles/FAISS%E5%90%91%E9%87%8F%E6%9F%A5%E8%AF%A2%E7%AE%80%E4%BB%8B/faiss1.png" class=""><p>在 Python 中，我们会IndexFlatL2用我们的向量维度（768——我们句子嵌入的输出大小）初始化我们的索引，如下所示：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> faiss</span><br><span class="line">d=sentence_embeddings.shape[<span class="number">1</span>]</span><br><span class="line"><span class="comment"># &gt;&gt; d=768</span></span><br><span class="line">index = faiss.IndexFlatL2(d)</span><br><span class="line">index.is_trained</span><br><span class="line"><span class="comment"># &gt;&gt; True</span></span><br></pre></td></tr></tbody></table></figure><p>通常，我们使用的索引需要我们在加载数据之前对其进行训练。</p><p>在 Faiss 中，**<code>Index</code>** 是建立在向量数据集上的索引结构，用于支持在向量数据集中进行快速相似性搜索。**<code>is_trained</code>** 是 <strong><code>Index</code></strong> 类的一个方法，用于检查索引结构是否已经被训练（即初始化）。</p><p>如果 <strong><code>index.is_trained</code></strong> 返回 True，则表示索引已经被训练并已经准备好接受查询。换句话说，这意味着索引结构已经被初始化，可以对其进行读取、添加或删除向量，并使用它执行相似性搜索操作。如果 <strong><code>index.is_trained</code></strong> 返回 False，则表示索引尚未被训练，并且需要使用向量数据集进行初始化才能进行查询操作。</p><p>准备就绪后，我们加载我们的嵌入和查询，如下所示：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">index.add(sentence_embeddings)</span><br><span class="line">index.ntotal</span><br></pre></td></tr></tbody></table></figure><p><strong><code>add()</code></strong> 是 <strong><code>Index</code></strong> 类的一个方法，用于将向量数据添加到索引中。**<code>sentence_embeddings</code>** 是一个包含向量的数组，每个向量对应一个句子的嵌入。</p><p><strong><code>index.ntotal</code></strong> 是 <strong><code>Index</code></strong> 类的另一个属性，用于返回当前索引中包含的向量数量。在使用 <strong><code>add()</code></strong> 方法将 <strong><code>sentence_embeddings</code></strong> 中的向量添加到索引中后，可以通过调用 <strong><code>index.ntotal</code></strong> 方法来获取索引中已包含的向量数量。这可以用于检查索引是否已正确地添加所有向量。</p><p>例如，如果 <strong><code>sentence_embeddings</code></strong> 中有100个句子的嵌入向量，并且这些向量已通过 <strong><code>add()</code></strong> 方法添加到索引中，则 <strong><code>index.ntotal</code></strong> 方法将返回100，表示索引中现在包含100个向量。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">k = <span class="number">4</span></span><br><span class="line">xq = model.encode([<span class="string">"Someone sprints with a football"</span>])</span><br></pre></td></tr></tbody></table></figure><p>在这段代码中，**<code>k</code>** 是一个整数变量，表示在进行相似性搜索时要返回的最近邻向量的数量。在 Faiss 中，相似性搜索可以使用 <strong><code>Index</code></strong> 类的 <strong><code>search()</code></strong> 方法来完成，该方法将查询向量作为输入，并返回与其最相似的 <strong><code>k</code></strong> 个向量。</p><p>另外，**<code>model.encode(["Someone sprints with a football"])</code>** 是用来计算输入句子的嵌入向量的方法调用。这个方法使用预先训练好的模型将输入的句子转换为一个向量表示，该向量表示包含输入句子的语义信息。</p><p>因此，将上述代码中的两个部分结合起来，可以得到一个查询向量 **<code>xq</code>**，它表示句子 “Someone sprints with a football” 的嵌入向量。然后，可以使用 <strong><code>Index</code></strong> 类的 <strong><code>search()</code></strong> 方法来查找与 <strong><code>xq</code></strong> 最相似的 <strong><code>k</code></strong> 个向量，并返回这些向量的索引列表和相似度得分列表。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line">D, I = index.search(xq, k)  <span class="comment"># search</span></span><br><span class="line"><span class="built_in">print</span>(I)</span><br></pre></td></tr></tbody></table></figure><p>在这段代码中，**<code>%%time</code>** 是 Jupyter Notebook 中的一种魔术命令，用于测量代码单元格的运行时间。**<code>D</code>** 和 <strong><code>I</code></strong> 是在使用 <strong><code>Index</code></strong> 类的 <strong><code>search()</code></strong> 方法进行相似性搜索时返回的两个结果。</p><p>具体来说，**<code>D</code>** 是一个包含相似度得分的数组，表示查询向量 <strong><code>xq</code></strong> 与检索到的 <strong><code>k</code></strong> 个最相似向量之间的相似度。**<code>I</code>** 是一个包含相应向量的索引的数组，表示与查询向量 <strong><code>xq</code></strong> 最相似的 <strong><code>k</code></strong> 个向量在索引数据集中的索引位置。</p><p>因此，将上述代码中的两个部分结合起来，可以使用 **<code>Index</code>**类的 **<code>search()</code>**方法在索引中查找与查询向量 **<code>xq</code>**最相似的 **<code>k</code>**个向量，并返回这些向量的索引列表和相似度得分列表。然后，使用 **<code>print(I)</code>**来输出检索到的最相似的向量的索引列表。由于 **<code>%time</code>**魔术命令被使用，该代码单元格还会打印出该代码单元格的执行时间。</p><h2 id="Partitioning-The-Index"><a href="#Partitioning-The-Index" class="headerlink" title="Partitioning The Index"></a><strong><strong>Partitioning The Index</strong></strong></h2><p>Faiss 允许我们添加多个步骤，这些步骤可以使用许多不同的方法优化我们的搜索。<br>一种流行的方法是将索引划分为 Voronoi 单元</p><img src="/2023/02/23/Code%20Chronicles/FAISS%E5%90%91%E9%87%8F%E6%9F%A5%E8%AF%A2%E7%AE%80%E4%BB%8B/faiss2.png" class=""><p>我们可以将我们的向量想象成每个向量都包含在一个 Voronoi 单元中——当我们引入一个新的查询向量时，我们首先测量它的质心之间的距离，然后将我们的搜索范围限制在该质心的单元内。</p><p>使用这种方法，我们将获取一个查询向量xq，识别它所属的单元格，然后使用我们的IndexFlatL2（或另一个度量）在查询向量和属于该特定单元格的所有其他向量之间进行搜索。</p><p>因此，我们正在缩小搜索范围，生成一个近似答案，而不是精确答案（通过详尽搜索得出）。</p><p>为了实现这一点，我们首先初始化我们的索引IndexFlatL2——但这次，我们使用 L2 索引作为量化器步骤——我们将其输入分区索引IndexIVFFlat。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nlist = <span class="number">50</span>  <span class="comment"># how many cells</span></span><br><span class="line">quantizer = faiss.IndexFlatL2(d)</span><br><span class="line">index = faiss.IndexIVFFlat(quantizer, d, nlist)</span><br></pre></td></tr></tbody></table></figure><p>在这段代码中，<code>nlist</code> 是一个整数变量，用于指定 IVF（inverted file）索引中存储的聚类中心数，即将数据集划分为多少个子集。<code>quantizer</code> 是一个 Faiss 索引对象，用于将向量分配到 IVF 索引的子集中。在这里，我们使用了 Faiss 提供的 <code>IndexFlatL2</code> 类型作为 <code>quantizer</code>，它使用欧几里得距离度量来计算向量之间的相似度，并将向量存储在一个平面的索引结构中。</p><p>另外，<code>d</code> 是一个整数变量，表示嵌入向量的维度大小。这个值是根据预训练的模型和嵌入向量的特征维度确定的。</p><p>最后，<code>index</code> 是一个 Faiss 索引对象，用于支持在向量数据集中进行快速相似性搜索。在这里，我们使用了 <code>IndexIVFFlat</code> 类型作为 <code>index</code>，它使用了一种称为倒排文件（inverted file）的数据结构来组织向量数据集，并使用 <code>quantizer</code> 来将向量分配到不同的子集中。这种索引结构可以加速相似性搜索，并且在存储大规模向量数据集时非常有效。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">index.is_trained</span><br><span class="line">&gt;&gt; <span class="literal">False</span></span><br></pre></td></tr></tbody></table></figure><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">index.train(sentence_embeddings)</span><br><span class="line">index.is_trained</span><br><span class="line">&gt;&gt; <span class="literal">True</span></span><br></pre></td></tr></tbody></table></figure><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">index.add(sentence_embeddings)</span><br><span class="line">index.ntotal</span><br><span class="line">&gt;&gt; <span class="number">14504</span></span><br></pre></td></tr></tbody></table></figure><p>现在我们的索引已经过训练，我们可以像以前一样添加数据。</p><p>让我们使用相同的索引句子嵌入和相同的查询向量再次搜索<code>xq</code></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line">D, I = index.search(xq, k)  <span class="comment"># search</span></span><br><span class="line"><span class="built_in">print</span>(I)</span><br></pre></td></tr></tbody></table></figure><h2 id="Quantization"><a href="#Quantization" class="headerlink" title="Quantization"></a><strong><strong>Quantization</strong></strong></h2><p>到目前为止，我们所有的索引都将我们的向量存储为完整的（例如<code>Flat</code>）向量。现在，在非常大的数据集中，这很快就会成为一个问题。</p><p>Faiss 具有使用乘积量化 (PQ)压缩向量的能力。我们可以将其视为一个额外的近似步骤，其结果与我们使用IVF的结果相似。在 IVF 允许我们通过缩小搜索范围进行近似的情况下，PQ 改为近似计算距离/相似性。</p><img src="/2023/02/23/Code%20Chronicles/FAISS%E5%90%91%E9%87%8F%E6%9F%A5%E8%AF%A2%E7%AE%80%E4%BB%8B/faiss3.png" class=""><ol><li>我们将原始向量拆分为几个子向量。 </li><li>对于每组子向量，我们执行聚类操作——为每个子向量集创建多个质心。 </li><li>在子向量中，我们用它最近的特定集合质心的 ID 替换每个子向量</li></ol><p>我们使用 <code>IndexIVFPQ</code> 训练索引——在添加嵌入之前我们还需要索引</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">m = <span class="number">8</span>  <span class="comment"># number of centroid IDs in final compressed vectors</span></span><br><span class="line">bits = <span class="number">8</span> <span class="comment"># number of bits in each centroid</span></span><br><span class="line"></span><br><span class="line">quantizer = faiss.IndexFlatL2(d)  <span class="comment"># we keep the same L2 distance flat index</span></span><br><span class="line">index = faiss.IndexIVFPQ(quantizer, d, nlist, m, bits)</span><br></pre></td></tr></tbody></table></figure><p>**<code>m</code>**是一个整数变量，表示对每个向量进行矢量量化后，要保留的聚类中心的数量。聚类中心是通过使用 K-means 聚类算法从向量数据集中选择的一组代表性向量，可以用来近似表示原始向量。</p><p>**<code>bits</code>**是一个整数变量，用于指定矢量量化后每个聚类中心的位数。较高的 <strong><code>bits</code></strong><br>值可以提高矢量量化的准确性，但也会增加存储和计算成本。</p><p>另外，**<code>quantizer</code>** 是一个 Faiss 索引对象，用于将向量分配到 IVF 索引的子集中。在这里，我们使用了 Faiss 提供的 <strong><code>IndexFlatL2</code></strong> 类型作为 **<code>quantizer</code>**，它使用欧几里得距离度量来计算向量之间的相似度，并将向量存储在一个平面的索引结构中。</p><p>最后，**<code>index</code>** 是一个 Faiss 索引对象，用于支持在向量数据集中进行快速相似性搜索。在这里，我们使用了 <strong><code>IndexIVFPQ</code></strong> 类型作为 **<code>index</code>**，它使用了一种称为倒排文件（inverted file）的数据结构来组织向量数据集，并使用矢量量化和乘积量化（product quantization）技术来压缩向量。这种索引结构可以加速相似性搜索，并且在存储大规模向量数据集时非常有效。</p><h2 id="Nearest-Neighbour-Indexes-for-Similarity-Search"><a href="#Nearest-Neighbour-Indexes-for-Similarity-Search" class="headerlink" title="Nearest Neighbour Indexes for Similarity Search"></a><strong><strong>Nearest Neighbour Indexes for Similarity Search</strong></strong></h2><h3 id="Flat"><a href="#Flat" class="headerlink" title="Flat"></a>Flat</h3><p>应该首先查看的索引是最简单的——平面索引。</p><p>Flat索引是“平面”的，我们不修改输入向量。由于向量没有近似值或聚类——这些索引产生最准确的结果。我们拥有完美的搜索质量，但这是以大量搜索时间为代价的。使用Flat索引，我们引入查询向量xq并将其与索引中的所有其他全尺寸向量进行比较——计算到每个向量的距离。</p><img src="/2023/02/23/Code%20Chronicles/FAISS%E5%90%91%E9%87%8F%E6%9F%A5%E8%AF%A2%E7%AE%80%E4%BB%8B/faiss4.png" class=""><p>使用Flat索引，我们将搜索查询<strong>xq</strong>与索引中的每个其他向量进行比较。</p><p>在计算完所有这些距离后，我们将返回最近的 k 个作为我们最近的匹配项。<br>k 最近邻 (kNN) 搜索。</p><img src="/2023/02/23/Code%20Chronicles/FAISS%E5%90%91%E9%87%8F%E6%9F%A5%E8%AF%A2%E7%AE%80%E4%BB%8B/faiss5.png" class=""><p>那么什么时候应该使用扁平索引呢？当搜索质量无疑是一个高优先级时——搜索速度就不那么重要了。此外对于较小的数据集，搜索速度可能是一个无关紧要的因素——尤其是在使用更强大的硬件时。</p><p>简而言之，在以下情况下使用平面索引：</p><ol><li>搜索质量是一个非常高的优先级。</li><li>搜索时间无关紧要或使用小索引（&lt;10K）时。</li></ol><p>怎样才能使我们的搜索更快呢？有两种主要方法：</p><ol><li>减少向量大小——通过降维或减少表示向量值的位数。</li><li>缩小搜索范围——我们可以根据某些属性、相似性或距离将向量聚类或组织成树结构——并将我们的搜索限制在最近的集群或过滤最相似的分支。</li></ol><p>使用这两种方法中的任何一种都意味着我们不再执行详尽的最近邻搜索，而是执行近似最近邻 (ANN) 搜索——因为我们不再搜索整个全数据集。</p><h3 id="Locality-Sensitive-Hashing"><a href="#Locality-Sensitive-Hashing" class="headerlink" title="Locality Sensitive Hashing"></a><strong>Locality Sensitive Hashing</strong></h3><p>局部敏感哈希（Locality-Sensitive Hashing，LSH）是一种用于在高维空间中快速近似搜索相似对象的技术。在很多现实世界的问题中，我们需要对高维向量（比如图像、音频、文本等）进行相似性搜索，但是传统的线性搜索方法在高维空间中效率非常低下，因为随着维度的增加，搜索的复杂度呈指数级增长。</p><p>LSH是一种通过哈希函数将相似的向量映射到同一个“桶”中的技术，因此可以大大减少需要比较的向量数量，从而提高搜索效率。具体来说，LSH将每个向量映射到多个哈希表中，每个哈希表由多个哈希函数组成。对于一个查询向量，LSH会将其映射到每个哈希表中，然后只对同一个桶中的向量进行相似性比较。</p><p>LSH可以根据不同的相似性度量来设计不同的哈希函数，例如欧几里得距离、余弦相似度等。不同的哈希函数可以在不同的空间中捕捉到向量的不同特征，从而适应不同的应用场景。</p><p>局部敏感哈希 (LSH) 的工作原理是将向量分组到桶中，方法是通过哈希函数处理每个向量，该哈希函数最大化哈希冲突，而不是像通常使用哈希函数那样最小化。</p><p>这意味着什么？假设我们有一个 Python 字典。当我们在字典中创建一个新的键值对时，我们使用散列函数对键进行散列。这个键的哈希值决定了我们存储其各自值的“桶”：</p><img src="/2023/02/23/Code%20Chronicles/FAISS%E5%90%91%E9%87%8F%E6%9F%A5%E8%AF%A2%E7%AE%80%E4%BB%8B/faiss6.png" class=""><p>Python 字典是哈希表的一个示例，它使用典型的哈希函数来<em>最小化</em>哈希冲突，即两个不同对象（键）产生相同哈希的哈希冲突。</p><p>在我们的字典中，我们希望避免这些冲突，因为这意味着我们会将多个对象映射到一个键——但对于 LSH，我们希望最大化<em>散列</em>冲突。</p><p>为什么我们要最大化碰撞？那么，对于搜索，我们使用 LSH 将相似的对象分组在一起。当我们引入一个新的查询对象（或向量）时，我们的 LSH 算法可以用来找到最接近的匹配组：</p><img src="/2023/02/23/Code%20Chronicles/FAISS%E5%90%91%E9%87%8F%E6%9F%A5%E8%AF%A2%E7%AE%80%E4%BB%8B/faiss7.png" class=""><p>我们的 LSH 散列函数试图最大化散列冲突，产生向量分组。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">nbits = d*<span class="number">4</span>  <span class="comment"># resolution of bucketed vectors</span></span><br><span class="line"><span class="comment"># initialize index and add vectors</span></span><br><span class="line">index = faiss.IndexLSH(d, nbits)</span><br><span class="line">index.add(sentence_embeddings)</span><br><span class="line"><span class="comment"># and search</span></span><br><span class="line">D, I = index.search(xq, k)</span><br></pre></td></tr></tbody></table></figure><p>这段代码使用了Faiss库中的LSH索引，其中d是向量的维度，nbits是哈希值的位数，nbits的取值通常是d的某个倍数，这里设置为d*4。</p><p>在初始化索引后，代码通过**<code>add</code><strong>方法将所有的sentence embeddings添加到LSH索引中。接下来，代码通过</strong><code>search</code>**方法在LSH索引中搜索与查询向量xq最相似的k个向量，返回的D是相似度分数，I是对应的向量索引。</p><p>值得注意的是，Faiss的LSH索引使用哈希函数将向量映射到桶（bucket）中，每个桶中包含一组相似的向量。因此，LSH索引适用于高维稀疏向量的相似性搜索，其中相似向量集中在少数的桶中，从而减少搜索的时间复杂度。但是，LSH索引的准确性可能会受到哈希冲突的影响，需要根据具体的应用需求进行调整。</p><p><code>nbits</code>是指散列向量的“分辨率”。更高的<code>nbits</code>值意味着更高的准确性，但会占用更多的内存和更慢的搜索速度。一般情况下，nbits越大，哈希计算复杂度也越高。这是因为nbits的增加会使得哈希值空间变得更大，从而增加计算哈希值所需要的运算量和存储空间。</p><h3 id="Hierarchical-Navigable-Small-World-Graphs"><a href="#Hierarchical-Navigable-Small-World-Graphs" class="headerlink" title="Hierarchical Navigable Small World Graphs"></a><strong><strong>Hierarchical Navigable Small World Graphs</strong></strong></h3><p>Hierarchical Navigable Small World（HNSW）是一种用于高维向量索引的算法，旨在提供快速和准确的相似度搜索。它是在Small World网络和Navigable Small World算法的基础上进一步发展而来的。</p><img src="/2023/02/23/Code%20Chronicles/FAISS%E5%90%91%E9%87%8F%E6%9F%A5%E8%AF%A2%E7%AE%80%E4%BB%8B/faiss8.png" class=""><p>HNSW算法将高维空间中的向量表示为节点，并构建一棵树结构来组织这些节点。树中的每个节点都表示一个向量，并保存该向量在索引中的位置以及与其他节点的相似度信息。HNSW使用近似的相似度计算方法来连接节点，这使得树的结构可以在高维空间中快速导航。</p><p>在构建HNSW索引时，首先构建一个稠密的初始图。然后，将节点逐步添加到图中，并使用近似的相似度计算方法来连接节点。这些连接在不同层次的树结构中被建立，从而形成了一组层次结构。HNSW使用这种层次结构来加速相似度搜索，从而提高了搜索效率。</p><img src="/2023/02/23/Code%20Chronicles/FAISS%E5%90%91%E9%87%8F%E6%9F%A5%E8%AF%A2%E7%AE%80%E4%BB%8B/faiss9.png" class=""><p>相比于传统的树型结构和线性扫描方法，HNSW具有更高的搜索效率和更好的可扩展性。它在大规模高维向量的相似度搜索任务中表现出色，并被广泛应用于图像、文本、语音等领域的数据挖掘和机器学习任务中。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># set HNSW index parameters</span></span><br><span class="line">M = <span class="number">64</span>  <span class="comment"># number of connections each vertex will have</span></span><br><span class="line">ef_search = <span class="number">32</span>  <span class="comment"># depth of layers explored during search</span></span><br><span class="line">ef_construction = <span class="number">64</span>  <span class="comment"># depth of layers explored during index construction</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># initialize index (d == 128)</span></span><br><span class="line">index = faiss.IndexHNSWFlat(d, M)</span><br><span class="line"><span class="comment"># set efConstruction and efSearch parameters</span></span><br><span class="line">index.hnsw.efConstruction = ef_construction</span><br><span class="line">index.hnsw.efSearch = ef_search</span><br><span class="line"><span class="comment"># add data to index</span></span><br><span class="line">index.add(wb)</span><br><span class="line"></span><br><span class="line"><span class="comment"># search as usual</span></span><br><span class="line">D, I = index.search(wb, k)</span><br></pre></td></tr></tbody></table></figure><p>这段代码使用了HNSW算法来构建高维向量的索引，并进行相似度搜索。</p><p>其中</p><ul><li><code>M</code>是每个节点连接的近邻数目，即每个节点在构建索引时最多连接M个最近邻节点。</li><li><code>ef_search</code>是在搜索时遍历的层数，即搜索的深度，</li><li><code>ef_construction</code>是在构建索引时使用的遍历层数。</li></ul><p>这些参数可以调整来平衡搜索时间和索引构建时间之间的权衡。</p><p><code>M</code>和<code>efSearch</code>对搜索时间有更大的影响；<code>efConstruction</code>主要是增加了索引构建时间（意味着更慢index.add）</p><p>接下来，使用<code>faiss.IndexHNSWFlat</code>初始化HNSW索引。然后，将<code>efConstruction</code>和<code>efSearch</code>参数设置为预定义的值。最后，使用<code>index.add</code>方法将向量数据添加到索引中。</p><p>最后一行代码使用<code>index.search</code>方法进行搜索。它会返回查询向量<code>wb</code>在索引中的k个最近邻向量的距离和索引位置。</p>]]></content>
      
      
      <categories>
          
          <category> Code Chronicles </category>
          
      </categories>
      
      
        <tags>
            
            <tag> FAISS </tag>
            
            <tag> Embedding </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Intersection &amp; Union</title>
      <link href="/2023/02/20/Code%20Chronicles/Python-IntersectionUnion/"/>
      <url>/2023/02/20/Code%20Chronicles/Python-IntersectionUnion/</url>
      
        <content type="html"><![CDATA[<h1 id="Intersection-Union"><a href="#Intersection-Union" class="headerlink" title="Intersection &amp; Union"></a>Intersection &amp; Union</h1><p>交集和并集</p><h1 id="Intersection"><a href="#Intersection" class="headerlink" title="Intersection"></a>Intersection</h1><p>Python中的<code>intersection()</code>方法是用于获取两个集合的交集。这个方法是set对象的方法，因此它只能用于set对象中。</p><p>如果您想在其他类型的容器中获取交集，可以使用Python的内置函数set()将它们转换为set对象。例如，您可以使用以下代码来获取两个列表的交集：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</span><br><span class="line">b = [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]</span><br><span class="line">intersection = <span class="built_in">set</span>(a).intersection(<span class="built_in">set</span>(b))</span><br></pre></td></tr></tbody></table></figure><p>这里，我们首先使用set()将两个列表a和b转换为set对象，然后使用intersection()方法获取它们的交集。注意，由于集合是无序的，因此结果集合中的元素顺序可能与原始列表中的顺序不同。</p><p>除了set对象之外，Python的字典类型也具有类似的intersection()方法，用于获取两个字典的相同键的交集，但这与上述情况不同。</p><h1 id="Union"><a href="#Union" class="headerlink" title="Union"></a>Union</h1><p>如果想获取两个容器的并集，可以使用Python的内置函数<code>set()</code>将它们转换为set对象，然后使用<code>union()</code>方法获取它们的并集。例如，以下代码可以获取两个列表的并集：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</span><br><span class="line">b = [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]</span><br><span class="line">union = <span class="built_in">set</span>(a).union(<span class="built_in">set</span>(b))</span><br></pre></td></tr></tbody></table></figure><p>这里，我们首先使用<code>set()</code>将两个列表<code>a</code>和<code>b</code>转换为<code>set</code>对象，然后使用<code>union()</code>方法获取它们的并集。注意，由于集合是无序的，因此结果集合中的元素顺序可能与原始列表中的顺序不同。</p><p>除了set对象之外，Python的字典类型不支持并集操作。如果您需要对字典进行并集操作，可以将其键或值转换为set对象，然后进行集合操作。例如，以下代码可以获取两个字典的键的并集：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = {<span class="string">'x'</span>: <span class="number">1</span>, <span class="string">'y'</span>: <span class="number">2</span>, <span class="string">'z'</span>: <span class="number">3</span>}</span><br><span class="line">b = {<span class="string">'z'</span>: <span class="number">4</span>, <span class="string">'w'</span>: <span class="number">5</span>}</span><br><span class="line">union = <span class="built_in">set</span>(a.keys()).union(<span class="built_in">set</span>(b.keys()))</span><br></pre></td></tr></tbody></table></figure><p>这里，我们将两个字典的键转换为set对象，然后使用union()方法获取它们的并集。</p>]]></content>
      
      
      <categories>
          
          <category> Code Chronicles </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python Basic </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python -m 参数解释</title>
      <link href="/2023/02/01/Code%20Chronicles/Python%20-m%20%E5%8F%82%E6%95%B0%E8%A7%A3%E9%87%8A/"/>
      <url>/2023/02/01/Code%20Chronicles/Python%20-m%20%E5%8F%82%E6%95%B0%E8%A7%A3%E9%87%8A/</url>
      
        <content type="html"><![CDATA[<h1 id="Python-m-参数解释"><a href="#Python-m-参数解释" class="headerlink" title="Python -m 参数解释"></a>Python -m 参数解释</h1><h2 id="1-Python-m-参数解释"><a href="#1-Python-m-参数解释" class="headerlink" title="1. Python -m 参数解释"></a>1. Python -m 参数解释</h2><h3 id="在shell调用时，python-m的参数是什么意思？"><a href="#在shell调用时，python-m的参数是什么意思？" class="headerlink" title="在shell调用时，python -m的参数是什么意思？"></a>在shell调用时，python -m的参数是什么意思？</h3><p>当在命令行中输入 “python -m <module-name>“ 时，这表示使用Python的内置模块运行程序，其中<module-name>是要运行的模块的名称。这类似于在Python脚本中使用 “import <module-name>“ 并运行 “module-name.run()”。</module-name></module-name></module-name></p><p>例如： “python -m http.server” 将启动Python内置的HTTP服务器，可以在浏览器中访问当前目录中的文件。 “python -m unittest discover” 将在当前目录中查找并运行所有以test_*.py命名的单元测试脚本。</p><h3 id="module-name-run-需要自己在模块中编写run函数吗？"><a href="#module-name-run-需要自己在模块中编写run函数吗？" class="headerlink" title="module-name.run() 需要自己在模块中编写run函数吗？"></a>module-name.run() 需要自己在模块中编写run函数吗？</h3><p>不需要。在调用 “python -m <module-name>“ 时，Python会自动在模块中寻找并运行名为 “run()” 的函数。如果该函数不存在，则会在模块中寻找并运行名为 “main()” 的函数。</module-name></p><p>在大多数情况下，如果模块是一个可执行程序，则应该在其中编写一个名为 “main()” 的函数，并在该函数中编写程序逻辑。</p><p>如果模块是一个库，而不是可执行程序，则不需要编写 “main()” 或 “run()” 函数。这些函数不会被调用，因为模块被导入时，其中的函数和变量都可以在其他代码中调用。</p>]]></content>
      
      
      <categories>
          
          <category> Code Chronicles </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python Basic </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python yield</title>
      <link href="/2023/02/01/Code%20Chronicles/Python-yield/"/>
      <url>/2023/02/01/Code%20Chronicles/Python-yield/</url>
      
        <content type="html"><![CDATA[<h1 id="Python-yield"><a href="#Python-yield" class="headerlink" title="Python yield"></a>Python yield</h1><p>在 Python 中，**<code>yield</code>** 是一个关键字，它通常用于生成器函数中，用于生成序列化的值而不需要将整个序列保存在内存中。</p><p>当函数被调用并包含 <strong><code>yield</code></strong> 语句时，它并不会立即执行函数体的所有代码。相反，它将返回一个生成器对象，每次调用生成器对象的 <strong><code>__next__()</code></strong> 方法时，函数体将从上次 <strong><code>yield</code></strong> 语句停止的位置继续执行，直到遇到下一个 <strong><code>yield</code></strong> 语句或函数结束。</p><p>举个例子，下面的代码展示了一个简单的生成器函数，它使用 <strong><code>yield</code></strong> 语句产生数字序列：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">number_generator</span>(<span class="params">n</span>):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        <span class="keyword">yield</span> i</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用生成器对象打印数字序列</span></span><br><span class="line">my_generator = number_generator(<span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">next</span>(my_generator))  <span class="comment"># 0</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">next</span>(my_generator))  <span class="comment"># 1</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">next</span>(my_generator))  <span class="comment"># 2</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">next</span>(my_generator))  <span class="comment"># 3</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">next</span>(my_generator))  <span class="comment"># 4</span></span><br></pre></td></tr></tbody></table></figure><p>这里，**<code>number_generator()</code>** 函数使用 <strong><code>yield</code></strong> 语句产生数字序列。当函数被调用时，它将返回一个生成器对象 **<code>my_generator</code>**。每次调用生成器对象的 <strong><code>__next__()</code></strong> 方法时，函数体将从上次 <strong><code>yield</code></strong> 语句停止的位置继续执行，直到函数结束或者再次遇到 <strong><code>yield</code></strong> 语句。由于生成器只在需要时才产生值，因此可以减少内存的占用。</p>]]></content>
      
      
      <categories>
          
          <category> Code Chronicles </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python Basic </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo博客的创建与部署</title>
      <link href="/2023/01/11/Tech%20Toolbox/Hexo%20%E5%8D%9A%E5%AE%A2%E7%9A%84%E5%88%9B%E5%BB%BA%E4%B8%8E%E9%83%A8%E7%BD%B2/"/>
      <url>/2023/01/11/Tech%20Toolbox/Hexo%20%E5%8D%9A%E5%AE%A2%E7%9A%84%E5%88%9B%E5%BB%BA%E4%B8%8E%E9%83%A8%E7%BD%B2/</url>
      
        <content type="html"><![CDATA[<p>（生活反思）<br>（代码编年史）<br>（流浪癖笔记）<br>NLP Insights（自然语言处理洞察）<br>Tech Toolbox（技术工具箱）<br>Travel Tales（旅行故事）<br>Debugging Diaries</p><blockquote><p>前言：大家好，我是博主黑头呆鱼。之前我的旧电脑退休了，这导致我之前博客的内容找不到了。所以，我决定在新博客的第一篇文章中分享如何创建博客并上传源代码到 GitHub。现在，让我们开始吧！</p></blockquote><h1 id="Hexo-博客的创建与部署"><a href="#Hexo-博客的创建与部署" class="headerlink" title="Hexo 博客的创建与部署"></a>Hexo 博客的创建与部署</h1><p>以下是创建新的 Hexo 博客并部署到 GitHub 的详细步骤：</p><h2 id="安装前置软件"><a href="#安装前置软件" class="headerlink" title="安装前置软件"></a>安装前置软件</h2><h3 id="安装-Node-js-和-npm"><a href="#安装-Node-js-和-npm" class="headerlink" title="安装 Node.js 和 npm"></a>安装 Node.js 和 npm</h3><p>Hexo 是基于 Node.js 构建的，所以首先你需要安装 Node.js 和 npm（Node 包管理器）。访问 <a href="https://nodejs.org/">Node.js 的官方网站</a> 进行下载安装。</p><h3 id="安装-Hexo"><a href="#安装-Hexo" class="headerlink" title="安装 Hexo"></a>安装 Hexo</h3><p>在 Node.js 和 npm 安装完成后，通过 npm 全局安装 Hexo。在命令行中运行以下命令：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install -g hexo-cli</span><br></pre></td></tr></tbody></table></figure><h2 id="创建新的-Hexo-博客"><a href="#创建新的-Hexo-博客" class="headerlink" title="创建新的 Hexo 博客"></a>创建新的 Hexo 博客</h2><h3 id="初始化新的-Hexo-博客"><a href="#初始化新的-Hexo-博客" class="headerlink" title="初始化新的 Hexo 博客"></a>初始化新的 Hexo 博客</h3><p>创建一个新的文件夹作为你的博客的根目录，然后在命令行中运行以下命令：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hexo init blog</span><br><span class="line"><span class="built_in">cd</span> blog</span><br></pre></td></tr></tbody></table></figure><p>这将在 “blog” 文件夹下创建一个新的 Hexo 博客。</p><h3 id="安装博客依赖"><a href="#安装博客依赖" class="headerlink" title="安装博客依赖"></a>安装博客依赖</h3><p>进入你的博客目录，然后运行以下命令来安装博客所需的依赖：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install</span><br></pre></td></tr></tbody></table></figure><h2 id="配置你的博客"><a href="#配置你的博客" class="headerlink" title="配置你的博客"></a>配置你的博客</h2><h3 id="配置-Hexo"><a href="#配置-Hexo" class="headerlink" title="配置 Hexo"></a>配置 Hexo</h3><p>使用你的文本编辑器打开 <code>_config.yml</code> 文件，这是 Hexo 博客的配置文件。你需要将 <code>url</code> 设置为你的 GitHub Pages 的 URL（通常是 <code>https://&lt;username&gt;.github.io</code>），并且你可能还想配置其他的一些选项，比如博客的标题、描述和作者信息。</p><h2 id="部署到-GitHub"><a href="#部署到-GitHub" class="headerlink" title="部署到 GitHub"></a>部署到 GitHub</h2><h3 id="安装-Hexo-部署插件"><a href="#安装-Hexo-部署插件" class="headerlink" title="安装 Hexo 部署插件"></a>安装 Hexo 部署插件</h3><p>首先，你需要安装 <code>hexo-deployer-git</code> 插件，这个插件可以让你直接将你的博客部署到 GitHub。在命令行中运行以下命令来安装：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-deployer-git --save</span><br></pre></td></tr></tbody></table></figure><h3 id="配置部署参数"><a href="#配置部署参数" class="headerlink" title="配置部署参数"></a>配置部署参数</h3><p>在 <code>_config.yml</code> 文件中添加以下配置：</p><figure class="highlight yml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">deploy:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">git</span></span><br><span class="line">  <span class="attr">repository:</span> <span class="string">git@github.com:&lt;username&gt;/&lt;username&gt;.github.io.git</span></span><br><span class="line">  <span class="attr">branch:</span> <span class="string">master</span></span><br></pre></td></tr></tbody></table></figure><p>将 <code>&lt;username&gt;</code> 替换为你的 GitHub 用户名。</p><h3 id="生成静态文件并部署"><a href="#生成静态文件并部署" class="headerlink" title="生成静态文件并部署"></a>生成静态文件并部署</h3><p>在命令行中运行以下命令来生成静态文件并将它们部署到 GitHub：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hexo generate</span><br><span class="line">hexo deploy</span><br></pre></td></tr></tbody></table></figure><p>或者你可以使用下面的单个命令来完成这两个步骤：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo g -d</span><br></pre></td></tr></tbody></table></figure><h2 id="添加新文章"><a href="#添加新文章" class="headerlink" title="添加新文章"></a>添加新文章</h2><p>你可以使用 Hexo 的 <code>new</code> 命令来快速创建新的文章。在命令行中运行以下命令：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new <span class="string">"文章标题"</span></span><br></pre></td></tr></tbody></table></figure><p>将 “文章标题” 替换为你想要的文章标题。这将在 <code>source/_posts</code> 目录下创建一个新的 Markdown 文件，文件名就是你指定的文章标题（把空格替换为 <code>-</code>）。</p><p>你可以使用任何你喜欢的文本编辑器打开这个文件，并在里面写下你的文章内容。Hexo 使用 Markdown 语法，你可以查看 <a href="https://markdown-zh.readthedocs.io/en/latest/">Markdown 语法手册</a> 来学习如何使用 Markdown。</p><p>完成后，你可以重新生成并部署你的博客，新的文章就会出现在你的博客上了。</p><h2 id="文章分类"><a href="#文章分类" class="headerlink" title="文章分类"></a>文章分类</h2><p>你可以在你的文章中使用 YAML 前置课（Front-matter）来为文章分配分类（categories）和标签（tags）。前置课应该放在每篇文章的顶部，举例如下：</p><figure class="highlight markdown"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: 文章标题</span><br><span class="line">date: 2023-07-06 00:00:00</span><br><span class="line">categories:</span><br><span class="line"><span class="bullet">-</span> 分类1</span><br><span class="line"><span class="bullet">-</span> 分类2</span><br><span class="line">tags:</span><br><span class="line"><span class="bullet">-</span> 标签1</span><br><span class="line"><span class="section">- 标签2</span></span><br><span class="line"><span class="section">---</span></span><br><span class="line"></span><br><span class="line">这里是文章的内容。</span><br></pre></td></tr></tbody></table></figure><p>在这个例子中，这篇文章被分配到了 “分类1” 和 “分类2” 这两个分类，同时也被分配了 “标签1” 和 “标签2” 这两个标签。</p><p>当你生成你的博客时，Hexo 会自动根据这些分类和标签创建索引，访问者可以通过分类和标签来查找文章。</p><h2 id="将博客源文件保存到-GitHub"><a href="#将博客源文件保存到-GitHub" class="headerlink" title="将博客源文件保存到 GitHub"></a>将博客源文件保存到 GitHub</h2><h3 id="创建一个新的-GitHub-仓库"><a href="#创建一个新的-GitHub-仓库" class="headerlink" title="创建一个新的 GitHub 仓库"></a>创建一个新的 GitHub 仓库</h3><p>登录到你的 GitHub 账号，然后创建一个新的仓库。你可以给这个仓库取任何你喜欢的名字，比如 <code>my-hexo-blog</code>。不需要初始化 README、.gitignore 或者许可证。</p><h3 id="初始化-Git"><a href="#初始化-Git" class="headerlink" title="初始化 Git"></a>初始化 Git</h3><p>在你的博客目录中，运行以下命令来初始化 Git：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git init</span><br></pre></td></tr></tbody></table></figure><h3 id="添加所有文件到-Git"><a href="#添加所有文件到-Git" class="headerlink" title="添加所有文件到 Git"></a>添加所有文件到 Git</h3><p>运行以下命令来添加所有文件到 Git：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git add .</span><br></pre></td></tr></tbody></table></figure><h3 id="提交你的更改"><a href="#提交你的更改" class="headerlink" title="提交你的更改"></a>提交你的更改</h3><p>运行以下命令来提交你的更改：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git commit -m <span class="string">"Initial commit"</span></span><br></pre></td></tr></tbody></table></figure><h3 id="添加远程仓库"><a href="#添加远程仓库" class="headerlink" title="添加远程仓库"></a>添加远程仓库</h3><p>运行以下命令来添加你刚才在 GitHub 上创建的仓库作为远程仓库：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git remote add origin git@github.com:&lt;username&gt;/my-hexo-blog.git</span><br></pre></td></tr></tbody></table></figure><p>将 <code>&lt;username&gt;</code> 替换为你的 GitHub 用户名。</p><h3 id="推送到-GitHub"><a href="#推送到-GitHub" class="headerlink" title="推送到 GitHub"></a>推送到 GitHub</h3><p>运行以下命令来将你的博客源文件推送到 GitHub：</p><pre><code class="bash">git push -u origin master</code></pre><p>完成这些步骤后，你的 Hexo 博客就已经部署到 GitHub Pages 上了。你可以访问 <code>https://&lt;username&gt;.github.io</code> 来查看你的博客。未来每次你想要添加新的文章，只需在 <code>source/_posts</code> 目录下添加新的 Markdown 文件，然后重新生成并部署你的博客就可以了。</p><p>在未来，每次你修改了博客源文件（比如添加新的文章），你都需要运行 <code>git add .</code>，<code>git commit -m "your message"</code> 和 <code>git push</code> 命令来更新你在 GitHub 上的备份。</p>]]></content>
      
      
      <categories>
          
          <category> Tech Toolbox </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Blog </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>南洋的椰风海韵</title>
      <link href="/2022/08/09/Wanderlust%20Adventures/%E5%8D%97%E6%B4%8B%E7%9A%84%E6%A4%B0%E9%A3%8E%E6%B5%B7%E9%9F%B5/"/>
      <url>/2022/08/09/Wanderlust%20Adventures/%E5%8D%97%E6%B4%8B%E7%9A%84%E6%A4%B0%E9%A3%8E%E6%B5%B7%E9%9F%B5/</url>
      
        <content type="html"><![CDATA[<h1 id="南洋的椰风海韵"><a href="#南洋的椰风海韵" class="headerlink" title="南洋的椰风海韵"></a>南洋的椰风海韵</h1><img src="/2022/08/09/Wanderlust%20Adventures/%E5%8D%97%E6%B4%8B%E7%9A%84%E6%A4%B0%E9%A3%8E%E6%B5%B7%E9%9F%B5/%E5%8D%97%E6%B4%8B%E7%9A%84%E6%A4%B0%E9%A3%8E%E6%B5%B7%E9%9F%B5.png" class="" title="南洋的椰风海韵"><p>轻柔的椰风，吹拂着南洋的海滨，<br>翠绿的棕榈，舞动在碧蓝的天际。<br>大海的波涛，如歌如泣，耳畔回荡。<br>潮起潮落间，悠远涛声如诗韵鸣响，</p><p>沿着海岸线，白色浪花轻轻拥抱沙滩，<br>细细沙粒，脚下轻轻润湿。<br>远处礁石，静静凝望海的无尽辽阔，<br>仿佛古老智者，守护秘密。</p><p>夜幕降临，星空如璀璨珠宝散落天穹，<br>海风带来盈盈月光，如银河倾泻而下，<br>椰树摇曳，带入无边诗画，<br>梦想和希望，交织成美丽图景，<br>永铭南洋椰风海韵，灵魂驿站。</p>]]></content>
      
      
      <categories>
          
          <category> Wanderlust Adventures </category>
          
      </categories>
      
      
    </entry>
    
    
  
  
</search>
