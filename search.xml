<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>LONGNET - Scaling Transformers to 1,000,000,000 Tokens</title>
      <link href="/2023/07/28/NLP%20Insights/LONGNET:%20Scaling%20Transformers%20to%201,000,000,000%20Tokens/"/>
      <url>/2023/07/28/NLP%20Insights/LONGNET:%20Scaling%20Transformers%20to%201,000,000,000%20Tokens/</url>
      
        <content type="html"><![CDATA[<h1 id="LONGNET：将Transformer扩展到10亿个标记"><a href="#LONGNET：将Transformer扩展到10亿个标记" class="headerlink" title="LONGNET：将Transformer扩展到10亿个标记"></a>LONGNET：将Transformer扩展到10亿个标记</h1><p>在本篇文章中，我们将详细讨论一个近期发布的先进模型——“LongNet”。该模型由微软亚洲研究院研发，于大约两周前正式公布。LongNet基于Transformer模型构建，其核心理念在于拓展Transformer的应用规模。值得一提的是，研究团队成功地将其扩展至处理10亿个令牌的规模。对于熟悉语言模型的人来说，会明白序列长度对模型性能的影响，因为序列长度决定了在执行注意力机制时，能够关联的令牌数量，从而影响模型可以获取的上下文信息长度。例如，我们希望像GPT这样的模型能拥有更长的上下文，使得模型可以参考更久之前的单词来预测下一个令牌。而LongNet就成功地将这个能力扩展到了10亿个令牌。以下图为例，可以清晰看出，GPT的序列长度仅为512，而Power Transformer的序列长度可扩展至12、000、64、262、000、甚至1000万，然而LongNet将序列长度扩展至惊人的10亿个令牌。试想一下，我们可以将所有维基百科的文本信息输入到模型中，模型可以利用所有这些令牌进行注意力计算。接下来，让我们首先来了解一下LongNet的工作原理。</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>在大型语言模型时代，扩展序列长度已成为一个重要需求。然而，现有方法在处理计算复杂性或模型表达能力时遇到困难，导致最大序列长度受限。为了解决这个问题，我们引入了LONGNET，这是一种Transformer变体，可以将序列长度扩展到10亿个标记以上，同时不损害对较短序列的性能。具体而言，我们提出了扩张注意力（dilated attention），随着距离增加，它以指数级扩展注意力范围。LONGNET有显著的优势：1）它具有线性计算复杂性，并且序列中任意两个标记之间存在对数依赖关系；2）它可以作为分布式训练器用于非常长的序列；3）它的扩张注意力可以无缝替换标准注意力，并且可以与现有基于Transformer的优化方法无缝集成。实验结果表明，LONGNET在长序列建模和一般语言任务上表现出强大的性能。我们的工作为建模非常长的序列打开了新的可能性，例如将整个语料库甚至整个互联网视为一个序列。</p><h3 id="LongNet的优点"><a href="#LongNet的优点" class="headerlink" title="LongNet的优点"></a>LongNet的优点</h3><p>LongNet具有多种优点。首先，其计算复杂度与序列长度呈线性关系，稍后将具体解释原因。其次，令牌之间存在对数依赖，也就是说，两个距离较远的令牌之间的依赖性较弱，而距离较近的令牌之间的依赖性较强。此外，它可在分布式网络中进行训练，这意味着我们可以利用分布式系统计算该注意力机制，如使用多个GPU或多台计算机。同时，LongNet可以作为标准注意力的替代品，这意味着如果我们已经有一个使用注意力机制的模型，我们只需将注意力机制替换为LongNet的机制，无需改变模型的其他部分，模型仍然能够像以前一样运行，但通过使用这种改进的注意力机制，可以处理更长的序列长度。</p><h3 id="对Transformer模型的回顾"><a href="#对Transformer模型的回顾" class="headerlink" title="对Transformer模型的回顾"></a>对Transformer模型的回顾</h3><p>如果您对Transformer模型还不太了解，建议您观看我之前发布的视频，其中详细解释了注意力机制和Transformer模型的基本原理。在这里，我将简要复习以前的注意力机制以便您理解其中存在的问题。在我之前的视频中，我详细解释了自注意力机制，我们使用了称为”Q、K和V”的矩阵。Q矩阵代表查询，其大小为”序列长度x模型大小”，模型大小表示每个词嵌入的向量表示。当我们计算查询与键（K）的乘积或与K的转置的乘积时，以生成这个矩阵，所需的操作次数是”n的平方乘以模型大小”，因为我们需要为矩阵中的每个元素计算点积。这就是为什么自注意力的复杂度是”n的平方乘以模型大小”。同时，这个比较也在相关文献中得到了详述，传统的注意力复杂度是”n的平方乘以模型大小”，但是LongNet这种新型模型的注意力机制复杂度是”n乘以模型大小”，我将在稍后解释如何实现这种线性复杂度。</p><h3 id="关于Transformer模型的梳理"><a href="#关于Transformer模型的梳理" class="headerlink" title="关于Transformer模型的梳理"></a>关于Transformer模型的梳理</h3><p>自注意力机制，我们使用了被称为“Q、K、V”的矩阵。其中，“Q”矩阵代表查询，其规模为“序列长度乘以模型大小”，模型大小指的是每个词嵌入的向量表示。当我们计算查询与键（K）的乘积，或者查询与K的转置的乘积来产生此矩阵时，所需的操作次数是“序列长度的平方乘以模型大小”，因为我们需要为矩阵中的每个元素计算点积。这就是为什么自注意力的复杂度是“序列长度的平方乘以模型大小”。这个比较在相关论文中也有详细描述，常规的注意力复杂度是“序列长度的平方乘以模型大小”，然而LongNet这种新模型，其注意力机制复杂度仅为“序列长度乘以模型大小”，下文我将说明如何实现这种线性复杂度。</p><h3 id="LongNet的注意力分配原理"><a href="#LongNet的注意力分配原理" class="headerlink" title="LongNet的注意力分配原理"></a>LongNet的注意力分配原理</h3><p>LongNet的核心原理是，令牌间的注意力分配会随着它们之间距离的增加而呈指数级地减小。让我们参照图表来理解它的运作方式。在传统方式中，我们计算所有令牌与其他所有令牌之间的注意力，但LongNet并未如此操作。它采用了一种将序列切分为不同大小窗口的方法。首先，以4为窗口大小为例，这里的“N”是序列的令牌数，我们将其分成四个大小为4的段，并计算这个小窗口内的所有词与其他词之间的注意力。然后，我们对所有这些小段中的词执行同样的操作，接着使用更大的窗口，这次窗口大小为8。如此类推，直到覆盖整个序列长度，然后我们再以增加窗口大小的方式进行操作，同时我们也增加了跳过的令牌数，即“R”。例如，我们可以先计算大小为8的窗口，然后将“R”设为2，这意味着我们会跳过一个令牌，然后计算注意力，再跳过一个令牌，继续计算注意力。以这种方式，随着窗口大小和跳过的令牌数的增加，计算的复杂度变得更小，因为我们并不是计算每个令牌与所有其他令牌之间的注意力，而是只计算在有限范围内的注意力。这样，LongNet的注意力分配就遵循了对数依赖的原则，即，距离较远的令牌之间的依赖性较弱，而距离较近的令牌之间的依赖性较强。这是LongNet能在更大序列长度上进行工作的关键。</p><h3 id="计算复杂度的优化"><a href="#计算复杂度的优化" class="headerlink" title="计算复杂度的优化"></a>计算复杂度的优化</h3><p>现在我们来看看为什么LongNet的计算复杂度是线性的。在传统的自注意力机制中，我们需要执行序列长度平方次数的点积操作，而LongNet通过使用窗口和跳过的方式，将计算的复杂度降低到了线性。如果我们假设窗口大小是固定的，例如为4，然后“R”也是固定的，例如为2，那么计算复杂度将是“O(N)”。当然，在实际操作中，窗口大小和跳过的令牌数可能会根据实际情况进行调整，但是它们是常数，不随序列长度增加而增加。这就是为什么LongNet的计算复杂度是线性的。</p><h3 id="LongNet的应用前景"><a href="#LongNet的应用前景" class="headerlink" title="LongNet的应用前景"></a>LongNet的应用前景</h3><p>LongNet的发布为自然语言处理领域带来了诸多潜在的应用前景。首先，它可以应用于更长文本的生成任务，如生成长篇小说或长篇新闻报道。其次，它可以应用于更复杂的对话任务，因为在对话中，我们往往需要处理大量的历史信息和上下文。另外，它还可能在翻译任务中发挥更大的作用，因为翻译往往涉及到处理长句子或长段落的情况。总的来说，LongNet的发布为我们提供了处理更长文本的新工具和可能性。</p><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>总的来说，LongNet是一个基于Transformer的模型，它成功地将自注意力机制扩展到了10亿个令牌，实现了处理更长文本的能力。它的优势包括计算复杂度是线性的、遵循对数依赖原则，以及可以在分布式系统上进行训练。通过LongNet，我们可以探索更多自然语言处理任务，并处理那些过去由于序列长度限制而难以处理的任务。这个新模型的发布为我们带来了更多可能性。</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul><li><a href="https://arxiv.org/pdf/2104.11178.pdf">LONGNET: Scaling Transformers to 1,000,000,000 Tokens</a></li><li><a href="https://www.youtube.com/watch?v=nC2nU9j9DVQ">Youtube Tutorial by Umar Jamil</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP Insights </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LLM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Prompt Engineering</title>
      <link href="/2023/07/27/NLP%20Insights/Prompt%20Engineering/"/>
      <url>/2023/07/27/NLP%20Insights/Prompt%20Engineering/</url>
      
        <content type="html"><![CDATA[<h1 id="Prompt-Engineering"><a href="#Prompt-Engineering" class="headerlink" title="Prompt Engineering"></a>Prompt Engineering</h1><p>Prompt Engineering, 也被称为上下文提示，是指在不更新模型权重的情况下，与LLM（语言模型）进行交互以引导其产生期望输出的方法。它是一门实证科学，提示工程方法的效果在不同模型之间可能会有很大的差异，因此需要进行大量的实验和试探。</p><p>本文仅关注自回归语言模型的提示工程，不涉及填空测试、图像生成或多模态模型。在本质上，提示工程的目标是实现模型的对齐和可操控性。您可以查阅我之前关于可控文本生成的帖子。</p><h2 id="基本提示方法"><a href="#基本提示方法" class="headerlink" title="基本提示方法"></a>基本提示方法</h2><p>zero-shot学习和few-shot学习是两种最基本的提示模型方法，这些方法由许多LLM论文首创，并且通常用于评估LLM性能。</p><h3 id="zero-shot学习"><a href="#zero-shot学习" class="headerlink" title="zero-shot学习"></a>zero-shot学习</h3><p>zero-shot学习是将任务文本直接输入模型并要求获得结果。</p><p>（所有情感分析示例来自于SST-2数据集）</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Text: i'll bet the video game is a lot more fun than the film.</span><br><span class="line">Sentiment:</span><br></pre></td></tr></tbody></table></figure><h3 id="few-shot学习"><a href="#few-shot学习" class="headerlink" title="few-shot学习"></a>few-shot学习</h3><p>few-shot学习通过提供一组高质量的示例演示，每个示例都包含目标任务的输入和期望输出。当模型首先看到好的示例时，它可以更好地理解人类的意图和期望的答案类型。因此，few-shot学习通常比zero-shot学习表现更好。然而，这样做的代价是更多的记号消耗，并且在输入和输出文本较长时可能会达到上下文长度限制。</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Text: (lawrence bounces) all over the stage, dancing, running, sweating, mopping his face and generally displaying the wacky talent that brought him fame in the first place.</span><br><span class="line">Sentiment: positive</span><br><span class="line"></span><br><span class="line">Text: despite all evidence to the contrary, this clunker has somehow managed to pose as an actual feature movie, the kind that charges full admission and gets hyped on tv and purports to amuse small children and ostensible adults.</span><br><span class="line">Sentiment: negative</span><br><span class="line"></span><br><span class="line">Text: for the first time in years, de niro digs deep emotionally, perhaps because he's been stirred by the powerful work of his co-stars.</span><br><span class="line">Sentiment: positive</span><br><span class="line"></span><br><span class="line">Text: i'll bet the video game is a lot more fun than the film.</span><br><span class="line">Sentiment:</span><br></pre></td></tr></tbody></table></figure><p>许多研究都探讨了如何构建上下文示例以最大化性能，并观察到提示格式、训练示例和示例的顺序选择可能会导致截然不同的性能，从几乎随机猜测到接近SOTA（State-of-the-Art）。</p><p>赵等人（2021年）研究了few-shot分类的情况，并提出了一些与LLM（他们在实验中使用了GPT-3）相关的偏差，这些偏差导致了高方差的情况：</p><ul><li>（1）多数类别偏差存在于示例的标签分布不平衡的情况下；</li><li>（2）最近偏差是指模型可能在结尾重复标签；</li><li>（3）常见记号偏差表明LLM倾向于更频繁地生成常见的记号而不是罕见的记号。为了克服这些偏差，他们提出了一种方法，通过对模型输出的标签概率进行校准，使其在输入字符串为N/A时保持均匀。</li></ul><h2 id="提示工程技巧"><a href="#提示工程技巧" class="headerlink" title="提示工程技巧"></a>提示工程技巧</h2><h3 id="示例选择的建议"><a href="#示例选择的建议" class="headerlink" title="示例选择的建议"></a>示例选择的建议</h3><ul><li><p>使用嵌入空间中的NN聚类（Liu等人，2021年）来选择与测试示例在语义上相似的示例。</p></li><li><p>Su等人（2022年）提出了一种基于图的方法来选择多样且代表性的示例：</p><ol><li>首先，根据样本之间的嵌入（例如SBERT或其他嵌入模型）余弦相似性构建一个有向图，其中每个节点指向其最近的邻居；</li><li>开始时有一组已选择的示例和一组剩余示例。每个示例都通过得分函数进行评分，其中得分函数的目标是保持低值，以鼓励选择多样化的示例。具体得分函数的计算公式未提供。</li></ol></li><li><p>Rubin等人（2022年）提出了针对上下文学习示例选择的对比学习方法。对于每个训练对（格式化的输入-输出对），可以通过LM分配的条件概率来衡量一个示例的质量。然后，可以根据得分对训练对进行排名，选择得分较高和得分较低的示例作为对比学习的正样本和负样本集。</p></li><li><p>有些研究人员尝试使用Q-Learning进行示例选择（Zhang等人，2022年）。</p></li><li><p>受不确定性主导的主动学习的启发，Diao等人（2023年）建议确定具有多次采样试验中高度不一致或熵值较高的示例，并注释这些示例以在few-shot提示中使用。</p></li></ul><h3 id="示例排序的建议"><a href="#示例排序的建议" class="headerlink" title="示例排序的建议"></a>示例排序的建议</h3><ul><li><p>一般建议保持示例选择的多样性，与测试示例相关，并以随机顺序进行排列，以避免多数类别偏差和最近偏差。</p></li><li><p>增加模型大小或包含更多训练示例并不能减少上下文示例不同排列之间的方差。同一顺序对一个模型可能有效，但对另一个模型可能无效。当验证集有限时，可以考虑选择顺序，以使模型不会产生极端不平衡的预测或对其预测过于自信（Lu等人，2022年）。</p></li></ul><h2 id="指令提示"><a href="#指令提示" class="headerlink" title="指令提示"></a>指令提示</h2><ul><li><p>在提示中展示few-shot示例的目的是向模型解释我们的意图；换句话说，用示例来描述任务指令，以便模型能够理解用户意图并遵循指令。然而，few-shot的使用可能会消耗较多的记号，并限制输入长度，因为上下文长度有限。所以，为什么不直接给出指令呢？</p></li><li><p>Instructed LM（例如InstructGPT，自然语言指令）使用高质量的（任务指令，输入，真实输出）元组对预训练模型进行微调，以使LM更好地理解用户意图并遵循指令。RLHF（人类反馈的强化学习）是一种常见的方法。采用指令遵循风格的微调使得模型更加符合人类意图，并极大地降低了通信成本。</p></li><li><p>在与指令模型进行交互时，我们应该详细描述任务要求，尽量具体和准确，并避免使用”不做某事”的表述，而是要明确指定要做什么。</p></li></ul><h1 id="Chain-of-Thought-CoT-Prompting"><a href="#Chain-of-Thought-CoT-Prompting" class="headerlink" title="Chain-of-Thought (CoT) Prompting"></a>Chain-of-Thought (CoT) Prompting</h1><p>Chain-of-Thought (CoT) Prompting（Wei等人，2022年）通过生成一系列简短的句子，逐步描述推理逻辑，即所谓的推理链或理由链，最终引导出最终答案。CoT在复杂的推理任务中效果更显著，特别是在使用大型模型（例如超过50亿参数的模型）时。对于简单的任务，CoT提示的受益较小。</p><p>CoT提示的两种主要类型：</p><h2 id="few-shot-CoT"><a href="#few-shot-CoT" class="headerlink" title="few-shot CoT"></a>few-shot CoT</h2><p>few-shot CoT是使用少量演示来引导模型，每个演示包含人工编写（或模型生成）的高质量推理链。</p><p>（以下所有数学推理示例来自GSM8k数据集）</p><p>问题：Tom和Elizabeth比赛爬山。Elizabeth花了30分钟爬上山。Tom花费的时间是Elizabeth的四倍。Tom爬上山需要多少小时？</p><p>答案：Tom需要30 * 4 = 120分钟爬上山。<br>Tom需要120/60 = 2小时爬上山。<br>所以答案是2。</p><p>===</p><p>问题：Jack是个足球运动员。他需要买两双袜子和一双足球鞋。每双袜子的价格是9.50美元，鞋子的价格是92美元。Jack有40美元。Jack还需要多少钱？</p><p>答案：两双袜子的总费用是9.50美元 x 2 = 19美元。<br>袜子和鞋子的总费用是19美元 + 92美元 = 111美元。<br>Jack还需要111美元 - 40美元 = 71美元。<br>所以答案是71。</p><p>===</p><p>问题：Marty有100厘米的缎带，他必须将其分成4等份。每个切割部分必须再分成5等份。每个最终切割部分将有多长？</p><p>答案：（待填写）</p><h2 id="zero-shot-CoT"><a href="#zero-shot-CoT" class="headerlink" title="zero-shot CoT"></a>zero-shot CoT</h2><p>zero-shot CoT是使用自然语言陈述，例如“让我们逐步思考”，明确地鼓励模型首先生成推理链，然后再通过“因此，答案是”等提示来产生答案（Kojima等人，2022年）。或者使用类似的语句“让我们一步一步来计算，确保我们得到正确的答案”（Zhou等人，2022年）。</p><p>问题：Marty有100厘米的缎带，他必须将其分成4等份。每个切割部分必须再分成5等份。每个最终切割部分将有多长？</p><p>答案：让我们逐步思考。</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">@article{weng2023prompt,</span><br><span class="line">  title   = "Prompt Engineering",</span><br><span class="line">  author  = "Weng, Lilian",</span><br><span class="line">  journal = "lilianweng.github.io",</span><br><span class="line">  year    = "2023",</span><br><span class="line">  month   = "Mar",</span><br><span class="line">  url     = "https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/"</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>]]></content>
      
      
      <categories>
          
          <category> NLP Insights </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Prompt </tag>
            
            <tag> LLM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ColoredLogger-彩色打印日志到控制台并记录到文件</title>
      <link href="/2023/07/10/Code%20Chronicles/ColoredLogger/"/>
      <url>/2023/07/10/Code%20Chronicles/ColoredLogger/</url>
      
        <content type="html"><![CDATA[<h1 id="彩色打印日志到控制台并记录到文件"><a href="#彩色打印日志到控制台并记录到文件" class="headerlink" title="彩色打印日志到控制台并记录到文件"></a>彩色打印日志到控制台并记录到文件</h1><p>本文档介绍了一个名为 ColoredLogger 的日志记录器类，它可以根据不同的消息类型以不同的颜色打印日志，并将日志记录到文件中。该类使用了 colorama 库来实现在控制台中显示带颜色的文本。为了使控制台输出的日志更加易于阅读和理解，我们通常会使用彩色的输出。同时，将日志记录到文件中可以方便我们后续的调试和分析。在Python中，我们可以使用<code>logging</code>和<code>colorama</code>库来实现这样的功能。</p><img src="/2023/07/10/Code%20Chronicles/ColoredLogger/image.png" class=""><p>以下是一个如何使用这两个库的详细介绍。</p><h2 id="功能"><a href="#功能" class="headerlink" title="功能"></a>功能</h2><ul><li>可以根据不同的消息类型以不同的颜色打印日志消息。</li><li>将日志消息记录到文件中，使用标准的 <code>logging</code> 模块进行记录。</li><li>在控制台中显示带颜色的日志消息。</li></ul><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p><code>logging</code>库提供了强大的日志记录功能，允许我们将日志记录到控制台、文件或者其他输出设备，并提供了详细的配置选项。</p><p><code>colorama</code>库可以使我们在控制台输出彩色的文本。它提供了对ANSI颜色编码的支持，可以在几乎所有的平台和终端中使用。</p><p>我们先初始化<code>colorama</code>，然后定义了一个<code>ColoredLogger</code>类，它包含了各种彩色的输出样式和对应的日志级别。然后，我们设置了<code>logging</code>的配置，定义了日志的输出级别、输出文件和文件模式。最后，我们使用<code>ColoredLogger</code>的<code>log</code>方法来记录日志，它会同时将彩色的文本输出到控制台，并去除颜色控制字符后写入文件。<br><code>ColoredLogger</code> 类使用了 <code>colorama</code> 库来设置控制台中的颜色输出。它通过定义不同类型消息的颜色，并使用 <code>Fore</code> 和 <code>Style</code> 类来应用相应的颜色。</p><p><code>ColoredLogger</code> 类的 <code>log</code> 方法接受两个参数：<code>msg_type</code> 和 <code>msg</code>。根据 <code>msg_type</code> 参数的值，方法将选择适当的颜色，并将带有颜色的消息打印到控制台上。然后，它使用正则表达式去除颜色控制字符，并使用 <code>logging.info</code> 方法将不带颜色的消息记录到文件中。</p><h2 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h2><ol><li><p>安装 <code>colorama</code> 库，使用以下命令：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install colorama</span><br></pre></td></tr></tbody></table></figure></li><li><p>导入所需的模块和类：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"><span class="keyword">from</span> colorama <span class="keyword">import</span> init, Fore, Style</span><br></pre></td></tr></tbody></table></figure></li><li><p>初始化 <code>colorama</code>：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">init(autoreset=<span class="literal">True</span>)</span><br></pre></td></tr></tbody></table></figure></li><li><p>定义 <code>ColoredLogger</code> 类，并设置不同类型消息的颜色。例如：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ColoredLogger</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">         self.type_A = Fore.CYAN + Style.BRIGHT</span><br><span class="line">         self.type_B = Fore.GREEN + Style.BRIGHT</span><br><span class="line">         self.type_C = Fore.YELLOW + Style.BRIGHT</span><br><span class="line">         self.type_D = Fore.MAGENTA + Style.BRIGHT</span><br><span class="line">         self.type_E = Fore.BLUE + Style.BRIGHT</span><br><span class="line">         self.RESET = Style.RESET_ALL</span><br></pre></td></tr></tbody></table></figure></li><li><p>实例化 <code>ColoredLogger</code> 类，并使用 <code>log</code> 方法打印和记录日志消息。例如：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">logger = ColoredLogger()</span><br><span class="line">logger.log(<span class="string">'type_A'</span>, <span class="string">'This is a test message for type A.'</span>)</span><br><span class="line">logger.log(<span class="string">'type_B'</span>, <span class="string">'This is a test message for type B.'</span>)</span><br><span class="line"><span class="comment"># ...</span></span><br></pre></td></tr></tbody></table></figure></li><li><p>通过以下命令配置日志记录器，将日志消息写入文件：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">logging.basicConfig(level=logging.INFO, filename=<span class="string">'example.log'</span>, filemode=<span class="string">'w'</span>)</span><br></pre></td></tr></tbody></table></figure></li><li><p>运行代码，查看在控制台和文件中显示的带颜色的日志消息。</p></li></ol><h2 id="完整代码实现"><a href="#完整代码实现" class="headerlink" title="完整代码实现"></a>完整代码实现</h2><pre><code class="python">import reimport loggingfrom colorama import init, Fore, Style# Initialize coloramainit(autoreset=True)class ColoredLogger:    def __init__(self):        self.type_A = Fore.CYAN + Style.BRIGHT        self.type_B = Fore.GREEN + Style.BRIGHT        self.type_C = Fore.YELLOW + Style.BRIGHT        self.type_D = Fore.MAGENTA + Style.BRIGHT        self.type_E = Fore.BLUE + Style.BRIGHT        self.RESET = Style.RESET_ALL    def log(self, msg_type, msg):        if msg_type == "type_A":            self.print_and_log(self.type_A + f"type_A: {msg}" + self.RESET)        elif msg_type == "type_B":            self.print_and_log(self.type_B + f"type_B: {msg}" + self.RESET)        elif msg_type == "type_C":            self.print_and_log(self.type_C + f"type_C: {msg}" + self.RESET)        elif msg_type == "type_D":            self.print_and_log(self.type_D + f"type_D: {msg}" + self.RESET)        elif msg_type == "type_E":            self.print_and_log(self.type_E + f"type_E: {msg}" + self.RESET)    def print_and_log(self, msg):        # Print to console with color        print(msg)        # Remove color control chars for logging to file        msg_without_color = re.sub('\x1b\[[0-9;]*m', '', msg)        logging.info(msg_without_color)# Setup logging configurationlogging.basicConfig(level=logging.INFO, filename='example.log', filemode='w')# Use the ColoredLoggerlogger = ColoredLogger()logger.log('type_A', 'This is a test message for type A.')logger.log('type_B', 'This is a test message for type B.')logger.log('type_C', 'This is a test message for type C.')logger.log('type_D', 'This is a test message for type D.')logger.log('type_E', 'This is a test message for type E.')```![Alt text](image.png)## 注意事项1. `colorama`库的颜色和样式可能在不同的平台和终端上有不同的效果，所以需要在目标环境上进行测试。2. 在写入文件时，需要去除颜色控制字符，否则会在文本中留下一些无法识别的字符。3. 在设置`logging`的配置时，需要注意文件模式的设置。'w'模式会在每次运行时覆盖之前的日志，而'a'模式则会在之前的日志后追加新的日志。4. 使用`logging`库记录日志时，需要注意日志的级别。不同级别的日志会有不同的输出效果和记录方式。5. 需要注意线程安全。如果在多线程环境下使用`logging`库，可能需要额外的配置来保证线程安全。</code></pre>]]></content>
      
      
      <categories>
          
          <category> Code Chronicles </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>详解梯度下降算法</title>
      <link href="/2023/07/09/NLP%20Insights/%E8%AF%A6%E8%A7%A3%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95/"/>
      <url>/2023/07/09/NLP%20Insights/%E8%AF%A6%E8%A7%A3%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h1 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h1><p>梯度是一个在微积分中使用的重要概念，它用于衡量函数在给定点上的方向导数沿各个方向最大时的最大值。对于一个标量函数，梯度的方向是函数增长最快的方向，而梯度的反方向则是函数减小最快的方向。</p><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>对于在点$x \in \mathbb{R}^n$可微的函数$f: \mathbb{R}^n \rightarrow \mathbb{R}$，其梯度被定义为一个向量，其各个分量为函数在该点上的偏导数。对于函数$f(x_1, x_2, …, x_n)$，它的梯度可以表示为：</p><p>$$<br>\nabla f(x) = \left[ \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, …, \frac{\partial f}{\partial x_n} \right]^T<br>$$</p><p>这里，$\nabla f(x)$表示$f(x)$的梯度，$\frac{\partial f}{\partial x_i}$表示$f$关于$x_i$的偏导数，$T$表示矩阵转置。</p><h2 id="物理含义"><a href="#物理含义" class="headerlink" title="物理含义"></a>物理含义</h2><p>梯度有一个重要的物理含义。在二维空间中，可以把函数$f(x, y)$看作地形的高度，那么梯度就是指向最陡峭上升方向的向量。而梯度的大小，则对应了最陡峭上升方向的斜率。因此，在优化问题中，我们通常沿着梯度的反方向更新参数，以最快地降低函数值。</p><h2 id="梯度在机器学习中的应用"><a href="#梯度在机器学习中的应用" class="headerlink" title="梯度在机器学习中的应用"></a>梯度在机器学习中的应用</h2><p>在机器学习中，我们的目标通常是找到一组参数，使得损失函数达到最小。为了实现这个目标，我们可以使用梯度下降算法，不断地沿着损失函数梯度的反方向更新参数。</p><p>在深度学习中，由于模型通常有大量的参数，我们需要使用反向传播算法来高效地计算梯度。这种算法基于链式法则，可以在计算图中从输出端到输入端，一层一层地计算各个参数的梯度。</p><h2 id="计算梯度"><a href="#计算梯度" class="headerlink" title="计算梯度"></a>计算梯度</h2><p>在实践中，我们通常使用自动微分（如PyTorch和TensorFlow提供的自动微分功能）来计算梯度。这使得我们无需手动推导和实现复杂的梯度公式，大大提高了编程的效率。以下是一个PyTorch中计算梯度的例子：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个张量，并设置requires_grad=True使得我们可以计算关于它的梯度</span></span><br><span class="line">x = torch.tensor([<span class="number">1.0</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义函数</span></span><br><span class="line">y = <span class="number">2</span> * x * x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过反向传播计算梯度</span></span><br><span class="line">y.backward()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出梯度</span></span><br><span class="line"><span class="built_in">print</span>(x.grad)  <span class="comment"># 输出：tensor([4.])，因为y = 2 * x^2, dy/dx = 4 * x = 4 * 1 = 4</span></span><br></pre></td></tr></tbody></table></figure><p>在上述例子中，<code>y.backward()</code>表示计算关于<code>y</code>的梯度，然后将这个梯度反向传播回其输入<code>x</code>。因此，<code>y</code>本身的梯度被认为是1（因为对于任何变量<code>x</code>，<code>dx/dx</code>都等于1），然后这个梯度被传递到<code>x</code>，得到的是<code>y</code>关于<code>x</code>的梯度，即<code>dy/dx</code>。</p><p>注意，<code>y</code>本身没有<code>.grad</code>属性，因为它不是通过<code>requires_grad=True</code>创建的。只有那些通过<code>requires_grad=True</code>创建，并且参与了运算的张量才有<code>.grad</code>属性，这个属性存储了梯度信息。</p><p>在PyTorch中，<code>backward()</code>函数的作用是计算梯度，并将梯度信息存储在<code>.grad</code>属性中。<code>backward()</code>函数的调用者（即<code>y</code>）自身的梯度被认为是1，然后这个梯度会被反向传播回所有参与了运算并需要计算梯度的张量。</p><h1 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a>梯度下降算法</h1><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>梯度下降算法是一种用于优化目标函数的迭代方法。具体来说，它是一个寻找函数最小值的算法。对于最大化问题，我们可以通过最小化目标函数的相反数来求解。</p><p>在梯度下降中，我们首先选择一个初始点（即初始参数值），然后我们迭代地将参数向负梯度方向移动，这样在每一步，我们都能够减小目标函数的值，直到找到函数的局部最小值。</p><p>在这个过程中，梯度（函数的一阶导数）给出了函数值下降最快的方向。我们使用一个叫做学习率的参数来控制每一步移动的大小。学习率决定了每次迭代时参数更新的步长，过大的学习率可能会使算法在最小值处震荡，过小则可能会导致算法收敛速度过慢。</p><h2 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h2><p>基本的梯度下降更新公式为：</p><p>$$<br>\theta_{new} = \theta_{old} - \alpha \nabla J(\theta_{old})<br>$$</p><p>其中，$\theta$ 表示我们试图优化的参数，$J(\theta)$ 是我们试图最小化的目标函数，$\nabla J(\theta_{old})$ 是在当前参数值 $\theta_{old}$ 处的目标函数的梯度，$\alpha$ 是学习率。</p><h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><p>考虑一个简单的线性回归问题。我们有一个目标函数（损失函数）为均方误差：</p><p>$$<br>J(\theta) = \frac{1}{2m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)}) - y^{(i)})^2<br>$$</p><p>其中，$h_{\theta}(x^{(i)}) = \theta^T x^{(i)}$ 是预测函数，$m$ 是训练样本数量。</p><p>对于这个问题，我们可以使用梯度下降算法来找到最小化损失函数的参数 $\theta$。在每次迭代中，我们首先计算损失函数的梯度，然后根据前面的公式更新参数。</p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>下面是使用 Python 实现梯度下降的示例代码：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradient_descent</span>(<span class="params">X, y, theta, alpha, num_iters</span>):</span><br><span class="line">    m = y.size</span><br><span class="line">    J_history = np.zeros(num_iters)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_iters):</span><br><span class="line">        theta = theta - alpha * (<span class="number">1</span>/m) * (X.T @ (X @ theta - y))</span><br><span class="line">        J_history[i] = compute_cost(X, y, theta)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> theta, J_history</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_cost</span>(<span class="params">X, y, theta</span>):</span><br><span class="line">    m = y.size</span><br><span class="line">    J = <span class="number">1</span>/(<span class="number">2</span>*m) * np.<span class="built_in">sum</span>(np.square(X @ theta - y))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> J</span><br></pre></td></tr></tbody></table></figure><p>在这个代码中，<code>gradient_descent</code> 函数实现了梯度下降算法，<code>compute_cost</code> 函数用于计算目标函数（损失函数）的值。</p><h1 id="梯度下降算法的分类"><a href="#梯度下降算法的分类" class="headerlink" title="梯度下降算法的分类"></a>梯度下降算法的分类</h1><p>梯度下降算法是一种优化算法，用于寻找损失函数的最小值。在机器学习和深度学习中，我们通常使用梯度下降算法来优化我们的模型，即调整模型参数以最小化损失函数。以下将介绍梯度下降的几种主要变体。</p><h2 id="批量梯度下降（Batch-Gradient-Descent）"><a href="#批量梯度下降（Batch-Gradient-Descent）" class="headerlink" title="批量梯度下降（Batch Gradient Descent）"></a>批量梯度下降（Batch Gradient Descent）</h2><p>批量梯度下降是最基本的形式，它在每一次迭代中都使用全量的训练数据来计算损失函数的梯度。因此，批量梯度下降的每一步都朝着全局最优的方向。然而，这也使得批量梯度下降在大规模数据集上非常慢，且无法在线（实时）更新模型。</p><p>更新公式：</p><p>$$<br>\theta = \theta - \alpha \nabla J(\theta)<br>$$</p><p>其中，$\theta$ 是参数，$\alpha$ 是学习率，$\nabla J(\theta)$ 是损失函数 $J$ 关于参数 $\theta$ 的梯度。</p><h2 id="随机梯度下降（Stochastic-Gradient-Descent，SGD）"><a href="#随机梯度下降（Stochastic-Gradient-Descent，SGD）" class="headerlink" title="随机梯度下降（Stochastic Gradient Descent，SGD）"></a>随机梯度下降（Stochastic Gradient Descent，SGD）</h2><p>随机梯度下降在每一次迭代中随机选择一个样本来计算梯度。因此，每一步的更新方向并不一定是全局最优的方向，结果会有一些噪音。然而，这使得随机梯度下降在大规模数据集上比批量梯度下降快很多，且能够在线更新模型。</p><p>更新公式与批量梯度下降一致，只是每次只对一个随机样本进行计算。</p><h2 id="小批量梯度下降（Mini-Batch-Gradient-Descent）"><a href="#小批量梯度下降（Mini-Batch-Gradient-Descent）" class="headerlink" title="小批量梯度下降（Mini-Batch Gradient Descent）"></a>小批量梯度下降（Mini-Batch Gradient Descent）</h2><p>小批量梯度下降是批量梯度下降和随机梯度下降的折中，它在每一次迭代中使用一部分（小批量）样本来计算梯度。小批量梯度下降比随机梯度下降更稳定，同时仍然具有相对较高的计算速度。</p><p>更新公式与前面两者一致，只是每次对一个小批量的样本进行计算。</p><h1 id="梯度下降算法的进阶变体"><a href="#梯度下降算法的进阶变体" class="headerlink" title="梯度下降算法的进阶变体"></a>梯度下降算法的进阶变体</h1><p>在梯度下降的基础上，研究者们引入了一些额外的概念以改善算法的性能。以下是一些广为使用的梯度下降算法的变体。</p><h2 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h2><p>动量（Momentum）是一种帮助优化器在相关方向上保持速度，从而抑制振荡并加快收敛的策略。其核心思想是引入一个新的变量（通常称为速度），它在每一步中都会增加当前梯度，然后参数更新会按照这个速度进行。</p><p>Momentum的更新公式如下：</p><p>$$<br>v = \beta v - \alpha \nabla J(\theta)<br>$$<br>$$<br>\theta = \theta + v<br>$$</p><p>其中，$\theta$ 是参数，$\nabla J(\theta)$ 是损失函数 $J$ 关于参数 $\theta$ 的梯度，$\alpha$ 是学习率，$v$ 是速度，$\beta$ 是动量因子，通常设为0.9。</p><h2 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h2><p>AdaGrad（Adaptive Gradient Algorithm）的主要思想是为每个参数分配一个自适应的学习率，这对于稀疏数据和处理非平稳目标函数非常有用。具体来说，对于经常出现且有大梯度的参数，其学习率会被降低；反之，对于稀疏或小梯度的参数，其学习率会被提高。</p><p>AdaGrad的更新公式如下：</p><p>$$<br>G_{t} = G_{t-1} + (\nabla J(\theta))^2<br>$$<br>$$<br>\theta = \theta - \frac{\alpha}{\sqrt{G_{t} + \epsilon}} \cdot \nabla J(\theta)<br>$$</p><p>其中，$G_{t}$ 是到目前为止所有梯度的平方和，$\epsilon$ 是一个很小的数，通常设为1e-8，用于防止除零错误。</p><h2 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h2><p>RMSProp（Root Mean Square Propagation）是AdaGrad的一个改进版本，主要解决了AdaGrad在非凸设置下学习率快速下降的问题。与AdaGrad一样，RMSProp也是为每个参数分配一个自适应的学习率，但是它使用了一个滑动平均的梯度平方来更新 $G_{t}$。</p><p>RMSProp的更新公式如下：</p><p>$$<br>G_{t} = \beta G_{t-1} + (1 - \beta) (\nabla J(\theta))^2<br>$$<br>$$<br>\theta = \theta - \frac{\alpha}{\sqrt{G_{t} + \epsilon}} \cdot \nabla J(\theta)<br>$$</p><p>其中，$\beta$ 是平方梯度的滑动平均因子，通常设为0.9。</p><h2 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h2><p>Adam（Adaptive Moment Estimation）结合了Momentum和RMSProp的思想。它计算了梯度的指数滑动平均（第一矩）和平方梯度的指数滑动平均（第二矩），并使用这两个量来更新参数。</p><p>Adam的更新公式如下：</p><p>$$<br>m = \beta_{1} m + (1 - \beta_{1}) \nabla J(\theta)<br>$$<br>$$<br>v = \beta_{2} v + (1 - \beta_{2}) (\nabla J(\theta))^2<br>$$<br>$$<br>\hat{m} = \frac{m}{1 - \beta_{1}^{t}}<br>$$<br>$$<br>\hat{v} = \frac{v}{1 - \beta_{2}^{t}}<br>$$<br>$$<br>\theta = \theta - \frac{\alpha \hat{m}}{\sqrt{\hat{v}} + \epsilon}<br>$$</p><p>其中，$m$ 和 $v$ 分别是第一矩和第二矩的估计，$\beta_{1}$ 和 $\beta_{2}$ 分别是第一矩和第二矩的滑动平均因子，通常设为0.9和0.999，$t$ 是当前的迭代步数。</p><h1 id="PyTorch优化器详细介绍"><a href="#PyTorch优化器详细介绍" class="headerlink" title="PyTorch优化器详细介绍"></a>PyTorch优化器详细介绍</h1><p>PyTorch提供了一些已经实现的优化器，这些优化器都在torch.optim模块中。优化器的主要作用是更新模型的参数以最小化目标函数（通常为损失函数）。</p><h2 id="常见优化器介绍及使用"><a href="#常见优化器介绍及使用" class="headerlink" title="常见优化器介绍及使用"></a>常见优化器介绍及使用</h2><ol><li><strong>随机梯度下降（SGD）</strong></li></ol><p>SGD是最基本的优化器，它对每一个参数使用相同的学习率进行更新。这是其更新公式：</p><p>$$<br>\theta_{new} = \theta_{old} - \alpha \nabla J(\theta_{old})<br>$$</p><p>在PyTorch中，可以这样使用SGD优化器：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.1</span>)</span><br></pre></td></tr></tbody></table></figure><ol start="2"><li><strong>带动量的随机梯度下降（Momentum SGD）</strong></li></ol><p>Momentum SGD是SGD的一种改进，它在更新参数时会考虑过去的梯度，从而达到平滑更新的效果。这是其更新公式：</p><p>$$<br>v = \beta v - \alpha \nabla J(\theta)<br>$$<br>$$<br>\theta = \theta + v<br>$$</p><p>其中，$v$是动量，$\beta$是动量衰减因子。在PyTorch中，可以这样使用Momentum SGD优化器：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.1</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></tbody></table></figure><ol start="3"><li><strong>自适应梯度算法（Adagrad）</strong></li></ol><p>Adagrad是一种自适应学习率的优化器，它会对每一个参数使用不同的学习率进行更新。这使得它在处理稀疏数据时有很好的表现。这是其更新公式：</p><p>$$<br>\theta_{new} = \theta_{old} - \frac{\alpha}{\sqrt{G_{t} + \epsilon}} \cdot \nabla J(\theta_{old})<br>$$</p><p>其中，$G_{t}$是到目前为止所有梯度的平方和，$\epsilon$是一个很小的数（如1e-8）用于防止除零错误。在PyTorch中，可以这样使用Adagrad优化器：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.Adagrad(model.parameters(), lr=<span class="number">0.1</span>)</span><br></pre></td></tr></tbody></table></figure><ol start="4"><li><strong>自适应动量估计（Adam）</strong></li></ol><p>Adam结合了Momentum SGD和Adagrad的思想，它对每个参数都有一个自适应的学习率，并且会考虑过去的梯度。这使得它在许多任务上都有很好的表现。这是其更新公式：</p><p>$$<br>m = \beta_{1} m + (1 - \beta_{1}) \nabla J(\theta)<br>$$<br>$$<br>v = \beta_{2} v + (1 - \beta_{2}) (\nabla J(\theta))^2<br>$$<br>$$<br>\hat{m} = \frac{m}{1 - \beta_{1}^{t}}<br>$$<br>$$<br>\hat{v} = \frac{v}{1 - \beta_{2}^{t}}<br>$$<br>$$<br>\theta = \theta - \frac{\alpha \hat{m}}{\sqrt{\hat{v}} + \epsilon}<br>$$</p><p>其中，$m$和$v$分别是第一和第二矩估计，$\beta_{1}$和$\beta_{2}$是衰减因子（一般设为0.9和0.999），$t$是迭代次数。在PyTorch中，可以这样使用Adam优化器：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br></pre></td></tr></tbody></table></figure><h2 id="优化器的使用方法"><a href="#优化器的使用方法" class="headerlink" title="优化器的使用方法"></a>优化器的使用方法</h2><p>优化器的基本使用流程如下：</p><ol><li>定义模型（model）。</li><li>定义损失函数（loss function）。</li><li>选择优化器（optimizer），并将模型的参数和学习率传入优化器。</li><li>在训练循环中，首先清空优化器的梯度（optimizer.zero_grad()），然后计算损失（loss.backward()），最后更新模型的参数（optimizer.step()）。</li></ol><h2 id="优化器的选择场景"><a href="#优化器的选择场景" class="headerlink" title="优化器的选择场景"></a>优化器的选择场景</h2><ol><li><strong>SGD</strong>：适用于大规模线性模型，或者在训练初期快速降低损失。</li><li><strong>Momentum SGD</strong>：适用于深度学习任务，比SGD有更快的收敛速度。</li><li><strong>Adagrad</strong>：适用于处理稀疏数据的任务，如自然语言处理中的词向量训练。</li><li><strong>Adam</strong>：适用于大多数深度学习任务，是一个比较通用的优化器。</li></ol><p>选择哪种优化器并没有固定的规则，具体要根据任务的特点和数据的特性来决定。一般来说，可以先试用Adam，如果效果不佳，再考虑其他优化器。</p><h1 id="反向传播算法"><a href="#反向传播算法" class="headerlink" title="反向传播算法"></a>反向传播算法</h1><p>反向传播（Backpropagation）是神经网络中用于训练模型的主要算法之一，它是许多现代深度学习框架（如TensorFlow和PyTorch）的核心部分。以下将详细介绍反向传播的工作原理。它的主要任务是通过计算损失函数（一个衡量模型预测与真实值差异的函数）对模型参数的梯度来有效地更新网络的权重和偏置，以最小化损失函数。</p><h2 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h2><p>反向传播的目标是计算损失函数关于神经网络参数的梯度，以便使用梯度下降或其他优化算法来更新参数。为此，反向传播从输出层开始，沿着神经网络反向传递梯度。</p><p>反向传播的关键思想是链式法则（chain rule），这是微积分的一个基本定理，用于计算复合函数的导数。在神经网络的上下文中，链式法则用于计算损失函数关于参数的梯度，通过将这个复合函数拆分成一系列更简单的函数，并将它们的导数相乘。</p><h2 id="算法过程"><a href="#算法过程" class="headerlink" title="算法过程"></a>算法过程</h2><p>以下是反向传播算法的一般步骤：</p><ol><li><p><strong>前向传播</strong>：从输入层开始，通过网络向前传播数据，计算每一层的输出，直到得到最终的预测结果。</p></li><li><p><strong>计算损失</strong>：使用损失函数计算预测结果和实际目标之间的误差。</p></li><li><p><strong>反向传播损失</strong>：从输出层开始，计算损失函数关于每一层输出的梯度，并反向传播这些梯度。具体来说，对于每一层，我们首先计算损失函数关于这一层输出的梯度，然后使用链式法则，计算这个梯度关于这一层输入的梯度，以及关于这一层参数的梯度。</p></li><li><p><strong>更新参数</strong>：使用梯度下降或其他优化算法，利用在步骤3中计算出的梯度来更新网络参数。</p></li></ol><p>这个过程在每个训练迭代中重复，直到网络参数收敛，或者达到预设的最大迭代次数。</p><h2 id="计算方法"><a href="#计算方法" class="headerlink" title="计算方法"></a>计算方法</h2><p>假设我们有一个损失函数 $L$，我们想要知道它关于权重 $w_{ij}$ （表示从第 $i$ 个神经元到第 $j$ 个神经元的权重）的梯度，我们可以使用以下的公式：</p><p>$$<br>\frac{\partial L}{\partial w_{ij}} = \frac{\partial L}{\partial z_j} \cdot \frac{\partial z_j}{\partial w_{ij}}<br>$$</p><p>其中，$z_j$ 是 $j$ 个神经元的输入，它等于 $\sum_i w_{ij} x_i + b_j$。</p><p>现在我们需要找到每个部分的导数。首先，$\frac{\partial L}{\partial z_j}$ 通常直接由网络的后面部分提供。这是因为我们一般会按照从后向前的顺序计算导数，也就是说，我们首先计算出损失函数对于最后一层的输出的导数，然后用这个导数来计算损失函数对于倒数第二层的输出的导数，以此类推。</p><p>然后，我们需要找到 $\frac{\partial z_j}{\partial w_{ij}}$。由于 $z_j = \sum_i w_{ij} x_i + b_j$，我们可以看到，如果我们改变 $w_{ij}$，$z_j$ 就会按照 $x_i$ 的大小改变。所以，$\frac{\partial z_j}{\partial w_{ij}} = x_i$。</p><p>这样，我们就得到了：</p><p>$$<br>\frac{\partial L}{\partial w_{ij}} = \frac{\partial L}{\partial z_j} \cdot x_i<br>$$</p><p>让我们来看一个具体的例子。假设我们有一个简单的神经网络，只有输入层和输出层，没有隐藏层。输入层有一个神经元，其值为 $x$，输出层也有一个神经元，其值为 $y$。他们之间的权重是 $w$，偏置为 $b$。那么，$y$ 的计算公式就是 $y = wx + b$。我们用均方误差作为损失函数，也就是说，如果真实值是 $t$，那么损失函数就是 $L = (t - y)^2$。</p><p>现在，我们想要计算 $\frac{\partial L}{\partial w}$。首先，我们需要找到 $\frac{\partial L}{\partial y}$。由于 $L = (t - y)^2$，我们可以得到 $\frac{\partial L}{\partial y} = -2(t - y)$。</p><p>然后，我们需要找到 $\frac{\partial y}{\partial w}$。由于 $y = wx + b$，我们可以得到 $\frac{\partial y}{\partial w} = x$。</p><p>所以，最后我们得到：</p><p>$$<br>\frac{\partial L}{\partial w} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial w} = -2(t - y) \cdot x<br>$$</p><p>这就是我们要找的梯度，我们可以用它来更新权重 $w$，以减小损失函数。</p><h2 id="代码实现-1"><a href="#代码实现-1" class="headerlink" title="代码实现"></a>代码实现</h2><h3 id="PyyTorch中的反向传播"><a href="#PyyTorch中的反向传播" class="headerlink" title="PyyTorch中的反向传播"></a>PyyTorch中的反向传播</h3><p>以下是一个简单的反向传播算法PyTorch代码示例进行详细分步解释：</p><ol><li><strong>创建一个简单的神经网络</strong>：</li></ol><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model = nn.Sequential(</span><br><span class="line">    nn.Linear(<span class="number">10</span>, <span class="number">20</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">20</span>, <span class="number">1</span>),</span><br><span class="line">)</span><br></pre></td></tr></tbody></table></figure><p>首先，我们使用PyTorch的<code>nn.Sequential</code>来定义一个简单的神经网络。这个网络由两个线性层（<code>nn.Linear</code>）和一个ReLU激活函数（<code>nn.ReLU</code>）组成。第一个线性层将输入的大小从10变为20，ReLU激活函数增加了模型的非线性，第二个线性层将大小为20的隐藏状态映射为大小为1的输出。</p><ol start="2"><li><strong>选择损失函数和优化器</strong>：</li></ol><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">criterion = nn.MSELoss()</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br></pre></td></tr></tbody></table></figure><p>接着，我们选择一个损失函数和一个优化器。损失函数用于度量模型的预测与实际目标之间的差距，优化器用于根据损失的梯度来更新模型的参数。在这个例子中，我们选择均方误差损失（<code>nn.MSELoss</code>）作为损失函数，选择随机梯度下降（<code>torch.optim.SGD</code>）作为优化器。</p><ol start="3"><li><strong>模拟输入和目标数据</strong>：</li></ol><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">inputs = torch.randn(<span class="number">5</span>, <span class="number">10</span>)</span><br><span class="line">targets = torch.randn(<span class="number">5</span>, <span class="number">1</span>)</span><br></pre></td></tr></tbody></table></figure><p>然后，我们生成一些模拟的输入和目标数据。在这个例子中，我们生成一个形状为[5, 10]的输入张量和一个形状为[5, 1]的目标张量。</p><ol start="4"><li><strong>前向传播</strong>：</li></ol><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">outputs = model(inputs)</span><br></pre></td></tr></tbody></table></figure><p>在前向传播阶段，我们将输入数据传入模型，得到模型的预测输出。</p><ol start="5"><li><strong>计算损失</strong>：</li></ol><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss = criterion(outputs, targets)</span><br></pre></td></tr></tbody></table></figure><p>接着，我们使用损失函数来计算模型的预测输出与实际目标之间的差距。</p><ol start="6"><li><strong>反向传播</strong>：</li></ol><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">loss.backward()</span><br></pre></td></tr></tbody></table></figure><p>在反向传播阶段，我们调用损失张量的<code>backward</code>方法，计算损失关于模型参数的梯度。这些梯度将存储在对应参数的<code>.grad</code>属性中。</p><ol start="7"><li><strong>更新参数</strong>：</li></ol><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer.step()</span><br></pre></td></tr></tbody></table></figure><p>然后，我们调用优化器的<code>step</code>方法，根据存储在模型参数的<code>.grad</code>属性中的梯度来更新参数。</p><ol start="8"><li><strong>清零梯度</strong>：</li></ol><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer.zero_grad()</span><br></pre></td></tr></tbody></table></figure><p>最后，我们调用优化器的<code>zero_grad</code>方法，将模型参数的<code>.grad</code>属性中的梯度清零。这一步是必要的，因为PyTorch默认会累加梯度，即每次调用<code>.backward</code>方法时，梯度都会累加到<code>.grad</code>属性中，而不是替换。如果不清零梯度，下一次迭代时计算的梯度将会与这一次迭代的梯度叠加，导致错误。</p><p>注意事项：以上的代码是一个完整的训练步骤，实际使用时通常需要将这个过程放入一个循环中，对整个训练数据集进行多次迭代，直到模型性能满足要求或达到预设的最大迭代次数。在每次迭代中，还可以添加一些代码来记录训练进度，如打印当前的损失值，或者在验证数据集上评估模型的性能。</p><p>完整代码如下：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个简单的神经网络</span></span><br><span class="line">model = nn.Sequential(</span><br><span class="line">    nn.Linear(<span class="number">10</span>, <span class="number">20</span>),</span><br><span class="line">    nn.ReLU(),</span><br><span class="line">    nn.Linear(<span class="number">20</span>, <span class="number">1</span>),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 选择损失函数和优化器</span></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟输入和目标数据</span></span><br><span class="line">inputs = torch.randn(<span class="number">5</span>, <span class="number">10</span>)</span><br><span class="line">targets = torch.randn(<span class="number">5</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 前向传播</span></span><br><span class="line">outputs = model(inputs)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算损失</span></span><br><span class="line">loss = criterion(outputs, targets)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 反向传播</span></span><br><span class="line">loss.backward()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 更新参数</span></span><br><span class="line">optimizer.step()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 清零梯度，为下一次迭代做准备</span></span><br><span class="line">optimizer.zero_grad()</span><br></pre></td></tr></tbody></table></figure><p>以上就是反向传播算法的基本介绍，其在深度学习训练中扮演着非常重要的角色，是理解和实现神经网络的基础知识。</p><h3 id="Python手写反向传播"><a href="#Python手写反向传播" class="headerlink" title="Python手写反向传播"></a>Python手写反向传播</h3><p>以下是一个简单的反向传播算法Python代码示例进行详细分步解释：</p><ol><li><p><strong>设定随机数种子和数据</strong>：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">X = np.array([[<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>],[<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]])</span><br><span class="line">y = np.array([[<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>]]).T</span><br></pre></td></tr></tbody></table></figure><p>这段代码首先设定了随机数种子，以确保每次运行程序时，初始化的权重值都是一样的。接着，我们设定了输入数据<code>X</code>和输出数据<code>y</code>。</p></li><li><p><strong>定义Sigmoid函数</strong>：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">x, deriv=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="keyword">if</span> deriv:</span><br><span class="line">        <span class="keyword">return</span> x*(<span class="number">1</span>-x)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>/(<span class="number">1</span>+np.exp(-x))</span><br></pre></td></tr></tbody></table></figure><p>这里我们定义了Sigmoid函数，该函数用于激活神经元的输出。在参数<code>deriv</code>为True时，该函数返回Sigmoid函数的导数，这对于反向传播计算梯度非常重要。</p></li><li><p><strong>初始化权重</strong>：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">w0 = <span class="number">2</span>*np.random.random((<span class="number">3</span>,<span class="number">4</span>)) - <span class="number">1</span></span><br><span class="line">w1 = <span class="number">2</span>*np.random.random((<span class="number">4</span>,<span class="number">1</span>)) - <span class="number">1</span></span><br></pre></td></tr></tbody></table></figure><p>在这里，我们初始化了权重<code>w0</code>和<code>w1</code>。我们的网络有两层，所以需要两组权重。这些权重的初始化值是在-1到1之间随机选择的。</p></li><li><p><strong>迭代训练</strong>：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">60000</span>):</span><br></pre></td></tr></tbody></table></figure><p>这是我们的训练循环，我们训练网络60000次。</p></li><li><p><strong>前向传播</strong>：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">l0 = X</span><br><span class="line">l1 = sigmoid(np.dot(l0, w0))</span><br><span class="line">l2 = sigmoid(np.dot(l1, w1))</span><br></pre></td></tr></tbody></table></figure><p>这是我们的前向传播步骤。首先，我们的输入<code>l0</code>就是数据<code>X</code>。然后，我们计算<code>l1</code>层，这是<code>l0</code>和<code>w0</code>的点积通过Sigmoid函数的结果。同样，我们计算<code>l2</code>，这是<code>l1</code>和<code>w1</code>的点积通过Sigmoid函数的结果。</p></li><li><p><strong>计算误差</strong>：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">l2_loss = y - l2</span><br></pre></td></tr></tbody></table></figure><p>在这里，我们计算了网络预测的误差，这就是实际值<code>y</code>减去预测值<code>l2</code>。</p></li><li><p><strong>打印误差</strong>：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> j % <span class="number">10000</span> == <span class="number">0</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f'Loss: <span class="subst">{np.mean(np.<span class="built_in">abs</span>(l2_loss))}</span>'</span>)</span><br></pre></td></tr></tbody></table></figure><p>每10000次迭代，我们计算并打印一次平均误差。</p></li><li><p><strong>反向传播</strong>：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">l2_delta = l2_loss * sigmoid(l2, deriv=<span class="literal">True</span>)</span><br><span class="line">l1_loss = l2_delta.dot(w1.T)</span><br><span class="line">l1_delta = l1_loss * sigmoid(l1, deriv=<span class="literal">True</span>)</span><br></pre></td></tr></tbody></table></figure><p>这是反向传播的步骤。我们首先计算<code>l2_delta</code>，这是<code>l2_loss</code>和<code>l2</code>通过Sigmoid导数函数的结果。然后，我们计算<code>l1_loss</code>，这是<code>l2_delta</code>和<code>w1</code>的转置的点积。最后，我们计算<code>l1_delta</code>，这是<code>l1_loss</code>和<code>l1</code>通过Sigmoid导数函数的结果。</p></li><li><p><strong>更新权重</strong>：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">w1 += l1.T.dot(l2_delta)</span><br><span class="line">w0 += l0.T.dot(l1_delta)</span><br></pre></td></tr></tbody></table></figure><p>在这里，我们根据反向传播的结果更新权重。<code>w1</code>增加的是<code>l1</code>的转置和<code>l2_delta</code>的点积，<code>w0</code>增加的是<code>l0</code>的转置和<code>l1_delta</code>的点积。</p></li></ol><p>这就是整个网络的训练过程，包括前向传播、计算误差、反向传播和更新权重。</p>]]></content>
      
      
      <categories>
          
          <category> NLP Insights </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
            <tag> Gradient Descent </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo博客添加可交互式足迹地图</title>
      <link href="/2023/07/08/Tech%20Toolbox/GeoMapToBlog/"/>
      <url>/2023/07/08/Tech%20Toolbox/GeoMapToBlog/</url>
      
        <content type="html"><![CDATA[<h1 id="在Blog中添加可交互式足迹地图"><a href="#在Blog中添加可交互式足迹地图" class="headerlink" title="在Blog中添加可交互式足迹地图"></a>在Blog中添加可交互式足迹地图</h1><p>这篇文章将向你展示如何在基于Hexo和Next的GitHub Pages博客中创建一个交互式的世界地图页面，这个地图将展示你曾经访问过的城市，你可以根据你对每个城市访问的频率在地图上显示不同颜色的标记，你还可以点击这些标记来显示更多关于这个城市的信息。</p><h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><p>你需要安装以下工具：</p><ul><li>Node.js 和 NPM</li><li>Hexo</li></ul><p>确保你的博客已经被部署到GitHub Pages，并且你在本地的开发环境已经正确设置。</p><h2 id="步骤一：创建新页面"><a href="#步骤一：创建新页面" class="headerlink" title="步骤一：创建新页面"></a>步骤一：创建新页面</h2><p>在你的Hexo项目的根目录下，运行以下命令：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new page <span class="string">"travel"</span></span><br></pre></td></tr></tbody></table></figure><p>这个命令将在<code>source</code>目录下创建一个名为”travel”的文件夹，并在该文件夹下创建一个<code>index.md</code>文件。</p><h2 id="步骤二：安装-Leaflet"><a href="#步骤二：安装-Leaflet" class="headerlink" title="步骤二：安装 Leaflet"></a>步骤二：安装 Leaflet</h2><p>在你的Hexo项目的根目录下，运行以下命令：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install leaflet</span><br></pre></td></tr></tbody></table></figure><p>然后在<code>index.md</code>文件的最顶部引入Leaflet的CSS和JS：</p><figure class="highlight markdown"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line"><span class="section">title: Travel</span></span><br><span class="line"><span class="section">---</span></span><br><span class="line"></span><br><span class="line">&lt;!-- 引入 Leaflet 的 CSS --&gt;</span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">link</span> <span class="attr">rel</span>=<span class="string">"stylesheet"</span> <span class="attr">href</span>=<span class="string">"/node_modules/leaflet/dist/leaflet.css"</span> /&gt;</span></span></span><br><span class="line"></span><br><span class="line">&lt;!-- 引入 Leaflet 的 JavaScript 文件 --&gt;</span><br><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">script</span> <span class="attr">src</span>=<span class="string">"/node_modules/leaflet/dist/leaflet.js"</span>&gt;</span></span><span class="language-xml"><span class="tag">&lt;/<span class="name">script</span>&gt;</span></span></span><br></pre></td></tr></tbody></table></figure><h2 id="步骤三：创建地图容器"><a href="#步骤三：创建地图容器" class="headerlink" title="步骤三：创建地图容器"></a>步骤三：创建地图容器</h2><p>在<code>index.md</code>中，创建一个用来承载地图的<code>&lt;div&gt;</code>元素，然后通过CSS给它设置一个明确的高度：</p><figure class="highlight markdown"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">style</span>&gt;</span></span></span><br><span class="line"><span class="code">    #map {</span></span><br><span class="line"><span class="code">        height: 500px;</span></span><br><span class="line"><span class="code">        width: 100%;</span></span><br><span class="line"><span class="code">        position: relative;</span></span><br><span class="line"><span class="code">    }</span></span><br><span class="line"><span class="code">&lt;/style&gt;</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="language-xml"><span class="language-xml"><span class="tag">&lt;<span class="name">div</span> <span class="attr">id</span>=<span class="string">"map"</span>&gt;</span></span></span><span class="language-xml"><span class="language-css">&lt;/<span class="selector-tag">div</span>&gt;</span></span></span><br></pre></td></tr></tbody></table></figure><h2 id="步骤四：初始化地图"><a href="#步骤四：初始化地图" class="headerlink" title="步骤四：初始化地图"></a>步骤四：初始化地图</h2><p>使用Leaflet的API初始化地图，并将其中心设置为经度0,纬度0（大西洋中部），并设置初始的缩放级别为2。</p><figure class="highlight markdown"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">script</span>&gt;</span></span></span><br><span class="line"><span class="code">    document.addEventListener('DOMContentLoaded', function() {</span></span><br><span class="line"><span class="code">        var map = L.map('map').setView([0, 0], 2);</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code">        L.tileLayer('https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png', {</span></span><br><span class="line"><span class="code">            maxZoom: 19,</span></span><br><span class="line"><span class="code">        }).addTo(map);</span></span><br><span class="line"><span class="code">    });</span></span><br><span class="line"><span class="code">&lt;/script&gt;</span></span><br></pre></td></tr></tbody></table></figure><h2 id="步骤五：添加城市数据"><a href="#步骤五：添加城市数据" class="headerlink" title="步骤五：添加城市数据"></a>步骤五：添加城市数据</h2><p>创建一个GeoJSON文件来存储城市的数据，包括城市的名称，经纬度，你访问的次数，以及一个图片的URL。文件的格式应该如下：</p><figure class="highlight json"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">{</span></span><br><span class="line">    <span class="attr">"type"</span><span class="punctuation">:</span> <span class="string">"FeatureCollection"</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">"features"</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">        <span class="punctuation">{</span></span><br><span class="line">            <span class="attr">"type"</span><span class="punctuation">:</span> <span class="string">"Feature"</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">"properties"</span><span class="punctuation">:</span> <span class="punctuation">{</span></span><br><span class="line">                <span class="attr">"name"</span><span class="punctuation">:</span> <span class="string">"北京"</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">"visits"</span><span class="punctuation">:</span> <span class="number">3</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">"image"</span><span class="punctuation">:</span> <span class="string">"https://your-image-url.com"</span></span><br><span class="line">            <span class="punctuation">}</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">"geometry"</span><span class="punctuation">:</span> <span class="punctuation">{</span></span><br><span class="line">                <span class="attr">"type"</span><span class="punctuation">:</span> <span class="string">"Point"</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">"coordinates"</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="number">116.4074</span><span class="punctuation">,</span> <span class="number">39.9042</span><span class="punctuation">]</span></span><br><span class="line">            <span class="punctuation">}</span></span><br><span class="line">        <span class="punctuation">}</span><span class="punctuation">,</span></span><br><span class="line">        ...</span><br><span class="line">    <span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">}</span></span><br></pre></td></tr></tbody></table></figure><p>将此文件保存在你的Hexo项目的<code>source</code>目录下，例如，你可以命名为<code>cities.json</code>。</p><h2 id="步骤六：加载数据并显示在地图上"><a href="#步骤六：加载数据并显示在地图上" class="headerlink" title="步骤六：加载数据并显示在地图上"></a>步骤六：加载数据并显示在地图上</h2><p>在<code>index.md</code>文件中，添加以下代码来加载GeoJSON文件，然后在地图上显示数据：</p><figure class="highlight markdown"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="language-xml"><span class="tag">&lt;<span class="name">script</span>&gt;</span></span></span><br><span class="line"><span class="code">    document.addEventListener('DOMContentLoaded', function() {</span></span><br><span class="line"><span class="code">        var map = L.map('map').setView([0, 0], 2);</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code">        L.tileLayer('https://{s}.tile.openstreetmap.org/{z}/{x}/{y}.png', {</span></span><br><span class="line"><span class="code">            maxZoom: 19,</span></span><br><span class="line"><span class="code">        }).addTo(map);</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code">        fetch('/travel/cities.json')</span></span><br><span class="line"><span class="code">            .then(response =&gt; response.json())</span></span><br><span class="line"><span class="code">            .then(data =&gt; {</span></span><br><span class="line"><span class="code">                L.geoJSON(data, {</span></span><br><span class="line"><span class="code">                    pointToLayer: function (feature, latlng) {</span></span><br><span class="line"><span class="code">                        var color = getColor(feature.properties.visits);</span></span><br><span class="line"><span class="code">                        return L.circleMarker(latlng, { fillColor: color, fillOpacity: 0.5 });</span></span><br><span class="line"><span class="code">                    },</span></span><br><span class="line"><span class="code">                    onEachFeature: function (feature, layer) {</span></span><br><span class="line"><span class="code">                        layer.bindPopup(`&lt;h2&gt;${feature.properties.name}&lt;/h2&gt;&lt;img src="${feature.properties.image}" width="200"&gt;`);</span></span><br><span class="line"><span class="code">                    }</span></span><br><span class="line"><span class="code">                }).addTo(map);</span></span><br><span class="line"><span class="code">            });</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code">        function getColor(visits) {</span></span><br><span class="line"><span class="code">            // 根据你访问的次数返回不同的颜色</span></span><br><span class="line"><span class="code">            return visits &gt; 10 ? '#800026' :</span></span><br><span class="line"><span class="code">                   visits &gt; 5  ? '#BD0026' :</span></span><br><span class="line"><span class="code">                   visits &gt; 2  ? '#E31A1C' :</span></span><br><span class="line"><span class="code">                   visits &gt; 1  ? '#FC4E2A' :</span></span><br><span class="line"><span class="code">                                 '#FFEDA0';</span></span><br><span class="line"><span class="code">        }</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code">        // 强制Leaflet重新计算地图的尺寸</span></span><br><span class="line"><span class="code">        setTimeout(function() {</span></span><br><span class="line"><span class="code">            map.invalidateSize();</span></span><br><span class="line"><span class="code">        }, 100);</span></span><br><span class="line"><span class="code">    });</span></span><br><span class="line"><span class="code">&lt;/script&gt;</span></span><br></pre></td></tr></tbody></table></figure><p>以上的JavaScript代码在页面加载完成后会运行，首先初始化地图，然后加载并解析GeoJSON文件，对于每一个城市，创建一个圆形的标记，颜色根据访问次数决定。当点击标记时，弹出一个包含城市名称和图片的窗口。</p><p>至此，你的博客就成功地添加了一个交互式的足迹地图页面。</p>]]></content>
      
      
      <categories>
          
          <category> Tech Toolbox </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Blog </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SUP体验</title>
      <link href="/2023/07/07/Life%20Reflections/SUP%E4%BD%93%E9%AA%8C/"/>
      <url>/2023/07/07/Life%20Reflections/SUP%E4%BD%93%E9%AA%8C/</url>
      
        <content type="html"><![CDATA[<h1 id="立式单桨冲浪-SUP"><a href="#立式单桨冲浪-SUP" class="headerlink" title="立式单桨冲浪 (SUP)"></a>立式单桨冲浪 (SUP)</h1><p>立式单桨冲浪 (Stand-Up Paddleboarding, 简称SUP) 是一项起源于夏威夷现代冲浪运动的水上活动。冲浪者站在漂浮在水面上的木板上，通过使用单桨来推动自己在水中前进。这项运动近年来在世界各地迅速流行起来，成为许多人喜爱的水上活动之一。</p><h2 id="体验-🌊"><a href="#体验-🌊" class="headerlink" title="体验 🌊"></a>体验 🌊</h2><p>如果你对立式单桨冲浪感兴趣，不妨考虑参加一个专门的课程来了解和体验这项活动。我最近参加了一个位于新加坡东海岸的立式单桨冲浪课程，以下是我的体验分享。</p><p>课程持续了两个小时，教练非常详细地介绍了SUP的基本知识和技巧。他们解释了如何站在板上保持平衡，正确使用桨来推动自己在水中前进，以及如何转向和控制板的方向。虽然一开始我感到有些不稳定，但很快就适应了这种站立的姿势，并且能够轻松地掌握桨的使用技巧。</p><style type="text/css">    .fancybox {        display: inline-block;    }</style><img src="/2023/07/07/Life%20Reflections/SUP%E4%BD%93%E9%AA%8C/Me.jpg" class=""><img src="/2023/07/07/Life%20Reflections/SUP%E4%BD%93%E9%AA%8C/certification.jpg" class=""><p>课程的安排非常合理，适合初学者。在教练的指导下，我能够很快上手，并且只掉进海里一次 😅，这让我感到有些尴尬。不过，SUP是一项非常有趣和挑战性的活动，掉进水里也是学习过程中的一部分。我相信随着练习的积累，我能够变得更加熟练和稳定。</p><p>课程在东海岸的开放海域进行，这意味着我们可以在广阔的海洋中自由冲浪。这种开放的环境让人感到非常舒适和自由。然而，我们也需要注意防晒 ☀️，因为长时间暴露在阳光下可能会对皮肤造成伤害。</p><h2 id="报名信息-📝"><a href="#报名信息-📝" class="headerlink" title="报名信息 📝"></a>报名信息 📝</h2><p>如果你对立式单桨冲浪课程感兴趣，你可以在以下网址报名：<a href="https://www.onepa.gov.sg/courses/sup-starter-course-open-water-c026977237">https://www.onepa.gov.sg/courses/sup-starter-course-open-water-c026977237</a>。这是新加坡一家名为PAssion WaVe @ East Coast的机构提供的课程。报名费用为61新币 💰。</p><p>在报名之前，需要满足一些课程要求</p><p>。你需要具备基本的游泳能力 🏊‍♂️，并且可以在穿着救生衣的情况下在海里游泳至少50米。这是为了确保你在水上活动中的安全。</p><p>课程地点位于新加坡东海岸的PAssion WaVe @ East Coast，具体地址是1390 ECP, Singapore 468961 📍。这个地点非常适合进行立式单桨冲浪，因为它提供了开放的海域和适宜的条件。</p><p>立式单桨冲浪是一项令人兴奋和刺激的水上活动，无论是对于冲浪爱好者还是初学者来说，都是一个很好的选择。通过参加课程，你将能够学习到正确的技巧和安全知识，享受冲浪的乐趣，并在海洋中留下美好的回忆。如果你对这项活动感兴趣，不妨考虑报名参加一个课程，开始你的立式单桨冲浪之旅吧！🏄‍♂️🌊</p>]]></content>
      
      
      <categories>
          
          <category> Life Reflections </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 坡岛生活指北 </tag>
            
            <tag> Sports </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DSSM 模型详解</title>
      <link href="/2023/07/07/NLP%20Insights/DSSM%20%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/"/>
      <url>/2023/07/07/NLP%20Insights/DSSM%20%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<h1 id="DSSM-Deep-Structured-Semantic-Models-模型详解"><a href="#DSSM-Deep-Structured-Semantic-Models-模型详解" class="headerlink" title="DSSM (Deep Structured Semantic Models) 模型详解"></a>DSSM (Deep Structured Semantic Models) 模型详解</h1><h2 id="一、模型简介"><a href="#一、模型简介" class="headerlink" title="一、模型简介"></a>一、模型简介</h2><p>DSSM（Deep Structured Semantic Models）是微软提出的一个深度学习模型，用于学习文本的语义表达。DSSM首次在信息检索领域中被提出，用于处理查询-文档匹配任务，但很快被应用到了各种其他场景，如广告点击率预测，推荐系统等。</p><h2 id="二、模型结构"><a href="#二、模型结构" class="headerlink" title="二、模型结构"></a>二、模型结构</h2><p>DSSM模型主要由三个部分构成：</p><ol><li><strong>输入层</strong>：在这一层，将输入的文本（查询或者文档）转化为词向量。</li><li><strong>深度神经网络</strong>：输入层之后是一系列全连接层，它们学习输入的语义表示。</li><li><strong>输出层</strong>：最后，输出层将深度神经网络的输出转化为概率分布，用于表示查询与文档之间的语义匹配度。</li></ol><h2 id="三、在推荐系统中的应用"><a href="#三、在推荐系统中的应用" class="headerlink" title="三、在推荐系统中的应用"></a>三、在推荐系统中的应用</h2><p>DSSM可应用在推荐系统中，它可以学习用户的行为特征与物品特征的语义匹配度，用于评估用户对物品的兴趣。在实际应用中，通常将用户行为序列作为查询，将候选物品的特征作为文档，通过DSSM学习用户的实时兴趣，并将兴趣与物品的匹配度用于排序。</p><h2 id="四、模型实现"><a href="#四、模型实现" class="headerlink" title="四、模型实现"></a>四、模型实现</h2><p>以下是一个使用PyTorch实现的DSSM模型的示例：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DSSM</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, hidden_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(DSSM, self).__init__()</span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        self.linear1 = nn.Linear(vocab_size, hidden_size)</span><br><span class="line">        self.linear2 = nn.Linear(hidden_size, hidden_size)</span><br><span class="line">        self.linear3 = nn.Linear(hidden_size, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, query, doc</span>):</span><br><span class="line">        query = self.linear1(query)</span><br><span class="line">        query = torch.relu(query)</span><br><span class="line">        query = self.linear2(query)</span><br><span class="line">        query = torch.relu(query)</span><br><span class="line"></span><br><span class="line">        doc = self.linear1(doc)</span><br><span class="line">        doc = torch.relu(doc)</span><br><span class="line">        doc = self.linear2(doc)</span><br><span class="line">        doc = torch.relu(doc)</span><br><span class="line"></span><br><span class="line">        out = self.linear3(query * doc)</span><br><span class="line">        out = torch.sigmoid(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></tbody></table></figure><h2 id="五、训练过程"><a href="#五、训练过程" class="headerlink" title="五、训练过程"></a>五、训练过程</h2><p>在训练DSSM模型时，我们通常会采用pairwise的训练方式，也就是利用正样本（正匹配对）和负样本（负匹配对）进行训练。在信息检索任务中，一个正样本可能是一个查询和一个相关的文档，负样本可能是同一个查询和一个不相关的文档。</p><p>对于每个训练样本，都将经过DSSM模型，得到查询和文档的向量表示，然后计算两者的余弦相似度作为预测的匹配分数。接下来，使用一个合适的损失函数，例如交叉熵损失，来优化模型参数。</p><p>以下是一个使用PyTorch训练DSSM模型的示例：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化模型</span></span><br><span class="line">model = DSSM(vocab_size=<span class="number">10000</span>, hidden_size=<span class="number">128</span>)</span><br><span class="line"><span class="comment"># 使用Adam优化器</span></span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line"><span class="comment"># 使用二元交叉熵作为损失函数</span></span><br><span class="line">criterion = torch.nn.BCELoss()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader, <span class="number">0</span>):</span><br><span class="line">        query, doc, label = data</span><br><span class="line">        <span class="comment"># 清零梯度</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        <span class="comment"># 前向传播</span></span><br><span class="line">        output = model(query, doc)</span><br><span class="line">        <span class="comment"># 计算损失</span></span><br><span class="line">        loss = criterion(output, label)</span><br><span class="line">        <span class="comment"># 反向传播</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        <span class="comment"># 更新权重</span></span><br><span class="line">        optimizer.step()</span><br></pre></td></tr></tbody></table></figure><p>以上代码中，首先初始化了一个DSSM模型，并定义了一个优化器和损失函数。在每个训练步骤中，我们对数据进行前向传播，计算损失，然后进行反向传播和优化。</p><p>在训练过程中，可以在验证集上评估模型的效果，并根据需要调整学习率和其他训练参数。训练结束后，通常会在测试集上对模型进行最后的评估，以确保模型能够很好地泛化到未见过的数据。</p><p>训练DSSM模型的主要挑战是选择合适的正样本和负样本，以及设置合理的训练参数。在实际应用中，可能需要使用一些策略来平衡正负样本的比例，或者使用更复杂的损失函数来处理不均衡的数据。</p><h2 id="六、线上推理方法"><a href="#六、线上推理方法" class="headerlink" title="六、线上推理方法"></a>六、线上推理方法</h2><p>DSSM模型在线上推理主要有以下两个步骤：</p><ol><li><strong>向量化</strong>：在模型训练完毕后，可以先对所有的物品进行向量化处理，保存为物品的向量表示。当新的用户请求到来时，将用户的行为序列转化为向量表示。</li><li><strong>计算相似度</strong>：然后，计算用户向量与各个物品向量的相似度，一般使用余弦相似度作为计算方法。相似度越高，表示用户对物品的兴趣越大。</li></ol><p>具体实施时，需要注意的是，为了提高在线推理的效率，通常会采用一些近似最近邻搜索的技术（如Faiss等）来快速找到与用户最相似的物品，而不是遍历所有的物品。</p><p>总的来说，DSSM是一个非常有效的深度学习模型，用于学习文本或其他类型数据的语义表示，广泛应用于信息检索、推荐系统等多种场景中。</p>]]></content>
      
      
      <categories>
          
          <category> NLP Insights </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DSSM </tag>
            
            <tag> Recommendation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>推荐系统离线评估指标详解</title>
      <link href="/2023/07/07/NLP%20Insights/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%A6%BB%E7%BA%BF%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87%E8%AF%A6%E8%A7%A3/"/>
      <url>/2023/07/07/NLP%20Insights/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%A6%BB%E7%BA%BF%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87%E8%AF%A6%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<h1 id="推荐系统离线评估指标详解"><a href="#推荐系统离线评估指标详解" class="headerlink" title="推荐系统离线评估指标详解"></a>推荐系统离线评估指标详解</h1><p>本文将详细介绍推荐系统中常用的离线评估指标，包括精确率（Precision）、召回率（Recall）、准确率（Accuracy）、F1-Score、NDCG、命中率（Hit Rate）、AUC、GAUC和对数损失（Log Loss）。这些指标对于评估推荐系统的性能和效果至关重要。这些指标对于评估推荐系统的性能和效果至关重要。</p><p>在推荐系统中，精确率衡量了推荐列表中真正符合用户兴趣的物品比例，召回率衡量了所有符合用户兴趣的物品中被成功推荐出的比例。准确率用于衡量模型预测结果与实际结果一致的比例。F1-Score综合考虑了精确率和召回率，对模型进行综合评价。NDCG则用于评价推荐系统排序质量，特别适用于考虑元素相关性排序的推荐系统。</p><p>我们将为每个指标提供详细的解释和计算公式，并给出Python实现的示例代码。这些指标的适用性将根据推荐系统的需求进行评估，以帮助您选择适合自己系统评估的指标。</p><p>通过深入了解这些离线评估指标，您将能够更好地评估和改进您的推荐系统，提供更准确和个性化的推荐服务。</p><h2 id="1-精确率-Precision"><a href="#1-精确率-Precision" class="headerlink" title="1. 精确率 (Precision)"></a>1. 精确率 (Precision)</h2><h3 id="功能"><a href="#功能" class="headerlink" title="功能"></a>功能</h3><p>精确率用于度量分类模型的准确程度，即模型预测为正类别的样本中实际为正类别的比例。在推荐系统中，精确率可以理解为在用户接收的推荐物品中，真正符合用户兴趣的物品比例。</p><h3 id="计算公式"><a href="#计算公式" class="headerlink" title="计算公式"></a>计算公式</h3><p>在二分类问题中，精确率的计算公式为：</p><p>$$ Precision = \frac{TP}{TP+FP} $$</p><p>其中，TP表示真正例（True Positives），FP表示假正例（False Positives）。</p><h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><p>假设一个推荐系统推荐出了10个商品，其中5个商品是用户真正感兴趣的。那么精确率为：5 / 10 = 0.5。</p><h3 id="Python-实现"><a href="#Python-实现" class="headerlink" title="Python 实现"></a>Python 实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># y_true 表示真实标签，y_pred 表示预测标签</span></span><br><span class="line">y_true = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">y_pred = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">precision = precision_score(y_true, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f'Precision: <span class="subst">{precision}</span>'</span>)</span><br></pre></td></tr></tbody></table></figure><h3 id="适用性"><a href="#适用性" class="headerlink" title="适用性"></a>适用性</h3><p>⭐️⭐️⭐️⭐️⭐️</p><p>精确率非常适用于推荐系统的评估。推荐系统的主要目标就是准确地推荐用户可能感兴趣的商品或服务。如果推荐的商品中大部分都是用户感兴趣的，那么精确率就高。</p><hr><h2 id="2-召回率-Recall"><a href="#2-召回率-Recall" class="headerlink" title="2. 召回率 (Recall)"></a>2. 召回率 (Recall)</h2><h3 id="功能-1"><a href="#功能-1" class="headerlink" title="功能"></a>功能</h3><p>召回率用于度量分类模型覆盖正类样本的能力，即在所有实际为正类别的样本中，模型预测为正类别的比例。在推荐系统中，召回率可以理解为所有符合用户兴趣的物品中，被模型成功推荐出的物品比例。</p><h3 id="计算公式-1"><a href="#计算公式-1" class="headerlink" title="计算公式"></a>计算公式</h3><p>在二分类问题中，召回率的计算公式为：</p><p>$$ Recall = \frac{TP}{TP+FN} $$</p><p>其中，TP表示真正例（True Positives），FN表示假反例（False Negatives）。</p><h3 id="示例-1"><a href="#示例-1" class="headerlink" title="示例"></a>示例</h3><p>假设一个用户真正感兴趣的商品有20个，推荐系统成功推荐出了10个。那么召回率为：10 / 20 = 0.5。</p><h3 id="Python-实现-1"><a href="#Python-实现-1" class="headerlink" title="Python 实现"></a>Python 实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> recall_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># y_true 表示真实标签，y_pred 表示预测标签</span></span><br><span class="line">y_true = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">y_pred = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">recall = recall_score(y_true, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f'Recall: <span class="subst">{recall}</span>'</span>)</span><br></pre></td></tr></tbody></table></figure><h3 id="适用性-1"><a href="#适用性-1" class="headerlink" title="适用性"></a>适用性</h3><p>⭐️⭐️⭐️⭐️⭐️</p><p>召回率也非常适用于推荐系统的评估。推荐系统的目标之一是覆盖尽可能多的用户感兴趣的商品或服务。如果能将用户感兴趣的商品大部分都推荐出来，那么召回率就高。</p><hr><h2 id="3-准确率-Accuracy"><a href="#3-准确率-Accuracy" class="headerlink" title="3. 准确率 (Accuracy)"></a>3. 准确率 (Accuracy)</h2><h3 id="功能-2"><a href="#功能-2" class="headerlink" title="功能"></a>功能</h3><p>准确率用于度量分类模型的预测结果与实际结果一致的比例，即在所有样本中，模型预测正确的比例。</p><h3 id="计算公式-2"><a href="#计算公式-2" class="headerlink" title="计算公式"></a>计算公式</h3><p>在二分类问题中，准确率的计算公式为：</p><p>$$ Accuracy = \frac{TP+TN}{TP+FP+TN+FN} $$</p><p>其中，TP表示真正例（True Positives），TN表示真反例（True Negatives），FP表示假正例（False Positives），FN表示假反例（False Negatives）。</p><h3 id="示例-2"><a href="#示例-2" class="headerlink" title="示例"></a>示例</h3><p>假设一个推荐系统对100个商品做出了预测，其中70个预测正确，那么准确率为：70 / 100 = 0.7。</p><h3 id="Python-实现-2"><a href="#Python-实现-2" class="headerlink" title="Python 实现"></a>Python 实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># y_true 表示真实标签，y_pred 表示预测标签</span></span><br><span class="line">y_true = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">y_pred = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">accuracy = accuracy_score(y_true, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f'Accuracy: <span class="subst">{accuracy}</span>'</span>)</span><br></pre></td></tr></tbody></table></figure><h3 id="适用性-2"><a href="#适用性-2" class="headerlink" title="适用性"></a>适用性</h3><p>⭐️⭐️⭐️</p><p>准确率在推荐系统中的适用性较低，因为推荐系统往往面临着不平衡的标签问题。比如，在一个商品推荐系统中，用户可能对大部分商品都没有兴趣，这样准确率就不能很好地反映出模型的性能。</p><hr><h2 id="4-F1-Score"><a href="#4-F1-Score" class="headerlink" title="4. F1-Score"></a>4. F1-Score</h2><h3 id="功能-3"><a href="#功能-3" class="headerlink" title="功能"></a>功能</h3><p>F1-Score 是精确率和召回率的调和平均值，用于同时考虑精确率和召回率，对模型进行综合评价。</p><h3 id="计算公式-3"><a href="#计算公式-3" class="headerlink" title="计算公式"></a>计算公式</h3><p>F1-Score 的计算公式为：</p><p>$$ F1-Score = 2 \times \frac{Precision \times Recall}{Precision + Recall} $$</p><h3 id="示例-3"><a href="#示例-3" class="headerlink" title="示例"></a>示例</h3><p>假设一个推荐系统的精确率为0.5，召回率为0.7，那么 F1-Score 为：2 * (0.5 * 0.7) / (0.5 + 0.7) = 0.583。</p><h3 id="Python-实现-3"><a href="#Python-实现-3" class="headerlink" title="Python 实现"></a>Python 实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> f1_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># y_true 表示真实标签，y_pred 表示预测标签</span></span><br><span class="line">y_true = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">y_pred = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">f1 = f1_score(y_true, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f'F1-Score: <span class="subst">{f1}</span>'</span>)</span><br></pre></td></tr></tbody></table></figure><h3 id=""><a href="#" class="headerlink" title=""></a></h3><p>适用性</p><p>⭐️⭐️⭐️⭐️⭐️</p><p>F1-Score 非常适用于推荐系统的评估。推荐系统需要平衡精确率和召回率，精确率高说明推荐的准确，召回率高说明推荐的全面，而 F1-Score 正是一个兼顾两者的评价指标。</p><hr><h2 id="5-NDCG-Normalized-Discounted-Cumulative-Gain"><a href="#5-NDCG-Normalized-Discounted-Cumulative-Gain" class="headerlink" title="5. NDCG (Normalized Discounted Cumulative Gain)"></a>5. NDCG (Normalized Discounted Cumulative Gain)</h2><h3 id="功能-4"><a href="#功能-4" class="headerlink" title="功能"></a>功能</h3><p>NDCG 是一个用于评价推荐系统排序质量的指标，特别是对于那些考虑元素相关性排序的推荐系统。它可以衡量模型预测的排序列表与真实的排序列表的相似程度。</p><h3 id="计算公式-4"><a href="#计算公式-4" class="headerlink" title="计算公式"></a>计算公式</h3><p>NDCG 的计算公式为：</p><p>$$ NDCG = \frac{DCG}{IDCG} $$</p><p>其中，DCG 表示推荐列表的 Discounted Cumulative Gain，计算公式为：</p><p>$$ DCG = \sum_{i=1}^{N} \frac{2^{rel_i} - 1}{log_2(i + 1)} $$</p><p>IDCG 表示理想情况下的最大 DCG，即所有相关性商品都排在前面，计算公式与 DCG 相同，只是商品的排序按照相关性从大到小。</p><h3 id="示例-4"><a href="#示例-4" class="headerlink" title="示例"></a>示例</h3><p>假设一个推荐系统对5个商品的相关性预测为 [3, 2, 3, 0, 1]，那么 DCG = (2^3 - 1)/log2(1+1) + (2^2 - 1)/log2(2+1) + (2^3 - 1)/log2(3+1) + (2^0 - 1)/log2(4+1) + (2^1 - 1)/log2(5+1)。如果理想的排序为 [3, 3, 2, 1, 0]，那么 IDCG 可以按照相同的方式计算，NDCG = DCG / IDCG。</p><h3 id="Python-实现-4"><a href="#Python-实现-4" class="headerlink" title="Python 实现"></a>Python 实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> ndcg_score</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># y_true 表示真实的相关性，y_score 表示预测的相关性</span></span><br><span class="line">y_true = np.array([[<span class="number">3</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">1</span>]])</span><br><span class="line">y_score = np.array([[<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">3</span>]])</span><br><span class="line"></span><br><span class="line">ndcg = ndcg_score(y_true, y_score)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f'NDCG: <span class="subst">{ndcg}</span>'</span>)</span><br></pre></td></tr></tbody></table></figure><h3 id="适用性-3"><a href="#适用性-3" class="headerlink" title="适用性"></a>适用性</h3><p>⭐️⭐️⭐️⭐️⭐️</p><p>NDCG 非常适用于推荐系统的评估，特别是对于需要考虑推荐排序的系统。NDCG 能够衡量推荐系统预测的排序列表与真实的排序列表的相似程度，从而评价模型的性能。</p><hr><p>以下几个指标（Hit Rate, AUC, GAUC, LogLoss）由于篇幅限制，我将在下一次</p><p>回复中继续介绍。</p><h2 id="6-命中率-Hit-Rate"><a href="#6-命中率-Hit-Rate" class="headerlink" title="6. 命中率 (Hit Rate)"></a>6. 命中率 (Hit Rate)</h2><h3 id="功能-5"><a href="#功能-5" class="headerlink" title="功能"></a>功能</h3><p>命中率是评价推荐系统是否能够推荐出用户感兴趣的物品的指标，可以理解为系统是否“命中”用户感兴趣的商品。</p><h3 id="计算公式-5"><a href="#计算公式-5" class="headerlink" title="计算公式"></a>计算公式</h3><p>命中率的计算公式为：</p><p>$$ HitRate = \frac{Number\ of\ Hits}{Total\ Number\ of\ Tests} $$</p><h3 id="示例-5"><a href="#示例-5" class="headerlink" title="示例"></a>示例</h3><p>假设一个推荐系统对10个商品做出了预测，其中3个预测命中，那么命中率为：3 / 10 = 0.3。</p><h3 id="Python-实现-5"><a href="#Python-实现-5" class="headerlink" title="Python 实现"></a>Python 实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">hit_rate</span>(<span class="params">recommended_items, true_items</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">int</span>(<span class="built_in">any</span>(item <span class="keyword">in</span> true_items <span class="keyword">for</span> item <span class="keyword">in</span> recommended_items))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 推荐的商品和真正感兴趣的商品</span></span><br><span class="line">recommended_items = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">true_items = [<span class="number">2</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]</span><br><span class="line"></span><br><span class="line">hit_rate = hit_rate(recommended_items, true_items)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f'Hit Rate: <span class="subst">{hit_rate}</span>'</span>)</span><br></pre></td></tr></tbody></table></figure><h3 id="适用性-4"><a href="#适用性-4" class="headerlink" title="适用性"></a>适用性</h3><p>⭐️⭐️⭐️⭐️⭐️</p><p>命中率对于推荐系统的评估非常重要。因为推荐系统的主要目标就是推荐出用户感兴趣的物品，如果推荐的物品中包含了用户真正感兴趣的物品，那么命中率就高。</p><hr><p>非常感谢您的建议，下面是根据您的建议修改后的 AUC 和 GAUC 的介绍。</p><h2 id="7-AUC-Area-Under-Curve"><a href="#7-AUC-Area-Under-Curve" class="headerlink" title="7. AUC (Area Under Curve)"></a>7. AUC (Area Under Curve)</h2><h3 id="功能-6"><a href="#功能-6" class="headerlink" title="功能"></a>功能</h3><p>AUC 是一种常用的分类问题的性能评估指标，特别是对于推荐系统，它是一个衡量模型对正负样本区分度的指标。对于每一个正样本，计算其预测分数高于多少比例的负样本，即正样本的“正样本率”。AUC 就是所有正样本的平均正样本率。</p><h3 id="计算公式-6"><a href="#计算公式-6" class="headerlink" title="计算公式"></a>计算公式</h3><p>AUC 的计算可以描述为：</p><ol><li>对于每个用户，计算推荐系统对于正样本和负样本的预测分数。</li><li>对于每个正样本，计算其预测分数高于多少比例的负样本，即正样本的“正样本率”。</li><li>计算所有正样本的平均正样本率，即 AUC。</li></ol><p>具体的公式可以表示为：</p><p>$$ AUC = \frac{1}{M}\sum_{i=1}^{M} \frac{1}{P_iN_i} \sum_{j=1}^{P_i} \sum_{k=1}^{N_i} I(s_{ij} &gt; s_{ik}) $$</p><p>其中，$M$ 是用户数，$P_i$ 和 $N_i$ 分别是用户 $i$ 的正样本数和负样本数，$s_{ij}$ 和 $s_{ik}$ 分别是用户 $i$ 的正样本 $j$ 和负样本 $k$ 的预测分数，$I(\cdot)$ 是指示函数。</p><h3 id="Python-实现-6"><a href="#Python-实现-6" class="headerlink" title="Python 实现"></a>Python 实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对每个用户计算 AUC，然后取平均</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calculate_auc</span>(<span class="params">users, y_true, y_score</span>):</span><br><span class="line">    auc_list = []</span><br><span class="line">    <span class="keyword">for</span> user <span class="keyword">in</span> users:</span><br><span class="line">        auc = roc_auc_score(y_true[user], y_score[user])</span><br><span class="line">        auc_list.append(auc)</span><br><span class="line">    <span class="keyword">return</span> np.mean(auc_list)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用户列表、真实标签、预测得分</span></span><br><span class="line">users = [<span class="string">'user1'</span>, <span class="string">'user2'</span>]</span><br><span class="line">y_true = {<span class="string">'user1'</span>: [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>], <span class="string">'user2'</span>: [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]}</span><br><span class="line">y_score = {<span class="string">'user1'</span>: [<span class="number">0.9</span>, <span class="number">0.8</span>, <span class="number">0.5</span>, <span class="number">0.4</span>], <span class="string">'user2'</span>: [<span class="number">0.7</span>, <span class="number">0.6</span>, <span class="number">0.3</span>, <span class="number">0.2</span>]}</span><br><span class="line"></span><br><span class="line">auc = calculate_auc(users, y_true, y_score)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f'AUC: <span class="subst">{auc}</span>'</span>)</span><br></pre></td></tr></tbody></table></figure><p>特别需要注意的是，在面试中经常遇到要求使用 Python 手写 AUC 计算面试题，下面展示一个实例，该示例主要考虑二分类问题。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">calculate_auc_manual</span>(<span class="params">y_true, y_score</span>):</span><br><span class="line">    <span class="comment"># 首先，获取所有正样本和负样本的索引</span></span><br><span class="line">    pos_indices = [i <span class="keyword">for</span> i, x <span class="keyword">in</span> <span class="built_in">enumerate</span>(y_true) <span class="keyword">if</span> x == <span class="number">1</span>]</span><br><span class="line">    neg_indices = [i <span class="keyword">for</span> i, x <span class="keyword">in</span> <span class="built_in">enumerate</span>(y_true) <span class="keyword">if</span> x == <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算所有正样本的分数高于多少比例的负样本</span></span><br><span class="line">    pos_count = <span class="built_in">len</span>(pos_indices)</span><br><span class="line">    neg_count = <span class="built_in">len</span>(neg_indices)</span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> pos_indices:</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> neg_indices:</span><br><span class="line">            <span class="keyword">if</span> y_score[i] &gt; y_score[j]:</span><br><span class="line">                count += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 正样本率就是所有正样本的分数高于负样本的数量比上总的正负样本对的数量</span></span><br><span class="line">    pos_rate = count / (pos_count * neg_count)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> pos_rate</span><br><span class="line"></span><br><span class="line"><span class="comment"># 真实标签</span></span><br><span class="line">y_true = [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line"><span class="comment"># 预测得分</span></span><br><span class="line">y_score = [<span class="number">0.9</span>, <span class="number">0.8</span>, <span class="number">0.1</span>, <span class="number">0.4</span>, <span class="number">0.95</span>, <span class="number">0.6</span>]</span><br><span class="line"></span><br><span class="line">auc = calculate_auc_manual(y_true, y_score)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f'AUC: <span class="subst">{auc}</span>'</span>)</span><br><span class="line">``````</span><br><span class="line"></span><br><span class="line">在上述代码中，我们首先获取所有正样本和负样本的索引。然后，对于每一个正样本，我们计算其预测分数高于多少比例的负样本。最后，所有正样本的平均正样本率即为 AUC。</span><br><span class="line"></span><br><span class="line">注意：在真实环境中，上述实现可能会非常慢，因为它需要对所有正负样本对进行比较。在实际应用中，我们通常会使用更有效的算法来计算 AUC，例如使用排序和计数方法。</span><br><span class="line"></span><br><span class="line"><span class="comment">### 适用性</span></span><br><span class="line"></span><br><span class="line">⭐️⭐️⭐️⭐️⭐️</span><br><span class="line"></span><br><span class="line">AUC 非常适用于推荐系统的评估。AUC 可以衡量模型对正负样本的区分能力，对于推荐系统来说，就是能否准确地找出用户感兴趣的物品。因此，AUC 是一个很好的评价指标。</span><br><span class="line"></span><br><span class="line">------</span><br><span class="line"></span><br><span class="line"><span class="comment">## 8. GAUC (Group Area Under Curve)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">###</span></span><br><span class="line"></span><br><span class="line">功能</span><br><span class="line"></span><br><span class="line">GAUC 是 AUC 的扩展，主要应用于推荐系统等场景。它的计算方法和 AUC 类似，只是在计算正样本率时，需要按照用户组进行计算，而不是按照单个样本进行计算。</span><br><span class="line"></span><br><span class="line"><span class="comment">### 计算公式</span></span><br><span class="line"></span><br><span class="line">GAUC 的计算步骤如下：</span><br><span class="line"></span><br><span class="line"><span class="number">1.</span> 对于每个用户，计算推荐系统对于正样本和负样本的预测分数。</span><br><span class="line"><span class="number">2.</span> 对于每个用户，计算其正样本的预测分数高于多少比例的负样本，即用户的“正样本率”。</span><br><span class="line"><span class="number">3.</span> 计算所有用户的平均正样本率，即 GAUC。</span><br><span class="line"></span><br><span class="line">具体的公式可以表示为：</span><br><span class="line"></span><br><span class="line">$$ GAUC = \frac{<span class="number">1</span>}{M}\sum_{i=<span class="number">1</span>}^{M} \left( \frac{<span class="number">1</span>}{P_iN_i} \sum_{j=<span class="number">1</span>}^{P_i} \sum_{k=<span class="number">1</span>}^{N_i} I(s_{ij} &gt; s_{ik}) \right) $$</span><br><span class="line"></span><br><span class="line">其中，$M$ 是用户数，$P_i$ 和 $N_i$ 分别是用户 $i$ 的正样本数和负样本数，$s_{ij}$ 和 $s_{ik}$ 分别是用户 $i$ 的正样本 $j$ 和负样本 $k$ 的预测分数，$I(\cdot)$ 是指示函数。</span><br><span class="line"></span><br><span class="line"><span class="comment">### Python 实现</span></span><br><span class="line"></span><br><span class="line">Python 实现需要根据具体的数据情况进行，以下是一个基本的示例。</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 对每个用户计算 AUC，然后取平均</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calculate_gauc</span>(<span class="params">users, y_true, y_score</span>):</span><br><span class="line">    auc_list = []</span><br><span class="line">    <span class="keyword">for</span> user <span class="keyword">in</span> users:</span><br><span class="line">        auc = roc_auc_score(y_true[user], y_score[user])</span><br><span class="line">        auc_list.append(auc)</span><br><span class="line">    <span class="keyword">return</span> np.mean(auc_list)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用户列表、真实标签、预测得分</span></span><br><span class="line">users = [<span class="string">'user1'</span>, <span class="string">'user2'</span>]</span><br><span class="line">y_true = {<span class="string">'user1'</span>: [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>], <span class="string">'user2'</span>: [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]}</span><br><span class="line">y_score = {<span class="string">'user1'</span>: [<span class="number">0.9</span>, <span class="number">0.8</span>, <span class="number">0.5</span>, <span class="number">0.4</span>], <span class="string">'user2'</span>: [<span class="number">0.7</span>, <span class="number">0.6</span>, <span class="number">0.3</span>, <span class="number">0.2</span>]}</span><br><span class="line"></span><br><span class="line">gauc = calculate_gauc(users, y_true, y_score)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f'GAUC: <span class="subst">{gauc}</span>'</span>)</span><br></pre></td></tr></tbody></table></figure><h3 id="适用性-5"><a href="#适用性-5" class="headerlink" title="适用性"></a>适用性</h3><p>⭐️⭐️⭐️⭐️⭐️</p><p>GAUC 也非常适用于推荐系统的评估，尤其是对于那些需要考虑个体差异的系统。通过计算每个用户的 AUC 并取平均，GAUC 能够更全面地反映推荐系统的性能。</p><h3 id="适用性-6"><a href="#适用性-6" class="headerlink" title="适用性"></a>适用性</h3><p>⭐️⭐️⭐️⭐️⭐️</p><p>GAUC 也非常适用于推荐系统的评估，尤其是对于那些需要考虑个体差异的系统。通过计算每个用户的 AUC 并取平均，GAUC 能够更全面地反映推荐系统的性能。</p><hr><h2 id="9-LogLoss-Logarithmic-Loss"><a href="#9-LogLoss-Logarithmic-Loss" class="headerlink" title="9. LogLoss (Logarithmic Loss)"></a>9. LogLoss (Logarithmic Loss)</h2><h3 id="功能-7"><a href="#功能-7" class="headerlink" title="功能"></a>功能</h3><p>LogLoss 是一种衡量分类模型的损失函数，它考虑了模型预测的概率值。对于二分类问题，其值越小，表示模型的性能越好。</p><h3 id="计算公式-7"><a href="#计算公式-7" class="headerlink" title="计算公式"></a>计算公式</h3><p>LogLoss 的计算公式为：</p><p>$$ LogLoss = -\frac{1}{n}\sum_{i=1}^{n}[y_i\log(p_i) + (1 - y_i)\log(1 - p_i)] $$</p><h3 id="示例-6"><a href="#示例-6" class="headerlink" title="示例"></a>示例</h3><p>假设一个推荐系统对3个样本的预测概率分别为 [0.8, 0.6, 0.3]，而这3个样本的真实标签分别为 [1, 1, 0]，那么 LogLoss 可以通过代入公式进行计算。</p><h3 id="Python-实现-7"><a href="#Python-实现-7" class="headerlink" title="Python 实现"></a>Python 实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> log_loss</span><br><span class="line"></span><br><span class="line"><span class="comment"># y_true 表示真实标签，y_pred 表示预测的概率</span></span><br><span class="line">y_true = [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line">y_pred = [<span class="number">0.8</span>, <span class="number">0.6</span>, <span class="number">0.3</span>]</span><br><span class="line"></span><br><span class="line">logloss = log_loss(y_true, y_pred)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f'LogLoss: <span class="subst">{logloss}</span>'</span>)</span><br></pre></td></tr></tbody></table></figure><h3 id="适用性-7"><a href="#适用性-7" class="headerlink" title="适用性"></a>适用性</h3><p>⭐️⭐️⭐️⭐️</p><p>LogLoss 在推荐系统的评估中适用，但可能并不是最主要的指标。LogLoss 更注重模型预测的概率值是否准确，而推荐系统除了预测的准确性外，还需要考虑其他因素，如覆盖率、新颖性等。因此，LogLoss 可以作为衡量推荐系统的一个辅助指标。</p>]]></content>
      
      
      <categories>
          
          <category> NLP Insights </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Recommendation </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>VS Code Remote SSH连接失败问题</title>
      <link href="/2023/07/06/Debugging%20Diaries/VS%20Code%20Remote%20SSH%E8%BF%9E%E6%8E%A5%E5%A4%B1%E8%B4%A5%E9%97%AE%E9%A2%98/"/>
      <url>/2023/07/06/Debugging%20Diaries/VS%20Code%20Remote%20SSH%E8%BF%9E%E6%8E%A5%E5%A4%B1%E8%B4%A5%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<h1 id="VS-Code-Remote-SSH连接失败的问题解决"><a href="#VS-Code-Remote-SSH连接失败的问题解决" class="headerlink" title="VS Code Remote SSH连接失败的问题解决"></a>VS Code Remote SSH连接失败的问题解决</h1><p>本文档针对VS Code中的Remote SSH插件在尝试连接远程服务器时出现”Failed to parse remote port from server output”错误的情况提供解决方案。作者在经过一系列的排查和尝试后，最终找到了解决的方法。</p><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>在使用VS Code的Remote SSH插件尝试连接远程服务器时，遇到了错误提示”Failed to parse remote port from server output”。此错误提示可能是由于VS Code不能正确地从SSH服务器的输出中解析出远程端口。</p><h2 id="尝试的解决方法"><a href="#尝试的解决方法" class="headerlink" title="尝试的解决方法"></a>尝试的解决方法</h2><ol><li>检查SSH配置文件</li><li>更新VS Code和Remote SSH扩展</li><li>手动SSH连接</li><li>检查远程服务器的状态</li><li>重启VS Code</li></ol><p>以上常见的解决方法都未能解决问题。</p><h2 id="成功的解决方案"><a href="#成功的解决方案" class="headerlink" title="成功的解决方案"></a>成功的解决方案</h2><p>最终，作者尝试了取消勾选VS Code设置中的<code>Remote.SSH: Use Local Server</code>选项，成功连接到了远程服务器。当该选项被选中（默认）时，VS Code会在本地机器上启动一个服务器，然后通过该本地服务器连接到远程SSH服务器。当取消勾选此选项时，VS Code会直接连接到远程SSH服务器，而不通过本地服务器。</p><h3 id="解决步骤"><a href="#解决步骤" class="headerlink" title="解决步骤"></a>解决步骤</h3><ol><li>打开VS Code。</li><li>在左侧的活动栏点击齿轮图标打开设置。</li><li>在设置搜索框中输入<code>Remote.SSH: Use Local Server</code>。</li><li>取消选中出现的<code>Remote.SSH: Use Local Server</code>复选框。</li></ol><h2 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h2><p>虽然直接连接到远程SSH服务器可以解决某些连接问题，但由于没有利用到本地服务器的优势，可能会导致VS Code的性能稍有下降。但只要没有遇到性能问题，这个设置就不需要过于担心。</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>如果你在使用VS Code的Remote SSH插件连接远程服务器时遇到了类似的问题，你也可以试试这个方法，希望这个解决方案能帮助到你。</p>]]></content>
      
      
      <categories>
          
          <category> Debugging Diaries </category>
          
      </categories>
      
      
        <tags>
            
            <tag> IssueFix </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kubernetes基础</title>
      <link href="/2023/06/06/Tech%20Toolbox/Kubernetes%E5%9F%BA%E7%A1%80/"/>
      <url>/2023/06/06/Tech%20Toolbox/Kubernetes%E5%9F%BA%E7%A1%80/</url>
      
        <content type="html"><![CDATA[<blockquote><p>前言：本文将介绍Kubernetes基础中的重要概念——Pod，以及它在Kubernetes中的应用和功能。首先，我们将详细解释Pod的基本概念和构成，包括它作为最小可部署单元的特点和包含的资源。然后，我们将探讨Pod在Kubernetes中的作用与功能，包括调度单位、网络单元、存储单元、生命周期管理和水平扩展。通过深入了解Pod，您将对Kubernetes中的核心概念有更全面的理解。</p></blockquote><h2 id="I-Kubernetes基础：Pod理解与应用"><a href="#I-Kubernetes基础：Pod理解与应用" class="headerlink" title="I. Kubernetes基础：Pod理解与应用"></a><strong>I. Kubernetes基础：Pod理解与应用</strong></h2><h3 id="1-1-Pod的基本概念与构成"><a href="#1-1-Pod的基本概念与构成" class="headerlink" title="1.1 Pod的基本概念与构成"></a><strong>1.1 Pod的基本概念与构成</strong></h3><p>在Kubernetes（简称K8s）中，Pod（容器组）是最小的可部署单元。它是Kubernetes集群中可以运行的一组一个或多个容器的逻辑主机。Pod提供了一个独立的环境，其中包含运行应用程序所需的所有资源，如存储、网络和其他依赖项。</p><p>Pod通常由一个或多个紧密相关的容器组成，这些容器共享相同的命名空间、网络和存储卷。它们可以通过本地主机上的localhost进行通信，并且可以共享文件系统的一部分或全部内容。</p><h3 id="1-2-Pod在Kubernetes中的作用与功能"><a href="#1-2-Pod在Kubernetes中的作用与功能" class="headerlink" title="1.2 Pod在Kubernetes中的作用与功能"></a><strong>1.2 Pod在Kubernetes中的作用与功能</strong></h3><p>Pod在Kubernetes中的作用是以下几个方面：</p><ol><li>调度单位：Kubernetes将Pod作为调度的基本单位，决定在哪个节点上运行Pod。</li><li>网络单元：每个Pod都有自己的IP地址，并且可以通过Kubernetes集群内部和外部的服务发现机制与其他Pod或外部服务通信。</li><li>存储单元：Pod可以共享存储卷，容器之间可以共享文件系统中的数据。</li><li>生命周期管理：Pod可以创建、启动、停止和销毁，它们的生命周期由Kubernetes控制器管理。</li><li>水平扩展：可以通过复制Pod的方式水平扩展应用程序的实例。</li></ol><h2 id="II-Kubernetes应用实践：在Kubernetes中安装和配置Miniconda"><a href="#II-Kubernetes应用实践：在Kubernetes中安装和配置Miniconda" class="headerlink" title="II. Kubernetes应用实践：在Kubernetes中安装和配置Miniconda"></a><strong><strong>II. Kubernetes应用实践：在Kubernetes中安装和配置Miniconda</strong></strong></h2><h3 id="2-1-安装和配置Miniconda的步骤"><a href="#2-1-安装和配置Miniconda的步骤" class="headerlink" title="2.1 安装和配置Miniconda的步骤"></a><strong><strong>2.1 安装和配置Miniconda的步骤</strong></strong></h3><p>本文介绍了如何在Kubernetes集群中安装和配置Miniconda。Miniconda是一个轻量级的Python环境管理工具，可用于创建和管理Python环境及其相关包。</p><h3 id="步骤-1：登录到Kubernetes-Pod的终端"><a href="#步骤-1：登录到Kubernetes-Pod的终端" class="headerlink" title="步骤 1：登录到Kubernetes Pod的终端"></a>步骤 1：登录到Kubernetes Pod的终端</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl <span class="built_in">exec</span> -it &lt;pod-name&gt; -- /bin/bash</span><br></pre></td></tr></tbody></table></figure><p>将 <code>&lt;pod-name&gt;</code> 替换为要登录的Pod的名称。</p><h3 id="步骤-2：下载和安装Miniconda"><a href="#步骤-2：下载和安装Miniconda" class="headerlink" title="步骤 2：下载和安装Miniconda"></a>步骤 2：下载和安装Miniconda</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh</span><br><span class="line">bash Miniconda3-latest-Linux-x86_64.sh</span><br></pre></td></tr></tbody></table></figure><p>这将下载Miniconda的安装脚本并启动安装过程。</p><h3 id="步骤-3：完成安装向导"><a href="#步骤-3：完成安装向导" class="headerlink" title="步骤 3：完成安装向导"></a>步骤 3：完成安装向导</h3><p>根据安装向导的提示，选择安装路径、环境变量配置等选项完成Miniconda的安装。</p><h3 id="步骤-4：激活Miniconda环境"><a href="#步骤-4：激活Miniconda环境" class="headerlink" title="步骤 4：激活Miniconda环境"></a>步骤 4：激活Miniconda环境</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> ~/.bashrc</span><br></pre></td></tr></tbody></table></figure><p>重新加载终端或执行上述命令来激活Miniconda环境。</p><h3 id="步骤-5：使用Miniconda"><a href="#步骤-5：使用Miniconda" class="headerlink" title="步骤 5：使用Miniconda"></a>步骤 5：使用Miniconda</h3><p>在激活的Miniconda环境中，您可以使用以下命令来管理环境和安装Python包：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">conda create --name myenv python=3.9</span><br><span class="line">conda activate myenv</span><br><span class="line">conda install package_name</span><br></pre></td></tr></tbody></table></figure><h3 id="2-2-安装和使用Miniconda的注意事项"><a href="#2-2-安装和使用Miniconda的注意事项" class="headerlink" title="2.2 安装和使用Miniconda的注意事项"></a><strong><strong>2.2 安装和使用Miniconda的注意事项</strong></strong></h3><ul><li>请根据Pod的操作系统和架构调整Miniconda的下载链接和安装命令。</li><li>为了自动化安装和配置Miniconda，请将相关步骤和环境配置包含在Pod的初始化脚本或容器镜像构建过程中。</li><li>请在安装和使用Miniconda时遵循适当的最佳实践和安全性措施，并根据具体需求进行配置和管理。</li></ul><h3 id="2-3-安装和配置Miniconda的总结"><a href="#2-3-安装和配置Miniconda的总结" class="headerlink" title="2.3 安装和配置Miniconda的总结"></a><strong><strong>2.3 安装和配置Miniconda的总结</strong></strong></h3><p>通过按照本文中的步骤，在Kubernetes中安装和配置Miniconda，您可以轻松管理Python环境和包，并为您的应用程序提供所需的依赖项。Miniconda的灵活性和可扩展性使其成为在Kubernetes环境中开发和部署Python应用程序的理想选择。</p><h2 id="III-Kubernetes数据操作：在本地Mac电脑将文件传输到Kubernetes集群的流程"><a href="#III-Kubernetes数据操作：在本地Mac电脑将文件传输到Kubernetes集群的流程" class="headerlink" title="III. Kubernetes数据操作：在本地Mac电脑将文件传输到Kubernetes集群的流程"></a><strong><strong>III. Kubernetes数据操作：在本地Mac电脑将文件传输到Kubernetes集群的流程</strong></strong></h2><p>本文介绍了如何在本地Mac电脑上将文件传输到Kubernetes集群中的Pod。我们使用lrzsz工具来实现文件的上传和下载操作。</p><h3 id="3-1-文件传输前的准备工作"><a href="#3-1-文件传输前的准备工作" class="headerlink" title="3.1 文件传输前的准备工作"></a><strong><strong>3.1 文件传输前的准备工作</strong></strong></h3><ul><li>本地Mac电脑已经安装了Homebrew。</li><li>Kubernetes集群已经安装了lrzsz工具。</li></ul><p><strong>[在本地执行]</strong></p><ol><li><p>打开终端应用程序。</p></li><li><p>安装lrzsz工具。在终端中执行以下命令：</p> <figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install lrzsz</span><br></pre></td></tr></tbody></table></figure></li><li><p>确保Kubernetes集群中已经安装了lrzsz工具。在Kubernetes集群中的终端中执行以下命令：</p> <figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">apt-get update</span><br><span class="line">apt-get install lrzsz</span><br></pre></td></tr></tbody></table></figure></li></ol><p>[<strong>在Kubernetes集群中执行]</strong></p><ol><li><p>登录到Pod的终端。在Kubernetes集群中的终端中执行以下命令：</p> <figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl <span class="built_in">exec</span> -it &lt;pod-name&gt; -- /bin/bash</span><br></pre></td></tr></tbody></table></figure><p> 将 <code>&lt;pod-name&gt;</code> 替换为目标Pod的名称。</p></li><li><p>在Pod的终端中，使用以下命令来接收文件：</p> <figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rz</span><br></pre></td></tr></tbody></table></figure><p> 执行该命令后，将弹出一个文件选择窗口。</p></li></ol><h3 id="3-2-文件传输的具体步骤"><a href="#3-2-文件传输的具体步骤" class="headerlink" title="3.2 文件传输的具体步骤"></a><strong><strong>3.2 文件传输的具体步骤</strong></strong></h3><p><strong>[在本地执行]</strong></p><ol><li><p>在本地终端中，使用以下命令将文件发送到Kubernetes集群的Pod：<br>将 <code>/path/to/environ.yaml</code> 替换为 <code>environ.yaml</code> 文件在本地计算机上的路径。</p> <figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sz /path/to/environ.yaml</span><br></pre></td></tr></tbody></table></figure></li></ol><p><strong>[在Kubernetes集群中执行]</strong></p><ol><li>在Kubernetes集群中的终端的文件选择窗口中，选择要上传的文件 <code>environ.yaml</code>。</li></ol><p>文件传输将在本地和Kubernetes集群之间进行，使用了lrzsz工具的上传和下载命令。请确保在本地和Kubernetes集群中都按照相应的步骤进行安装和操作，并使用正确的命令进行文件传输。</p><p>请在进行文件传输操作时，遵循适当的安全性和最佳实践，以保护数据和系统的安全性。</p><h2 id="IV-如何检查在终端断开连接后-Linux-命令是否继续执行"><a href="#IV-如何检查在终端断开连接后-Linux-命令是否继续执行" class="headerlink" title="IV. 如何检查在终端断开连接后 Linux 命令是否继续执行"></a><strong>IV. 如何检查在终端断开连接后 Linux 命令是否继续执行</strong></h2><p>在 Linux 终端中运行命令或脚本时，如果终端连接断开，您可能会想知道命令或脚本是否仍在后台执行。以下是几种方法来检查 Linux 命令在终端断开连接后是否继续执行。</p><h3 id="4-1-方法一：使用-ps-命令"><a href="#4-1-方法一：使用-ps-命令" class="headerlink" title="4.1 方法一：使用 ps 命令"></a>4.1 <strong>方法一：使用 ps 命令</strong></h3><ol><li>打开新的终端窗口。</li><li>运行以下命令：<code>ps aux | grep &lt;命令或脚本关键词&gt;</code></li><li>检查输出结果中是否存在与命令或脚本相关的进程。如果存在，表示命令或脚本仍在后台执行。</li></ol><h3 id="4-2-方法二：使用-pgrep-命令"><a href="#4-2-方法二：使用-pgrep-命令" class="headerlink" title="4.2 方法二：使用 pgrep 命令"></a>4.2 <strong>方法二：使用 pgrep 命令</strong></h3><ol><li>打开新的终端窗口。</li><li>运行以下命令：<code>pgrep -f &lt;命令或脚本关键词&gt;</code></li><li>检查输出结果中是否存在与命令或脚本相关的进程 ID。如果存在，表示命令或脚本仍在后台执行。</li></ol><h3 id="4-3-方法三：使用日志文件或输出文件"><a href="#4-3-方法三：使用日志文件或输出文件" class="headerlink" title="4.3 方法三：使用日志文件或输出文件"></a>4.3 <strong>方法三：使用日志文件或输出文件</strong></h3><ol><li>如果在命令或脚本中使用了输出重定向（如 <code>tee</code>），请检查日志文件或输出文件。</li><li>打开新的终端窗口。</li><li>使用 <code>tail</code> 命令查看日志文件或输出文件的最后几行：<code>tail -n &lt;行数&gt; &lt;文件路径&gt;</code></li><li>检查最后几行是否包含与命令或脚本的输出相关的内容。如果有新的输出，表示命令或脚本仍在执行。</li></ol><p>需要注意的是，即使命令或脚本在终端断开连接后仍在后台执行，如果发生错误或问题，它们可能会终止或停止运行。因此，还应检查命令或脚本本身是否存在问题。</p><p>总结：<br>通过使用 ps 命令、pgrep 命令或查看日志文件或输出文件，您可以检查在终端断开连接后 Linux 命令是否继续执行。这些方法提供了一种了解命令或脚本是否在后台持续执行的方式，以确保任务能够正常进行。</p><h2 id="V-使用-Bash-脚本执行-Python-脚本"><a href="#V-使用-Bash-脚本执行-Python-脚本" class="headerlink" title="V. 使用 Bash 脚本执行 Python 脚本"></a><strong>V.</strong> 使用 Bash 脚本执行 Python 脚本</h2><p>本文档介绍了如何使用 Bash 脚本来执行指定的 Python 脚本，并提供了一个示例脚本。该脚本还涉及使用 conda 环境来运行 Python。</p><h3 id="5-1-简介"><a href="#5-1-简介" class="headerlink" title="5.1 简介"></a>5.1 简介</h3><p>在某些情况下，您可能需要在终端中执行长时间运行的 Python 脚本。为了确保持久性并方便管理，可以编写一个 Bash 脚本来运行 Python 脚本。本文档提供了一个示例脚本，演示如何使用 Bash 脚本来执行 Python 脚本。</p><h3 id="5-2-脚本示例"><a href="#5-2-脚本示例" class="headerlink" title="5. 2 脚本示例"></a>5. 2 脚本示例</h3><p>以下是一个示例 Bash 脚本，用于执行特定的 Python 脚本，并使用 conda 环境：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义 conda 环境名称</span></span><br><span class="line">conda_env=<span class="string">"python3.10"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 激活 conda 环境</span></span><br><span class="line"><span class="built_in">source</span> activate <span class="variable">$conda_env</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行 Python 脚本，并使用 tee 将输出同时重定向到文件和控制台</span></span><br><span class="line">python data_gen_updated.py conversations_0607_v1_500 500 | <span class="built_in">tee</span> output.log</span><br><span class="line"></span><br><span class="line"><span class="comment"># 停用 conda 环境</span></span><br><span class="line">conda deactivate</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><p>上述脚本包含以下步骤：</p><ol><li>定义 conda 环境的名称（根据需要进行修改）。</li><li>使用 <code>source activate</code> 命令激活指定的 conda 环境。</li><li>使用 <code>python</code> 命令执行特定的 Python 脚本。同时，使用 <code>tee</code> 命令将输出同时重定向到文件和控制台。</li><li>使用 <code>conda deactivate</code> 命令停用 conda 环境。</li></ol><h3 id="5-3-使用脚本"><a href="#5-3-使用脚本" class="headerlink" title="5.3 使用脚本"></a>5.3 使用脚本</h3><p>按照以下步骤在终端中使用脚本：</p><ol><li><p>使用文本编辑器创建一个新文件，并将上述示例脚本粘贴进去。</p></li><li><p>保存文件并关闭文本编辑器。</p></li><li><p>在终端中，赋予脚本执行权限：</p> <figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">chmod</span> +x script.sh</span><br></pre></td></tr></tbody></table></figure></li><li><p>运行脚本：</p> <figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./script.sh</span><br></pre></td></tr></tbody></table></figure></li></ol><p>确保在运行脚本之前，已正确安装并配置所需的 conda 环境，并将脚本中的 <code>python data_gen_updated.py conversations_0607_v1_500 500</code> 替换为您要执行的实际命令。</p><h3 id="5-4-结论"><a href="#5-4-结论" class="headerlink" title="5.4 结论"></a>5.4 结论</h3><p>使用 Bash 脚本可以在终端中执行 Python 脚本，并提供持久性和管理灵活性。本文档提供了一个示例脚本，帮助您开始使用 Bash 脚本来执行 Python 脚本，并演示了使用 conda 环境的方法。根据您的实际需求，可以修改和调整脚</p>]]></content>
      
      
      <categories>
          
          <category> Tech Toolbox </category>
          
      </categories>
      
      
        <tags>
            
            <tag> K8s </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GitLog</title>
      <link href="/2023/05/23/Tech%20Toolbox/GitLog/"/>
      <url>/2023/05/23/Tech%20Toolbox/GitLog/</url>
      
        <content type="html"><![CDATA[<h1 id="Git-Log"><a href="#Git-Log" class="headerlink" title="Git Log"></a>Git Log</h1><p>当使用Git进行版本控制时，**<code>git log</code>**命令是一个有用的工具，它可以显示提交历史记录和分支之间的关系。</p><h3 id="基本用法"><a href="#基本用法" class="headerlink" title="基本用法"></a>基本用法</h3><p>在终端或命令行中使用以下命令格式来调用<code>git log</code>：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">log</span></span><br></pre></td></tr></tbody></table></figure><p>这将显示包含所有提交历史记录的列表，最新的提交显示在最上面。</p><h3 id="限制输出"><a href="#限制输出" class="headerlink" title="限制输出"></a>限制输出</h3><p><code>git log</code>提供了一些选项来限制输出，以满足不同的需求。</p><ul><li><code>-oneline</code>：以紧凑的一行摘要形式显示提交历史记录。</li></ul><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">log</span> --oneline</span><br></pre></td></tr></tbody></table></figure><ul><li><code>-decorate</code>：在输出中显示分支和标签的引用名称。</li></ul><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git log --decorate</span><br></pre></td></tr></tbody></table></figure><ul><li><code>-graph</code>：使用图形表示法展示分支和合并的关系。</li></ul><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">log</span> --graph</span><br></pre></td></tr></tbody></table></figure><p>可以将这些选项组合在一起使用：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">log</span> --oneline --decorate --graph</span><br></pre></td></tr></tbody></table></figure><p>当您行 <strong><code>git log --oneline --decorate --graph</code></strong> 命令时，输出结果会以一种紧凑、图形化的方式显示提交历史记录和分支之间的关系。下面是对输出结果中每个部分的解释：</p><ol><li>Commit Hash（提交哈希值）：每个提交的唯一标识符，通常使用短的哈希值。这些哈希值是提交的独特标识，可以用来引用和检索特定的提交。</li><li>Commit Message（提交消息）：提交时输入的描述性消息，用于说明该提交所做的更改和目的。</li><li>Branches and Tags（分支和标签）：显示当前提交所在的分支和相关标签的引用名称。这些引用名称显示在提交哈希值后的括号内，以及在分支和标签之前的装饰符 **<code>decorate</code>**。</li><li>Graphical Representation（图形表示）：使用字符（如斜线、反斜线、竖线和星号）表示分支和合并的关系。这部分使用图形表示法展示了提交历史记录中不同分支的发展和合并情况。斜线（/）和反斜线（\）表示分支的发展，竖线（|）表示分支的分叉，星号（*）表示合并点。</li></ol><p>命令的输出结果可以通过以下方式进行阅读：</p><ol><li>每行表示一个提交，包含简短的提交哈希值和提交消息。例如：**<code>579ac2d Resolved merge conflicts with master branch</code>**。</li><li>分支和标签的引用名称显示在每个提交的后面。它们用括号括起来，并在引用名称前加上 <strong><code>tag:</code></strong> 或 <strong><code>HEAD -&gt;</code></strong> 的标识符。例如：**<code>(HEAD -&gt; huiyu/product_search_similarity_test, origin/huiyu/product_search_similarity_test)</code>** 表示当前所在的分支和远程分支。</li><li>图形表示法展示了分支和合并的关系。合并提交显示为一个或多个分支合并在一起的线条。例如，**<code>\</code>** 和 <strong><code>/</code></strong> 字符表示不同的分支合并。**<code>|</code>** 字符表示分支的分叉。这种图形表示法可以帮助您理解提交历史中不同分支之间的关系。</li></ol><p>通过阅读这些输出结果，您可以了解每个提交的信息，包括提交哈希值、提交消息、分支和标签的引用名称，以及分支和合并的关系。这有助于您跟踪代码的发展历程、分支的合并情况以及不同分支之间的关系。</p><h3 id="过滤和排序提交"><a href="#过滤和排序提交" class="headerlink" title="过滤和排序提交"></a>过滤和排序提交</h3><p>您可以使用一些选项来过滤和排序提交历史记录。</p><ul><li><code>-author=&lt;author&gt;</code>：仅显示特定作者的提交历史记录。</li></ul><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">log</span> --author=John</span><br></pre></td></tr></tbody></table></figure><ul><li><code>-since=&lt;date&gt;</code>：仅显示指定日期之后的提交历史记录。</li></ul><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">log</span> --since=<span class="string">"2023-01-01"</span></span><br></pre></td></tr></tbody></table></figure><ul><li><code>-until=&lt;date&gt;</code>：仅显示指定日期之前的提交历史记录。</li></ul><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">log</span> --<span class="keyword">until</span>=<span class="string">"2023-02-01"</span></span><br></pre></td></tr></tbody></table></figure><ul><li><code>-grep=&lt;pattern&gt;</code>：仅显示包含指定模式的提交消息。</li></ul><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">log</span> --grep=<span class="string">"bug fix"</span></span><br></pre></td></tr></tbody></table></figure><ul><li><code>-follow &lt;file&gt;</code>：跟踪指定文件的改动历史记录。</li></ul><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">log</span> --follow file.txt</span><br></pre></td></tr></tbody></table></figure><ul><li><code>-reverse</code>：按照提交时间的逆序显示提交历史记录。</li></ul><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">log</span> --reverse</span><br></pre></td></tr></tbody></table></figure><h3 id="分支和标签"><a href="#分支和标签" class="headerlink" title="分支和标签"></a>分支和标签</h3><p>默认情况下，<code>git log</code>显示所有分支的提交历史记录。您还可以指定特定的分支或标签。</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">log</span> &lt;branch-name&gt;</span><br></pre></td></tr></tbody></table></figure><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">log</span> &lt;tag-name&gt;</span><br></pre></td></tr></tbody></table></figure><h3 id="其他选项"><a href="#其他选项" class="headerlink" title="其他选项"></a>其他选项</h3><p><code>git log</code>命令还提供其他一些有用的选项，例如：</p><ul><li><code>-stat</code>：显示每个提交的简要统计信息，包括改动的文件和插入/删除的行数。</li></ul><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">log</span> --<span class="built_in">stat</span></span><br></pre></td></tr></tbody></table></figure><ul><li><code>-pretty=&lt;format&gt;</code>：使用自定义的输出格式显示提交历史记录。</li></ul><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">log</span> --pretty=format:<span class="string">"%h - %an, %ar : %s"</span></span><br></pre></td></tr></tbody></table></figure><ul><li><code>-graph</code>和<code>-oneline</code>可以与其他选项组合使用。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Tech Toolbox </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FAISS向量查询简介</title>
      <link href="/2023/02/23/Code%20Chronicles/FAISS%E5%90%91%E9%87%8F%E6%9F%A5%E8%AF%A2%E7%AE%80%E4%BB%8B/"/>
      <url>/2023/02/23/Code%20Chronicles/FAISS%E5%90%91%E9%87%8F%E6%9F%A5%E8%AF%A2%E7%AE%80%E4%BB%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="Faiss"><a href="#Faiss" class="headerlink" title="Faiss"></a>Faiss</h1><h2 id="Install"><a href="#Install" class="headerlink" title="Install"></a>Install</h2><ul><li>支持 CUDA 的 Linux ：<br><code>conda install -c pytorch faiss-gpu</code></li><li>其他：<br><code>conda install -c pytorch faiss-cpu</code></li></ul><h2 id="IndexFlatL2"><a href="#IndexFlatL2" class="headerlink" title="IndexFlatL2"></a><strong><strong>IndexFlatL2</strong></strong></h2><p><strong>IndexFlatL2</strong><br>测量查询向量与加载到索引中的向量之间所有给定点之间的 L2（或欧几里得）距离。它很简单，非常准确，但也不会太快。</p><p>给定一组维度为$d$的向量${ x_1,…, x_n }$，Faiss在Ram中构架一个数据结构——<code>index</code> ，构造完结构后，当给定一个新的维度为$d$向量$x$时，可以高效的执行以下操作：</p><p>$$<br>i = \mathrm{argmin}_i || x - x_i ||<br>$$</p><p>其中$||.||$表示欧氏（Euclidean distance）距离（L2）</p><p>用 Faiss 术语来说，数据结构是一个*<code>index</code><em>，<code>index</code> 是一个具有</em><code>add方法</code>的对象。*add可以用于添加 <code>x_i</code>向量。请注意，假定 <code>x_i</code>是固定的。</p><img src="/2023/02/23/Code%20Chronicles/FAISS%E5%90%91%E9%87%8F%E6%9F%A5%E8%AF%A2%E7%AE%80%E4%BB%8B/faiss1.png" class=""><p>在 Python 中，我们会IndexFlatL2用我们的向量维度（768——我们句子嵌入的输出大小）初始化我们的索引，如下所示：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> faiss</span><br><span class="line">d=sentence_embeddings.shape[<span class="number">1</span>]</span><br><span class="line"><span class="comment"># &gt;&gt; d=768</span></span><br><span class="line">index = faiss.IndexFlatL2(d)</span><br><span class="line">index.is_trained</span><br><span class="line"><span class="comment"># &gt;&gt; True</span></span><br></pre></td></tr></tbody></table></figure><p>通常，我们使用的索引需要我们在加载数据之前对其进行训练。</p><p>在 Faiss 中，**<code>Index</code>** 是建立在向量数据集上的索引结构，用于支持在向量数据集中进行快速相似性搜索。**<code>is_trained</code>** 是 <strong><code>Index</code></strong> 类的一个方法，用于检查索引结构是否已经被训练（即初始化）。</p><p>如果 <strong><code>index.is_trained</code></strong> 返回 True，则表示索引已经被训练并已经准备好接受查询。换句话说，这意味着索引结构已经被初始化，可以对其进行读取、添加或删除向量，并使用它执行相似性搜索操作。如果 <strong><code>index.is_trained</code></strong> 返回 False，则表示索引尚未被训练，并且需要使用向量数据集进行初始化才能进行查询操作。</p><p>准备就绪后，我们加载我们的嵌入和查询，如下所示：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">index.add(sentence_embeddings)</span><br><span class="line">index.ntotal</span><br></pre></td></tr></tbody></table></figure><p><strong><code>add()</code></strong> 是 <strong><code>Index</code></strong> 类的一个方法，用于将向量数据添加到索引中。**<code>sentence_embeddings</code>** 是一个包含向量的数组，每个向量对应一个句子的嵌入。</p><p><strong><code>index.ntotal</code></strong> 是 <strong><code>Index</code></strong> 类的另一个属性，用于返回当前索引中包含的向量数量。在使用 <strong><code>add()</code></strong> 方法将 <strong><code>sentence_embeddings</code></strong> 中的向量添加到索引中后，可以通过调用 <strong><code>index.ntotal</code></strong> 方法来获取索引中已包含的向量数量。这可以用于检查索引是否已正确地添加所有向量。</p><p>例如，如果 <strong><code>sentence_embeddings</code></strong> 中有100个句子的嵌入向量，并且这些向量已通过 <strong><code>add()</code></strong> 方法添加到索引中，则 <strong><code>index.ntotal</code></strong> 方法将返回100，表示索引中现在包含100个向量。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">k = <span class="number">4</span></span><br><span class="line">xq = model.encode([<span class="string">"Someone sprints with a football"</span>])</span><br></pre></td></tr></tbody></table></figure><p>在这段代码中，**<code>k</code>** 是一个整数变量，表示在进行相似性搜索时要返回的最近邻向量的数量。在 Faiss 中，相似性搜索可以使用 <strong><code>Index</code></strong> 类的 <strong><code>search()</code></strong> 方法来完成，该方法将查询向量作为输入，并返回与其最相似的 <strong><code>k</code></strong> 个向量。</p><p>另外，**<code>model.encode(["Someone sprints with a football"])</code>** 是用来计算输入句子的嵌入向量的方法调用。这个方法使用预先训练好的模型将输入的句子转换为一个向量表示，该向量表示包含输入句子的语义信息。</p><p>因此，将上述代码中的两个部分结合起来，可以得到一个查询向量 **<code>xq</code>**，它表示句子 “Someone sprints with a football” 的嵌入向量。然后，可以使用 <strong><code>Index</code></strong> 类的 <strong><code>search()</code></strong> 方法来查找与 <strong><code>xq</code></strong> 最相似的 <strong><code>k</code></strong> 个向量，并返回这些向量的索引列表和相似度得分列表。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line">D, I = index.search(xq, k)  <span class="comment"># search</span></span><br><span class="line"><span class="built_in">print</span>(I)</span><br></pre></td></tr></tbody></table></figure><p>在这段代码中，**<code>%%time</code>** 是 Jupyter Notebook 中的一种魔术命令，用于测量代码单元格的运行时间。**<code>D</code>** 和 <strong><code>I</code></strong> 是在使用 <strong><code>Index</code></strong> 类的 <strong><code>search()</code></strong> 方法进行相似性搜索时返回的两个结果。</p><p>具体来说，**<code>D</code>** 是一个包含相似度得分的数组，表示查询向量 <strong><code>xq</code></strong> 与检索到的 <strong><code>k</code></strong> 个最相似向量之间的相似度。**<code>I</code>** 是一个包含相应向量的索引的数组，表示与查询向量 <strong><code>xq</code></strong> 最相似的 <strong><code>k</code></strong> 个向量在索引数据集中的索引位置。</p><p>因此，将上述代码中的两个部分结合起来，可以使用 **<code>Index</code>**类的 **<code>search()</code>**方法在索引中查找与查询向量 **<code>xq</code>**最相似的 **<code>k</code>**个向量，并返回这些向量的索引列表和相似度得分列表。然后，使用 **<code>print(I)</code>**来输出检索到的最相似的向量的索引列表。由于 **<code>%time</code>**魔术命令被使用，该代码单元格还会打印出该代码单元格的执行时间。</p><h2 id="Partitioning-The-Index"><a href="#Partitioning-The-Index" class="headerlink" title="Partitioning The Index"></a><strong><strong>Partitioning The Index</strong></strong></h2><p>Faiss 允许我们添加多个步骤，这些步骤可以使用许多不同的方法优化我们的搜索。<br>一种流行的方法是将索引划分为 Voronoi 单元</p><img src="/2023/02/23/Code%20Chronicles/FAISS%E5%90%91%E9%87%8F%E6%9F%A5%E8%AF%A2%E7%AE%80%E4%BB%8B/faiss2.png" class=""><p>我们可以将我们的向量想象成每个向量都包含在一个 Voronoi 单元中——当我们引入一个新的查询向量时，我们首先测量它的质心之间的距离，然后将我们的搜索范围限制在该质心的单元内。</p><p>使用这种方法，我们将获取一个查询向量xq，识别它所属的单元格，然后使用我们的IndexFlatL2（或另一个度量）在查询向量和属于该特定单元格的所有其他向量之间进行搜索。</p><p>因此，我们正在缩小搜索范围，生成一个近似答案，而不是精确答案（通过详尽搜索得出）。</p><p>为了实现这一点，我们首先初始化我们的索引IndexFlatL2——但这次，我们使用 L2 索引作为量化器步骤——我们将其输入分区索引IndexIVFFlat。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">nlist = <span class="number">50</span>  <span class="comment"># how many cells</span></span><br><span class="line">quantizer = faiss.IndexFlatL2(d)</span><br><span class="line">index = faiss.IndexIVFFlat(quantizer, d, nlist)</span><br></pre></td></tr></tbody></table></figure><p>在这段代码中，<code>nlist</code> 是一个整数变量，用于指定 IVF（inverted file）索引中存储的聚类中心数，即将数据集划分为多少个子集。<code>quantizer</code> 是一个 Faiss 索引对象，用于将向量分配到 IVF 索引的子集中。在这里，我们使用了 Faiss 提供的 <code>IndexFlatL2</code> 类型作为 <code>quantizer</code>，它使用欧几里得距离度量来计算向量之间的相似度，并将向量存储在一个平面的索引结构中。</p><p>另外，<code>d</code> 是一个整数变量，表示嵌入向量的维度大小。这个值是根据预训练的模型和嵌入向量的特征维度确定的。</p><p>最后，<code>index</code> 是一个 Faiss 索引对象，用于支持在向量数据集中进行快速相似性搜索。在这里，我们使用了 <code>IndexIVFFlat</code> 类型作为 <code>index</code>，它使用了一种称为倒排文件（inverted file）的数据结构来组织向量数据集，并使用 <code>quantizer</code> 来将向量分配到不同的子集中。这种索引结构可以加速相似性搜索，并且在存储大规模向量数据集时非常有效。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">index.is_trained</span><br><span class="line">&gt;&gt; <span class="literal">False</span></span><br></pre></td></tr></tbody></table></figure><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">index.train(sentence_embeddings)</span><br><span class="line">index.is_trained</span><br><span class="line">&gt;&gt; <span class="literal">True</span></span><br></pre></td></tr></tbody></table></figure><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">index.add(sentence_embeddings)</span><br><span class="line">index.ntotal</span><br><span class="line">&gt;&gt; <span class="number">14504</span></span><br></pre></td></tr></tbody></table></figure><p>现在我们的索引已经过训练，我们可以像以前一样添加数据。</p><p>让我们使用相同的索引句子嵌入和相同的查询向量再次搜索<code>xq</code></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line">D, I = index.search(xq, k)  <span class="comment"># search</span></span><br><span class="line"><span class="built_in">print</span>(I)</span><br></pre></td></tr></tbody></table></figure><h2 id="Quantization"><a href="#Quantization" class="headerlink" title="Quantization"></a><strong><strong>Quantization</strong></strong></h2><p>到目前为止，我们所有的索引都将我们的向量存储为完整的（例如<code>Flat</code>）向量。现在，在非常大的数据集中，这很快就会成为一个问题。</p><p>Faiss 具有使用乘积量化 (PQ)压缩向量的能力。我们可以将其视为一个额外的近似步骤，其结果与我们使用IVF的结果相似。在 IVF 允许我们通过缩小搜索范围进行近似的情况下，PQ 改为近似计算距离/相似性。</p><img src="/2023/02/23/Code%20Chronicles/FAISS%E5%90%91%E9%87%8F%E6%9F%A5%E8%AF%A2%E7%AE%80%E4%BB%8B/faiss3.png" class=""><ol><li>我们将原始向量拆分为几个子向量。 </li><li>对于每组子向量，我们执行聚类操作——为每个子向量集创建多个质心。 </li><li>在子向量中，我们用它最近的特定集合质心的 ID 替换每个子向量</li></ol><p>我们使用 <code>IndexIVFPQ</code> 训练索引——在添加嵌入之前我们还需要索引</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">m = <span class="number">8</span>  <span class="comment"># number of centroid IDs in final compressed vectors</span></span><br><span class="line">bits = <span class="number">8</span> <span class="comment"># number of bits in each centroid</span></span><br><span class="line"></span><br><span class="line">quantizer = faiss.IndexFlatL2(d)  <span class="comment"># we keep the same L2 distance flat index</span></span><br><span class="line">index = faiss.IndexIVFPQ(quantizer, d, nlist, m, bits)</span><br></pre></td></tr></tbody></table></figure><p>**<code>m</code>**是一个整数变量，表示对每个向量进行矢量量化后，要保留的聚类中心的数量。聚类中心是通过使用 K-means 聚类算法从向量数据集中选择的一组代表性向量，可以用来近似表示原始向量。</p><p>**<code>bits</code>**是一个整数变量，用于指定矢量量化后每个聚类中心的位数。较高的 <strong><code>bits</code></strong><br>值可以提高矢量量化的准确性，但也会增加存储和计算成本。</p><p>另外，**<code>quantizer</code>** 是一个 Faiss 索引对象，用于将向量分配到 IVF 索引的子集中。在这里，我们使用了 Faiss 提供的 <strong><code>IndexFlatL2</code></strong> 类型作为 **<code>quantizer</code>**，它使用欧几里得距离度量来计算向量之间的相似度，并将向量存储在一个平面的索引结构中。</p><p>最后，**<code>index</code>** 是一个 Faiss 索引对象，用于支持在向量数据集中进行快速相似性搜索。在这里，我们使用了 <strong><code>IndexIVFPQ</code></strong> 类型作为 **<code>index</code>**，它使用了一种称为倒排文件（inverted file）的数据结构来组织向量数据集，并使用矢量量化和乘积量化（product quantization）技术来压缩向量。这种索引结构可以加速相似性搜索，并且在存储大规模向量数据集时非常有效。</p><h2 id="Nearest-Neighbour-Indexes-for-Similarity-Search"><a href="#Nearest-Neighbour-Indexes-for-Similarity-Search" class="headerlink" title="Nearest Neighbour Indexes for Similarity Search"></a><strong><strong>Nearest Neighbour Indexes for Similarity Search</strong></strong></h2><h3 id="Flat"><a href="#Flat" class="headerlink" title="Flat"></a>Flat</h3><p>应该首先查看的索引是最简单的——平面索引。</p><p>Flat索引是“平面”的，我们不修改输入向量。由于向量没有近似值或聚类——这些索引产生最准确的结果。我们拥有完美的搜索质量，但这是以大量搜索时间为代价的。使用Flat索引，我们引入查询向量xq并将其与索引中的所有其他全尺寸向量进行比较——计算到每个向量的距离。</p><img src="/2023/02/23/Code%20Chronicles/FAISS%E5%90%91%E9%87%8F%E6%9F%A5%E8%AF%A2%E7%AE%80%E4%BB%8B/faiss4.png" class=""><p>使用Flat索引，我们将搜索查询<strong>xq</strong>与索引中的每个其他向量进行比较。</p><p>在计算完所有这些距离后，我们将返回最近的 k 个作为我们最近的匹配项。<br>k 最近邻 (kNN) 搜索。</p><img src="/2023/02/23/Code%20Chronicles/FAISS%E5%90%91%E9%87%8F%E6%9F%A5%E8%AF%A2%E7%AE%80%E4%BB%8B/faiss5.png" class=""><p>那么什么时候应该使用扁平索引呢？当搜索质量无疑是一个高优先级时——搜索速度就不那么重要了。此外对于较小的数据集，搜索速度可能是一个无关紧要的因素——尤其是在使用更强大的硬件时。</p><p>简而言之，在以下情况下使用平面索引：</p><ol><li>搜索质量是一个非常高的优先级。</li><li>搜索时间无关紧要或使用小索引（&lt;10K）时。</li></ol><p>怎样才能使我们的搜索更快呢？有两种主要方法：</p><ol><li>减少向量大小——通过降维或减少表示向量值的位数。</li><li>缩小搜索范围——我们可以根据某些属性、相似性或距离将向量聚类或组织成树结构——并将我们的搜索限制在最近的集群或过滤最相似的分支。</li></ol><p>使用这两种方法中的任何一种都意味着我们不再执行详尽的最近邻搜索，而是执行近似最近邻 (ANN) 搜索——因为我们不再搜索整个全数据集。</p><h3 id="Locality-Sensitive-Hashing"><a href="#Locality-Sensitive-Hashing" class="headerlink" title="Locality Sensitive Hashing"></a><strong>Locality Sensitive Hashing</strong></h3><p>局部敏感哈希（Locality-Sensitive Hashing，LSH）是一种用于在高维空间中快速近似搜索相似对象的技术。在很多现实世界的问题中，我们需要对高维向量（比如图像、音频、文本等）进行相似性搜索，但是传统的线性搜索方法在高维空间中效率非常低下，因为随着维度的增加，搜索的复杂度呈指数级增长。</p><p>LSH是一种通过哈希函数将相似的向量映射到同一个“桶”中的技术，因此可以大大减少需要比较的向量数量，从而提高搜索效率。具体来说，LSH将每个向量映射到多个哈希表中，每个哈希表由多个哈希函数组成。对于一个查询向量，LSH会将其映射到每个哈希表中，然后只对同一个桶中的向量进行相似性比较。</p><p>LSH可以根据不同的相似性度量来设计不同的哈希函数，例如欧几里得距离、余弦相似度等。不同的哈希函数可以在不同的空间中捕捉到向量的不同特征，从而适应不同的应用场景。</p><p>局部敏感哈希 (LSH) 的工作原理是将向量分组到桶中，方法是通过哈希函数处理每个向量，该哈希函数最大化哈希冲突，而不是像通常使用哈希函数那样最小化。</p><p>这意味着什么？假设我们有一个 Python 字典。当我们在字典中创建一个新的键值对时，我们使用散列函数对键进行散列。这个键的哈希值决定了我们存储其各自值的“桶”：</p><img src="/2023/02/23/Code%20Chronicles/FAISS%E5%90%91%E9%87%8F%E6%9F%A5%E8%AF%A2%E7%AE%80%E4%BB%8B/faiss6.png" class=""><p>Python 字典是哈希表的一个示例，它使用典型的哈希函数来<em>最小化</em>哈希冲突，即两个不同对象（键）产生相同哈希的哈希冲突。</p><p>在我们的字典中，我们希望避免这些冲突，因为这意味着我们会将多个对象映射到一个键——但对于 LSH，我们希望最大化<em>散列</em>冲突。</p><p>为什么我们要最大化碰撞？那么，对于搜索，我们使用 LSH 将相似的对象分组在一起。当我们引入一个新的查询对象（或向量）时，我们的 LSH 算法可以用来找到最接近的匹配组：</p><img src="/2023/02/23/Code%20Chronicles/FAISS%E5%90%91%E9%87%8F%E6%9F%A5%E8%AF%A2%E7%AE%80%E4%BB%8B/faiss7.png" class=""><p>我们的 LSH 散列函数试图最大化散列冲突，产生向量分组。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">nbits = d*<span class="number">4</span>  <span class="comment"># resolution of bucketed vectors</span></span><br><span class="line"><span class="comment"># initialize index and add vectors</span></span><br><span class="line">index = faiss.IndexLSH(d, nbits)</span><br><span class="line">index.add(sentence_embeddings)</span><br><span class="line"><span class="comment"># and search</span></span><br><span class="line">D, I = index.search(xq, k)</span><br></pre></td></tr></tbody></table></figure><p>这段代码使用了Faiss库中的LSH索引，其中d是向量的维度，nbits是哈希值的位数，nbits的取值通常是d的某个倍数，这里设置为d*4。</p><p>在初始化索引后，代码通过**<code>add</code><strong>方法将所有的sentence embeddings添加到LSH索引中。接下来，代码通过</strong><code>search</code>**方法在LSH索引中搜索与查询向量xq最相似的k个向量，返回的D是相似度分数，I是对应的向量索引。</p><p>值得注意的是，Faiss的LSH索引使用哈希函数将向量映射到桶（bucket）中，每个桶中包含一组相似的向量。因此，LSH索引适用于高维稀疏向量的相似性搜索，其中相似向量集中在少数的桶中，从而减少搜索的时间复杂度。但是，LSH索引的准确性可能会受到哈希冲突的影响，需要根据具体的应用需求进行调整。</p><p><code>nbits</code>是指散列向量的“分辨率”。更高的<code>nbits</code>值意味着更高的准确性，但会占用更多的内存和更慢的搜索速度。一般情况下，nbits越大，哈希计算复杂度也越高。这是因为nbits的增加会使得哈希值空间变得更大，从而增加计算哈希值所需要的运算量和存储空间。</p><h3 id="Hierarchical-Navigable-Small-World-Graphs"><a href="#Hierarchical-Navigable-Small-World-Graphs" class="headerlink" title="Hierarchical Navigable Small World Graphs"></a><strong><strong>Hierarchical Navigable Small World Graphs</strong></strong></h3><p>Hierarchical Navigable Small World（HNSW）是一种用于高维向量索引的算法，旨在提供快速和准确的相似度搜索。它是在Small World网络和Navigable Small World算法的基础上进一步发展而来的。</p><img src="/2023/02/23/Code%20Chronicles/FAISS%E5%90%91%E9%87%8F%E6%9F%A5%E8%AF%A2%E7%AE%80%E4%BB%8B/faiss8.png" class=""><p>HNSW算法将高维空间中的向量表示为节点，并构建一棵树结构来组织这些节点。树中的每个节点都表示一个向量，并保存该向量在索引中的位置以及与其他节点的相似度信息。HNSW使用近似的相似度计算方法来连接节点，这使得树的结构可以在高维空间中快速导航。</p><p>在构建HNSW索引时，首先构建一个稠密的初始图。然后，将节点逐步添加到图中，并使用近似的相似度计算方法来连接节点。这些连接在不同层次的树结构中被建立，从而形成了一组层次结构。HNSW使用这种层次结构来加速相似度搜索，从而提高了搜索效率。</p><img src="/2023/02/23/Code%20Chronicles/FAISS%E5%90%91%E9%87%8F%E6%9F%A5%E8%AF%A2%E7%AE%80%E4%BB%8B/faiss9.png" class=""><p>相比于传统的树型结构和线性扫描方法，HNSW具有更高的搜索效率和更好的可扩展性。它在大规模高维向量的相似度搜索任务中表现出色，并被广泛应用于图像、文本、语音等领域的数据挖掘和机器学习任务中。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># set HNSW index parameters</span></span><br><span class="line">M = <span class="number">64</span>  <span class="comment"># number of connections each vertex will have</span></span><br><span class="line">ef_search = <span class="number">32</span>  <span class="comment"># depth of layers explored during search</span></span><br><span class="line">ef_construction = <span class="number">64</span>  <span class="comment"># depth of layers explored during index construction</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># initialize index (d == 128)</span></span><br><span class="line">index = faiss.IndexHNSWFlat(d, M)</span><br><span class="line"><span class="comment"># set efConstruction and efSearch parameters</span></span><br><span class="line">index.hnsw.efConstruction = ef_construction</span><br><span class="line">index.hnsw.efSearch = ef_search</span><br><span class="line"><span class="comment"># add data to index</span></span><br><span class="line">index.add(wb)</span><br><span class="line"></span><br><span class="line"><span class="comment"># search as usual</span></span><br><span class="line">D, I = index.search(wb, k)</span><br></pre></td></tr></tbody></table></figure><p>这段代码使用了HNSW算法来构建高维向量的索引，并进行相似度搜索。</p><p>其中</p><ul><li><code>M</code>是每个节点连接的近邻数目，即每个节点在构建索引时最多连接M个最近邻节点。</li><li><code>ef_search</code>是在搜索时遍历的层数，即搜索的深度，</li><li><code>ef_construction</code>是在构建索引时使用的遍历层数。</li></ul><p>这些参数可以调整来平衡搜索时间和索引构建时间之间的权衡。</p><p><code>M</code>和<code>efSearch</code>对搜索时间有更大的影响；<code>efConstruction</code>主要是增加了索引构建时间（意味着更慢index.add）</p><p>接下来，使用<code>faiss.IndexHNSWFlat</code>初始化HNSW索引。然后，将<code>efConstruction</code>和<code>efSearch</code>参数设置为预定义的值。最后，使用<code>index.add</code>方法将向量数据添加到索引中。</p><p>最后一行代码使用<code>index.search</code>方法进行搜索。它会返回查询向量<code>wb</code>在索引中的k个最近邻向量的距离和索引位置。</p>]]></content>
      
      
      <categories>
          
          <category> Code Chronicles </category>
          
      </categories>
      
      
        <tags>
            
            <tag> FAISS </tag>
            
            <tag> Embedding </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Intersection &amp; Union</title>
      <link href="/2023/02/20/Code%20Chronicles/Python-IntersectionUnion/"/>
      <url>/2023/02/20/Code%20Chronicles/Python-IntersectionUnion/</url>
      
        <content type="html"><![CDATA[<h1 id="Intersection-amp-Union"><a href="#Intersection-amp-Union" class="headerlink" title="Intersection &amp; Union"></a>Intersection &amp; Union</h1><p>交集和并集</p><h1 id="Intersection"><a href="#Intersection" class="headerlink" title="Intersection"></a>Intersection</h1><p>Python中的<code>intersection()</code>方法是用于获取两个集合的交集。这个方法是set对象的方法，因此它只能用于set对象中。</p><p>如果您想在其他类型的容器中获取交集，可以使用Python的内置函数set()将它们转换为set对象。例如，您可以使用以下代码来获取两个列表的交集：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</span><br><span class="line">b = [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]</span><br><span class="line">intersection = <span class="built_in">set</span>(a).intersection(<span class="built_in">set</span>(b))</span><br></pre></td></tr></tbody></table></figure><p>这里，我们首先使用set()将两个列表a和b转换为set对象，然后使用intersection()方法获取它们的交集。注意，由于集合是无序的，因此结果集合中的元素顺序可能与原始列表中的顺序不同。</p><p>除了set对象之外，Python的字典类型也具有类似的intersection()方法，用于获取两个字典的相同键的交集，但这与上述情况不同。</p><h1 id="Union"><a href="#Union" class="headerlink" title="Union"></a>Union</h1><p>如果想获取两个容器的并集，可以使用Python的内置函数<code>set()</code>将它们转换为set对象，然后使用<code>union()</code>方法获取它们的并集。例如，以下代码可以获取两个列表的并集：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</span><br><span class="line">b = [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]</span><br><span class="line">union = <span class="built_in">set</span>(a).union(<span class="built_in">set</span>(b))</span><br></pre></td></tr></tbody></table></figure><p>这里，我们首先使用<code>set()</code>将两个列表<code>a</code>和<code>b</code>转换为<code>set</code>对象，然后使用<code>union()</code>方法获取它们的并集。注意，由于集合是无序的，因此结果集合中的元素顺序可能与原始列表中的顺序不同。</p><p>除了set对象之外，Python的字典类型不支持并集操作。如果您需要对字典进行并集操作，可以将其键或值转换为set对象，然后进行集合操作。例如，以下代码可以获取两个字典的键的并集：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = {<span class="string">'x'</span>: <span class="number">1</span>, <span class="string">'y'</span>: <span class="number">2</span>, <span class="string">'z'</span>: <span class="number">3</span>}</span><br><span class="line">b = {<span class="string">'z'</span>: <span class="number">4</span>, <span class="string">'w'</span>: <span class="number">5</span>}</span><br><span class="line">union = <span class="built_in">set</span>(a.keys()).union(<span class="built_in">set</span>(b.keys()))</span><br></pre></td></tr></tbody></table></figure><p>这里，我们将两个字典的键转换为set对象，然后使用union()方法获取它们的并集。</p>]]></content>
      
      
      <categories>
          
          <category> Code Chronicles </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python Basic </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python -m 参数解释</title>
      <link href="/2023/02/01/Code%20Chronicles/Python%20-m%20%E5%8F%82%E6%95%B0%E8%A7%A3%E9%87%8A/"/>
      <url>/2023/02/01/Code%20Chronicles/Python%20-m%20%E5%8F%82%E6%95%B0%E8%A7%A3%E9%87%8A/</url>
      
        <content type="html"><![CDATA[<h1 id="Python-m-参数解释"><a href="#Python-m-参数解释" class="headerlink" title="Python -m 参数解释"></a>Python -m 参数解释</h1><h2 id="1-Python-m-参数解释"><a href="#1-Python-m-参数解释" class="headerlink" title="1. Python -m 参数解释"></a>1. Python -m 参数解释</h2><h3 id="在shell调用时，python-m的参数是什么意思？"><a href="#在shell调用时，python-m的参数是什么意思？" class="headerlink" title="在shell调用时，python -m的参数是什么意思？"></a>在shell调用时，python -m的参数是什么意思？</h3><p>当在命令行中输入 “python -m <module-name>“ 时，这表示使用Python的内置模块运行程序，其中<module-name>是要运行的模块的名称。这类似于在Python脚本中使用 “import <module-name>“ 并运行 “module-name.run()”。</module-name></module-name></module-name></p><p>例如： “python -m http.server” 将启动Python内置的HTTP服务器，可以在浏览器中访问当前目录中的文件。 “python -m unittest discover” 将在当前目录中查找并运行所有以test_*.py命名的单元测试脚本。</p><h3 id="module-name-run-需要自己在模块中编写run函数吗？"><a href="#module-name-run-需要自己在模块中编写run函数吗？" class="headerlink" title="module-name.run() 需要自己在模块中编写run函数吗？"></a>module-name.run() 需要自己在模块中编写run函数吗？</h3><p>不需要。在调用 “python -m <module-name>“ 时，Python会自动在模块中寻找并运行名为 “run()” 的函数。如果该函数不存在，则会在模块中寻找并运行名为 “main()” 的函数。</module-name></p><p>在大多数情况下，如果模块是一个可执行程序，则应该在其中编写一个名为 “main()” 的函数，并在该函数中编写程序逻辑。</p><p>如果模块是一个库，而不是可执行程序，则不需要编写 “main()” 或 “run()” 函数。这些函数不会被调用，因为模块被导入时，其中的函数和变量都可以在其他代码中调用。</p>]]></content>
      
      
      <categories>
          
          <category> Code Chronicles </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python Basic </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python yield</title>
      <link href="/2023/02/01/Code%20Chronicles/Python-yield/"/>
      <url>/2023/02/01/Code%20Chronicles/Python-yield/</url>
      
        <content type="html"><![CDATA[<h1 id="Python-yield"><a href="#Python-yield" class="headerlink" title="Python yield"></a>Python yield</h1><p>在 Python 中，**<code>yield</code>** 是一个关键字，它通常用于生成器函数中，用于生成序列化的值而不需要将整个序列保存在内存中。</p><p>当函数被调用并包含 <strong><code>yield</code></strong> 语句时，它并不会立即执行函数体的所有代码。相反，它将返回一个生成器对象，每次调用生成器对象的 <strong><code>__next__()</code></strong> 方法时，函数体将从上次 <strong><code>yield</code></strong> 语句停止的位置继续执行，直到遇到下一个 <strong><code>yield</code></strong> 语句或函数结束。</p><p>举个例子，下面的代码展示了一个简单的生成器函数，它使用 <strong><code>yield</code></strong> 语句产生数字序列：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">number_generator</span>(<span class="params">n</span>):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        <span class="keyword">yield</span> i</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用生成器对象打印数字序列</span></span><br><span class="line">my_generator = number_generator(<span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">next</span>(my_generator))  <span class="comment"># 0</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">next</span>(my_generator))  <span class="comment"># 1</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">next</span>(my_generator))  <span class="comment"># 2</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">next</span>(my_generator))  <span class="comment"># 3</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">next</span>(my_generator))  <span class="comment"># 4</span></span><br></pre></td></tr></tbody></table></figure><p>这里，**<code>number_generator()</code>** 函数使用 <strong><code>yield</code></strong> 语句产生数字序列。当函数被调用时，它将返回一个生成器对象 **<code>my_generator</code>**。每次调用生成器对象的 <strong><code>__next__()</code></strong> 方法时，函数体将从上次 <strong><code>yield</code></strong> 语句停止的位置继续执行，直到函数结束或者再次遇到 <strong><code>yield</code></strong> 语句。由于生成器只在需要时才产生值，因此可以减少内存的占用。</p>]]></content>
      
      
      <categories>
          
          <category> Code Chronicles </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python Basic </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo博客的创建与部署</title>
      <link href="/2023/01/11/Tech%20Toolbox/Hexo%20%E5%8D%9A%E5%AE%A2%E7%9A%84%E5%88%9B%E5%BB%BA%E4%B8%8E%E9%83%A8%E7%BD%B2/"/>
      <url>/2023/01/11/Tech%20Toolbox/Hexo%20%E5%8D%9A%E5%AE%A2%E7%9A%84%E5%88%9B%E5%BB%BA%E4%B8%8E%E9%83%A8%E7%BD%B2/</url>
      
        <content type="html"><![CDATA[<p>（生活反思）<br>（代码编年史）<br>（流浪癖笔记）<br>NLP Insights（自然语言处理洞察）<br>Tech Toolbox（技术工具箱）<br>Travel Tales（旅行故事）<br>Debugging Diaries</p><blockquote><p>前言：大家好，我是博主黑头呆鱼。之前我的旧电脑退休了，这导致我之前博客的内容找不到了。所以，我决定在新博客的第一篇文章中分享如何创建博客并上传源代码到 GitHub。现在，让我们开始吧！</p></blockquote><h1 id="Hexo-博客的创建与部署"><a href="#Hexo-博客的创建与部署" class="headerlink" title="Hexo 博客的创建与部署"></a>Hexo 博客的创建与部署</h1><p>以下是创建新的 Hexo 博客并部署到 GitHub 的详细步骤：</p><h2 id="安装前置软件"><a href="#安装前置软件" class="headerlink" title="安装前置软件"></a>安装前置软件</h2><h3 id="安装-Node-js-和-npm"><a href="#安装-Node-js-和-npm" class="headerlink" title="安装 Node.js 和 npm"></a>安装 Node.js 和 npm</h3><p>Hexo 是基于 Node.js 构建的，所以首先你需要安装 Node.js 和 npm（Node 包管理器）。访问 <a href="https://nodejs.org/">Node.js 的官方网站</a> 进行下载安装。</p><h3 id="安装-Hexo"><a href="#安装-Hexo" class="headerlink" title="安装 Hexo"></a>安装 Hexo</h3><p>在 Node.js 和 npm 安装完成后，通过 npm 全局安装 Hexo。在命令行中运行以下命令：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install -g hexo-cli</span><br></pre></td></tr></tbody></table></figure><h2 id="创建新的-Hexo-博客"><a href="#创建新的-Hexo-博客" class="headerlink" title="创建新的 Hexo 博客"></a>创建新的 Hexo 博客</h2><h3 id="初始化新的-Hexo-博客"><a href="#初始化新的-Hexo-博客" class="headerlink" title="初始化新的 Hexo 博客"></a>初始化新的 Hexo 博客</h3><p>创建一个新的文件夹作为你的博客的根目录，然后在命令行中运行以下命令：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hexo init blog</span><br><span class="line"><span class="built_in">cd</span> blog</span><br></pre></td></tr></tbody></table></figure><p>这将在 “blog” 文件夹下创建一个新的 Hexo 博客。</p><h3 id="安装博客依赖"><a href="#安装博客依赖" class="headerlink" title="安装博客依赖"></a>安装博客依赖</h3><p>进入你的博客目录，然后运行以下命令来安装博客所需的依赖：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install</span><br></pre></td></tr></tbody></table></figure><h2 id="配置你的博客"><a href="#配置你的博客" class="headerlink" title="配置你的博客"></a>配置你的博客</h2><h3 id="配置-Hexo"><a href="#配置-Hexo" class="headerlink" title="配置 Hexo"></a>配置 Hexo</h3><p>使用你的文本编辑器打开 <code>_config.yml</code> 文件，这是 Hexo 博客的配置文件。你需要将 <code>url</code> 设置为你的 GitHub Pages 的 URL（通常是 <code>https://&lt;username&gt;.github.io</code>），并且你可能还想配置其他的一些选项，比如博客的标题、描述和作者信息。</p><h2 id="部署到-GitHub"><a href="#部署到-GitHub" class="headerlink" title="部署到 GitHub"></a>部署到 GitHub</h2><h3 id="安装-Hexo-部署插件"><a href="#安装-Hexo-部署插件" class="headerlink" title="安装 Hexo 部署插件"></a>安装 Hexo 部署插件</h3><p>首先，你需要安装 <code>hexo-deployer-git</code> 插件，这个插件可以让你直接将你的博客部署到 GitHub。在命令行中运行以下命令来安装：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-deployer-git --save</span><br></pre></td></tr></tbody></table></figure><h3 id="配置部署参数"><a href="#配置部署参数" class="headerlink" title="配置部署参数"></a>配置部署参数</h3><p>在 <code>_config.yml</code> 文件中添加以下配置：</p><figure class="highlight yml"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">deploy:</span></span><br><span class="line">  <span class="attr">type:</span> <span class="string">git</span></span><br><span class="line">  <span class="attr">repository:</span> <span class="string">git@github.com:&lt;username&gt;/&lt;username&gt;.github.io.git</span></span><br><span class="line">  <span class="attr">branch:</span> <span class="string">master</span></span><br></pre></td></tr></tbody></table></figure><p>将 <code>&lt;username&gt;</code> 替换为你的 GitHub 用户名。</p><h3 id="生成静态文件并部署"><a href="#生成静态文件并部署" class="headerlink" title="生成静态文件并部署"></a>生成静态文件并部署</h3><p>在命令行中运行以下命令来生成静态文件并将它们部署到 GitHub：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hexo generate</span><br><span class="line">hexo deploy</span><br></pre></td></tr></tbody></table></figure><p>或者你可以使用下面的单个命令来完成这两个步骤：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo g -d</span><br></pre></td></tr></tbody></table></figure><h2 id="添加新文章"><a href="#添加新文章" class="headerlink" title="添加新文章"></a>添加新文章</h2><p>你可以使用 Hexo 的 <code>new</code> 命令来快速创建新的文章。在命令行中运行以下命令：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new <span class="string">"文章标题"</span></span><br></pre></td></tr></tbody></table></figure><p>将 “文章标题” 替换为你想要的文章标题。这将在 <code>source/_posts</code> 目录下创建一个新的 Markdown 文件，文件名就是你指定的文章标题（把空格替换为 <code>-</code>）。</p><p>你可以使用任何你喜欢的文本编辑器打开这个文件，并在里面写下你的文章内容。Hexo 使用 Markdown 语法，你可以查看 <a href="https://markdown-zh.readthedocs.io/en/latest/">Markdown 语法手册</a> 来学习如何使用 Markdown。</p><p>完成后，你可以重新生成并部署你的博客，新的文章就会出现在你的博客上了。</p><h2 id="文章分类"><a href="#文章分类" class="headerlink" title="文章分类"></a>文章分类</h2><p>你可以在你的文章中使用 YAML 前置课（Front-matter）来为文章分配分类（categories）和标签（tags）。前置课应该放在每篇文章的顶部，举例如下：</p><figure class="highlight markdown"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: 文章标题</span><br><span class="line">date: 2023-07-06 00:00:00</span><br><span class="line">categories:</span><br><span class="line"><span class="bullet">-</span> 分类1</span><br><span class="line"><span class="bullet">-</span> 分类2</span><br><span class="line">tags:</span><br><span class="line"><span class="bullet">-</span> 标签1</span><br><span class="line"><span class="section">- 标签2</span></span><br><span class="line"><span class="section">---</span></span><br><span class="line"></span><br><span class="line">这里是文章的内容。</span><br></pre></td></tr></tbody></table></figure><p>在这个例子中，这篇文章被分配到了 “分类1” 和 “分类2” 这两个分类，同时也被分配了 “标签1” 和 “标签2” 这两个标签。</p><p>当你生成你的博客时，Hexo 会自动根据这些分类和标签创建索引，访问者可以通过分类和标签来查找文章。</p><h2 id="将博客源文件保存到-GitHub"><a href="#将博客源文件保存到-GitHub" class="headerlink" title="将博客源文件保存到 GitHub"></a>将博客源文件保存到 GitHub</h2><h3 id="创建一个新的-GitHub-仓库"><a href="#创建一个新的-GitHub-仓库" class="headerlink" title="创建一个新的 GitHub 仓库"></a>创建一个新的 GitHub 仓库</h3><p>登录到你的 GitHub 账号，然后创建一个新的仓库。你可以给这个仓库取任何你喜欢的名字，比如 <code>my-hexo-blog</code>。不需要初始化 README、.gitignore 或者许可证。</p><h3 id="初始化-Git"><a href="#初始化-Git" class="headerlink" title="初始化 Git"></a>初始化 Git</h3><p>在你的博客目录中，运行以下命令来初始化 Git：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git init</span><br></pre></td></tr></tbody></table></figure><h3 id="添加所有文件到-Git"><a href="#添加所有文件到-Git" class="headerlink" title="添加所有文件到 Git"></a>添加所有文件到 Git</h3><p>运行以下命令来添加所有文件到 Git：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git add .</span><br></pre></td></tr></tbody></table></figure><h3 id="提交你的更改"><a href="#提交你的更改" class="headerlink" title="提交你的更改"></a>提交你的更改</h3><p>运行以下命令来提交你的更改：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git commit -m <span class="string">"Initial commit"</span></span><br></pre></td></tr></tbody></table></figure><h3 id="添加远程仓库"><a href="#添加远程仓库" class="headerlink" title="添加远程仓库"></a>添加远程仓库</h3><p>运行以下命令来添加你刚才在 GitHub 上创建的仓库作为远程仓库：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git remote add origin git@github.com:&lt;username&gt;/my-hexo-blog.git</span><br></pre></td></tr></tbody></table></figure><p>将 <code>&lt;username&gt;</code> 替换为你的 GitHub 用户名。</p><h3 id="推送到-GitHub"><a href="#推送到-GitHub" class="headerlink" title="推送到 GitHub"></a>推送到 GitHub</h3><p>运行以下命令来将你的博客源文件推送到 GitHub：</p><pre><code class="bash">git push -u origin master</code></pre><p>完成这些步骤后，你的 Hexo 博客就已经部署到 GitHub Pages 上了。你可以访问 <code>https://&lt;username&gt;.github.io</code> 来查看你的博客。未来每次你想要添加新的文章，只需在 <code>source/_posts</code> 目录下添加新的 Markdown 文件，然后重新生成并部署你的博客就可以了。</p><p>在未来，每次你修改了博客源文件（比如添加新的文章），你都需要运行 <code>git add .</code>，<code>git commit -m "your message"</code> 和 <code>git push</code> 命令来更新你在 GitHub 上的备份。</p>]]></content>
      
      
      <categories>
          
          <category> Tech Toolbox </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Blog </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>南洋的椰风海韵</title>
      <link href="/2022/08/09/Wanderlust%20Adventures/%E5%8D%97%E6%B4%8B%E7%9A%84%E6%A4%B0%E9%A3%8E%E6%B5%B7%E9%9F%B5/"/>
      <url>/2022/08/09/Wanderlust%20Adventures/%E5%8D%97%E6%B4%8B%E7%9A%84%E6%A4%B0%E9%A3%8E%E6%B5%B7%E9%9F%B5/</url>
      
        <content type="html"><![CDATA[<h1 id="南洋的椰风海韵"><a href="#南洋的椰风海韵" class="headerlink" title="南洋的椰风海韵"></a>南洋的椰风海韵</h1><img src="/2022/08/09/Wanderlust%20Adventures/%E5%8D%97%E6%B4%8B%E7%9A%84%E6%A4%B0%E9%A3%8E%E6%B5%B7%E9%9F%B5/%E5%8D%97%E6%B4%8B%E7%9A%84%E6%A4%B0%E9%A3%8E%E6%B5%B7%E9%9F%B5.png" class="" title="南洋的椰风海韵"><p>轻柔的椰风，吹拂着南洋的海滨，<br>翠绿的棕榈，舞动在碧蓝的天际。<br>大海的波涛，如歌如泣，耳畔回荡。<br>潮起潮落间，悠远涛声如诗韵鸣响，</p><p>沿着海岸线，白色浪花轻轻拥抱沙滩，<br>细细沙粒，脚下轻轻润湿。<br>远处礁石，静静凝望海的无尽辽阔，<br>仿佛古老智者，守护秘密。</p><p>夜幕降临，星空如璀璨珠宝散落天穹，<br>海风带来盈盈月光，如银河倾泻而下，<br>椰树摇曳，带入无边诗画，<br>梦想和希望，交织成美丽图景，<br>永铭南洋椰风海韵，灵魂驿站。</p>]]></content>
      
      
      <categories>
          
          <category> Wanderlust Adventures </category>
          
      </categories>
      
      
    </entry>
    
    
  
  
</search>
