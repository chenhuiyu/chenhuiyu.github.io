<!DOCTYPE html><html lang="[&quot;zh-CN&quot;,&quot;en&quot;]" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>SeCom: 重新定义对话AI的记忆管理 | 黑头呆鱼进化之旅</title><meta name="author" content="Huiyu Chen"><meta name="copyright" content="Huiyu Chen"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="SeCom: 重新定义对话AI的记忆管理写在前面最近笔者一直在研究对话AI中的内存管理问题，特别是长期对话场景下的记忆构建与检索技术。发现了一篇令人眼前一亮的ICLR 2025论文——《SeCom: On Memory Construction and Retrieval for Personalized Conversational Agents》，由Microsoft和清华大学的研究团队联合发">
<meta property="og:type" content="article">
<meta property="og:title" content="SeCom: 重新定义对话AI的记忆管理">
<meta property="og:url" content="https://chenhuiyu.github.io/2025/06/24/NLP%20Insights/SeCom:%20%E9%87%8D%E6%96%B0%E5%AE%9A%E4%B9%89%E5%AF%B9%E8%AF%9DAI%E7%9A%84%E8%AE%B0%E5%BF%86%E7%AE%A1%E7%90%86.zh-CN/index.html">
<meta property="og:site_name" content="黑头呆鱼进化之旅">
<meta property="og:description" content="SeCom: 重新定义对话AI的记忆管理写在前面最近笔者一直在研究对话AI中的内存管理问题，特别是长期对话场景下的记忆构建与检索技术。发现了一篇令人眼前一亮的ICLR 2025论文——《SeCom: On Memory Construction and Retrieval for Personalized Conversational Agents》，由Microsoft和清华大学的研究团队联合发">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://chenhuiyu.github.io/img/butterfly-icon.png">
<meta property="article:published_time" content="2025-06-24T08:00:00.000Z">
<meta property="article:modified_time" content="2026-02-20T21:47:32.629Z">
<meta property="article:author" content="Huiyu Chen">
<meta property="article:tag" content="Conversational AI">
<meta property="article:tag" content="Memory Management">
<meta property="article:tag" content="SeCom">
<meta property="article:tag" content="RAG">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://chenhuiyu.github.io/img/butterfly-icon.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "SeCom: 重新定义对话AI的记忆管理",
  "url": "https://chenhuiyu.github.io/2025/06/24/NLP%20Insights/SeCom:%20%E9%87%8D%E6%96%B0%E5%AE%9A%E4%B9%89%E5%AF%B9%E8%AF%9DAI%E7%9A%84%E8%AE%B0%E5%BF%86%E7%AE%A1%E7%90%86.zh-CN/",
  "image": "https://chenhuiyu.github.io/img/butterfly-icon.png",
  "datePublished": "2025-06-24T08:00:00.000Z",
  "dateModified": "2026-02-20T21:47:32.629Z",
  "author": [
    {
      "@type": "Person",
      "name": "Huiyu Chen",
      "url": "https://chenhuiyu.github.io"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://chenhuiyu.github.io/2025/06/24/NLP%20Insights/SeCom:%20%E9%87%8D%E6%96%B0%E5%AE%9A%E4%B9%89%E5%AF%B9%E8%AF%9DAI%E7%9A%84%E8%AE%B0%E5%BF%86%E7%AE%A1%E7%90%86.zh-CN/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=5.5.4"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.13.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'SeCom: 重新定义对话AI的记忆管理',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="黑头呆鱼进化之旅" type="application/atom+xml">

<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head><body><div class="bg-animation" id="web_bg" style="background-image: url(/img/site-bg.jpg);"></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">黑头呆鱼进化之旅</span></a><a class="nav-page-title" href="/"><span class="site-name">SeCom: 重新定义对话AI的记忆管理</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"></div></nav><div id="post-info"><h1 class="post-title">SeCom: 重新定义对话AI的记忆管理</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-06-24T08:00:00.000Z" title="发表于 2025-06-24 16:00:00">2025-06-24</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2026-02-20T21:47:32.629Z" title="更新于 2026-02-21 05:47:32">2026-02-21</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/NLP-Insights/">NLP Insights</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="SeCom-重新定义对话AI的记忆管理"><a href="#SeCom-重新定义对话AI的记忆管理" class="headerlink" title="SeCom: 重新定义对话AI的记忆管理"></a>SeCom: 重新定义对话AI的记忆管理</h1><h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>最近笔者一直在研究对话AI中的内存管理问题，特别是长期对话场景下的记忆构建与检索技术。发现了一篇令人眼前一亮的ICLR 2025论文——<strong>《SeCom: On Memory Construction and Retrieval for Personalized Conversational Agents》</strong>，由Microsoft和清华大学的研究团队联合发表。</p>
<p>这篇论文提出的SeCom方法巧妙地解决了一个核心问题：<strong>如何在长期对话中有效管理和检索历史信息</strong>？今天想和大家分享一下这个方法的技术细节和创新点，希望能为从事相关研究的朋友们提供一些启发。</p>
<h2 id="1-为什么我们需要关注对话内存管理？"><a href="#1-为什么我们需要关注对话内存管理？" class="headerlink" title="1. 为什么我们需要关注对话内存管理？"></a>1. 为什么我们需要关注对话内存管理？</h2><h3 id="1-1-长期对话的现实挑战"><a href="#1-1-长期对话的现实挑战" class="headerlink" title="1.1 长期对话的现实挑战"></a>1.1 长期对话的现实挑战</h3><p>在与LLMs的日常交互中，相信大家都遇到过这样的困扰：当对话变得很长时，AI似乎”忘记”了之前讨论的内容，或者给出的回答与前面的上下文不够连贯。这背后反映的正是长期对话中的内存管理挑战。</p>
<p>随着大语言模型技术的成熟，基于LLM的对话代理已经深入到我们生活的方方面面。但是，当我们希望与AI进行真正的长期、个性化交互时——比如跨越数天、数周的项目讨论，现有的技术就显得力不从心了。</p>
<p>长期对话面临的主要技术挑战包括：</p>
<ul>
<li><strong>上下文长度限制</strong>：即使是支持长上下文的模型，在处理超长对话时也面临计算成本和性能下降的问题</li>
<li><strong>信息相关性</strong>：历史对话中可能包含大量与当前查询无关的信息</li>
<li><strong>语义连贯性</strong>：相关信息可能分散在多个不连续的对话轮次中</li>
<li><strong>个性化记忆</strong>：需要记住用户的偏好、习惯和历史交互模式</li>
</ul>
<h3 id="1-2-笔者对Memory管理领域的观察"><a href="#1-2-笔者对Memory管理领域的观察" class="headerlink" title="1.2 笔者对Memory管理领域的观察"></a>1.2 笔者对Memory管理领域的观察</h3><p>在深入研究这个领域的过程中，笔者发现对话内存管理其实是一个相当复杂的系统工程。它的核心目标听起来很简单：从历史对话中提取、存储和检索相关信息，以支持当前对话的生成。但实际实现起来，需要解决三个关键问题：</p>
<ol>
<li><strong>内存构建（Memory Construction）</strong>：如何将自然语言对话转换为结构化的内存单元？</li>
<li><strong>内存检索（Memory Retrieval）</strong>：面对海量历史信息，如何快速准确地找到相关内容？</li>
<li><strong>响应生成（Response Generation）</strong>：如何基于检索到的记忆生成连贯、个性化的回复？</li>
</ol>
<p>听起来是不是很像人类的记忆机制？确实如此，这也是为什么这个问题如此有趣和具有挑战性。</p>
<h4 id="1-2-2-现有方法的”三国演义”"><a href="#1-2-2-现有方法的”三国演义”" class="headerlink" title="1.2.2 现有方法的”三国演义”"></a>1.2.2 现有方法的”三国演义”</h4><p>在研究过程中，笔者发现现有的方法大致可以分为三大流派，每个都有自己的”哲学”：</p>
<p><strong>“全盘托出”派（基于完整历史）</strong>：</p>
<ul>
<li><strong>核心思想</strong>：既然不知道什么重要，那就全部给你！</li>
<li><strong>优势</strong>：信息完整，绝不遗漏</li>
<li><strong>问题</strong>：就像把整个图书馆搬给你找一本书，效率可想而知</li>
</ul>
<p><strong>“提纲挈领”派（基于摘要）</strong>：</p>
<ul>
<li><strong>核心思想</strong>：重要的信息浓缩成摘要就够了</li>
<li><strong>优势</strong>：信息压缩，计算高效</li>
<li><strong>问题</strong>：摘要过程中重要细节可能”意外失踪”</li>
</ul>
<p><strong>“精准打击”派（基于检索）</strong>：</p>
<ul>
<li><strong>代表方法</strong>：轮次级检索、会话级检索</li>
<li><strong>核心思想</strong>：需要什么就检索什么，按需取用</li>
<li><strong>优势</strong>：计算效率高，定位精确</li>
<li><strong>问题</strong>：关键在于如何确定检索的”粒度”——这正是SeCom要解决的核心问题！</li>
</ul>
<h4 id="1-2-3-检索增强生成（RAG）在对话中的应用"><a href="#1-2-3-检索增强生成（RAG）在对话中的应用" class="headerlink" title="1.2.3 检索增强生成（RAG）在对话中的应用"></a>1.2.3 检索增强生成（RAG）在对话中的应用</h4><p>检索增强生成技术在对话系统中的应用日益广泛，主要包括：</p>
<ul>
<li>**Dense Passage Retrieval (DPR)**：使用预训练的密集检索模型</li>
<li><strong>BM25</strong>：基于词频统计的稀疏检索方法</li>
<li><strong>Hybrid Retrieval</strong>：结合密集检索和稀疏检索的优势</li>
</ul>
<p>然而，现有RAG方法在对话场景中面临独特挑战：</p>
<ul>
<li><strong>分块策略（Chunking Strategy）</strong>：如何将对话分割为检索单元</li>
<li><strong>相关性判断</strong>：对话的相关性判断比文档检索更复杂</li>
<li><strong>时序依赖</strong>：对话具有强时序性，前后文关系重要</li>
</ul>
<h3 id="1-3-内存粒度问题的深层分析"><a href="#1-3-内存粒度问题的深层分析" class="headerlink" title="1.3 内存粒度问题的深层分析"></a>1.3 内存粒度问题的深层分析</h3><h4 id="1-3-1-轮次级内存的局限性"><a href="#1-3-1-轮次级内存的局限性" class="headerlink" title="1.3.1 轮次级内存的局限性"></a>1.3.1 轮次级内存的局限性</h4><p>轮次级内存将每个用户-代理交互（turn）作为独立的内存单元：</p>
<p><strong>数学表示</strong>：<br>设对话历史 $\mathcal{H} = {\mathbf{c}<em>i}</em>{i=1}^C$，其中每个会话 $\mathbf{c}<em>i = {\mathbf{t}<em>j}</em>{j=1}^{T_i}$<br>轮次级内存：$|\mathcal{M}| = \sum</em>{i=1}^C T_i$，每个 $\mathbf{m} \in \mathcal{M}$ 对应一个轮次 $\mathbf{t}$</p>
<p><strong>主要问题</strong>：</p>
<ul>
<li><strong>信息碎片化</strong>：相关信息分散在多个轮次中，单个轮次可能缺乏完整语义</li>
<li><strong>上下文缺失</strong>：轮次间的依赖关系丢失</li>
<li><strong>检索精度低</strong>：查询词汇可能不直接出现在相关轮次中</li>
</ul>
<p><strong>具体示例</strong>：<br>用户在第3轮询问”什么是机器学习”，第5轮询问”监督学习的例子”，第8轮询问”如何选择算法”。当用户在第10轮询问”之前提到的分类算法性能如何评估”时，轮次级检索可能无法找到完整的上下文。</p>
<h4 id="1-3-2-会话级内存的局限性"><a href="#1-3-2-会话级内存的局限性" class="headerlink" title="1.3.2 会话级内存的局限性"></a>1.3.2 会话级内存的局限性</h4><p>会话级内存将整个对话会话作为内存单元：</p>
<p><strong>数学表示</strong>：<br>会话级内存：$|\mathcal{M}| = C$，每个 $\mathbf{m} \in \mathcal{M}$ 对应一个会话 $\mathbf{c}$</p>
<p><strong>主要问题</strong>：</p>
<ul>
<li><strong>主题混杂</strong>：单个会话可能包含多个不相关主题</li>
<li><strong>噪声干扰</strong>：大量无关信息影响检索和生成质量</li>
<li><strong>检索粗糙</strong>：无法精确定位到具体相关内容</li>
</ul>
<p><strong>具体示例</strong>：<br>一个会话中用户讨论了机器学习、烹饪食谱、旅行计划和电影推荐。当查询机器学习相关问题时，检索到的会话包含大量无关的烹饪和旅行信息。</p>
<h4 id="1-3-3-摘要化方法的信息损失"><a href="#1-3-3-摘要化方法的信息损失" class="headerlink" title="1.3.3 摘要化方法的信息损失"></a>1.3.3 摘要化方法的信息损失</h4><p>摘要化方法通过压缩对话内容来减少信息量：</p>
<p><strong>主要问题</strong>：</p>
<ul>
<li><strong>细节丢失</strong>：摘要过程中重要细节可能被省略</li>
<li><strong>主观性</strong>：摘要质量依赖于模型的理解能力</li>
<li><strong>不可逆性</strong>：一旦信息被摘要，原始细节无法恢复</li>
</ul>
<h2 id="2-SeCom的设计"><a href="#2-SeCom的设计" class="headerlink" title="2. SeCom的设计"></a>2. SeCom的设计</h2><h3 id="2-1-核心发现"><a href="#2-1-核心发现" class="headerlink" title="2.1 核心发现"></a>2.1 核心发现</h3><p>SeCom（<strong>Se</strong>gmentation + <strong>Com</strong>pression）的两个核心发现：</p>
<p><strong>洞察一：对话天然具有”段落”结构</strong><br>就像我们写文章会分段一样，人类的对话其实也有天然的主题边界。比如在一次长对话中，我们可能先讨论工作项目，然后转到周末计划，再聊到最近看的电影。每个主题就是一个天然的”段落”。</p>
<p>传统方法要么把每句话当作独立单元（太碎片化），要么把整个对话当作一个整体（太粗糙），而SeCom找到了中间的最佳平衡点——<strong>段落级的语义单元</strong>。</p>
<p><strong>洞察二：自然语言充满”废话”</strong><br>这听起来有点刻薄，但确实如此。我们日常对话中充满了”嗯”、”那个”、”你知道的”这样的冗余表达，还有大量的重复、确认、客套话。这些在人际交流中很重要，但对机器检索来说就是噪声。</p>
<p>SeCom通过智能压缩，保留关键信息的同时去除这些”噪声”，让检索更加精准。</p>
<h3 id="2-2-系统设计"><a href="#2-2-系统设计" class="headerlink" title="2.2 系统设计"></a>2.2 系统设计</h3><p>SeCom的整体架构设计非常优雅，就像一条高效的流水线：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">历史对话 → [分段器] → 段落级内存单元 → [压缩器] → 去噪内存单元 → [检索器] → 相关上下文 → [生成器] → 最终回复</span><br></pre></td></tr></tbody></table></figure>

<p>用更直观的话来解释这个流程：</p>
<ol>
<li><strong>分段器</strong>：将杂乱的对话历史按主题”切块”</li>
<li><strong>压缩器</strong>：将每个”块”中的废话去掉，保留精华</li>
<li><strong>检索器</strong>：根据当前问题找到最相关的”块”</li>
<li><strong>生成器</strong>：基于相关信息生成回答</li>
</ol>
<p><strong>技术表示</strong>（没什么用，写给喜欢数学的朋友）：<br>设 $f_{\mathcal{I}}$ 为分段器，$f_{Comp}$ 为压缩器，$f_R$ 为检索器，$f_{LLM}$ 为生成器</p>
<p>完整流程：</p>
<ol>
<li>${\mathbf{s}<em>k}</em>{k=1}^K \leftarrow f_{\mathcal{I}}(\mathcal{H})$ （对话分段）</li>
<li>${\mathbf{m}<em>k}</em>{k=1}^K \leftarrow f_{Comp}({\mathbf{s}<em>k}</em>{k=1}^K)$ （压缩去噪）</li>
<li>${\mathbf{m}<em>n}</em>{n=1}^N \leftarrow f_R(u^*, {\mathbf{m}<em>k}</em>{k=1}^K, N)$ （内存检索）</li>
<li>$r^* = f_{LLM}(u^*, {\mathbf{m}<em>n}</em>{n=1}^N)$ （响应生成）</li>
</ol>
<h3 id="2-3-分段算法：教AI学会”断句”"><a href="#2-3-分段算法：教AI学会”断句”" class="headerlink" title="2.3 分段算法：教AI学会”断句”"></a>2.3 分段算法：教AI学会”断句”</h3><h4 id="2-3-1-零样本分段"><a href="#2-3-1-零样本分段" class="headerlink" title="2.3.1 零样本分段"></a>2.3.1 零样本分段</h4><p>如何让AI自动识别对话中的主题边界？传统方法需要大量标注数据训练专门的分段模型，而SeCom采用了一个非常聪明的”零样本”方法。</p>
<p><strong>核心思路</strong>：<br>既然GPT-4这样的大模型已经具备了强大的文本理解能力，为什么不直接让它来判断对话的主题边界呢？就像让一个文学老师来给文章分段一样。</p>
<p><strong>输入预处理</strong>：<br>将对话会话增强为结构化格式：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Turn j: </span><br><span class="line">[user]: u_j</span><br><span class="line">[agent]: r_j</span><br></pre></td></tr></tbody></table></figure>

<p><strong>分段提示设计</strong>：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">分析以下对话，识别主题边界，将对话分割为语义连贯的段落。</span><br><span class="line">每个段落应该：</span><br><span class="line">1. 围绕单一主题或相关主题</span><br><span class="line">2. 包含完整的交互序列</span><br><span class="line">3. 具有明确的开始和结束边界</span><br><span class="line"></span><br><span class="line">对话内容：</span><br><span class="line">[对话内容]</span><br><span class="line"></span><br><span class="line">请输出每个段落的起始和结束轮次编号。</span><br></pre></td></tr></tbody></table></figure>

<p><strong>优势</strong>：</p>
<ul>
<li>无需训练数据，适用于开放域对话</li>
<li>利用LLM的强大理解能力</li>
<li>可处理复杂的主题转换模式</li>
</ul>
<h4 id="2-3-2-基于反思的分段优化"><a href="#2-3-2-基于反思的分段优化" class="headerlink" title="2.3.2 基于反思的分段优化"></a>2.3.2 基于反思的分段优化</h4><p>当有少量标注数据时，采用反思机制优化分段效果：</p>
<p><strong>算法步骤</strong>：</p>
<ol>
<li><strong>初始分段</strong>：使用零样本方法对批量数据进行分段</li>
<li><strong>错误识别</strong>：基于WindowDiff指标选择top-K个分段错误最大的样本</li>
<li><strong>反思学习</strong>：让LLM分析分段错误，更新分段指导原则</li>
<li><strong>迭代优化</strong>：重复上述过程直到收敛</li>
</ol>
<p><strong>反思提示设计</strong>：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">分析以下分段错误，并更新分段指导原则：</span><br><span class="line"></span><br><span class="line">错误案例：</span><br><span class="line">[分段结果] vs [标准答案]</span><br><span class="line"></span><br><span class="line">请分析错误原因并提供改进的分段指导原则。</span><br></pre></td></tr></tbody></table></figure>

<p><strong>数学表示</strong>：<br>设 $\boldsymbol{G}<em>m$ 为第m轮的分段指导原则，更新公式为：<br>$$\boldsymbol{G}</em>{m+1} = \boldsymbol{G}_m - \eta \nabla \mathcal{L}(\boldsymbol{G}_m)$$</p>
<p>其中 $\nabla \mathcal{L}(\boldsymbol{G}_m)$ 为LLM隐式估计的分段损失梯度。</p>
<h4 id="2-3-3-增量分段算法"><a href="#2-3-3-增量分段算法" class="headerlink" title="2.3.3 增量分段算法"></a>2.3.3 增量分段算法</h4><p>对于新增的对话轮次，设计增量分段算法：</p>
<p><strong>算法流程</strong>：</p>
<ol>
<li>输入新轮次 $\mathbf{t}<em>{new}$ 和前一段落 $\mathbf{s}</em>{prev}$</li>
<li>判断是否应该合并：$binary = f_{judge}(\mathbf{t}<em>{new}, \mathbf{s}</em>{prev})$</li>
<li>如果合并：$\mathbf{s}<em>{prev} \leftarrow \mathbf{s}</em>{prev} \cup {\mathbf{t}_{new}}$</li>
<li>否则：创建新段落 $\mathbf{s}<em>{new} = {\mathbf{t}</em>{new}}$</li>
</ol>
<p><strong>判断提示</strong>：</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">判断新的用户-机器人轮次是否应该与前一段落合并：</span><br><span class="line"></span><br><span class="line">新轮次：[新轮次内容]</span><br><span class="line">前一段落：[前一段落内容]</span><br><span class="line"></span><br><span class="line">如果属于同一主题，回答"Yes"，否则回答"No"。</span><br></pre></td></tr></tbody></table></figure>

<h3 id="2-4-压缩式内存去噪"><a href="#2-4-压缩式内存去噪" class="headerlink" title="2.4 压缩式内存去噪"></a>2.4 压缩式内存去噪</h3><h4 id="2-4-1-自然语言冗余性分析"><a href="#2-4-1-自然语言冗余性分析" class="headerlink" title="2.4.1 自然语言冗余性分析"></a>2.4.1 自然语言冗余性分析</h4><p><strong>理论基础</strong>：<br>根据Shannon信息论，自然语言具有高度冗余性，冗余率约为50-75%。这种冗余在人类交流中有助于错误纠正和理解，但在机器检索中构成噪声。</p>
<p><strong>冗余类型</strong>：</p>
<ol>
<li><strong>词汇冗余</strong>：同义词、重复表达</li>
<li><strong>语法冗余</strong>：冗余的语法结构</li>
<li><strong>语义冗余</strong>：重复的语义信息</li>
<li><strong>对话冗余</strong>：客套话、确认性回复</li>
</ol>
<h4 id="2-4-2-LLMLingua-2压缩原理"><a href="#2-4-2-LLMLingua-2压缩原理" class="headerlink" title="2.4.2 LLMLingua-2压缩原理"></a>2.4.2 LLMLingua-2压缩原理</h4><p><strong>算法核心</strong>：<br>LLMLingua-2基于token重要性评分进行压缩：</p>
<ol>
<li><p><strong>重要性评分</strong>：<br>$$s_i = f_{score}(x_i | x_{&lt;i}, x_{&gt;i})$$<br>其中 $x_i$ 为第i个token，$x_{&lt;i}$ 和 $x_{&gt;i}$ 为上下文</p>
</li>
<li><p><strong>动态压缩</strong>：<br>根据目标压缩率 $r$，保留top $(1-r) \times N$ 个重要token</p>
</li>
<li><p><strong>语义保持</strong>：<br>通过双向上下文建模确保关键语义信息不丢失</p>
</li>
</ol>
<p><strong>压缩效果分析</strong>：<br>实验表明，75%压缩率下：</p>
<ul>
<li>关键信息保留率 &gt; 95%</li>
<li>检索相关性提升 9.46分（GPT4Score）</li>
<li>计算效率提升 4倍</li>
</ul>
<h4 id="2-4-3-压缩对检索性能的影响"><a href="#2-4-3-压缩对检索性能的影响" class="headerlink" title="2.4.3 压缩对检索性能的影响"></a>2.4.3 压缩对检索性能的影响</h4><p><strong>相似性变化分析</strong>：<br>设 $sim(q, s)$ 为查询q与段落s的相似性</p>
<p>压缩前：$sim_{before}(q, s_{relevant})$，$sim_{before}(q, s_{irrelevant})$<br>压缩后：$sim_{after}(q, s’<em>{relevant})$，$sim</em>{after}(q, s’_{irrelevant})$</p>
<p>实验结果显示：</p>
<ul>
<li>$sim_{after}(q, s’<em>{relevant}) &gt; sim</em>{before}(q, s_{relevant})$ （相关段落相似性提升）</li>
<li>$sim_{after}(q, s’<em>{irrelevant}) &lt; sim</em>{before}(q, s_{irrelevant})$ （无关段落相似性降低）</li>
</ul>
<h3 id="2-5-多模态检索系统"><a href="#2-5-多模态检索系统" class="headerlink" title="2.5 多模态检索系统"></a>2.5 多模态检索系统</h3><h4 id="2-5-1-检索器选择与配置"><a href="#2-5-1-检索器选择与配置" class="headerlink" title="2.5.1 检索器选择与配置"></a>2.5.1 检索器选择与配置</h4><p><strong>BM25检索器</strong>：<br>$$BM25(q, d) = \sum_{i=1}^{|q|} IDF(q_i) \cdot \frac{tf(q_i, d) \cdot (k_1 + 1)}{tf(q_i, d) + k_1 \cdot (1 - b + b \cdot \frac{|d|}{avgdl})}$$</p>
<p>参数设置：$k_1 = 1.2$，$b = 0.75$</p>
<p><strong>MPNet检索器</strong>：<br>基于MPNet模型的密集检索：<br>$$score = \cos(\mathbf{e}_q, \mathbf{e}_d)$$<br>其中 $\mathbf{e}_q$ 和 $\mathbf{e}_d$ 分别为查询和文档的向量表示</p>
<h4 id="2-5-2-混合检索策略"><a href="#2-5-2-混合检索策略" class="headerlink" title="2.5.2 混合检索策略"></a>2.5.2 混合检索策略</h4><p>结合稀疏检索和密集检索的优势：<br>$$score_{hybrid} = \alpha \cdot score_{BM25} + (1-\alpha) \cdot score_{MPNet}$$</p>
<p>通过实验确定最优权重 $\alpha = 0.6$</p>
<h2 id="3-写在最后：一些思考"><a href="#3-写在最后：一些思考" class="headerlink" title="3. 写在最后：一些思考"></a>3. 写在最后：一些思考</h2><h3 id="3-1-SeCom给我们的启发"><a href="#3-1-SeCom给我们的启发" class="headerlink" title="3.1 SeCom给我们的启发"></a>3.1 SeCom给我们的启发</h3><p>研读这篇论文后，笔者有几点深刻的感悟：</p>
<p><strong>简单往往是最有效的</strong>：SeCom的核心思想其实很简单——分段+压缩，但正是这种简单的组合解决了复杂的问题。这提醒我们，在面对技术挑战时，有时候最朴素的想法反而是最有效的。</p>
<p><strong>理解问题比解决问题更重要</strong>：作者团队深入分析了内存粒度问题的本质，发现了段落级内存的最优性。这种对问题本质的深刻理解是技术创新的基础。</p>
<p>笔者认为未来可能的发展方向包括：</p>
<ul>
<li><strong>个性化分段策略</strong>：不同用户的对话模式不同，能否学习个性化的分段方式？</li>
<li><strong>实时优化机制</strong>：能否根据对话质量动态调整压缩率和分段策略？</li>
</ul>
<hr>
<h2 id="参考资源"><a href="#参考资源" class="headerlink" title="参考资源"></a>参考资源</h2><ul>
<li><strong>论文链接</strong>：<a target="_blank" rel="noopener" href="https://www.arxiv.org/abs/2502.05589">SeCom: On Memory Construction and Retrieval for Personalized Conversational Agents (ICLR 2025)</a></li>
<li><strong>项目主页</strong>：<a target="_blank" rel="noopener" href="https://llmlingua.com/secom.html">https://llmlingua.com/secom.html</a></li>
<li><strong>代码仓库</strong>：SeCom-main项目</li>
<li><strong>数据集</strong>：LOCOMO、Long-MT-Bench+、DialSeg711、TIAGE、SuperDialSeg</li>
</ul>
<p><em>本文基于Microsoft和清华大学联合研究团队在ICLR 2025发表的论文撰写，详细技术实现请参考原始论文和开源代码。</em> </p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://chenhuiyu.github.io">Huiyu Chen</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://chenhuiyu.github.io/2025/06/24/NLP%20Insights/SeCom:%20%E9%87%8D%E6%96%B0%E5%AE%9A%E4%B9%89%E5%AF%B9%E8%AF%9DAI%E7%9A%84%E8%AE%B0%E5%BF%86%E7%AE%A1%E7%90%86.zh-CN/">https://chenhuiyu.github.io/2025/06/24/NLP%20Insights/SeCom:%20%E9%87%8D%E6%96%B0%E5%AE%9A%E4%B9%89%E5%AF%B9%E8%AF%9DAI%E7%9A%84%E8%AE%B0%E5%BF%86%E7%AE%A1%E7%90%86.zh-CN/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://chenhuiyu.github.io" target="_blank">黑头呆鱼进化之旅</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Conversational-AI/">Conversational AI</a><a class="post-meta__tags" href="/tags/Memory-Management/">Memory Management</a><a class="post-meta__tags" href="/tags/SeCom/">SeCom</a><a class="post-meta__tags" href="/tags/RAG/">RAG</a></div><div class="post-share"><div class="social-share" data-image="/img/butterfly-icon.png" data-sites="facebook,x,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2026/02/21/NLP%20Insights/Evaluation%20of%20Generation-Based%20Large%20Language%20Models%20(LLMs):%20Opportunities%20and%20Challenges%20from%20Generation%20to%20Judgment/" title=""><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2"></div></div><div class="info-2"><div class="info-item-1">Evaluation of Generation-Based Large Language Models (LLMs): Opportunities and Challenges from Generation to JudgmentAbstractEvaluation tasks in artificial intelligence (AI) and natural language processing (NLP) have long been challenging. Traditional evaluation methods, such as those based on matching or embeddings, are limited in assessing complex attributes. The recent development of large language models (LLMs) has given rise to the “LLM-as-a-Judge” paradigm, which utilizes LLMs for scori...</div></div></div></a><a class="pagination-related" href="/2025/06/24/NLP%20Insights/SeCom:%20%E9%87%8D%E6%96%B0%E5%AE%9A%E4%B9%89%E5%AF%B9%E8%AF%9DAI%E7%9A%84%E8%AE%B0%E5%BF%86%E7%AE%A1%E7%90%86/" title="SeCom: 重新定义对话AI的记忆管理"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">SeCom: 重新定义对话AI的记忆管理</div></div><div class="info-2"><div class="info-item-1">SeCom: 重新定义对话AI的记忆管理写在前面最近笔者一直在研究对话AI中的内存管理问题，特别是长期对话场景下的记忆构建与检索技术。发现了一篇令人眼前一亮的ICLR 2025论文——《SeCom: On Memory Construction and Retrieval for Personalized Conversational Agents》，由Microsoft和清华大学的研究团队联合发表。 这篇论文提出的SeCom方法巧妙地解决了一个核心问题：如何在长期对话中有效管理和检索历史信息？今天想和大家分享一下这个方法的技术细节和创新点，希望能为从事相关研究的朋友们提供一些启发。 1. 为什么我们需要关注对话内存管理？1.1 长期对话的现实挑战在与LLMs的日常交互中，相信大家都遇到过这样的困扰：当对话变得很长时，AI似乎”忘记”了之前讨论的内容，或者给出的回答与前面的上下文不够连贯。这背后反映的正是长期对话中的内存管理挑战。 随着大语言模型技术的成熟，基于LLM的对话代理已经深入到我们生活的方方面面。但是，当我们希望与AI进行真正的长期、个性化交互时——比如跨越数天、数周的...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/06/24/NLP%20Insights/SeCom:%20%E9%87%8D%E6%96%B0%E5%AE%9A%E4%B9%89%E5%AF%B9%E8%AF%9DAI%E7%9A%84%E8%AE%B0%E5%BF%86%E7%AE%A1%E7%90%86.en/" title="SeCom: 重新定义对话AI的记忆管理"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-24</div><div class="info-item-2">SeCom: 重新定义对话AI的记忆管理</div></div><div class="info-2"><div class="info-item-1">SeCom: 重新定义对话AI的记忆管理写在前面最近笔者一直在研究对话AI中的内存管理问题，特别是长期对话场景下的记忆构建与检索技术。发现了一篇令人眼前一亮的ICLR 2025论文——《SeCom: On Memory Construction and Retrieval for Personalized Conversational Agents》，由Microsoft和清华大学的研究团队联合发表。 这篇论文提出的SeCom方法巧妙地解决了一个核心问题：如何在长期对话中有效管理和检索历史信息？今天想和大家分享一下这个方法的技术细节和创新点，希望能为从事相关研究的朋友们提供一些启发。 1. 为什么我们需要关注对话内存管理？1.1 长期对话的现实挑战在与LLMs的日常交互中，相信大家都遇到过这样的困扰：当对话变得很长时，AI似乎”忘记”了之前讨论的内容，或者给出的回答与前面的上下文不够连贯。这背后反映的正是长期对话中的内存管理挑战。 随着大语言模型技术的成熟，基于LLM的对话代理已经深入到我们生活的方方面面。但是，当我们希望与AI进行真正的长期、个性化交互时——比如跨越数天、数周的...</div></div></div></a><a class="pagination-related" href="/2025/06/24/NLP%20Insights/SeCom%20Redefining%20Memory%20Management%20in%20Conversational%20AI.en/" title="SeCom: Redefining Memory Management in Conversational AI"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-24</div><div class="info-item-2">SeCom: Redefining Memory Management in Conversational AI</div></div><div class="info-2"><div class="info-item-1">SeCom: Redefining Memory Management in Conversational AIForewordI’ve recently been diving into memory management for dialog-based AI, especially how to construct and retrieve memories in long-term conversations. During my exploration I came across an eye-opening ICLR 2025 paper—**”SeCom: On Memory Construction and Retrieval for Personalized Conversational Agents”**—a collaboration between Microsoft and Tsinghua University. SeCom solves a core problem: How can an agent effectively manage and r...</div></div></div></a><a class="pagination-related" href="/2025/06/24/NLP%20Insights/SeCom%20Redefining%20Memory%20Management%20in%20Conversational%20AI/" title="SeCom: Redefining Memory Management in Conversational AI"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-24</div><div class="info-item-2">SeCom: Redefining Memory Management in Conversational AI</div></div><div class="info-2"><div class="info-item-1">SeCom: Redefining Memory Management in Conversational AIForewordI’ve recently been diving into memory management for dialog-based AI, especially how to construct and retrieve memories in long-term conversations. During my exploration I came across an eye-opening ICLR 2025 paper—**”SeCom: On Memory Construction and Retrieval for Personalized Conversational Agents”**—a collaboration between Microsoft and Tsinghua University. SeCom solves a core problem: How can an agent effectively manage and r...</div></div></div></a><a class="pagination-related" href="/2025/06/24/NLP%20Insights/SeCom:%20%E9%87%8D%E6%96%B0%E5%AE%9A%E4%B9%89%E5%AF%B9%E8%AF%9DAI%E7%9A%84%E8%AE%B0%E5%BF%86%E7%AE%A1%E7%90%86/" title="SeCom: 重新定义对话AI的记忆管理"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-24</div><div class="info-item-2">SeCom: 重新定义对话AI的记忆管理</div></div><div class="info-2"><div class="info-item-1">SeCom: 重新定义对话AI的记忆管理写在前面最近笔者一直在研究对话AI中的内存管理问题，特别是长期对话场景下的记忆构建与检索技术。发现了一篇令人眼前一亮的ICLR 2025论文——《SeCom: On Memory Construction and Retrieval for Personalized Conversational Agents》，由Microsoft和清华大学的研究团队联合发表。 这篇论文提出的SeCom方法巧妙地解决了一个核心问题：如何在长期对话中有效管理和检索历史信息？今天想和大家分享一下这个方法的技术细节和创新点，希望能为从事相关研究的朋友们提供一些启发。 1. 为什么我们需要关注对话内存管理？1.1 长期对话的现实挑战在与LLMs的日常交互中，相信大家都遇到过这样的困扰：当对话变得很长时，AI似乎”忘记”了之前讨论的内容，或者给出的回答与前面的上下文不够连贯。这背后反映的正是长期对话中的内存管理挑战。 随着大语言模型技术的成熟，基于LLM的对话代理已经深入到我们生活的方方面面。但是，当我们希望与AI进行真正的长期、个性化交互时——比如跨越数天、数周的...</div></div></div></a><a class="pagination-related" href="/2025/06/24/NLP%20Insights/SeCom%20Redefining%20Memory%20Management%20in%20Conversational%20AI.zh-CN/" title="SeCom: Redefining Memory Management in Conversational AI"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-24</div><div class="info-item-2">SeCom: Redefining Memory Management in Conversational AI</div></div><div class="info-2"><div class="info-item-1">SeCom: Redefining Memory Management in Conversational AIForewordI’ve recently been diving into memory management for dialog-based AI, especially how to construct and retrieve memories in long-term conversations. During my exploration I came across an eye-opening ICLR 2025 paper—**”SeCom: On Memory Construction and Retrieval for Personalized Conversational Agents”**—a collaboration between Microsoft and Tsinghua University. SeCom solves a core problem: How can an agent effectively manage and r...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Huiyu Chen</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">186</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">46</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#SeCom-%E9%87%8D%E6%96%B0%E5%AE%9A%E4%B9%89%E5%AF%B9%E8%AF%9DAI%E7%9A%84%E8%AE%B0%E5%BF%86%E7%AE%A1%E7%90%86"><span class="toc-number">1.</span> <span class="toc-text">SeCom: 重新定义对话AI的记忆管理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%86%99%E5%9C%A8%E5%89%8D%E9%9D%A2"><span class="toc-number">1.1.</span> <span class="toc-text">写在前面</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E4%B8%BA%E4%BB%80%E4%B9%88%E6%88%91%E4%BB%AC%E9%9C%80%E8%A6%81%E5%85%B3%E6%B3%A8%E5%AF%B9%E8%AF%9D%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%EF%BC%9F"><span class="toc-number">1.2.</span> <span class="toc-text">1. 为什么我们需要关注对话内存管理？</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-%E9%95%BF%E6%9C%9F%E5%AF%B9%E8%AF%9D%E7%9A%84%E7%8E%B0%E5%AE%9E%E6%8C%91%E6%88%98"><span class="toc-number">1.2.1.</span> <span class="toc-text">1.1 长期对话的现实挑战</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-%E7%AC%94%E8%80%85%E5%AF%B9Memory%E7%AE%A1%E7%90%86%E9%A2%86%E5%9F%9F%E7%9A%84%E8%A7%82%E5%AF%9F"><span class="toc-number">1.2.2.</span> <span class="toc-text">1.2 笔者对Memory管理领域的观察</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-2-%E7%8E%B0%E6%9C%89%E6%96%B9%E6%B3%95%E7%9A%84%E2%80%9D%E4%B8%89%E5%9B%BD%E6%BC%94%E4%B9%89%E2%80%9D"><span class="toc-number">1.2.2.1.</span> <span class="toc-text">1.2.2 现有方法的”三国演义”</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-3-%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90%EF%BC%88RAG%EF%BC%89%E5%9C%A8%E5%AF%B9%E8%AF%9D%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8"><span class="toc-number">1.2.2.2.</span> <span class="toc-text">1.2.3 检索增强生成（RAG）在对话中的应用</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-%E5%86%85%E5%AD%98%E7%B2%92%E5%BA%A6%E9%97%AE%E9%A2%98%E7%9A%84%E6%B7%B1%E5%B1%82%E5%88%86%E6%9E%90"><span class="toc-number">1.2.3.</span> <span class="toc-text">1.3 内存粒度问题的深层分析</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-1-%E8%BD%AE%E6%AC%A1%E7%BA%A7%E5%86%85%E5%AD%98%E7%9A%84%E5%B1%80%E9%99%90%E6%80%A7"><span class="toc-number">1.2.3.1.</span> <span class="toc-text">1.3.1 轮次级内存的局限性</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-2-%E4%BC%9A%E8%AF%9D%E7%BA%A7%E5%86%85%E5%AD%98%E7%9A%84%E5%B1%80%E9%99%90%E6%80%A7"><span class="toc-number">1.2.3.2.</span> <span class="toc-text">1.3.2 会话级内存的局限性</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-3-%E6%91%98%E8%A6%81%E5%8C%96%E6%96%B9%E6%B3%95%E7%9A%84%E4%BF%A1%E6%81%AF%E6%8D%9F%E5%A4%B1"><span class="toc-number">1.2.3.3.</span> <span class="toc-text">1.3.3 摘要化方法的信息损失</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-SeCom%E7%9A%84%E8%AE%BE%E8%AE%A1"><span class="toc-number">1.3.</span> <span class="toc-text">2. SeCom的设计</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-%E6%A0%B8%E5%BF%83%E5%8F%91%E7%8E%B0"><span class="toc-number">1.3.1.</span> <span class="toc-text">2.1 核心发现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E7%B3%BB%E7%BB%9F%E8%AE%BE%E8%AE%A1"><span class="toc-number">1.3.2.</span> <span class="toc-text">2.2 系统设计</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E5%88%86%E6%AE%B5%E7%AE%97%E6%B3%95%EF%BC%9A%E6%95%99AI%E5%AD%A6%E4%BC%9A%E2%80%9D%E6%96%AD%E5%8F%A5%E2%80%9D"><span class="toc-number">1.3.3.</span> <span class="toc-text">2.3 分段算法：教AI学会”断句”</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-1-%E9%9B%B6%E6%A0%B7%E6%9C%AC%E5%88%86%E6%AE%B5"><span class="toc-number">1.3.3.1.</span> <span class="toc-text">2.3.1 零样本分段</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-2-%E5%9F%BA%E4%BA%8E%E5%8F%8D%E6%80%9D%E7%9A%84%E5%88%86%E6%AE%B5%E4%BC%98%E5%8C%96"><span class="toc-number">1.3.3.2.</span> <span class="toc-text">2.3.2 基于反思的分段优化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-3-3-%E5%A2%9E%E9%87%8F%E5%88%86%E6%AE%B5%E7%AE%97%E6%B3%95"><span class="toc-number">1.3.3.3.</span> <span class="toc-text">2.3.3 增量分段算法</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-%E5%8E%8B%E7%BC%A9%E5%BC%8F%E5%86%85%E5%AD%98%E5%8E%BB%E5%99%AA"><span class="toc-number">1.3.4.</span> <span class="toc-text">2.4 压缩式内存去噪</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4-1-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%86%97%E4%BD%99%E6%80%A7%E5%88%86%E6%9E%90"><span class="toc-number">1.3.4.1.</span> <span class="toc-text">2.4.1 自然语言冗余性分析</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4-2-LLMLingua-2%E5%8E%8B%E7%BC%A9%E5%8E%9F%E7%90%86"><span class="toc-number">1.3.4.2.</span> <span class="toc-text">2.4.2 LLMLingua-2压缩原理</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-4-3-%E5%8E%8B%E7%BC%A9%E5%AF%B9%E6%A3%80%E7%B4%A2%E6%80%A7%E8%83%BD%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-number">1.3.4.3.</span> <span class="toc-text">2.4.3 压缩对检索性能的影响</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-5-%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A3%80%E7%B4%A2%E7%B3%BB%E7%BB%9F"><span class="toc-number">1.3.5.</span> <span class="toc-text">2.5 多模态检索系统</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-5-1-%E6%A3%80%E7%B4%A2%E5%99%A8%E9%80%89%E6%8B%A9%E4%B8%8E%E9%85%8D%E7%BD%AE"><span class="toc-number">1.3.5.1.</span> <span class="toc-text">2.5.1 检索器选择与配置</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-5-2-%E6%B7%B7%E5%90%88%E6%A3%80%E7%B4%A2%E7%AD%96%E7%95%A5"><span class="toc-number">1.3.5.2.</span> <span class="toc-text">2.5.2 混合检索策略</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E5%86%99%E5%9C%A8%E6%9C%80%E5%90%8E%EF%BC%9A%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83"><span class="toc-number">1.4.</span> <span class="toc-text">3. 写在最后：一些思考</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-SeCom%E7%BB%99%E6%88%91%E4%BB%AC%E7%9A%84%E5%90%AF%E5%8F%91"><span class="toc-number">1.4.1.</span> <span class="toc-text">3.1 SeCom给我们的启发</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%BA%90"><span class="toc-number">1.5.</span> <span class="toc-text">参考资源</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/21/NLP%20Insights/Evaluation%20of%20Generation-Based%20Large%20Language%20Models%20(LLMs):%20Opportunities%20and%20Challenges%20from%20Generation%20to%20Judgment.en/" title="无标题">无标题</a><time datetime="2026-02-20T21:47:32.623Z" title="发表于 2026-02-21 05:47:32">2026-02-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/21/NLP%20Insights/Evaluation%20of%20Generation-Based%20Large%20Language%20Models%20(LLMs):%20Opportunities%20and%20Challenges%20from%20Generation%20to%20Judgment.zh-CN/" title="无标题">无标题</a><time datetime="2026-02-20T21:47:32.623Z" title="发表于 2026-02-21 05:47:32">2026-02-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/21/NLP%20Insights/Evaluation%20of%20Generation-Based%20Large%20Language%20Models%20(LLMs):%20Opportunities%20and%20Challenges%20from%20Generation%20to%20Judgment/" title="无标题">无标题</a><time datetime="2026-02-20T21:46:38.687Z" title="发表于 2026-02-21 05:46:38">2026-02-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/24/NLP%20Insights/SeCom%20Redefining%20Memory%20Management%20in%20Conversational%20AI.en/" title="SeCom: Redefining Memory Management in Conversational AI">SeCom: Redefining Memory Management in Conversational AI</a><time datetime="2025-06-24T08:00:00.000Z" title="发表于 2025-06-24 16:00:00">2025-06-24</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/24/NLP%20Insights/SeCom%20Redefining%20Memory%20Management%20in%20Conversational%20AI/" title="SeCom: Redefining Memory Management in Conversational AI">SeCom: Redefining Memory Management in Conversational AI</a><time datetime="2025-06-24T08:00:00.000Z" title="发表于 2025-06-24 16:00:00">2025-06-24</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 - 2026 By Huiyu Chen</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 6.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.5.4</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.4"></script><script src="/js/main.js?v=5.5.4"></script><div class="js-pjax"></div><script src="/js/lang-switch.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>