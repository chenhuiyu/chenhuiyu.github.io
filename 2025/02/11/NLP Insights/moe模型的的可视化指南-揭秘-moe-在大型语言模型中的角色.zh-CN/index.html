<!DOCTYPE html><html lang="[&quot;zh-CN&quot;,&quot;en&quot;,&quot;default&quot;]" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>MoE模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色 | 黑头呆鱼进化之旅</title><meta name="author" content="Huiyu Chen"><meta name="copyright" content="Huiyu Chen"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="MoE 模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色原文地址：A Visual Guide to Mixture of Experts (MoE) 📅 作者：Maarten Grootendorst 📆 日期：2024 年 10 月 7 日  探索语言模型：混合专家模型（MoE）可视化指南目录 MoE 模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色 探索语言模型：混合专家">
<meta property="og:type" content="article">
<meta property="og:title" content="MoE模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色">
<meta property="og:url" content="https://chenhuiyu.github.io/2025/02/11/NLP%20Insights/moe%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97-%E6%8F%AD%E7%A7%98-moe-%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2.zh-CN/index.html">
<meta property="og:site_name" content="黑头呆鱼进化之旅">
<meta property="og:description" content="MoE 模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色原文地址：A Visual Guide to Mixture of Experts (MoE) 📅 作者：Maarten Grootendorst 📆 日期：2024 年 10 月 7 日  探索语言模型：混合专家模型（MoE）可视化指南目录 MoE 模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色 探索语言模型：混合专家">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://chenhuiyu.github.io/img/butterfly-icon.png">
<meta property="article:published_time" content="2025-02-11T03:50:29.000Z">
<meta property="article:modified_time" content="2026-02-20T22:19:17.273Z">
<meta property="article:author" content="Huiyu Chen">
<meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://chenhuiyu.github.io/img/butterfly-icon.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "MoE模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色",
  "url": "https://chenhuiyu.github.io/2025/02/11/NLP%20Insights/moe%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97-%E6%8F%AD%E7%A7%98-moe-%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2.zh-CN/",
  "image": "https://chenhuiyu.github.io/img/butterfly-icon.png",
  "datePublished": "2025-02-11T03:50:29.000Z",
  "dateModified": "2026-02-20T22:19:17.273Z",
  "author": [
    {
      "@type": "Person",
      "name": "Huiyu Chen",
      "url": "https://chenhuiyu.github.io"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://chenhuiyu.github.io/2025/02/11/NLP%20Insights/moe%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97-%E6%8F%AD%E7%A7%98-moe-%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2.zh-CN/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=5.5.4"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.13.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'MoE模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 6.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="黑头呆鱼进化之旅" type="application/atom+xml">
</head><body><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">黑头呆鱼进化之旅</span></a><a class="nav-page-title" href="/"><span class="site-name">MoE模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"></div></nav><div id="post-info"><h1 class="post-title">MoE模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-02-11T03:50:29.000Z" title="发表于 2025-02-11 11:50:29">2025-02-11</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2026-02-20T22:19:17.273Z" title="更新于 2026-02-21 06:19:17">2026-02-21</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/NLP-Insights/">NLP Insights</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="MoE-模型的的可视化指南：揭秘-MoE-在大型语言模型中的角色"><a href="#MoE-模型的的可视化指南：揭秘-MoE-在大型语言模型中的角色" class="headerlink" title="MoE 模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色"></a>MoE 模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色</h1><p><strong>原文地址</strong>：<a target="_blank" rel="noopener" href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts">A Visual Guide to Mixture of Experts (MoE)</a></p>
<p>📅 作者：Maarten Grootendorst</p>
<p>📆 日期：2024 年 10 月 7 日</p>
<hr>
<h1 id="探索语言模型：混合专家模型（MoE）可视化指南"><a href="#探索语言模型：混合专家模型（MoE）可视化指南" class="headerlink" title="探索语言模型：混合专家模型（MoE）可视化指南"></a>探索语言模型：混合专家模型（MoE）可视化指南</h1><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul>
<li><a href="#moe-%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%E6%8F%AD%E7%A7%98-moe-%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2">MoE 模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色</a></li>
<li><a href="#%E6%8E%A2%E7%B4%A2%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%B7%B7%E5%90%88%E4%B8%93%E5%AE%B6%E6%A8%A1%E5%9E%8Bmoe%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97">探索语言模型：混合专家模型（MoE）可视化指南</a><ul>
<li><a href="#%E7%9B%AE%E5%BD%95">目录</a></li>
<li><a href="#%E4%BB%80%E4%B9%88%E6%98%AF%E6%B7%B7%E5%90%88%E4%B8%93%E5%AE%B6moe%E6%A8%A1%E5%9E%8B">什么是混合专家（MoE）模型？</a></li>
<li><a href="#experts">Experts</a><ul>
<li><a href="#dense-layers">Dense Layers</a></li>
<li><a href="#sparse-layers">Sparse Layers</a></li>
<li><a href="#what-does-an-expert-learn">What does an Expert Learn?</a></li>
<li><a href="#%E4%B8%93%E5%AE%B6%E7%9A%84%E6%9E%B6%E6%9E%84architecture-of-experts">专家的架构（Architecture of Experts）</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p>当我们查看最新发布的大型语言模型（<strong>LLMs</strong>，Large Language Models）时，常常会在标题中看到 “<strong>MoE</strong>”。这个 “MoE” 代表什么？为什么这么多 LLM 都在使用它？</p>
<p>在这份可视化指南中，我们会通过 50 多个可视化图示，逐步探索这一关键组件：**Mixture of Experts (MoE)**。</p>


<p><strong>图示内容</strong>：在这张图中，可以看到一个典型 <strong>MoE</strong> 结构的两个主要组成部分：<strong>Experts</strong>（专家）和 <strong>Router</strong>（路由器或门控网络）。图中显示了一个 <strong>Router</strong>，以及下方并列的多个 <strong>Experts</strong>，表明在 <strong>LLM</strong> 架构中，MoE 会将输入根据需要路由到合适的专家。<br><strong>图 1 详细说明</strong>：</p>
<ol>
<li><strong>Router</strong>：决定将输入（例如 token）发送给哪一个或哪几个专家。</li>
<li><strong>Experts</strong>：若干个不同的子模型（通常是 <strong>FFNN</strong> 结构），每个专家可能在不同方面具有专长。</li>
<li><strong>工作流程</strong>：输入先通过 <strong>Router</strong>，再被分配到不同的专家进行处理，最后汇总结果。</li>
</ol>
<h2 id="什么是混合专家（MoE）模型？"><a href="#什么是混合专家（MoE）模型？" class="headerlink" title="什么是混合专家（MoE）模型？"></a>什么是混合专家（MoE）模型？</h2><p><strong>Mixture of Experts (MoE)</strong> 是一种技术，它使用许多不同的子模型（或“<strong>experts</strong>”）来提升大型语言模型的质量。</p>
<p>在 MoE 中，有两个主要组件：</p>
<ol>
<li><strong>Experts</strong><ul>
<li>每个 <strong>FFNN</strong> 层都不再是一个单独的网络，而是有一组“专家”可供选择。</li>
<li>这些“专家”通常也是 <strong>FFNN</strong>（Feedforward Neural Network）结构。</li>
</ul>
</li>
<li><strong>Router</strong> 或 <strong>gate network</strong><ul>
<li>负责决定哪些 <strong>tokens</strong> 被发送到哪些专家。</li>
</ul>
</li>
</ol>
<p>在一个带有 MoE 的 <strong>LLM</strong> 的每一层，我们都能看到（在某种程度上）有所专门化的专家：</p>


<p><strong>图示内容</strong>：展示了在 <strong>LLM</strong> 的每一层都可以拥有多个 <strong>Experts</strong>。它强调了这些专家在不同的上下文中能够处理不同的输入 token。<br><strong>图2详细说明</strong>：  </p>
<ol>
<li><strong>层结构</strong>：图中用不同的层级（Layer 1、Layer 2、Layer 3……）表示多层模型。  </li>
<li><strong>Experts</strong>：在每一层，都有若干个专家（Expert 1、Expert 2、Expert 3、Expert 4），这些专家并行存在。  </li>
<li><strong>目标</strong>：强调专家在特定上下文或特定输入时更具备“专业性”，从而被选中来处理该输入。</li>
</ol>
<p>尽管 MoE 并不会在特定领域（如心理学或生物学）上专门训练专家，但它们仍可能在词法或句法级别上形成一定的偏向：</p>


<ul>
<li><strong>MoE 专家可能学习到不同的语言特征</strong><ul>
<li><strong>Expert 1</strong> 处理<strong>标点符号</strong>（Punctuation）：如 <code>, . : &amp; - ?</code> 等。</li>
<li><strong>Expert 2</strong> 处理<strong>动词</strong>（Verbs）：如 <code>said, read, miss</code> 等。</li>
<li><strong>Expert 3</strong> 处理<strong>连接词</strong>（Conjunctions）：如 <code>the, and, if, not</code> 等。</li>
<li><strong>Expert 4</strong> 处理<strong>视觉描述词</strong>（Visual Descriptions）：如 <code>dark, outer, yellow</code> 等。</li>
</ul>
</li>
</ul>
<p>更具体地说，他们的专长是在特定上下文中处理特定的标记（tokens）。</p>
<hr>
<p><strong>Router (gate network)</strong> 选择最适合给定输入的专家或专家组合：</p>


<p><strong>图示内容</strong>：展示了 <strong>Router</strong> 如何在每一层根据输入选择合适的专家。图中高亮了被选中的专家，以及输入 token 的流动过程。<br><strong>图3详细说明</strong>：  </p>
<ol>
<li><strong>输入</strong>：图顶部的 Input 代表模型接收到的 token 或向量表示。  </li>
<li><strong>Router</strong>：位于网络结构中，起到决策作用。  </li>
<li><strong>专家选择</strong>：被选中的专家会接收输入，其余专家则不被激活。  </li>
<li><strong>输出</strong>：来自被激活专家的结果被汇总或继续流向下游层。</li>
</ol>
<p>需要注意的是，每个专家并不是整个 LLM，而是 <strong>LLM</strong> 架构中的一个子模型部分。</p>
<hr>
<h2 id="Experts"><a href="#Experts" class="headerlink" title="Experts"></a>Experts</h2><p>为了理解专家（<strong>Experts</strong>）是什么以及它们如何工作，我们先来看看 MoE 希望替代的东西：<strong>dense layers</strong>。</p>
<h3 id="Dense-Layers"><a href="#Dense-Layers" class="headerlink" title="Dense Layers"></a>Dense Layers</h3><p>所有的 <strong>Mixture of Experts (MoE)</strong> 都基于 LLM 中一个相对基础的功能：**Feedforward Neural Network (FFNN)**。</p>
<p>回忆一下，一个标准的 <strong>decoder-only Transformer</strong> 架构中，<strong>FFNN</strong> 通常是在 <strong>layer normalization</strong> 之后应用的：</p>

<p><strong>图示内容</strong>：展示了一个典型的 <strong>decoder</strong> 结构，每个 <strong>decoder block</strong> 包含 <strong>Masked Self-Attention</strong> 和 <strong>FFNN</strong>（中间会有 <strong>Layer Norm</strong>）。  </p>
<ol>
<li><strong>Position Embedding</strong>：在输入 token 之前或同时加入位置编码信息。  </li>
<li><strong>Decoder Block</strong>：包含 <strong>Masked Self-Attention</strong>、<strong>Layer Norm</strong> 和 <strong>FFNN</strong>。  </li>
<li><strong>FFNN</strong>：在图中用紫色方块表示，是该层对输入进一步变换以捕捉更复杂关系的关键组件。</li>
</ol>
<p><strong>FFNN</strong> 可以利用注意力机制产生的上下文信息，对其进行进一步的转换，以捕捉数据中更复杂的关系。</p>
<p>不过，为了学习这些复杂关系，<strong>FFNN</strong> 的规模会随之增长，通常会在输入上进行扩张（例如，中间层维度会变大）：</p>

<p><strong>图示内容</strong>：展示了一个 <strong>FFNN</strong> 的结构，输入先被映射到更高维度，然后再被映射回输出维度。  </p>
<ol>
<li><strong>输入维度</strong>：图中显示有 512 个输入单元。  </li>
<li><strong>隐藏层</strong>：通常会有 4 倍或更多的扩张（图中示例为 4 倍扩张到 2048 维）。  </li>
<li><strong>输出维度</strong>：再映射回 512 维的输出。</li>
</ol>
<h3 id="Sparse-Layers"><a href="#Sparse-Layers" class="headerlink" title="Sparse Layers"></a>Sparse Layers</h3><p>在传统的 Transformer 中，<strong>FFNN</strong> 称为 <strong>dense model</strong>，因为它的所有参数（权重和偏置）都会被激活。也就是说，模型的全部参数都参与计算输出。</p>
<p>如果我们仔细观察 <strong>dense model</strong>，可以看到输入会激活所有的参数：</p>

<p><strong>图示内容</strong>：展示了一个“密集”模型，输入层的每个神经元都与隐藏层所有神经元相连，隐藏层所有神经元又与输出层神经元相连。<br><strong>图6详细说明</strong>：  </p>
<ol>
<li><strong>全连接</strong>：图中所有节点都连接到下一层的所有节点，表示无稀疏性。  </li>
<li><strong>所有参数被激活</strong>：没有任何“闲置”或“未激活”的参数。</li>
</ol>
<p>与之对比，<strong>sparse models</strong>（稀疏模型）只激活一部分总参数，这与 <strong>Mixture of Experts</strong> 密切相关。</p>
<p>为了说明这一点，我们可以把 <strong>dense model</strong> 切分成多个部分（即专家，<strong>experts</strong>），重新训练它，并且在推理（inference）时只激活其中一部分：</p>


<p><strong>图示内容</strong>：将原本的密集模型分割成多个专家（Expert 1、Expert 2、Expert 3、Expert 4）。在推理阶段，只选择一部分专家进行激活。  </p>
<ol>
<li><strong>模型切分</strong>：原有的大网络被拆分成多个较小的“专家”。  </li>
<li><strong>稀疏激活</strong>：并不是所有专家都被激活，只有部分专家在某些输入下被激活。  </li>
<li><strong>好处</strong>：通过稀疏激活，可以在不显著增加计算成本的情况下，拥有更多的潜在参数容量。</li>
</ol>
<p>其核心思想是：在训练期间，每个专家学习不同的信息；在推理时，只用到与当前任务最相关的那些专家。</p>
<p>当我们提出一个问题时，就会选择最适合该任务的专家：</p>


<p><strong>图示内容</strong>：展示了一个示例：当输入是 “What is 1 + 1?” 这样的数字相关问题时，路由器只激活与数字相关的专家。  </p>
<ol>
<li><strong>输入</strong>：一个表示算术问题的句子或 token。  </li>
<li><strong>专家选择</strong>：只激活 “Numbers” 领域的专家。  </li>
<li><strong>输出</strong>：专家给出结果 “2”。</li>
</ol>
<h3 id="What-does-an-Expert-Learn"><a href="#What-does-an-Expert-Learn" class="headerlink" title="What does an Expert Learn?"></a>What does an Expert Learn?</h3><p>正如前面所提到的，专家（<strong>Experts</strong>）往往学习到比整个领域更细致的知识。有人会觉得称它们为“专家”可能会带来误解，但这是因为每个专家往往只专注于某些特定类型的输入特征或上下文。</p>


<p><strong>图示内容</strong>：展示了一个表格或对照，说明在某些情况下，不同的专家可能学习到不同的特征（比如标点符号、动词、数字等）。  </p>
<ol>
<li><strong>示例化专家</strong>：Punctuation、Conjunctions、Verbs、Numbers 等。  </li>
<li><strong>分层位置</strong>：不同专家可能出现在模型的不同层。  </li>
<li><strong>分配</strong>：某些 token 会路由到某些专家，以获得更有效的处理。</li>
</ol>
<p>在 <strong>decoder</strong> 模型中，专家之间可能没有那么明显的领域分工。然而，这并不意味着所有专家都完全相同。<br>在 <strong>Mixtral 8x7B</strong> 这篇论文中，有一个很好的示例：每个 token 会被标记为其首选专家，这些专家并不一定对应直观的语义领域，但在统计上表现出某些倾向。</p>

<p>这张可视化示例还展示了，experts（专家）更倾向于关注句法（syntax），而不是特定的领域（domain）。因此，虽然 decoder experts（解码器专家）似乎并没有明确的“专业领域（specialism）”，但它们似乎会在某些特定类型的 tokens（标记）上被持续地使用。</p>
<p>在[图1]中，展示了一段关于 MoELayer 的示例代码或可视化结果，色块区分了不同部分，强调了<strong>专家（experts）与路由器（router）</strong>之间的关系。通过色块可以看出：</p>
<ul>
<li>experts 列表（在代码中用 nn.ModuleList 表示）包含了多个子网络（即多个 FFNN，Feed-Forward Neural Network，前馈神经网络）。</li>
<li>gate（门控网络，也称 router）负责选择哪些专家会被激活。</li>
<li>整体上可以看到，这些专家通常关注到输入句子的句法层面，而非特定主题或领域。</li>
</ul>
<h3 id="专家的架构（Architecture-of-Experts）"><a href="#专家的架构（Architecture-of-Experts）" class="headerlink" title="专家的架构（Architecture of Experts）"></a>专家的架构（Architecture of Experts）</h3></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://chenhuiyu.github.io">Huiyu Chen</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://chenhuiyu.github.io/2025/02/11/NLP%20Insights/moe%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97-%E6%8F%AD%E7%A7%98-moe-%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2.zh-CN/">https://chenhuiyu.github.io/2025/02/11/NLP%20Insights/moe%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97-%E6%8F%AD%E7%A7%98-moe-%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2.zh-CN/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://chenhuiyu.github.io" target="_blank">黑头呆鱼进化之旅</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/LLM/">LLM</a></div><div class="post-share"><div class="social-share" data-image="/img/butterfly-icon.png" data-sites="facebook,x,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86-llm-%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97-%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E-deepseek-r1.en/" title="推理 LLM 的可视化指南：探索推理时计算技术与 DeepSeek-R1"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">推理 LLM 的可视化指南：探索推理时计算技术与 DeepSeek-R1</div></div><div class="info-2"><div class="info-item-1">推理 LLM 的可视化指南：探索推理时计算技术与 DeepSeek-R1原文地址：A Visual Guide to Reasoning LLMs 📅 作者：Maarten Grootendorst 📆 日期：2025 年 2 月 3 日  📌 引言DeepSeek-R1、OpenAI o3-mini 和 Google Gemini 2.0 Flash Thinking 是如何通过“推理”框架将 LLM（大型语言模型, Large Language Models） 扩展到新高度的典型示例。 它们标志着从 扩展训练时计算（train-time compute） 到 扩展推理时计算（test-time compute） 的范式转变。 在本篇文章中，我们提供了 超过 40 张定制可视化图表，带你深入探索：  推理 LLM（Reasoning LLMs） 领域 推理时计算（Test-Time Compute） 机制 DeepSeek-R1 的核心思想  我们将逐步介绍相关概念，帮助你建立对这一新范式的直觉理解。    📖 什么是推理 LLM？与普通 LLM（Large Langu...</div></div></div></a><a class="pagination-related" href="/2025/02/11/NLP%20Insights/moe%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97-%E6%8F%AD%E7%A7%98-moe-%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2.en/" title="MoE模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">MoE模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色</div></div><div class="info-2"><div class="info-item-1">MoE 模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色原文地址：A Visual Guide to Mixture of Experts (MoE) 📅 作者：Maarten Grootendorst 📆 日期：2024 年 10 月 7 日  探索语言模型：混合专家模型（MoE）可视化指南目录 MoE 模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色 探索语言模型：混合专家模型（MoE）可视化指南 目录 什么是混合专家（MoE）模型？ Experts Dense Layers Sparse Layers What does an Expert Learn? 专家的架构（Architecture of Experts）      当我们查看最新发布的大型语言模型（LLMs，Large Language Models）时，常常会在标题中看到 “MoE”。这个 “MoE” 代表什么？为什么这么多 LLM 都在使用它？ 在这份可视化指南中，我们会通过 50 多个可视化图示，逐步探索这一关键组件：**Mixture of Experts (MoE)**。   图示内...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2023/07/28/NLP%20Insights/longnet-scaling-transformers-to-1-000-000-000-tokens.zh-CN/" title="LONGNET - Scaling Transformers to 1,000,000,000 Tokens"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2023-07-28</div><div class="info-item-2">LONGNET - Scaling Transformers to 1,000,000,000 Tokens</div></div><div class="info-2"><div class="info-item-1">LONGNET：将Transformer扩展到10亿个标记在本篇文章中，我们将详细讨论一个近期发布的先进模型——“LongNet”。该模型由微软亚洲研究院研发，于大约两周前正式公布。LongNet基于Transformer模型构建，其核心理念在于拓展Transformer的应用规模。值得一提的是，研究团队成功地将其扩展至处理10亿个令牌的规模。对于熟悉语言模型的人来说，会明白序列长度对模型性能的影响，因为序列长度决定了在执行注意力机制时，能够关联的令牌数量，从而影响模型可以获取的上下文信息长度。例如，我们希望像GPT这样的模型能拥有更长的上下文，使得模型可以参考更久之前的单词来预测下一个令牌。而LongNet就成功地将这个能力扩展到了10亿个令牌。以下图为例，可以清晰看出，GPT的序列长度仅为512，而Power Transformer的序列长度可扩展至12、000、64、262、000、甚至1000万，然而LongNet将序列长度扩展至惊人的10亿个令牌。试想一下，我们可以将所有维基百科的文本信息输入到模型中，模型可以利用所有这些令牌进行注意力计算。接下来，让我们首先来了解一下...</div></div></div></a><a class="pagination-related" href="/2024/02/28/NLP%20Insights/gorilla-llm-%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B.en/" title="Gorilla LLM 大语言模型简介"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-28</div><div class="info-item-2">Gorilla LLM 大语言模型简介</div></div><div class="info-2"><div class="info-item-1">Gorilla LLM 大语言模型简介🦍 Gorilla: Large Language Model Connected with Massive APIsLink: https://gorilla.cs.berkeley.edu/blogs/7_open_functions_v2.html  Berkeley 功能调用排行榜Berkeley 功能调用排行榜 在线体验模型：Gorilla OpenFunctions-v2 网络演示 项目详情：GitHub 模型（7B 参数）在 HuggingFace 上的页面：gorilla-llm/gorilla-openfunctions-v2  1. 伯克利函数调用排行榜自 2022 年底以来，大语言模型（LLMs）凭借其执行通用任务的强大能力，成为众人关注的焦点。不仅限于聊天应用，将这些模型应用于开发各类 AI 应用和软件（如 Langchain, Llama Index, AutoGPT, Voyager）已成为一种趋势。GPT, Gemini, Llama, Mistral 等模型通过与外部世界的交互，如函数调用和执行，展现了其巨大...</div></div></div></a><a class="pagination-related" href="/2025/03/06/NLP%20Insights/differences-in-padding-strategies-between-decoder-only-and-encoder-only-models.zh-CN/" title="Decoder-only与Encoder-only模型Padding策略的差异"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-06</div><div class="info-item-2">Decoder-only与Encoder-only模型Padding策略的差异</div></div><div class="info-2"><div class="info-item-1">📌 Padding 的含义在大模型 (LLM) 中，padding 是用于将不同长度的序列调整为同一长度的方法，以便于批量 (batch) 处理。 例如： 12句子1: "I love NLP"句子2: "Padding is useful in LLM training"  使用 &lt;pad&gt; token 进行对齐： 12"I love NLP &lt;pad&gt; &lt;pad&gt; &lt;pad&gt;""Padding is useful in LLM training"   📌 Padding 位置的选择：Left vs RightPadding 有两种常见方式：  Right padding（右填充）： 1"I love NLP &lt;pad&gt; &lt;pad&gt;"  Left padding（左填充）： 1"&lt;pad&gt; &lt;pad&gt; I love NLP"  通常：  Decoder-only 模型（如 GPT, Llama）：采用 Left padding Encoder-only 模型（如 BERT）：采用...</div></div></div></a><a class="pagination-related" href="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86-llm-%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97-%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E-deepseek-r1.en/" title="推理 LLM 的可视化指南：探索推理时计算技术与 DeepSeek-R1"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-02-11</div><div class="info-item-2">推理 LLM 的可视化指南：探索推理时计算技术与 DeepSeek-R1</div></div><div class="info-2"><div class="info-item-1">推理 LLM 的可视化指南：探索推理时计算技术与 DeepSeek-R1原文地址：A Visual Guide to Reasoning LLMs 📅 作者：Maarten Grootendorst 📆 日期：2025 年 2 月 3 日  📌 引言DeepSeek-R1、OpenAI o3-mini 和 Google Gemini 2.0 Flash Thinking 是如何通过“推理”框架将 LLM（大型语言模型, Large Language Models） 扩展到新高度的典型示例。 它们标志着从 扩展训练时计算（train-time compute） 到 扩展推理时计算（test-time compute） 的范式转变。 在本篇文章中，我们提供了 超过 40 张定制可视化图表，带你深入探索：  推理 LLM（Reasoning LLMs） 领域 推理时计算（Test-Time Compute） 机制 DeepSeek-R1 的核心思想  我们将逐步介绍相关概念，帮助你建立对这一新范式的直觉理解。    📖 什么是推理 LLM？与普通 LLM（Large Langu...</div></div></div></a><a class="pagination-related" href="/2023/07/28/NLP%20Insights/longnet-scaling-transformers-to-1-000-000-000-tokens.en/" title="LONGNET - Scaling Transformers to 1,000,000,000 Tokens"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2023-07-28</div><div class="info-item-2">LONGNET - Scaling Transformers to 1,000,000,000 Tokens</div></div><div class="info-2"><div class="info-item-1">LONGNET：将Transformer扩展到10亿个标记在本篇文章中，我们将详细讨论一个近期发布的先进模型——“LongNet”。该模型由微软亚洲研究院研发，于大约两周前正式公布。LongNet基于Transformer模型构建，其核心理念在于拓展Transformer的应用规模。值得一提的是，研究团队成功地将其扩展至处理10亿个令牌的规模。对于熟悉语言模型的人来说，会明白序列长度对模型性能的影响，因为序列长度决定了在执行注意力机制时，能够关联的令牌数量，从而影响模型可以获取的上下文信息长度。例如，我们希望像GPT这样的模型能拥有更长的上下文，使得模型可以参考更久之前的单词来预测下一个令牌。而LongNet就成功地将这个能力扩展到了10亿个令牌。以下图为例，可以清晰看出，GPT的序列长度仅为512，而Power Transformer的序列长度可扩展至12、000、64、262、000、甚至1000万，然而LongNet将序列长度扩展至惊人的10亿个令牌。试想一下，我们可以将所有维基百科的文本信息输入到模型中，模型可以利用所有这些令牌进行注意力计算。接下来，让我们首先来了解一下...</div></div></div></a><a class="pagination-related" href="/2025/02/11/NLP%20Insights/moe%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97-%E6%8F%AD%E7%A7%98-moe-%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2.en/" title="MoE模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-02-11</div><div class="info-item-2">MoE模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色</div></div><div class="info-2"><div class="info-item-1">MoE 模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色原文地址：A Visual Guide to Mixture of Experts (MoE) 📅 作者：Maarten Grootendorst 📆 日期：2024 年 10 月 7 日  探索语言模型：混合专家模型（MoE）可视化指南目录 MoE 模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色 探索语言模型：混合专家模型（MoE）可视化指南 目录 什么是混合专家（MoE）模型？ Experts Dense Layers Sparse Layers What does an Expert Learn? 专家的架构（Architecture of Experts）      当我们查看最新发布的大型语言模型（LLMs，Large Language Models）时，常常会在标题中看到 “MoE”。这个 “MoE” 代表什么？为什么这么多 LLM 都在使用它？ 在这份可视化指南中，我们会通过 50 多个可视化图示，逐步探索这一关键组件：**Mixture of Experts (MoE)**。   图示内...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Huiyu Chen</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">102</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">49</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#MoE-%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98-MoE-%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2"><span class="toc-number">1.</span> <span class="toc-text">MoE 模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%8E%A2%E7%B4%A2%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%9A%E6%B7%B7%E5%90%88%E4%B8%93%E5%AE%B6%E6%A8%A1%E5%9E%8B%EF%BC%88MoE%EF%BC%89%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97"><span class="toc-number">2.</span> <span class="toc-text">探索语言模型：混合专家模型（MoE）可视化指南</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%AE%E5%BD%95"><span class="toc-number">2.1.</span> <span class="toc-text">目录</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E6%B7%B7%E5%90%88%E4%B8%93%E5%AE%B6%EF%BC%88MoE%EF%BC%89%E6%A8%A1%E5%9E%8B%EF%BC%9F"><span class="toc-number">2.2.</span> <span class="toc-text">什么是混合专家（MoE）模型？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Experts"><span class="toc-number">2.3.</span> <span class="toc-text">Experts</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Dense-Layers"><span class="toc-number">2.3.1.</span> <span class="toc-text">Dense Layers</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Sparse-Layers"><span class="toc-number">2.3.2.</span> <span class="toc-text">Sparse Layers</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#What-does-an-Expert-Learn"><span class="toc-number">2.3.3.</span> <span class="toc-text">What does an Expert Learn?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%93%E5%AE%B6%E7%9A%84%E6%9E%B6%E6%9E%84%EF%BC%88Architecture-of-Experts%EF%BC%89"><span class="toc-number">2.3.4.</span> <span class="toc-text">专家的架构（Architecture of Experts）</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/21/NLP%20Insights/paper-deep-dive-2026-02-21.en/" title="Paper Deep Dive | SLA2: Sparse-Linear Attention with Learnable Routing and QAT">Paper Deep Dive | SLA2: Sparse-Linear Attention with Learnable Routing and QAT</a><time datetime="2026-02-21T04:28:28.000Z" title="发表于 2026-02-21 12:28:28">2026-02-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/21/NLP%20Insights/paper-deep-dive-2026-02-21.zh-CN/" title="论文深读｜SLA2: Sparse-Linear Attention with Learnable Routing and QAT">论文深读｜SLA2: Sparse-Linear Attention with Learnable Routing and QAT</a><time datetime="2026-02-21T04:28:28.000Z" title="发表于 2026-02-21 12:28:28">2026-02-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/21/NLP%20Insights/evaluation-of-generation-based-large-language-models-llms-opportunities-and-challenges-from-generation-to-judgment.en/" title="evaluation-of-generation-based-large-language-models-llms-opportunities-and-challenges-from-generation-to-judgment">evaluation-of-generation-based-large-language-models-llms-opportunities-and-challenges-from-generation-to-judgment</a><time datetime="2026-02-20T16:00:00.000Z" title="发表于 2026-02-21 00:00:00">2026-02-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/21/NLP%20Insights/evaluation-of-generation-based-large-language-models-llms-opportunities-and-challenges-from-generation-to-judgment.zh-CN/" title="evaluation-of-generation-based-large-language-models-llms-opportunities-and-challenges-from-generation-to-judgment">evaluation-of-generation-based-large-language-models-llms-opportunities-and-challenges-from-generation-to-judgment</a><time datetime="2026-02-20T16:00:00.000Z" title="发表于 2026-02-21 00:00:00">2026-02-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/24/NLP%20Insights/secom-redefining-memory-management-in-conversational-ai.en/" title="SeCom: Redefining Memory Management in Conversational AI">SeCom: Redefining Memory Management in Conversational AI</a><time datetime="2025-06-24T08:00:00.000Z" title="发表于 2025-06-24 16:00:00">2025-06-24</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 - 2026 By Huiyu Chen</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 6.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.5.4</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.4"></script><script src="/js/main.js?v=5.5.4"></script><div class="js-pjax"></div><script src="/js/lang-switch.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>