<!DOCTYPE html><html class="appearance-auto" lang="[&quot;en&quot;]"><head><meta charset="UTF-8"><title>MoE模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色</title><meta name="description" content="May the Force be with you"><meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no, initial-scale=1"><!-- Google Analytics --><!-- End Google Analytics -->
<!-- Baidu Analytics --><!-- End Baidu Analytics --><link rel="icon" href="/images/favicon.ico"><link rel="stylesheet" href="/style/common/bulma.css"><link rel="stylesheet" href="/style/base.css"><link rel="stylesheet" href="/style/common/helper.css"><script src="/js/common.js"></script><link rel="stylesheet" href="/style/post.css"><link rel="stylesheet" href="/style/themes/highlight-theme-light.css"><link rel="stylesheet" href="/style/common/jquery.fancybox.min.css"><script src="/js/highlight.pack.js"></script><meta name="description" content="MoE 模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色原文地址：A Visual Guide to Mixture of Experts (MoE)
📅 作者：Maarten Grootendorst
📆 日期：2024 年 10 月 7 日

探索语言模型：混合专家模型（MoE）可视化指南目录
MoE 模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色
探索语言模型：混合专家模型（MoE）可视化指南
目录
什么是混合专家（MoE）模型？
Experts
Dense Layers
Sparse Layers
What does an Expert Learn?
专家的架构（Architecture of Experts）





当我们查看最新发布的大型语言模型（LLMs，Large .."><meta name="generator" content="Hexo 6.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="黑头呆鱼进化之旅" type="application/atom+xml">
</head><body class="is-flex is-flex-direction-column"><header class="header-widget is-flex-shrink-0 is-hidden-mobile"><div class="container is-fullhd is-flex is-justify-content-space-between is-align-items-center is-full-height"><section class="is-hidden-mobile is-flex-shrink-0"><h2><a href="/">user's blog</a></h2></section><h3 class="is-hidden-mobile is-family-serif is-full-height is-flex is-align-items-center is-flex-shrink-0"><div class="is-full-height" id="postTopic"><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">MoE模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色</p><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">Click back to the top</p></div></h3><aside class="is-flex-shrink-0"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></aside></div></header><header class="is-flex header-widget is-flex-shrink-0 is-align-items-center is-justify-content-center is-hidden-tablet"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></header><main><main class="container is-max-widescreen content section post-page pt-4 px-4"><div class="columns is-flex-desktop is-justify-content-center is-flex-direction-row-reverse"><div class="column is-3 is-hidden-mobile"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#MoE-%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98-MoE-%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2"><span class="toc-text">MoE 模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%8E%A2%E7%B4%A2%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%9A%E6%B7%B7%E5%90%88%E4%B8%93%E5%AE%B6%E6%A8%A1%E5%9E%8B%EF%BC%88MoE%EF%BC%89%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97"><span class="toc-text">探索语言模型：混合专家模型（MoE）可视化指南</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%AE%E5%BD%95"><span class="toc-text">目录</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E6%B7%B7%E5%90%88%E4%B8%93%E5%AE%B6%EF%BC%88MoE%EF%BC%89%E6%A8%A1%E5%9E%8B%EF%BC%9F"><span class="toc-text">什么是混合专家（MoE）模型？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Experts"><span class="toc-text">Experts</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Dense-Layers"><span class="toc-text">Dense Layers</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Sparse-Layers"><span class="toc-text">Sparse Layers</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#What-does-an-Expert-Learn"><span class="toc-text">What does an Expert Learn?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%93%E5%AE%B6%E7%9A%84%E6%9E%B6%E6%9E%84%EF%BC%88Architecture-of-Experts%EF%BC%89"><span class="toc-text">专家的架构（Architecture of Experts）</span></a></li></ol></li></ol></li></ol></div><div class="column is-9"><header class="my-4"><a href="/tags/LLM"><i class="tag post-item-tag">LLM</i></a></header><h1 class="mt-0 mb-1 is-family-serif" id="postTitle">MoE模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色</h1><time class="has-text-grey" datetime="2025-02-11T03:50:29.000Z">2025-02-11</time><article class="mt-2 post-content"><h1 id="MoE-模型的的可视化指南：揭秘-MoE-在大型语言模型中的角色"><a href="#MoE-模型的的可视化指南：揭秘-MoE-在大型语言模型中的角色" class="headerlink" title="MoE 模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色"></a>MoE 模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色</h1><p><strong>原文地址</strong>：<a target="_blank" rel="noopener" href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts">A Visual Guide to Mixture of Experts (MoE)</a></p>
<p>📅 作者：Maarten Grootendorst</p>
<p>📆 日期：2024 年 10 月 7 日</p>
<hr>
<h1 id="探索语言模型：混合专家模型（MoE）可视化指南"><a href="#探索语言模型：混合专家模型（MoE）可视化指南" class="headerlink" title="探索语言模型：混合专家模型（MoE）可视化指南"></a>探索语言模型：混合专家模型（MoE）可视化指南</h1><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul>
<li><a href="#moe-%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%E6%8F%AD%E7%A7%98-moe-%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2">MoE 模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色</a></li>
<li><a href="#%E6%8E%A2%E7%B4%A2%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%B7%B7%E5%90%88%E4%B8%93%E5%AE%B6%E6%A8%A1%E5%9E%8Bmoe%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97">探索语言模型：混合专家模型（MoE）可视化指南</a><ul>
<li><a href="#%E7%9B%AE%E5%BD%95">目录</a></li>
<li><a href="#%E4%BB%80%E4%B9%88%E6%98%AF%E6%B7%B7%E5%90%88%E4%B8%93%E5%AE%B6moe%E6%A8%A1%E5%9E%8B">什么是混合专家（MoE）模型？</a></li>
<li><a href="#experts">Experts</a><ul>
<li><a href="#dense-layers">Dense Layers</a></li>
<li><a href="#sparse-layers">Sparse Layers</a></li>
<li><a href="#what-does-an-expert-learn">What does an Expert Learn?</a></li>
<li><a href="#%E4%B8%93%E5%AE%B6%E7%9A%84%E6%9E%B6%E6%9E%84architecture-of-experts">专家的架构（Architecture of Experts）</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p>当我们查看最新发布的大型语言模型（<strong>LLMs</strong>，Large Language Models）时，常常会在标题中看到 “<strong>MoE</strong>”。这个 “MoE” 代表什么？为什么这么多 LLM 都在使用它？</p>
<p>在这份可视化指南中，我们会通过 50 多个可视化图示，逐步探索这一关键组件：**Mixture of Experts (MoE)**。</p>
<img src="/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2/20250224_145859.png" class="">

<p><strong>图示内容</strong>：在这张图中，可以看到一个典型 <strong>MoE</strong> 结构的两个主要组成部分：<strong>Experts</strong>（专家）和 <strong>Router</strong>（路由器或门控网络）。图中显示了一个 <strong>Router</strong>，以及下方并列的多个 <strong>Experts</strong>，表明在 <strong>LLM</strong> 架构中，MoE 会将输入根据需要路由到合适的专家。<br><strong>图 1 详细说明</strong>：</p>
<ol>
<li><strong>Router</strong>：决定将输入（例如 token）发送给哪一个或哪几个专家。</li>
<li><strong>Experts</strong>：若干个不同的子模型（通常是 <strong>FFNN</strong> 结构），每个专家可能在不同方面具有专长。</li>
<li><strong>工作流程</strong>：输入先通过 <strong>Router</strong>，再被分配到不同的专家进行处理，最后汇总结果。</li>
</ol>
<h2 id="什么是混合专家（MoE）模型？"><a href="#什么是混合专家（MoE）模型？" class="headerlink" title="什么是混合专家（MoE）模型？"></a>什么是混合专家（MoE）模型？</h2><p><strong>Mixture of Experts (MoE)</strong> 是一种技术，它使用许多不同的子模型（或“<strong>experts</strong>”）来提升大型语言模型的质量。</p>
<p>在 MoE 中，有两个主要组件：</p>
<ol>
<li><strong>Experts</strong><ul>
<li>每个 <strong>FFNN</strong> 层都不再是一个单独的网络，而是有一组“专家”可供选择。</li>
<li>这些“专家”通常也是 <strong>FFNN</strong>（Feedforward Neural Network）结构。</li>
</ul>
</li>
<li><strong>Router</strong> 或 <strong>gate network</strong><ul>
<li>负责决定哪些 <strong>tokens</strong> 被发送到哪些专家。</li>
</ul>
</li>
</ol>
<p>在一个带有 MoE 的 <strong>LLM</strong> 的每一层，我们都能看到（在某种程度上）有所专门化的专家：</p>
<img src="/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2/20250224_150409.png" class="">

<p><strong>图示内容</strong>：展示了在 <strong>LLM</strong> 的每一层都可以拥有多个 <strong>Experts</strong>。它强调了这些专家在不同的上下文中能够处理不同的输入 token。<br><strong>图2详细说明</strong>：  </p>
<ol>
<li><strong>层结构</strong>：图中用不同的层级（Layer 1、Layer 2、Layer 3……）表示多层模型。  </li>
<li><strong>Experts</strong>：在每一层，都有若干个专家（Expert 1、Expert 2、Expert 3、Expert 4），这些专家并行存在。  </li>
<li><strong>目标</strong>：强调专家在特定上下文或特定输入时更具备“专业性”，从而被选中来处理该输入。</li>
</ol>
<p>尽管 MoE 并不会在特定领域（如心理学或生物学）上专门训练专家，但它们仍可能在词法或句法级别上形成一定的偏向：</p>
<img src="/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2/20250224_153715.png" class="">

<ul>
<li><strong>MoE 专家可能学习到不同的语言特征</strong><ul>
<li><strong>Expert 1</strong> 处理<strong>标点符号</strong>（Punctuation）：如 <code>, . : &amp; - ?</code> 等。</li>
<li><strong>Expert 2</strong> 处理<strong>动词</strong>（Verbs）：如 <code>said, read, miss</code> 等。</li>
<li><strong>Expert 3</strong> 处理<strong>连接词</strong>（Conjunctions）：如 <code>the, and, if, not</code> 等。</li>
<li><strong>Expert 4</strong> 处理<strong>视觉描述词</strong>（Visual Descriptions）：如 <code>dark, outer, yellow</code> 等。</li>
</ul>
</li>
</ul>
<p>更具体地说，他们的专长是在特定上下文中处理特定的标记（tokens）。</p>
<hr>
<p><strong>Router (gate network)</strong> 选择最适合给定输入的专家或专家组合：</p>
<img src="/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2/20250224_153924.png" class="">

<p><strong>图示内容</strong>：展示了 <strong>Router</strong> 如何在每一层根据输入选择合适的专家。图中高亮了被选中的专家，以及输入 token 的流动过程。<br><strong>图3详细说明</strong>：  </p>
<ol>
<li><strong>输入</strong>：图顶部的 Input 代表模型接收到的 token 或向量表示。  </li>
<li><strong>Router</strong>：位于网络结构中，起到决策作用。  </li>
<li><strong>专家选择</strong>：被选中的专家会接收输入，其余专家则不被激活。  </li>
<li><strong>输出</strong>：来自被激活专家的结果被汇总或继续流向下游层。</li>
</ol>
<p>需要注意的是，每个专家并不是整个 LLM，而是 <strong>LLM</strong> 架构中的一个子模型部分。</p>
<hr>
<h2 id="Experts"><a href="#Experts" class="headerlink" title="Experts"></a>Experts</h2><p>为了理解专家（<strong>Experts</strong>）是什么以及它们如何工作，我们先来看看 MoE 希望替代的东西：<strong>dense layers</strong>。</p>
<h3 id="Dense-Layers"><a href="#Dense-Layers" class="headerlink" title="Dense Layers"></a>Dense Layers</h3><p>所有的 <strong>Mixture of Experts (MoE)</strong> 都基于 LLM 中一个相对基础的功能：**Feedforward Neural Network (FFNN)**。</p>
<p>回忆一下，一个标准的 <strong>decoder-only Transformer</strong> 架构中，<strong>FFNN</strong> 通常是在 <strong>layer normalization</strong> 之后应用的：</p>
<img src="/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2/20250224_154729.png" class="">
<p><strong>图示内容</strong>：展示了一个典型的 <strong>decoder</strong> 结构，每个 <strong>decoder block</strong> 包含 <strong>Masked Self-Attention</strong> 和 <strong>FFNN</strong>（中间会有 <strong>Layer Norm</strong>）。  </p>
<ol>
<li><strong>Position Embedding</strong>：在输入 token 之前或同时加入位置编码信息。  </li>
<li><strong>Decoder Block</strong>：包含 <strong>Masked Self-Attention</strong>、<strong>Layer Norm</strong> 和 <strong>FFNN</strong>。  </li>
<li><strong>FFNN</strong>：在图中用紫色方块表示，是该层对输入进一步变换以捕捉更复杂关系的关键组件。</li>
</ol>
<p><strong>FFNN</strong> 可以利用注意力机制产生的上下文信息，对其进行进一步的转换，以捕捉数据中更复杂的关系。</p>
<p>不过，为了学习这些复杂关系，<strong>FFNN</strong> 的规模会随之增长，通常会在输入上进行扩张（例如，中间层维度会变大）：</p>
<img src="/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2/20250224_154923.png" class="">
<p><strong>图示内容</strong>：展示了一个 <strong>FFNN</strong> 的结构，输入先被映射到更高维度，然后再被映射回输出维度。  </p>
<ol>
<li><strong>输入维度</strong>：图中显示有 512 个输入单元。  </li>
<li><strong>隐藏层</strong>：通常会有 4 倍或更多的扩张（图中示例为 4 倍扩张到 2048 维）。  </li>
<li><strong>输出维度</strong>：再映射回 512 维的输出。</li>
</ol>
<h3 id="Sparse-Layers"><a href="#Sparse-Layers" class="headerlink" title="Sparse Layers"></a>Sparse Layers</h3><p>在传统的 Transformer 中，<strong>FFNN</strong> 称为 <strong>dense model</strong>，因为它的所有参数（权重和偏置）都会被激活。也就是说，模型的全部参数都参与计算输出。</p>
<p>如果我们仔细观察 <strong>dense model</strong>，可以看到输入会激活所有的参数：</p>
<img src="/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2/20250224_163850.png" class="">
<p><strong>图示内容</strong>：展示了一个“密集”模型，输入层的每个神经元都与隐藏层所有神经元相连，隐藏层所有神经元又与输出层神经元相连。<br><strong>图6详细说明</strong>：  </p>
<ol>
<li><strong>全连接</strong>：图中所有节点都连接到下一层的所有节点，表示无稀疏性。  </li>
<li><strong>所有参数被激活</strong>：没有任何“闲置”或“未激活”的参数。</li>
</ol>
<p>与之对比，<strong>sparse models</strong>（稀疏模型）只激活一部分总参数，这与 <strong>Mixture of Experts</strong> 密切相关。</p>
<p>为了说明这一点，我们可以把 <strong>dense model</strong> 切分成多个部分（即专家，<strong>experts</strong>），重新训练它，并且在推理（inference）时只激活其中一部分：</p>
<img src="/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2/20250224_164042.png" class="">

<p><strong>图示内容</strong>：将原本的密集模型分割成多个专家（Expert 1、Expert 2、Expert 3、Expert 4）。在推理阶段，只选择一部分专家进行激活。  </p>
<ol>
<li><strong>模型切分</strong>：原有的大网络被拆分成多个较小的“专家”。  </li>
<li><strong>稀疏激活</strong>：并不是所有专家都被激活，只有部分专家在某些输入下被激活。  </li>
<li><strong>好处</strong>：通过稀疏激活，可以在不显著增加计算成本的情况下，拥有更多的潜在参数容量。</li>
</ol>
<p>其核心思想是：在训练期间，每个专家学习不同的信息；在推理时，只用到与当前任务最相关的那些专家。</p>
<p>当我们提出一个问题时，就会选择最适合该任务的专家：</p>
<img src="/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2/20250224_170127.png" class="">

<p><strong>图示内容</strong>：展示了一个示例：当输入是 “What is 1 + 1?” 这样的数字相关问题时，路由器只激活与数字相关的专家。  </p>
<ol>
<li><strong>输入</strong>：一个表示算术问题的句子或 token。  </li>
<li><strong>专家选择</strong>：只激活 “Numbers” 领域的专家。  </li>
<li><strong>输出</strong>：专家给出结果 “2”。</li>
</ol>
<h3 id="What-does-an-Expert-Learn"><a href="#What-does-an-Expert-Learn" class="headerlink" title="What does an Expert Learn?"></a>What does an Expert Learn?</h3><p>正如前面所提到的，专家（<strong>Experts</strong>）往往学习到比整个领域更细致的知识。有人会觉得称它们为“专家”可能会带来误解，但这是因为每个专家往往只专注于某些特定类型的输入特征或上下文。</p>
<img src="/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2/20250224_170229.png" class="">

<p><strong>图示内容</strong>：展示了一个表格或对照，说明在某些情况下，不同的专家可能学习到不同的特征（比如标点符号、动词、数字等）。  </p>
<ol>
<li><strong>示例化专家</strong>：Punctuation、Conjunctions、Verbs、Numbers 等。  </li>
<li><strong>分层位置</strong>：不同专家可能出现在模型的不同层。  </li>
<li><strong>分配</strong>：某些 token 会路由到某些专家，以获得更有效的处理。</li>
</ol>
<p>在 <strong>decoder</strong> 模型中，专家之间可能没有那么明显的领域分工。然而，这并不意味着所有专家都完全相同。<br>在 <strong>Mixtral 8x7B</strong> 这篇论文中，有一个很好的示例：每个 token 会被标记为其首选专家，这些专家并不一定对应直观的语义领域，但在统计上表现出某些倾向。</p>
<img src="/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2/20250224_182219.png" class="">
<p>这张可视化示例还展示了，experts（专家）更倾向于关注句法（syntax），而不是特定的领域（domain）。因此，虽然 decoder experts（解码器专家）似乎并没有明确的“专业领域（specialism）”，但它们似乎会在某些特定类型的 tokens（标记）上被持续地使用。</p>
<p>在[图1]中，展示了一段关于 MoELayer 的示例代码或可视化结果，色块区分了不同部分，强调了<strong>专家（experts）与路由器（router）</strong>之间的关系。通过色块可以看出：</p>
<ul>
<li>experts 列表（在代码中用 nn.ModuleList 表示）包含了多个子网络（即多个 FFNN，Feed-Forward Neural Network，前馈神经网络）。</li>
<li>gate（门控网络，也称 router）负责选择哪些专家会被激活。</li>
<li>整体上可以看到，这些专家通常关注到输入句子的句法层面，而非特定主题或领域。</li>
</ul>
<h3 id="专家的架构（Architecture-of-Experts）"><a href="#专家的架构（Architecture-of-Experts）" class="headerlink" title="专家的架构（Architecture of Experts）"></a>专家的架构（Architecture of Experts）</h3></article><section class="jump-container is-flex is-justify-content-space-between my-6"><!-- em is empty placeholder--><a class="button is-default" href="/2025/03/06/NLP%20Insights/Differences%20in%20Padding%20Strategies%20Between%20Decoder-only%20and%20Encoder-only%20Models/" title="Differences in Padding Strategies Between Decoder-only and Encoder-only Models"><i class="iconfont icon-prev mr-2 has-text-grey"></i><span class="has-text-weight-semibold">Previous: Differences in Padding Strategies Between Decoder-only and Encoder-only Models</span></a><a class="button is-default" href="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/" title="推理 LLM 的可视化指南：探索推理时计算技术与 DeepSeek-R1"><span class="has-text-weight-semibold">Next: 推理 LLM 的可视化指南：探索推理时计算技术与 DeepSeek-R1</span><i class="iconfont icon-next ml-2 has-text-grey"></i></a></section><article class="mt-6 comment-container"><script async repo="Haojen/Claudia-theme-blog" src="https://utteranc.es/client.js" issue-term="pathname" theme="preferred-color-scheme"></script></article></div></div></main></main><footer class="is-flex is-flex-direction-column is-align-items-center is-flex-shrink-0 is-family-serif"><section class="sns-container"><a title="twitter" target="_blank" rel="noopener nofollow" href="//twitter.com//"><i class="iconfont icon-twitter"></i></a><!-- Github--><a title="github" target="_blank" rel="noopener nofollow" href="//github.com/haojen"><i class="iconfont icon-github"></i></a><!-- Ins--><a title="instagram" target="_blank" rel="noopener nofollow" href="//www.instagram.com//"><i class="iconfont icon-ins"></i></a><!-- RSS--><!-- 知乎--><!-- 领英--><!-- 脸书--><a title="facebook" target="_blank" rel="noopener nofollow" href="//www.facebook.com//"><i class="iconfont icon-tian7_facebook"></i></a></section><p><span>Copyright ©</span><span> user 2026</span></p><div class="is-flex is-justify-content-center is-flex-wrap-wrap"><p>Powered by Hexo &verbar;&nbsp;</p><p class="is-flex is-justify-content-center"><a title="Hexo theme author" target="_blank" rel="noopener" href="//github.com/haojen">Theme by Haojen&nbsp;</a></p><div style="margin-top: 2px"><a class="github-button" title="github-button" target="_blank" rel="noopener" href="https://github.com/haojen/hexo-theme-Claudia" data-color-scheme="no-preference: light; light: light; dark: dark;" data-show-count="true"></a></div></div><div><span></span></div></footer><script async defer src="https://buttons.github.io/buttons.js"></script><script src="/js/jquery-3.6.1.min.js"></script><script src="/js/jquery-fancybox.min.js"></script><script src="/js/img_zoom.js"></script><script src="/js/post.js"></script></body></html>