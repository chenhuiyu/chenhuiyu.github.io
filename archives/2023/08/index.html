<!DOCTYPE html><html class="appearance-auto" lang="zh-CN"><head><meta charset="UTF-8"><title>user's blog</title><meta name="description" content="May the Force be with you"><meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no, initial-scale=1"><!-- Google Analytics --><!-- End Google Analytics -->
<!-- Baidu Analytics --><!-- End Baidu Analytics --><link rel="icon" href="/images/favicon.ico"><link rel="stylesheet" href="/style/common/bulma.css"><link rel="stylesheet" href="/style/base.css"><link rel="stylesheet" href="/style/common/helper.css"><script src="/js/common.js"></script><link rel="stylesheet" href="/style/widget-post-list.css"><meta name="generator" content="Hexo 6.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="黑头呆鱼进化之旅" type="application/atom+xml">
</head><body class="is-flex is-flex-direction-column"><header class="header-widget is-flex-shrink-0 is-hidden-mobile"><div class="container is-fullhd is-flex is-justify-content-space-between is-align-items-center is-full-height"><section class="is-hidden-mobile is-flex-shrink-0"><h2><a href="/">user's blog</a></h2></section><h3 class="is-hidden-mobile is-family-serif is-full-height is-flex is-align-items-center is-flex-shrink-0"><span>归档 · 2023</span></h3><aside class="is-flex-shrink-0"><h3 class="is-inline-block"><a href="/">首页</a></h3><h3 class="is-inline-block"><a href="/about">关于</a></h3><h3 class="is-inline-block"><a href="/archives">归档</a></h3></aside></div></header><header class="is-flex header-widget is-flex-shrink-0 is-align-items-center is-justify-content-center is-hidden-tablet"><h3 class="is-inline-block"><a href="/">首页</a></h3><h3 class="is-inline-block"><a href="/about">关于</a></h3><h3 class="is-inline-block"><a href="/archives">归档</a></h3></header><main><article class="post-container is-flex is-justify-content-center section container is-max-widescreen pt-4 px-2"><div class="columns is-variable is-1-tablet is-3-desktop-only is-2-widescreen is-full-width"><section class="column"><article class="post-item-card"><section class="content post-card-content p-4 pb-5"><header><a href="/tags/Python%20Basic"><i class="tag post-item-tag">Python Basic</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2023/08/23/Code%20Chronicles/Python-Uninterrupted%20Remote%20Program%20Execution:%203%20Methods.en/">无断开烦恼！远程服务器后台运行程序的3种方法：`nohup`、`tmux`和`screen`</a></h2><time class="has-text-grey" datetime="2023-08-23T06:46:03.000Z">2023-08-23</time><p class="is-flex-grow-2 mt-2">无断开烦恼！远程服务器后台运行程序的3种方法：nohup、tmux和screenUninterrupted Remote Program Execution: 3 Methods在数据分析或机器学习项目中，经常需要在远程服务器上运行耗时长、计算密集型的任务。通过SSH连接到远程服务器是常见的操作方式。但是，如何确保在断开SSH连接之后，远程服务器上的程序能够继续运行呢？本文详细介绍了三种方法：nohup、tmux和screen。
使用nohup命令开始
连接到远程机器：在本地终端中执行以下命令。
 1ssh username@remote-server-address

启动后台程序：在远程机器上执行。
 1nohup your-command-to-run-the-program &amp;amp;

例如：
1..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2023/08/23/Code%20Chronicles/Python-Uninterrupted%20Remote%20Program%20Execution:%203%20Methods.en/">更多</a></section></article><article class="post-item-card"><section class="content post-card-content p-4 pb-5"><header><a href="/tags/Python%20Basic"><i class="tag post-item-tag">Python Basic</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2023/08/23/Code%20Chronicles/Python-Uninterrupted%20Remote%20Program%20Execution:%203%20Methods.zh-CN/">无断开烦恼！远程服务器后台运行程序的3种方法：`nohup`、`tmux`和`screen`</a></h2><time class="has-text-grey" datetime="2023-08-23T06:46:03.000Z">2023-08-23</time><p class="is-flex-grow-2 mt-2">无断开烦恼！远程服务器后台运行程序的3种方法：nohup、tmux和screenUninterrupted Remote Program Execution: 3 Methods在数据分析或机器学习项目中，经常需要在远程服务器上运行耗时长、计算密集型的任务。通过SSH连接到远程服务器是常见的操作方式。但是，如何确保在断开SSH连接之后，远程服务器上的程序能够继续运行呢？本文详细介绍了三种方法：nohup、tmux和screen。
使用nohup命令开始
连接到远程机器：在本地终端中执行以下命令。
 1ssh username@remote-server-address

启动后台程序：在远程机器上执行。
 1nohup your-command-to-run-the-program &amp;amp;

例如：
1..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2023/08/23/Code%20Chronicles/Python-Uninterrupted%20Remote%20Program%20Execution:%203%20Methods.zh-CN/">更多</a></section></article><article class="post-item-card"><section class="content post-card-content p-4 pb-5"><header><a href="/tags/Python%20Basic"><i class="tag post-item-tag">Python Basic</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2023/08/22/Code%20Chronicles/Python-subprocess.en/">超越Python的边界：`subprocess` 助你一键执行外部命令</a></h2><time class="has-text-grey" datetime="2023-08-22T11:06:03.000Z">2023-08-22</time><p class="is-flex-grow-2 mt-2">超越Python的边界：subprocess 助你一键执行外部命令在日常开发中，有时候我们希望能够从 Python 脚本中执行系统命令或者其他程序。Python 提供了 subprocess 模块，使得这一操作变得既简单又安全。
介绍subprocess 模块是 Python 标准库的一部分，它提供了一种简单统一的方法来执行外部命令，与进程交互，读取它的输出，并获取它的返回码。无论你是在自动化某个系统任务，还是简单地想要从另一个程序中获取数据，subprocess 都能助你一臂之力。
功能与用途
执行外部命令：你可以轻易地从 Python 脚本中运行任何外部命令，就像在命令行中输入命令一样。这种能力使得你能够在你的 Python 程序中调用并集成其他命令行工具，扩展你的应用的功能。

捕获命令的输出：如果你..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2023/08/22/Code%20Chronicles/Python-subprocess.en/">更多</a></section></article><article class="post-item-card"><section class="content post-card-content p-4 pb-5"><header><a href="/tags/Python%20Basic"><i class="tag post-item-tag">Python Basic</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2023/08/22/Code%20Chronicles/Python-subprocess.zh-CN/">超越Python的边界：`subprocess` 助你一键执行外部命令</a></h2><time class="has-text-grey" datetime="2023-08-22T11:06:03.000Z">2023-08-22</time><p class="is-flex-grow-2 mt-2">超越Python的边界：subprocess 助你一键执行外部命令在日常开发中，有时候我们希望能够从 Python 脚本中执行系统命令或者其他程序。Python 提供了 subprocess 模块，使得这一操作变得既简单又安全。
介绍subprocess 模块是 Python 标准库的一部分，它提供了一种简单统一的方法来执行外部命令，与进程交互，读取它的输出，并获取它的返回码。无论你是在自动化某个系统任务，还是简单地想要从另一个程序中获取数据，subprocess 都能助你一臂之力。
功能与用途
执行外部命令：你可以轻易地从 Python 脚本中运行任何外部命令，就像在命令行中输入命令一样。这种能力使得你能够在你的 Python 程序中调用并集成其他命令行工具，扩展你的应用的功能。

捕获命令的输出：如果你..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2023/08/22/Code%20Chronicles/Python-subprocess.zh-CN/">更多</a></section></article><article class="post-item-card"><section class="content post-card-content p-4 pb-5"><header><a href="/tags/Onnx"><i class="tag post-item-tag">Onnx</i></a><a href="/tags/Deployment"><i class="tag post-item-tag">Deployment</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2023/08/21/NLP%20Insights/Conver%20Pytorch%20Model%20to%20ONNX%20Format.en/">Conver Pytorch Model to ONNX Format</a></h2><time class="has-text-grey" datetime="2023-08-21T06:34:18.000Z">2023-08-21</time><p class="is-flex-grow-2 mt-2">使用 PyTorch 和 ONNX 检查模型一致性在机器学习和深度学习的开发过程中，模型的互操作性变得越来越重要。ONNX (Open Neural Network Exchange) 是一种开放格式，用于表示机器学习和深度学习模型。它允许开发者在各种深度学习框架之间轻松地共享模型，从而提高了模型的可移植性和互操作性。
本教程将指导您完成以下步骤：

将 PyTorch 模型转换为 ONNX 格式。
验证转换后的 ONNX 模型与原始 PyTorch 模型的输出是否一致。

1. 导入必要的库首先，我们导入为模型转换和验证所需的所有库。
123456import osimport sysimport torchimport onnximport onnxruntimeimport numpy as np

..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2023/08/21/NLP%20Insights/Conver%20Pytorch%20Model%20to%20ONNX%20Format.en/">更多</a></section></article><article class="post-item-card"><section class="content post-card-content p-4 pb-5"><header><a href="/tags/Onnx"><i class="tag post-item-tag">Onnx</i></a><a href="/tags/Deployment"><i class="tag post-item-tag">Deployment</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2023/08/21/NLP%20Insights/Conver%20Pytorch%20Model%20to%20ONNX%20Format.zh-CN/">Conver Pytorch Model to ONNX Format</a></h2><time class="has-text-grey" datetime="2023-08-21T06:34:18.000Z">2023-08-21</time><p class="is-flex-grow-2 mt-2">使用 PyTorch 和 ONNX 检查模型一致性在机器学习和深度学习的开发过程中，模型的互操作性变得越来越重要。ONNX (Open Neural Network Exchange) 是一种开放格式，用于表示机器学习和深度学习模型。它允许开发者在各种深度学习框架之间轻松地共享模型，从而提高了模型的可移植性和互操作性。
本教程将指导您完成以下步骤：

将 PyTorch 模型转换为 ONNX 格式。
验证转换后的 ONNX 模型与原始 PyTorch 模型的输出是否一致。

1. 导入必要的库首先，我们导入为模型转换和验证所需的所有库。
123456import osimport sysimport torchimport onnximport onnxruntimeimport numpy as np

..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2023/08/21/NLP%20Insights/Conver%20Pytorch%20Model%20to%20ONNX%20Format.zh-CN/">更多</a></section></article><article class="post-item-card"><section class="content post-card-content p-4 pb-5"><header><a href="/tags/LLM"><i class="tag post-item-tag">LLM</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2023/08/02/NLP%20Insights/LLAMA2.en/">Training Llama 2 Model on Single GPU with int8 Quantization and LoRA</a></h2><time class="has-text-grey" datetime="2023-08-02T07:38:29.000Z">2023-08-02</time><p class="is-flex-grow-2 mt-2">Training Llama 2 Model on Single GPU with int8 Quantization and LoRALlama 2概述Llama 2 是一个包含预训练和微调的生成式文本模型的集合，其规模从 70 亿到 700 亿个参数不等。Llama2模型是由Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert等人在Llama 2: Open Foundation and Fine-Tuned Chat Models中提出的。
该论文的摘要如下：
在这项工作中，我们开发并发布了Llama 2，这是一组从70亿到700亿参数的预训练和微调的大型语言模型（LLMs）。我们的微调LLMs，称为Llama 2-Chat，针对对话用例进行了优化..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2023/08/02/NLP%20Insights/LLAMA2.en/">更多</a></section></article><article class="post-item-card"><section class="content post-card-content p-4 pb-5"><header><a href="/tags/LLM"><i class="tag post-item-tag">LLM</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2023/08/02/NLP%20Insights/LLAMA2.zh-CN/">Training Llama 2 Model on Single GPU with int8 Quantization and LoRA</a></h2><time class="has-text-grey" datetime="2023-08-02T07:38:29.000Z">2023-08-02</time><p class="is-flex-grow-2 mt-2">Training Llama 2 Model on Single GPU with int8 Quantization and LoRALlama 2概述Llama 2 是一个包含预训练和微调的生成式文本模型的集合，其规模从 70 亿到 700 亿个参数不等。Llama2模型是由Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert等人在Llama 2: Open Foundation and Fine-Tuned Chat Models中提出的。
该论文的摘要如下：
在这项工作中，我们开发并发布了Llama 2，这是一组从70亿到700亿参数的预训练和微调的大型语言模型（LLMs）。我们的微调LLMs，称为Llama 2-Chat，针对对话用例进行了优化..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2023/08/02/NLP%20Insights/LLAMA2.zh-CN/">更多</a></section></article></section><aside class="column is-hidden-mobile is-4-tablet is-3-widescreen"><div style="position: sticky; top: 50px;"><main class="aside-card-container archives-widget is-in-archive-page"><h3>归档</h3><section><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2026/02/">二月 2026</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/06/">六月 2025</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/03/">三月 2025</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/02/">二月 2025</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/12/">十二月 2024</a><span class="archive-list-count">14</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/10/">十月 2024</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/08/">八月 2024</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/04/">四月 2024</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/03/">三月 2024</a><span class="archive-list-count">12</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2024/02/">二月 2024</a><span class="archive-list-count">22</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/09/">九月 2023</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/08/">八月 2023</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/07/">七月 2023</a><span class="archive-list-count">19</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/06/">六月 2023</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/05/">五月 2023</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/02/">二月 2023</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/01/">一月 2023</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/08/">八月 2022</a><span class="archive-list-count">2</span></li></ul></section></main></div></aside></div></article><script>$claudia.fadeInImage(null, $claudia.blurBackdropImg)

window.addEventListener('resize', $claudia.throttle(function () {
    var images = document.querySelectorAll('.js-img-fadeIn')

    images.forEach($claudia.blurBackdropImg)
}, 150))</script></main><footer class="is-flex is-flex-direction-column is-align-items-center is-flex-shrink-0 is-family-serif"><section class="sns-container"><a title="twitter" target="_blank" rel="noopener nofollow" href="//twitter.com//"><i class="iconfont icon-twitter"></i></a><!-- Github--><a title="github" target="_blank" rel="noopener nofollow" href="//github.com/haojen"><i class="iconfont icon-github"></i></a><!-- Ins--><a title="instagram" target="_blank" rel="noopener nofollow" href="//www.instagram.com//"><i class="iconfont icon-ins"></i></a><!-- RSS--><!-- 知乎--><!-- 领英--><!-- 脸书--><a title="facebook" target="_blank" rel="noopener nofollow" href="//www.facebook.com//"><i class="iconfont icon-tian7_facebook"></i></a></section><p><span>Copyright ©</span><span> user 2026</span></p><div class="is-flex is-justify-content-center is-flex-wrap-wrap"><p>Powered by Hexo &verbar;&nbsp;</p><p class="is-flex is-justify-content-center"><a title="Hexo theme author" target="_blank" rel="noopener" href="//github.com/haojen">Theme by Haojen&nbsp;</a></p><div style="margin-top: 2px"><a class="github-button" title="github-button" target="_blank" rel="noopener" href="https://github.com/haojen/hexo-theme-Claudia" data-color-scheme="no-preference: light; light: light; dark: dark;" data-show-count="true"></a></div></div><div><span></span></div></footer><script src="/js/lang-switch.js"></script><script async defer src="https://buttons.github.io/buttons.js"></script><script>$claudia.fadeInImage(null, $claudia.blurBackdropImg)

window.addEventListener('resize', $claudia.throttle(function () {
    var images = document.querySelectorAll('.js-img-fadeIn')

    images.forEach($claudia.blurBackdropImg)
}, 150))</script></body></html>