<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>黑头呆鱼进化之旅</title>
  
  <subtitle>只身打码过草原</subtitle>
  <link href="https://chenhuiyu.github.io/atom.xml" rel="self"/>
  
  <link href="https://chenhuiyu.github.io/"/>
  <updated>2026-02-20T21:52:05.310Z</updated>
  <id>https://chenhuiyu.github.io/</id>
  
  <author>
    <name>Huiyu Chen</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Evaluation of Generation-Based Large Language Models (LLMs): Opportunities and Challenges from Generation to Judgment</title>
    <link href="https://chenhuiyu.github.io/2026/02/21/NLP%20Insights/Evaluation%20of%20Generation-Based%20Large%20Language%20Models%20(LLMs):%20Opportunities%20and%20Challenges%20from%20Generation%20to%20Judgment.en/"/>
    <id>https://chenhuiyu.github.io/2026/02/21/NLP%20Insights/Evaluation%20of%20Generation-Based%20Large%20Language%20Models%20(LLMs):%20Opportunities%20and%20Challenges%20from%20Generation%20to%20Judgment.en/</id>
    <published>2026-02-20T22:00:00.000Z</published>
    <updated>2026-02-20T21:52:05.310Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Evaluation-of-Generation-Based-Large-Language-Models-LLMs-Opportunities-and-Challenges-from-Generation-to-Judgment"><a href="#Evaluation-of-Generation-Based-Large-Language-Models-LLMs-Opportunities-and-Challenges-from-Generation-to-Judgment" class="headerlink" title="Evaluation of Generation-Based Large Language Models (LLMs): Opportunities and Challenges from Generation to Judgment"></a>Evaluation of Generation-Based Large Language Models (LLMs): Opportunities and Challenges from Generation to Judgment</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>Evaluation tasks in artificial intelligence (AI) and natural language processing (NLP) have long been challenging. Traditional evaluation methods, such as those based on matching or embeddings, are limited in assessing complex attributes. The recent development of large language models (LLMs) has given rise to the “LLM-as-a-Judge” paradigm, which utilizes LLMs for scoring, ranking, or selection tasks. This paper provides a comprehensive review of LLM evaluation methodologies, including their definitions, classification frameworks, benchmarks, and future research directions.</p><hr><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><h3 id="1-1-Background"><a href="#1-1-Background" class="headerlink" title="1.1 Background"></a>1.1 Background</h3><p>Evaluation is one of the core issues in machine learning and NLP. Traditional evaluation methods such as BLEU and ROUGE often rely on text overlap and lack applicability in complex scenarios. With the development of deep learning and LLMs (e.g., GPT-4), researchers have proposed the “LLM-as-a-Judge” paradigm to address the limitations of traditional evaluation methods.</p><h3 id="1-2-Research-Questions"><a href="#1-2-Research-Questions" class="headerlink" title="1.2 Research Questions"></a>1.2 Research Questions</h3><p>This paper aims to explore the following questions:</p><ul><li><strong>What do LLMs evaluate?</strong></li><li><strong>How is evaluation conducted?</strong></li><li><strong>Where are LLMs applied for evaluation?</strong></li></ul><hr><h2 id="2-Preliminary-Knowledge"><a href="#2-Preliminary-Knowledge" class="headerlink" title="2. Preliminary Knowledge"></a>2. Preliminary Knowledge</h2><h3 id="2-1-Input-Formats"><a href="#2-1-Input-Formats" class="headerlink" title="2.1 Input Formats"></a>2.1 Input Formats</h3><p>Evaluation inputs can be categorized as follows:</p><ul><li><strong>Point-Wise</strong>: Evaluation of a single sample.</li><li><strong>Pair/List-Wise</strong>: Comparative evaluation of multiple samples.</li></ul><h3 id="2-2-Output-Formats"><a href="#2-2-Output-Formats" class="headerlink" title="2.2 Output Formats"></a>2.2 Output Formats</h3><p>Evaluation outputs include:</p><ul><li><strong>Scores</strong>: Quantitative scoring of samples.</li><li><strong>Ranking</strong>: Ordering based on merit.</li><li><strong>Selection</strong>: Choosing the best option among candidates.</li></ul><hr><h2 id="3-Evaluation-Attributes"><a href="#3-Evaluation-Attributes" class="headerlink" title="3. Evaluation Attributes"></a>3. Evaluation Attributes</h2><h3 id="3-1-Helpfulness"><a href="#3-1-Helpfulness" class="headerlink" title="3.1 Helpfulness"></a>3.1 Helpfulness</h3><p>LLMs evaluate the helpfulness of responses by guiding user tasks and generating feedback, which is crucial in AI alignment.</p><h3 id="3-2-Harmlessness"><a href="#3-2-Harmlessness" class="headerlink" title="3.2 Harmlessness"></a>3.2 Harmlessness</h3><p>Evaluating the harmlessness of text is key to generating safe content. LLMs assist in data labeling or directly assess potential harmful content.</p><h3 id="3-3-Reliability"><a href="#3-3-Reliability" class="headerlink" title="3.3 Reliability"></a>3.3 Reliability</h3><p>LLMs detect factual accuracy and consistency, e.g., generating supporting evidence or conducting conversation-level reliability evaluations.</p><h3 id="3-4-Relevance"><a href="#3-4-Relevance" class="headerlink" title="3.4 Relevance"></a>3.4 Relevance</h3><p>LLMs assess the relevance of generated or retrieved content, applicable in scenarios like conversations and retrieval-augmented generation (RAG).</p><h3 id="3-5-Feasibility"><a href="#3-5-Feasibility" class="headerlink" title="3.5 Feasibility"></a>3.5 Feasibility</h3><p>In complex tasks, LLMs judge the feasibility of candidate steps or actions to optimize decision paths.</p><h3 id="3-6-Overall-Quality"><a href="#3-6-Overall-Quality" class="headerlink" title="3.6 Overall Quality"></a>3.6 Overall Quality</h3><p>By scoring across multiple dimensions, LLMs provide an overall evaluation, suitable for comprehensive comparisons in generation tasks.</p><hr><h3 id="4-Methodology"><a href="#4-Methodology" class="headerlink" title="4. Methodology"></a>4. Methodology</h3><h4 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h4><p>The methodology section focuses on optimizing the capabilities of LLMs as evaluators (LLM-as-a-Judge) through two approaches: fine-tuning and prompt engineering.</p><ol><li><strong>Fine-Tuning Techniques</strong>: Enhancing LLM judgment capabilities using supervised fine-tuning (SFT) and preference learning with labeled or synthetic feedback.</li><li><strong>Prompt Engineering</strong>: Designing effective prompt strategies, such as operation swapping, rule enhancement, and multi-agent collaboration, to improve inference and evaluation accuracy and reliability.</li></ol><hr><h4 id="4-1-Fine-Tuning-Techniques"><a href="#4-1-Fine-Tuning-Techniques" class="headerlink" title="4.1 Fine-Tuning Techniques"></a>4.1 Fine-Tuning Techniques</h4><h5 id="Data-Sources"><a href="#Data-Sources" class="headerlink" title="Data Sources"></a>Data Sources</h5><h6 id="1-Human-Labeled-Data"><a href="#1-Human-Labeled-Data" class="headerlink" title="1. Human-Labeled Data"></a>1. <strong>Human-Labeled Data</strong></h6><p>Human-labeled data provides high-quality training samples that help LLMs learn human preferences. Key studies and innovations include:</p><ol><li><p><strong>PandaLM</strong> [Wang et al., 2024h]:</p><ul><li>Collected a diverse dataset with 300,000 samples for instruction-generation tasks.</li><li>Enhanced generalization by integrating data sources like open-domain QA and dialogue generation.</li><li>Introduced standardized annotation workflows for consistency and emphasized multilingual support.</li></ul></li><li><p><strong>AspectInstruct</strong> [Liu et al., 2024a]:</p><ul><li>Introduced a dataset tailored for multi-dimensional evaluation, covering 65 tasks and 27 evaluation dimensions.</li><li>Designed a unique task segmentation mechanism for contextual understanding and dimension prioritization.</li></ul></li></ol><h6 id="2-Synthetic-Data"><a href="#2-Synthetic-Data" class="headerlink" title="2. Synthetic Data"></a>2. <strong>Synthetic Data</strong></h6><p>Synthetic data generated by LLMs reduces dependency on human labeling and expands data coverage. Key studies and innovations include:</p><ol><li><p><strong>JudgeLM</strong> [Zhu et al., 2023]:</p><ul><li>Generated a dataset with 100,000 samples, covering various instruction-generation scenarios.</li><li>Introduced task-seeding methods to ensure diversity and specificity.</li></ul></li><li><p><strong>Meta-Rewarding</strong> [Wu et al., 2024]:</p><ul><li>Proposed “meta-rewarding,” using LLM self-evaluation signals to enhance training effectiveness.</li></ul></li></ol><h5 id="Fine-Tuning-Methods"><a href="#Fine-Tuning-Methods" class="headerlink" title="Fine-Tuning Methods"></a>Fine-Tuning Methods</h5><h6 id="1-Supervised-Fine-Tuning-SFT"><a href="#1-Supervised-Fine-Tuning-SFT" class="headerlink" title="1. Supervised Fine-Tuning (SFT)"></a>1. <strong>Supervised Fine-Tuning (SFT)</strong></h6><p>SFT trains LLMs using human-labeled or synthetic data to learn evaluation criteria. Key studies include:</p><ol><li><p><strong>FLAMe</strong> [Vu et al., 2024]:</p><ul><li>Leveraged a multi-task learning framework with 5 million samples for multi-task SFT.</li><li>Unified evaluation standards across diverse tasks.</li></ul></li><li><p><strong>JSFT</strong> [Lee et al., 2024]:</p><ul><li>Combined SFT with preference learning to optimize performance on diverse evaluation tasks.</li></ul></li></ol><h6 id="2-Preference-Learning"><a href="#2-Preference-Learning" class="headerlink" title="2. Preference Learning"></a>2. <strong>Preference Learning</strong></h6><p>Preference learning optimizes LLM comparison and ranking capabilities for complex evaluations. Key studies include:</p><ol><li><p><strong>HALU-J</strong> [Wang et al., 2024a]:</p><ul><li>Employed directed preference optimization (DPO) with multi-evidence selection mechanisms.</li></ul></li><li><p><strong>Self-Taught Evaluators</strong> [Wang et al., 2024f]:</p><ul><li>Used self-generated suboptimal responses as negative samples for dynamic improvement.</li></ul></li></ol><hr><h3 id="5-Applications"><a href="#5-Applications" class="headerlink" title="5. Applications"></a>5. Applications</h3><h4 id="Overview-1"><a href="#Overview-1" class="headerlink" title="Overview"></a>Overview</h4><p>The applications of LLM-as-a-Judge have expanded from generation evaluation to alignment, retrieval, and reasoning. This section systematically introduces these applications, their specific tasks, and representative studies.</p><hr><h4 id="5-1-Evaluation"><a href="#5-1-Evaluation" class="headerlink" title="5.1 Evaluation"></a>5.1 Evaluation</h4><h5 id="Overview-2"><a href="#Overview-2" class="headerlink" title="Overview"></a>Overview</h5><p>LLM-as-a-Judge was initially applied to evaluation tasks like dialogue generation and summarization. Key studies include:</p><ol><li><p><strong>MD-Judge</strong> [Li et al., 2024f]:</p><ul><li>Evaluated safety-related Q&amp;A frameworks, focusing on harmfulness and ethical risks.</li></ul></li><li><p><strong>Chan Framework</strong> [Chan et al., 2023]:</p><ul><li>Introduced a multi-agent debate framework for improved evaluation quality.</li></ul></li><li><p><strong>ICE</strong> [Jain et al., 2023b]:</p><ul><li>Used few-shot examples for interactive multi-dimensional evaluation.</li></ul></li></ol><hr><h3 id="7-Challenges-and-Future-Directions"><a href="#7-Challenges-and-Future-Directions" class="headerlink" title="7. Challenges and Future Directions"></a>7. Challenges and Future Directions</h3><h4 id="Overview-3"><a href="#Overview-3" class="headerlink" title="Overview"></a>Overview</h4><p>Despite its powerful capabilities, LLM-as-a-Judge faces challenges such as evaluation bias, adaptability to dynamic tasks, and the potential of human-AI collaborative evaluation. This section explores these challenges and outlines future research directions.</p><h5 id="7-1-Bias-and-Vulnerabilities"><a href="#7-1-Bias-and-Vulnerabilities" class="headerlink" title="7.1 Bias and Vulnerabilities"></a>7.1 Bias and Vulnerabilities</h5><ol><li><strong>OffsetBias</strong> [Park et al., 2024]:<ul><li>Proposed a de-biasing framework to mitigate positional and content biases.</li></ul></li></ol><h5 id="7-2-Dynamic-and-Complex-Evaluations"><a href="#7-2-Dynamic-and-Complex-Evaluations" class="headerlink" title="7.2 Dynamic and Complex Evaluations"></a>7.2 Dynamic and Complex Evaluations</h5><ol><li><strong>Tree of Thought (ToT)</strong> [Yao et al., 2023a]:<ul><li>Enhanced multi-step reasoning with dynamic state evaluation mechanisms.</li></ul></li></ol><h5 id="7-3-Self-Evaluation-and-Human-AI-Collaboration"><a href="#7-3-Self-Evaluation-and-Human-AI-Collaboration" class="headerlink" title="7.3 Self-Evaluation and Human-AI Collaboration"></a>7.3 Self-Evaluation and Human-AI Collaboration</h5><ol><li><p><strong>Self-Taught Evaluators</strong> [Wang et al., 2024f]:</p><ul><li>Highlighted the potential for models to improve through self-learning mechanisms.</li></ul></li><li><p><strong>Meta-Rewarding</strong> [Wu et al., 2024]:</p><ul><li>Demonstrated the advantages of integrating self-evaluation signals into optimization.</li></ul></li></ol><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Evaluation-of-Generation-Based-Large-Language-Models-LLMs-Opportunities-and-Challenges-from-Generation-to-Judgment&quot;&gt;&lt;a href=&quot;#Evalua</summary>
      
    
    
    
    <category term="NLP Insights" scheme="https://chenhuiyu.github.io/categories/NLP-Insights/"/>
    
    
  </entry>
  
  <entry>
    <title>Evaluation of Generation-Based Large Language Models (LLMs): Opportunities and Challenges from Generation to Judgment</title>
    <link href="https://chenhuiyu.github.io/2026/02/21/NLP%20Insights/Evaluation%20of%20Generation-Based%20Large%20Language%20Models%20(LLMs):%20Opportunities%20and%20Challenges%20from%20Generation%20to%20Judgment/"/>
    <id>https://chenhuiyu.github.io/2026/02/21/NLP%20Insights/Evaluation%20of%20Generation-Based%20Large%20Language%20Models%20(LLMs):%20Opportunities%20and%20Challenges%20from%20Generation%20to%20Judgment/</id>
    <published>2026-02-20T22:00:00.000Z</published>
    <updated>2026-02-20T21:51:54.079Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Evaluation-of-Generation-Based-Large-Language-Models-LLMs-Opportunities-and-Challenges-from-Generation-to-Judgment"><a href="#Evaluation-of-Generation-Based-Large-Language-Models-LLMs-Opportunities-and-Challenges-from-Generation-to-Judgment" class="headerlink" title="Evaluation of Generation-Based Large Language Models (LLMs): Opportunities and Challenges from Generation to Judgment"></a>Evaluation of Generation-Based Large Language Models (LLMs): Opportunities and Challenges from Generation to Judgment</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>Evaluation tasks in artificial intelligence (AI) and natural language processing (NLP) have long been challenging. Traditional evaluation methods, such as those based on matching or embeddings, are limited in assessing complex attributes. The recent development of large language models (LLMs) has given rise to the “LLM-as-a-Judge” paradigm, which utilizes LLMs for scoring, ranking, or selection tasks. This paper provides a comprehensive review of LLM evaluation methodologies, including their definitions, classification frameworks, benchmarks, and future research directions.</p><hr><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><h3 id="1-1-Background"><a href="#1-1-Background" class="headerlink" title="1.1 Background"></a>1.1 Background</h3><p>Evaluation is one of the core issues in machine learning and NLP. Traditional evaluation methods such as BLEU and ROUGE often rely on text overlap and lack applicability in complex scenarios. With the development of deep learning and LLMs (e.g., GPT-4), researchers have proposed the “LLM-as-a-Judge” paradigm to address the limitations of traditional evaluation methods.</p><h3 id="1-2-Research-Questions"><a href="#1-2-Research-Questions" class="headerlink" title="1.2 Research Questions"></a>1.2 Research Questions</h3><p>This paper aims to explore the following questions:</p><ul><li><strong>What do LLMs evaluate?</strong></li><li><strong>How is evaluation conducted?</strong></li><li><strong>Where are LLMs applied for evaluation?</strong></li></ul><hr><h2 id="2-Preliminary-Knowledge"><a href="#2-Preliminary-Knowledge" class="headerlink" title="2. Preliminary Knowledge"></a>2. Preliminary Knowledge</h2><h3 id="2-1-Input-Formats"><a href="#2-1-Input-Formats" class="headerlink" title="2.1 Input Formats"></a>2.1 Input Formats</h3><p>Evaluation inputs can be categorized as follows:</p><ul><li><strong>Point-Wise</strong>: Evaluation of a single sample.</li><li><strong>Pair/List-Wise</strong>: Comparative evaluation of multiple samples.</li></ul><h3 id="2-2-Output-Formats"><a href="#2-2-Output-Formats" class="headerlink" title="2.2 Output Formats"></a>2.2 Output Formats</h3><p>Evaluation outputs include:</p><ul><li><strong>Scores</strong>: Quantitative scoring of samples.</li><li><strong>Ranking</strong>: Ordering based on merit.</li><li><strong>Selection</strong>: Choosing the best option among candidates.</li></ul><hr><h2 id="3-Evaluation-Attributes"><a href="#3-Evaluation-Attributes" class="headerlink" title="3. Evaluation Attributes"></a>3. Evaluation Attributes</h2><h3 id="3-1-Helpfulness"><a href="#3-1-Helpfulness" class="headerlink" title="3.1 Helpfulness"></a>3.1 Helpfulness</h3><p>LLMs evaluate the helpfulness of responses by guiding user tasks and generating feedback, which is crucial in AI alignment.</p><h3 id="3-2-Harmlessness"><a href="#3-2-Harmlessness" class="headerlink" title="3.2 Harmlessness"></a>3.2 Harmlessness</h3><p>Evaluating the harmlessness of text is key to generating safe content. LLMs assist in data labeling or directly assess potential harmful content.</p><h3 id="3-3-Reliability"><a href="#3-3-Reliability" class="headerlink" title="3.3 Reliability"></a>3.3 Reliability</h3><p>LLMs detect factual accuracy and consistency, e.g., generating supporting evidence or conducting conversation-level reliability evaluations.</p><h3 id="3-4-Relevance"><a href="#3-4-Relevance" class="headerlink" title="3.4 Relevance"></a>3.4 Relevance</h3><p>LLMs assess the relevance of generated or retrieved content, applicable in scenarios like conversations and retrieval-augmented generation (RAG).</p><h3 id="3-5-Feasibility"><a href="#3-5-Feasibility" class="headerlink" title="3.5 Feasibility"></a>3.5 Feasibility</h3><p>In complex tasks, LLMs judge the feasibility of candidate steps or actions to optimize decision paths.</p><h3 id="3-6-Overall-Quality"><a href="#3-6-Overall-Quality" class="headerlink" title="3.6 Overall Quality"></a>3.6 Overall Quality</h3><p>By scoring across multiple dimensions, LLMs provide an overall evaluation, suitable for comprehensive comparisons in generation tasks.</p><hr><h3 id="4-Methodology"><a href="#4-Methodology" class="headerlink" title="4. Methodology"></a>4. Methodology</h3><h4 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h4><p>The methodology section focuses on optimizing the capabilities of LLMs as evaluators (LLM-as-a-Judge) through two approaches: fine-tuning and prompt engineering.</p><ol><li><strong>Fine-Tuning Techniques</strong>: Enhancing LLM judgment capabilities using supervised fine-tuning (SFT) and preference learning with labeled or synthetic feedback.</li><li><strong>Prompt Engineering</strong>: Designing effective prompt strategies, such as operation swapping, rule enhancement, and multi-agent collaboration, to improve inference and evaluation accuracy and reliability.</li></ol><hr><h4 id="4-1-Fine-Tuning-Techniques"><a href="#4-1-Fine-Tuning-Techniques" class="headerlink" title="4.1 Fine-Tuning Techniques"></a>4.1 Fine-Tuning Techniques</h4><h5 id="Data-Sources"><a href="#Data-Sources" class="headerlink" title="Data Sources"></a>Data Sources</h5><h6 id="1-Human-Labeled-Data"><a href="#1-Human-Labeled-Data" class="headerlink" title="1. Human-Labeled Data"></a>1. <strong>Human-Labeled Data</strong></h6><p>Human-labeled data provides high-quality training samples that help LLMs learn human preferences. Key studies and innovations include:</p><ol><li><p><strong>PandaLM</strong> [Wang et al., 2024h]:</p><ul><li>Collected a diverse dataset with 300,000 samples for instruction-generation tasks.</li><li>Enhanced generalization by integrating data sources like open-domain QA and dialogue generation.</li><li>Introduced standardized annotation workflows for consistency and emphasized multilingual support.</li></ul></li><li><p><strong>AspectInstruct</strong> [Liu et al., 2024a]:</p><ul><li>Introduced a dataset tailored for multi-dimensional evaluation, covering 65 tasks and 27 evaluation dimensions.</li><li>Designed a unique task segmentation mechanism for contextual understanding and dimension prioritization.</li></ul></li></ol><h6 id="2-Synthetic-Data"><a href="#2-Synthetic-Data" class="headerlink" title="2. Synthetic Data"></a>2. <strong>Synthetic Data</strong></h6><p>Synthetic data generated by LLMs reduces dependency on human labeling and expands data coverage. Key studies and innovations include:</p><ol><li><p><strong>JudgeLM</strong> [Zhu et al., 2023]:</p><ul><li>Generated a dataset with 100,000 samples, covering various instruction-generation scenarios.</li><li>Introduced task-seeding methods to ensure diversity and specificity.</li></ul></li><li><p><strong>Meta-Rewarding</strong> [Wu et al., 2024]:</p><ul><li>Proposed “meta-rewarding,” using LLM self-evaluation signals to enhance training effectiveness.</li></ul></li></ol><h5 id="Fine-Tuning-Methods"><a href="#Fine-Tuning-Methods" class="headerlink" title="Fine-Tuning Methods"></a>Fine-Tuning Methods</h5><h6 id="1-Supervised-Fine-Tuning-SFT"><a href="#1-Supervised-Fine-Tuning-SFT" class="headerlink" title="1. Supervised Fine-Tuning (SFT)"></a>1. <strong>Supervised Fine-Tuning (SFT)</strong></h6><p>SFT trains LLMs using human-labeled or synthetic data to learn evaluation criteria. Key studies include:</p><ol><li><p><strong>FLAMe</strong> [Vu et al., 2024]:</p><ul><li>Leveraged a multi-task learning framework with 5 million samples for multi-task SFT.</li><li>Unified evaluation standards across diverse tasks.</li></ul></li><li><p><strong>JSFT</strong> [Lee et al., 2024]:</p><ul><li>Combined SFT with preference learning to optimize performance on diverse evaluation tasks.</li></ul></li></ol><h6 id="2-Preference-Learning"><a href="#2-Preference-Learning" class="headerlink" title="2. Preference Learning"></a>2. <strong>Preference Learning</strong></h6><p>Preference learning optimizes LLM comparison and ranking capabilities for complex evaluations. Key studies include:</p><ol><li><p><strong>HALU-J</strong> [Wang et al., 2024a]:</p><ul><li>Employed directed preference optimization (DPO) with multi-evidence selection mechanisms.</li></ul></li><li><p><strong>Self-Taught Evaluators</strong> [Wang et al., 2024f]:</p><ul><li>Used self-generated suboptimal responses as negative samples for dynamic improvement.</li></ul></li></ol><hr><h3 id="5-Applications"><a href="#5-Applications" class="headerlink" title="5. Applications"></a>5. Applications</h3><h4 id="Overview-1"><a href="#Overview-1" class="headerlink" title="Overview"></a>Overview</h4><p>The applications of LLM-as-a-Judge have expanded from generation evaluation to alignment, retrieval, and reasoning. This section systematically introduces these applications, their specific tasks, and representative studies.</p><hr><h4 id="5-1-Evaluation"><a href="#5-1-Evaluation" class="headerlink" title="5.1 Evaluation"></a>5.1 Evaluation</h4><h5 id="Overview-2"><a href="#Overview-2" class="headerlink" title="Overview"></a>Overview</h5><p>LLM-as-a-Judge was initially applied to evaluation tasks like dialogue generation and summarization. Key studies include:</p><ol><li><p><strong>MD-Judge</strong> [Li et al., 2024f]:</p><ul><li>Evaluated safety-related Q&amp;A frameworks, focusing on harmfulness and ethical risks.</li></ul></li><li><p><strong>Chan Framework</strong> [Chan et al., 2023]:</p><ul><li>Introduced a multi-agent debate framework for improved evaluation quality.</li></ul></li><li><p><strong>ICE</strong> [Jain et al., 2023b]:</p><ul><li>Used few-shot examples for interactive multi-dimensional evaluation.</li></ul></li></ol><hr><h3 id="7-Challenges-and-Future-Directions"><a href="#7-Challenges-and-Future-Directions" class="headerlink" title="7. Challenges and Future Directions"></a>7. Challenges and Future Directions</h3><h4 id="Overview-3"><a href="#Overview-3" class="headerlink" title="Overview"></a>Overview</h4><p>Despite its powerful capabilities, LLM-as-a-Judge faces challenges such as evaluation bias, adaptability to dynamic tasks, and the potential of human-AI collaborative evaluation. This section explores these challenges and outlines future research directions.</p><h5 id="7-1-Bias-and-Vulnerabilities"><a href="#7-1-Bias-and-Vulnerabilities" class="headerlink" title="7.1 Bias and Vulnerabilities"></a>7.1 Bias and Vulnerabilities</h5><ol><li><strong>OffsetBias</strong> [Park et al., 2024]:<ul><li>Proposed a de-biasing framework to mitigate positional and content biases.</li></ul></li></ol><h5 id="7-2-Dynamic-and-Complex-Evaluations"><a href="#7-2-Dynamic-and-Complex-Evaluations" class="headerlink" title="7.2 Dynamic and Complex Evaluations"></a>7.2 Dynamic and Complex Evaluations</h5><ol><li><strong>Tree of Thought (ToT)</strong> [Yao et al., 2023a]:<ul><li>Enhanced multi-step reasoning with dynamic state evaluation mechanisms.</li></ul></li></ol><h5 id="7-3-Self-Evaluation-and-Human-AI-Collaboration"><a href="#7-3-Self-Evaluation-and-Human-AI-Collaboration" class="headerlink" title="7.3 Self-Evaluation and Human-AI Collaboration"></a>7.3 Self-Evaluation and Human-AI Collaboration</h5><ol><li><p><strong>Self-Taught Evaluators</strong> [Wang et al., 2024f]:</p><ul><li>Highlighted the potential for models to improve through self-learning mechanisms.</li></ul></li><li><p><strong>Meta-Rewarding</strong> [Wu et al., 2024]:</p><ul><li>Demonstrated the advantages of integrating self-evaluation signals into optimization.</li></ul></li></ol><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Evaluation-of-Generation-Based-Large-Language-Models-LLMs-Opportunities-and-Challenges-from-Generation-to-Judgment&quot;&gt;&lt;a href=&quot;#Evalua</summary>
      
    
    
    
    <category term="NLP Insights" scheme="https://chenhuiyu.github.io/categories/NLP-Insights/"/>
    
    
  </entry>
  
  <entry>
    <title>SeCom: Redefining Memory Management in Conversational AI</title>
    <link href="https://chenhuiyu.github.io/2025/06/24/NLP%20Insights/SeCom%20Redefining%20Memory%20Management%20in%20Conversational%20AI.en/"/>
    <id>https://chenhuiyu.github.io/2025/06/24/NLP%20Insights/SeCom%20Redefining%20Memory%20Management%20in%20Conversational%20AI.en/</id>
    <published>2025-06-24T08:00:00.000Z</published>
    <updated>2026-02-20T21:47:32.621Z</updated>
    
    <content type="html"><![CDATA[<h1 id="SeCom-Redefining-Memory-Management-in-Conversational-AI"><a href="#SeCom-Redefining-Memory-Management-in-Conversational-AI" class="headerlink" title="SeCom: Redefining Memory Management in Conversational AI"></a>SeCom: Redefining Memory Management in Conversational AI</h1><h2 id="Foreword"><a href="#Foreword" class="headerlink" title="Foreword"></a>Foreword</h2><p>I’ve recently been diving into memory management for dialog-based AI, especially how to construct and retrieve memories in long-term conversations. During my exploration I came across an eye-opening ICLR 2025 paper—**”SeCom: On Memory Construction and Retrieval for Personalized Conversational Agents”**—a collaboration between Microsoft and Tsinghua University.</p><p>SeCom solves a core problem: <strong>How can an agent effectively manage and retrieve historical information in prolonged conversations?</strong> In this post I’ll unpack the method’s key ideas and technical innovations, hoping to spark inspiration for researchers working in this arena.</p><h2 id="1-Why-Should-We-Care-About-Dialog-Memory-Management"><a href="#1-Why-Should-We-Care-About-Dialog-Memory-Management" class="headerlink" title="1. Why Should We Care About Dialog Memory Management?"></a>1. Why Should We Care About Dialog Memory Management?</h2><h3 id="1-1-Real-World-Challenges-in-Long-Conversations"><a href="#1-1-Real-World-Challenges-in-Long-Conversations" class="headerlink" title="1.1 Real-World Challenges in Long Conversations"></a>1.1 Real-World Challenges in Long Conversations</h3><p>Anyone who chats with LLMs regularly has probably experienced this: once a conversation grows long, the agent seems to “forget” earlier context or respond incoherently. That’s the memory problem in action.</p><p>Even with long-context models, super-long dialogs increase compute cost and often degrade quality. Key challenges include:</p><ul><li><strong>Context length limits</strong>: Token budgets remain finite.</li><li><strong>Information relevance</strong>: History contains plenty of facts irrelevant to the current query.</li><li><strong>Semantic coherence</strong>: Related information may be scattered across non-contiguous turns.</li><li><strong>Personalization</strong>: The agent must remember user preferences and interaction patterns.</li></ul><h3 id="1-2-A-Quick-Landscape-of-Existing-Approaches"><a href="#1-2-A-Quick-Landscape-of-Existing-Approaches" class="headerlink" title="1.2 A Quick Landscape of Existing Approaches"></a>1.2 A Quick Landscape of Existing Approaches</h3><p>The community’s strategies roughly split into three camps:</p><ol><li><strong>“Give Me Everything” (full history)</strong><ul><li>Complete information, zero recall loss.</li><li>But like moving an entire library just to find one book—computational overkill.</li></ul></li><li><strong>“Bullet-Point Digest” (summaries)</strong><ul><li>Compact and efficient.</li><li>Risk of omitting crucial details during abstraction.</li></ul></li><li><strong>“Precision Strike” (retrieval-based)</strong><ul><li>Fetch only what you need, exactly when you need it.</li><li>Success hinges on choosing the right retrieval granularity—precisely the issue SeCom addresses.</li></ul></li></ol><h4 id="1-2-3-Retrieval-Augmented-Generation-RAG-in-Dialog"><a href="#1-2-3-Retrieval-Augmented-Generation-RAG-in-Dialog" class="headerlink" title="1.2.3 Retrieval-Augmented Generation (RAG) in Dialog"></a>1.2.3 Retrieval-Augmented Generation (RAG) in Dialog</h4><p>RAG faces dialog-specific hurdles:</p><ul><li><strong>Chunking strategy</strong>: How to segment a dialog into retrievable units.</li><li><strong>Relevance estimation</strong>: Harder than in static docs due to dialog dynamics.</li><li><strong>Temporal dependency</strong>: Order matters; turns refer to earlier context.</li></ul><h3 id="1-3-The-Granularity-Dilemma"><a href="#1-3-The-Granularity-Dilemma" class="headerlink" title="1.3 The Granularity Dilemma"></a>1.3 The Granularity Dilemma</h3><p>We often index memories at the turn-level or at the whole-conversation level. Both extremes break down:</p><ul><li><strong>Turn-level</strong> → fragments context, loses dependencies, retrieval recall suffers.</li><li><strong>Conversation-level</strong> → topic mixture, lots of noise, retrieval becomes coarse.</li><li><strong>Summaries</strong> → irreversible information loss.</li></ul><p>SeCom’s insight: dialog naturally contains <strong>paragraph-level thematic boundaries</strong>. Segmenting at this “just-right” granularity preserves coherence without exploding memory size.</p><h2 id="2-Inside-SeCom"><a href="#2-Inside-SeCom" class="headerlink" title="2. Inside SeCom"></a>2. Inside SeCom</h2><h3 id="2-1-Two-Key-Insights"><a href="#2-1-Two-Key-Insights" class="headerlink" title="2.1 Two Key Insights"></a>2.1 Two Key Insights</h3><ol><li><strong>Paragraph-like Topic Shifts</strong> exist in dialog just as in essays.</li><li><strong>Natural Language Is Redundant</strong>—filler words, confirmations, small talk, etc. Removing them boosts retrieval precision.</li></ol><p>Hence <strong>SeCom = Segmentation + Compression</strong>.</p><h3 id="2-2-System-Pipeline"><a href="#2-2-System-Pipeline" class="headerlink" title="2.2 System Pipeline"></a>2.2 System Pipeline</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">History → [Segmenter] → Paragraph-level units → [Compressor] → Denoised memories → [Retriever] → Relevant context → [Generator] → Final reply</span><br></pre></td></tr></tbody></table></figure><p>Technically:</p><ol><li>Segmenter $f_{\mathcal I}$ splits the dialog.</li><li>Compressor $f_{comp}$ denoises each segment.</li><li>Retriever $f_R$ ranks memories for the current user utterance $u^*$.</li><li>LLM $f_{LLM}$ produces the answer based on top-N memories.</li></ol><h3 id="2-3-How-to-Segment-Without-Labels"><a href="#2-3-How-to-Segment-Without-Labels" class="headerlink" title="2.3 How to Segment Without Labels"></a>2.3 How to Segment Without Labels</h3><p>SeCom leverages GPT-4 in a <strong>zero-shot</strong> fashion: craft a prompt asking the model to mark topic boundaries and output span indices. No training data required.</p><p>When limited gold data are available, a <strong>reflection-based</strong> loop iteratively refines the guidelines using WindowDiff scores and GPT-4 reasoning.</p><p>An <strong>incremental segmenter</strong> decides on-the-fly whether a new turn merges into the previous segment or starts a fresh one.</p><h3 id="2-4-Denoising-via-LLMLingua-2"><a href="#2-4-Denoising-via-LLMLingua-2" class="headerlink" title="2.4 Denoising via LLMLingua-2"></a>2.4 Denoising via LLMLingua-2</h3><p>LLMLingua-2 scores token importance and keeps the top $(1-r)$ fraction (e.g., 25 %) accordingly. Empirically, retaining just 25 % tokens preserves <strong>&gt;95 %</strong> key information, lifts retrieval GPT4Score by <strong>+9.46</strong>, and yields 4 × speed-up.</p><h3 id="2-5-Hybrid-Retrieval"><a href="#2-5-Hybrid-Retrieval" class="headerlink" title="2.5 Hybrid Retrieval"></a>2.5 Hybrid Retrieval</h3><p>BM25 (sparse) and MPNet (dense) scores are linearly combined:</p><p>$$\text{score}_{hybrid}=\alpha,\text{BM25}+(1-\alpha),\text{MPNet}, \quad \alpha=0.6$$</p><h2 id="3-Final-Thoughts"><a href="#3-Final-Thoughts" class="headerlink" title="3. Final Thoughts"></a>3. Final Thoughts</h2><h3 id="3-1-What-SeCom-Teaches-Us"><a href="#3-1-What-SeCom-Teaches-Us" class="headerlink" title="3.1 What SeCom Teaches Us"></a>3.1 What SeCom Teaches Us</h3><ul><li><strong>Simplicity Wins</strong>: Segment + Compress, nothing fancy, yet highly effective.</li><li><strong>Understand the Problem First</strong>: The authors nailed the granularity pain-point before designing a solution.</li></ul><p>Future directions:</p><ul><li><strong>Personalized segmentation</strong> tuned to each user’s dialog style.</li><li><strong>Real-time adaptation</strong> of compression and segmentation based on quality metrics.</li></ul><hr><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><strong>Paper</strong>: <a href="https://www.arxiv.org/abs/2502.05589">SeCom: On Memory Construction and Retrieval for Personalized Conversational Agents (ICLR 2025)</a></li><li><strong>Project Page</strong>: <a href="https://llmlingua.com/secom.html">https://llmlingua.com/secom.html</a></li><li><strong>Code</strong>: SeCom-main</li><li><strong>Datasets</strong>: LOCOMO, Long-MT-Bench+, DialSeg711, TIAGE, SuperDialSeg</li></ul><p><em>This post is based on Microsoft &amp; Tsinghua University’s ICLR 2025 paper. Please refer to the original publication and open-source repo for implementation details.</em> </p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;SeCom-Redefining-Memory-Management-in-Conversational-AI&quot;&gt;&lt;a href=&quot;#SeCom-Redefining-Memory-Management-in-Conversational-AI&quot; class=&quot;h</summary>
      
    
    
    
    <category term="NLP Insights" scheme="https://chenhuiyu.github.io/categories/NLP-Insights/"/>
    
    
    <category term="Conversational AI" scheme="https://chenhuiyu.github.io/tags/Conversational-AI/"/>
    
    <category term="Memory Management" scheme="https://chenhuiyu.github.io/tags/Memory-Management/"/>
    
    <category term="SeCom" scheme="https://chenhuiyu.github.io/tags/SeCom/"/>
    
    <category term="RAG" scheme="https://chenhuiyu.github.io/tags/RAG/"/>
    
  </entry>
  
  <entry>
    <title>SeCom: Redefining Memory Management in Conversational AI</title>
    <link href="https://chenhuiyu.github.io/2025/06/24/NLP%20Insights/SeCom%20Redefining%20Memory%20Management%20in%20Conversational%20AI/"/>
    <id>https://chenhuiyu.github.io/2025/06/24/NLP%20Insights/SeCom%20Redefining%20Memory%20Management%20in%20Conversational%20AI/</id>
    <published>2025-06-24T08:00:00.000Z</published>
    <updated>2026-02-20T21:51:54.079Z</updated>
    
    <content type="html"><![CDATA[<h1 id="SeCom-Redefining-Memory-Management-in-Conversational-AI"><a href="#SeCom-Redefining-Memory-Management-in-Conversational-AI" class="headerlink" title="SeCom: Redefining Memory Management in Conversational AI"></a>SeCom: Redefining Memory Management in Conversational AI</h1><h2 id="Foreword"><a href="#Foreword" class="headerlink" title="Foreword"></a>Foreword</h2><p>I’ve recently been diving into memory management for dialog-based AI, especially how to construct and retrieve memories in long-term conversations. During my exploration I came across an eye-opening ICLR 2025 paper—**”SeCom: On Memory Construction and Retrieval for Personalized Conversational Agents”**—a collaboration between Microsoft and Tsinghua University.</p><p>SeCom solves a core problem: <strong>How can an agent effectively manage and retrieve historical information in prolonged conversations?</strong> In this post I’ll unpack the method’s key ideas and technical innovations, hoping to spark inspiration for researchers working in this arena.</p><h2 id="1-Why-Should-We-Care-About-Dialog-Memory-Management"><a href="#1-Why-Should-We-Care-About-Dialog-Memory-Management" class="headerlink" title="1. Why Should We Care About Dialog Memory Management?"></a>1. Why Should We Care About Dialog Memory Management?</h2><h3 id="1-1-Real-World-Challenges-in-Long-Conversations"><a href="#1-1-Real-World-Challenges-in-Long-Conversations" class="headerlink" title="1.1 Real-World Challenges in Long Conversations"></a>1.1 Real-World Challenges in Long Conversations</h3><p>Anyone who chats with LLMs regularly has probably experienced this: once a conversation grows long, the agent seems to “forget” earlier context or respond incoherently. That’s the memory problem in action.</p><p>Even with long-context models, super-long dialogs increase compute cost and often degrade quality. Key challenges include:</p><ul><li><strong>Context length limits</strong>: Token budgets remain finite.</li><li><strong>Information relevance</strong>: History contains plenty of facts irrelevant to the current query.</li><li><strong>Semantic coherence</strong>: Related information may be scattered across non-contiguous turns.</li><li><strong>Personalization</strong>: The agent must remember user preferences and interaction patterns.</li></ul><h3 id="1-2-A-Quick-Landscape-of-Existing-Approaches"><a href="#1-2-A-Quick-Landscape-of-Existing-Approaches" class="headerlink" title="1.2 A Quick Landscape of Existing Approaches"></a>1.2 A Quick Landscape of Existing Approaches</h3><p>The community’s strategies roughly split into three camps:</p><ol><li><strong>“Give Me Everything” (full history)</strong><ul><li>Complete information, zero recall loss.</li><li>But like moving an entire library just to find one book—computational overkill.</li></ul></li><li><strong>“Bullet-Point Digest” (summaries)</strong><ul><li>Compact and efficient.</li><li>Risk of omitting crucial details during abstraction.</li></ul></li><li><strong>“Precision Strike” (retrieval-based)</strong><ul><li>Fetch only what you need, exactly when you need it.</li><li>Success hinges on choosing the right retrieval granularity—precisely the issue SeCom addresses.</li></ul></li></ol><h4 id="1-2-3-Retrieval-Augmented-Generation-RAG-in-Dialog"><a href="#1-2-3-Retrieval-Augmented-Generation-RAG-in-Dialog" class="headerlink" title="1.2.3 Retrieval-Augmented Generation (RAG) in Dialog"></a>1.2.3 Retrieval-Augmented Generation (RAG) in Dialog</h4><p>RAG faces dialog-specific hurdles:</p><ul><li><strong>Chunking strategy</strong>: How to segment a dialog into retrievable units.</li><li><strong>Relevance estimation</strong>: Harder than in static docs due to dialog dynamics.</li><li><strong>Temporal dependency</strong>: Order matters; turns refer to earlier context.</li></ul><h3 id="1-3-The-Granularity-Dilemma"><a href="#1-3-The-Granularity-Dilemma" class="headerlink" title="1.3 The Granularity Dilemma"></a>1.3 The Granularity Dilemma</h3><p>We often index memories at the turn-level or at the whole-conversation level. Both extremes break down:</p><ul><li><strong>Turn-level</strong> → fragments context, loses dependencies, retrieval recall suffers.</li><li><strong>Conversation-level</strong> → topic mixture, lots of noise, retrieval becomes coarse.</li><li><strong>Summaries</strong> → irreversible information loss.</li></ul><p>SeCom’s insight: dialog naturally contains <strong>paragraph-level thematic boundaries</strong>. Segmenting at this “just-right” granularity preserves coherence without exploding memory size.</p><h2 id="2-Inside-SeCom"><a href="#2-Inside-SeCom" class="headerlink" title="2. Inside SeCom"></a>2. Inside SeCom</h2><h3 id="2-1-Two-Key-Insights"><a href="#2-1-Two-Key-Insights" class="headerlink" title="2.1 Two Key Insights"></a>2.1 Two Key Insights</h3><ol><li><strong>Paragraph-like Topic Shifts</strong> exist in dialog just as in essays.</li><li><strong>Natural Language Is Redundant</strong>—filler words, confirmations, small talk, etc. Removing them boosts retrieval precision.</li></ol><p>Hence <strong>SeCom = Segmentation + Compression</strong>.</p><h3 id="2-2-System-Pipeline"><a href="#2-2-System-Pipeline" class="headerlink" title="2.2 System Pipeline"></a>2.2 System Pipeline</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">History → [Segmenter] → Paragraph-level units → [Compressor] → Denoised memories → [Retriever] → Relevant context → [Generator] → Final reply</span><br></pre></td></tr></tbody></table></figure><p>Technically:</p><ol><li>Segmenter $f_{\mathcal I}$ splits the dialog.</li><li>Compressor $f_{comp}$ denoises each segment.</li><li>Retriever $f_R$ ranks memories for the current user utterance $u^*$.</li><li>LLM $f_{LLM}$ produces the answer based on top-N memories.</li></ol><h3 id="2-3-How-to-Segment-Without-Labels"><a href="#2-3-How-to-Segment-Without-Labels" class="headerlink" title="2.3 How to Segment Without Labels"></a>2.3 How to Segment Without Labels</h3><p>SeCom leverages GPT-4 in a <strong>zero-shot</strong> fashion: craft a prompt asking the model to mark topic boundaries and output span indices. No training data required.</p><p>When limited gold data are available, a <strong>reflection-based</strong> loop iteratively refines the guidelines using WindowDiff scores and GPT-4 reasoning.</p><p>An <strong>incremental segmenter</strong> decides on-the-fly whether a new turn merges into the previous segment or starts a fresh one.</p><h3 id="2-4-Denoising-via-LLMLingua-2"><a href="#2-4-Denoising-via-LLMLingua-2" class="headerlink" title="2.4 Denoising via LLMLingua-2"></a>2.4 Denoising via LLMLingua-2</h3><p>LLMLingua-2 scores token importance and keeps the top $(1-r)$ fraction (e.g., 25 %) accordingly. Empirically, retaining just 25 % tokens preserves <strong>&gt;95 %</strong> key information, lifts retrieval GPT4Score by <strong>+9.46</strong>, and yields 4 × speed-up.</p><h3 id="2-5-Hybrid-Retrieval"><a href="#2-5-Hybrid-Retrieval" class="headerlink" title="2.5 Hybrid Retrieval"></a>2.5 Hybrid Retrieval</h3><p>BM25 (sparse) and MPNet (dense) scores are linearly combined:</p><p>$$\text{score}_{hybrid}=\alpha,\text{BM25}+(1-\alpha),\text{MPNet}, \quad \alpha=0.6$$</p><h2 id="3-Final-Thoughts"><a href="#3-Final-Thoughts" class="headerlink" title="3. Final Thoughts"></a>3. Final Thoughts</h2><h3 id="3-1-What-SeCom-Teaches-Us"><a href="#3-1-What-SeCom-Teaches-Us" class="headerlink" title="3.1 What SeCom Teaches Us"></a>3.1 What SeCom Teaches Us</h3><ul><li><strong>Simplicity Wins</strong>: Segment + Compress, nothing fancy, yet highly effective.</li><li><strong>Understand the Problem First</strong>: The authors nailed the granularity pain-point before designing a solution.</li></ul><p>Future directions:</p><ul><li><strong>Personalized segmentation</strong> tuned to each user’s dialog style.</li><li><strong>Real-time adaptation</strong> of compression and segmentation based on quality metrics.</li></ul><hr><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><strong>Paper</strong>: <a href="https://www.arxiv.org/abs/2502.05589">SeCom: On Memory Construction and Retrieval for Personalized Conversational Agents (ICLR 2025)</a></li><li><strong>Project Page</strong>: <a href="https://llmlingua.com/secom.html">https://llmlingua.com/secom.html</a></li><li><strong>Code</strong>: SeCom-main</li><li><strong>Datasets</strong>: LOCOMO, Long-MT-Bench+, DialSeg711, TIAGE, SuperDialSeg</li></ul><p><em>This post is based on Microsoft &amp; Tsinghua University’s ICLR 2025 paper. Please refer to the original publication and open-source repo for implementation details.</em> </p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;SeCom-Redefining-Memory-Management-in-Conversational-AI&quot;&gt;&lt;a href=&quot;#SeCom-Redefining-Memory-Management-in-Conversational-AI&quot; class=&quot;h</summary>
      
    
    
    
    <category term="NLP Insights" scheme="https://chenhuiyu.github.io/categories/NLP-Insights/"/>
    
    
    <category term="Conversational AI" scheme="https://chenhuiyu.github.io/tags/Conversational-AI/"/>
    
    <category term="Memory Management" scheme="https://chenhuiyu.github.io/tags/Memory-Management/"/>
    
    <category term="SeCom" scheme="https://chenhuiyu.github.io/tags/SeCom/"/>
    
    <category term="RAG" scheme="https://chenhuiyu.github.io/tags/RAG/"/>
    
  </entry>
  
  <entry>
    <title>SeCom: 重新定义对话AI的记忆管理</title>
    <link href="https://chenhuiyu.github.io/2025/06/24/NLP%20Insights/SeCom:%20%E9%87%8D%E6%96%B0%E5%AE%9A%E4%B9%89%E5%AF%B9%E8%AF%9DAI%E7%9A%84%E8%AE%B0%E5%BF%86%E7%AE%A1%E7%90%86.en/"/>
    <id>https://chenhuiyu.github.io/2025/06/24/NLP%20Insights/SeCom:%20%E9%87%8D%E6%96%B0%E5%AE%9A%E4%B9%89%E5%AF%B9%E8%AF%9DAI%E7%9A%84%E8%AE%B0%E5%BF%86%E7%AE%A1%E7%90%86.en/</id>
    <published>2025-06-24T08:00:00.000Z</published>
    <updated>2026-02-20T21:47:32.629Z</updated>
    
    <content type="html"><![CDATA[<h1 id="SeCom-重新定义对话AI的记忆管理"><a href="#SeCom-重新定义对话AI的记忆管理" class="headerlink" title="SeCom: 重新定义对话AI的记忆管理"></a>SeCom: 重新定义对话AI的记忆管理</h1><h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>最近笔者一直在研究对话AI中的内存管理问题，特别是长期对话场景下的记忆构建与检索技术。发现了一篇令人眼前一亮的ICLR 2025论文——<strong>《SeCom: On Memory Construction and Retrieval for Personalized Conversational Agents》</strong>，由Microsoft和清华大学的研究团队联合发表。</p><p>这篇论文提出的SeCom方法巧妙地解决了一个核心问题：<strong>如何在长期对话中有效管理和检索历史信息</strong>？今天想和大家分享一下这个方法的技术细节和创新点，希望能为从事相关研究的朋友们提供一些启发。</p><h2 id="1-为什么我们需要关注对话内存管理？"><a href="#1-为什么我们需要关注对话内存管理？" class="headerlink" title="1. 为什么我们需要关注对话内存管理？"></a>1. 为什么我们需要关注对话内存管理？</h2><h3 id="1-1-长期对话的现实挑战"><a href="#1-1-长期对话的现实挑战" class="headerlink" title="1.1 长期对话的现实挑战"></a>1.1 长期对话的现实挑战</h3><p>在与LLMs的日常交互中，相信大家都遇到过这样的困扰：当对话变得很长时，AI似乎”忘记”了之前讨论的内容，或者给出的回答与前面的上下文不够连贯。这背后反映的正是长期对话中的内存管理挑战。</p><p>随着大语言模型技术的成熟，基于LLM的对话代理已经深入到我们生活的方方面面。但是，当我们希望与AI进行真正的长期、个性化交互时——比如跨越数天、数周的项目讨论，现有的技术就显得力不从心了。</p><p>长期对话面临的主要技术挑战包括：</p><ul><li><strong>上下文长度限制</strong>：即使是支持长上下文的模型，在处理超长对话时也面临计算成本和性能下降的问题</li><li><strong>信息相关性</strong>：历史对话中可能包含大量与当前查询无关的信息</li><li><strong>语义连贯性</strong>：相关信息可能分散在多个不连续的对话轮次中</li><li><strong>个性化记忆</strong>：需要记住用户的偏好、习惯和历史交互模式</li></ul><h3 id="1-2-笔者对Memory管理领域的观察"><a href="#1-2-笔者对Memory管理领域的观察" class="headerlink" title="1.2 笔者对Memory管理领域的观察"></a>1.2 笔者对Memory管理领域的观察</h3><p>在深入研究这个领域的过程中，笔者发现对话内存管理其实是一个相当复杂的系统工程。它的核心目标听起来很简单：从历史对话中提取、存储和检索相关信息，以支持当前对话的生成。但实际实现起来，需要解决三个关键问题：</p><ol><li><strong>内存构建（Memory Construction）</strong>：如何将自然语言对话转换为结构化的内存单元？</li><li><strong>内存检索（Memory Retrieval）</strong>：面对海量历史信息，如何快速准确地找到相关内容？</li><li><strong>响应生成（Response Generation）</strong>：如何基于检索到的记忆生成连贯、个性化的回复？</li></ol><p>听起来是不是很像人类的记忆机制？确实如此，这也是为什么这个问题如此有趣和具有挑战性。</p><h4 id="1-2-2-现有方法的”三国演义”"><a href="#1-2-2-现有方法的”三国演义”" class="headerlink" title="1.2.2 现有方法的”三国演义”"></a>1.2.2 现有方法的”三国演义”</h4><p>在研究过程中，笔者发现现有的方法大致可以分为三大流派，每个都有自己的”哲学”：</p><p><strong>“全盘托出”派（基于完整历史）</strong>：</p><ul><li><strong>核心思想</strong>：既然不知道什么重要，那就全部给你！</li><li><strong>优势</strong>：信息完整，绝不遗漏</li><li><strong>问题</strong>：就像把整个图书馆搬给你找一本书，效率可想而知</li></ul><p><strong>“提纲挈领”派（基于摘要）</strong>：</p><ul><li><strong>核心思想</strong>：重要的信息浓缩成摘要就够了</li><li><strong>优势</strong>：信息压缩，计算高效</li><li><strong>问题</strong>：摘要过程中重要细节可能”意外失踪”</li></ul><p><strong>“精准打击”派（基于检索）</strong>：</p><ul><li><strong>代表方法</strong>：轮次级检索、会话级检索</li><li><strong>核心思想</strong>：需要什么就检索什么，按需取用</li><li><strong>优势</strong>：计算效率高，定位精确</li><li><strong>问题</strong>：关键在于如何确定检索的”粒度”——这正是SeCom要解决的核心问题！</li></ul><h4 id="1-2-3-检索增强生成（RAG）在对话中的应用"><a href="#1-2-3-检索增强生成（RAG）在对话中的应用" class="headerlink" title="1.2.3 检索增强生成（RAG）在对话中的应用"></a>1.2.3 检索增强生成（RAG）在对话中的应用</h4><p>检索增强生成技术在对话系统中的应用日益广泛，主要包括：</p><ul><li>**Dense Passage Retrieval (DPR)**：使用预训练的密集检索模型</li><li><strong>BM25</strong>：基于词频统计的稀疏检索方法</li><li><strong>Hybrid Retrieval</strong>：结合密集检索和稀疏检索的优势</li></ul><p>然而，现有RAG方法在对话场景中面临独特挑战：</p><ul><li><strong>分块策略（Chunking Strategy）</strong>：如何将对话分割为检索单元</li><li><strong>相关性判断</strong>：对话的相关性判断比文档检索更复杂</li><li><strong>时序依赖</strong>：对话具有强时序性，前后文关系重要</li></ul><h3 id="1-3-内存粒度问题的深层分析"><a href="#1-3-内存粒度问题的深层分析" class="headerlink" title="1.3 内存粒度问题的深层分析"></a>1.3 内存粒度问题的深层分析</h3><h4 id="1-3-1-轮次级内存的局限性"><a href="#1-3-1-轮次级内存的局限性" class="headerlink" title="1.3.1 轮次级内存的局限性"></a>1.3.1 轮次级内存的局限性</h4><p>轮次级内存将每个用户-代理交互（turn）作为独立的内存单元：</p><p><strong>数学表示</strong>：<br>设对话历史 $\mathcal{H} = {\mathbf{c}<em>i}</em>{i=1}^C$，其中每个会话 $\mathbf{c}<em>i = {\mathbf{t}<em>j}</em>{j=1}^{T_i}$<br>轮次级内存：$|\mathcal{M}| = \sum</em>{i=1}^C T_i$，每个 $\mathbf{m} \in \mathcal{M}$ 对应一个轮次 $\mathbf{t}$</p><p><strong>主要问题</strong>：</p><ul><li><strong>信息碎片化</strong>：相关信息分散在多个轮次中，单个轮次可能缺乏完整语义</li><li><strong>上下文缺失</strong>：轮次间的依赖关系丢失</li><li><strong>检索精度低</strong>：查询词汇可能不直接出现在相关轮次中</li></ul><p><strong>具体示例</strong>：<br>用户在第3轮询问”什么是机器学习”，第5轮询问”监督学习的例子”，第8轮询问”如何选择算法”。当用户在第10轮询问”之前提到的分类算法性能如何评估”时，轮次级检索可能无法找到完整的上下文。</p><h4 id="1-3-2-会话级内存的局限性"><a href="#1-3-2-会话级内存的局限性" class="headerlink" title="1.3.2 会话级内存的局限性"></a>1.3.2 会话级内存的局限性</h4><p>会话级内存将整个对话会话作为内存单元：</p><p><strong>数学表示</strong>：<br>会话级内存：$|\mathcal{M}| = C$，每个 $\mathbf{m} \in \mathcal{M}$ 对应一个会话 $\mathbf{c}$</p><p><strong>主要问题</strong>：</p><ul><li><strong>主题混杂</strong>：单个会话可能包含多个不相关主题</li><li><strong>噪声干扰</strong>：大量无关信息影响检索和生成质量</li><li><strong>检索粗糙</strong>：无法精确定位到具体相关内容</li></ul><p><strong>具体示例</strong>：<br>一个会话中用户讨论了机器学习、烹饪食谱、旅行计划和电影推荐。当查询机器学习相关问题时，检索到的会话包含大量无关的烹饪和旅行信息。</p><h4 id="1-3-3-摘要化方法的信息损失"><a href="#1-3-3-摘要化方法的信息损失" class="headerlink" title="1.3.3 摘要化方法的信息损失"></a>1.3.3 摘要化方法的信息损失</h4><p>摘要化方法通过压缩对话内容来减少信息量：</p><p><strong>主要问题</strong>：</p><ul><li><strong>细节丢失</strong>：摘要过程中重要细节可能被省略</li><li><strong>主观性</strong>：摘要质量依赖于模型的理解能力</li><li><strong>不可逆性</strong>：一旦信息被摘要，原始细节无法恢复</li></ul><h2 id="2-SeCom的设计"><a href="#2-SeCom的设计" class="headerlink" title="2. SeCom的设计"></a>2. SeCom的设计</h2><h3 id="2-1-核心发现"><a href="#2-1-核心发现" class="headerlink" title="2.1 核心发现"></a>2.1 核心发现</h3><p>SeCom（<strong>Se</strong>gmentation + <strong>Com</strong>pression）的两个核心发现：</p><p><strong>洞察一：对话天然具有”段落”结构</strong><br>就像我们写文章会分段一样，人类的对话其实也有天然的主题边界。比如在一次长对话中，我们可能先讨论工作项目，然后转到周末计划，再聊到最近看的电影。每个主题就是一个天然的”段落”。</p><p>传统方法要么把每句话当作独立单元（太碎片化），要么把整个对话当作一个整体（太粗糙），而SeCom找到了中间的最佳平衡点——<strong>段落级的语义单元</strong>。</p><p><strong>洞察二：自然语言充满”废话”</strong><br>这听起来有点刻薄，但确实如此。我们日常对话中充满了”嗯”、”那个”、”你知道的”这样的冗余表达，还有大量的重复、确认、客套话。这些在人际交流中很重要，但对机器检索来说就是噪声。</p><p>SeCom通过智能压缩，保留关键信息的同时去除这些”噪声”，让检索更加精准。</p><h3 id="2-2-系统设计"><a href="#2-2-系统设计" class="headerlink" title="2.2 系统设计"></a>2.2 系统设计</h3><p>SeCom的整体架构设计非常优雅，就像一条高效的流水线：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">历史对话 → [分段器] → 段落级内存单元 → [压缩器] → 去噪内存单元 → [检索器] → 相关上下文 → [生成器] → 最终回复</span><br></pre></td></tr></tbody></table></figure><p>用更直观的话来解释这个流程：</p><ol><li><strong>分段器</strong>：将杂乱的对话历史按主题”切块”</li><li><strong>压缩器</strong>：将每个”块”中的废话去掉，保留精华</li><li><strong>检索器</strong>：根据当前问题找到最相关的”块”</li><li><strong>生成器</strong>：基于相关信息生成回答</li></ol><p><strong>技术表示</strong>（没什么用，写给喜欢数学的朋友）：<br>设 $f_{\mathcal{I}}$ 为分段器，$f_{Comp}$ 为压缩器，$f_R$ 为检索器，$f_{LLM}$ 为生成器</p><p>完整流程：</p><ol><li>${\mathbf{s}<em>k}</em>{k=1}^K \leftarrow f_{\mathcal{I}}(\mathcal{H})$ （对话分段）</li><li>${\mathbf{m}<em>k}</em>{k=1}^K \leftarrow f_{Comp}({\mathbf{s}<em>k}</em>{k=1}^K)$ （压缩去噪）</li><li>${\mathbf{m}<em>n}</em>{n=1}^N \leftarrow f_R(u^*, {\mathbf{m}<em>k}</em>{k=1}^K, N)$ （内存检索）</li><li>$r^* = f_{LLM}(u^*, {\mathbf{m}<em>n}</em>{n=1}^N)$ （响应生成）</li></ol><h3 id="2-3-分段算法：教AI学会”断句”"><a href="#2-3-分段算法：教AI学会”断句”" class="headerlink" title="2.3 分段算法：教AI学会”断句”"></a>2.3 分段算法：教AI学会”断句”</h3><h4 id="2-3-1-零样本分段"><a href="#2-3-1-零样本分段" class="headerlink" title="2.3.1 零样本分段"></a>2.3.1 零样本分段</h4><p>如何让AI自动识别对话中的主题边界？传统方法需要大量标注数据训练专门的分段模型，而SeCom采用了一个非常聪明的”零样本”方法。</p><p><strong>核心思路</strong>：<br>既然GPT-4这样的大模型已经具备了强大的文本理解能力，为什么不直接让它来判断对话的主题边界呢？就像让一个文学老师来给文章分段一样。</p><p><strong>输入预处理</strong>：<br>将对话会话增强为结构化格式：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Turn j: </span><br><span class="line">[user]: u_j</span><br><span class="line">[agent]: r_j</span><br></pre></td></tr></tbody></table></figure><p><strong>分段提示设计</strong>：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">分析以下对话，识别主题边界，将对话分割为语义连贯的段落。</span><br><span class="line">每个段落应该：</span><br><span class="line">1. 围绕单一主题或相关主题</span><br><span class="line">2. 包含完整的交互序列</span><br><span class="line">3. 具有明确的开始和结束边界</span><br><span class="line"></span><br><span class="line">对话内容：</span><br><span class="line">[对话内容]</span><br><span class="line"></span><br><span class="line">请输出每个段落的起始和结束轮次编号。</span><br></pre></td></tr></tbody></table></figure><p><strong>优势</strong>：</p><ul><li>无需训练数据，适用于开放域对话</li><li>利用LLM的强大理解能力</li><li>可处理复杂的主题转换模式</li></ul><h4 id="2-3-2-基于反思的分段优化"><a href="#2-3-2-基于反思的分段优化" class="headerlink" title="2.3.2 基于反思的分段优化"></a>2.3.2 基于反思的分段优化</h4><p>当有少量标注数据时，采用反思机制优化分段效果：</p><p><strong>算法步骤</strong>：</p><ol><li><strong>初始分段</strong>：使用零样本方法对批量数据进行分段</li><li><strong>错误识别</strong>：基于WindowDiff指标选择top-K个分段错误最大的样本</li><li><strong>反思学习</strong>：让LLM分析分段错误，更新分段指导原则</li><li><strong>迭代优化</strong>：重复上述过程直到收敛</li></ol><p><strong>反思提示设计</strong>：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">分析以下分段错误，并更新分段指导原则：</span><br><span class="line"></span><br><span class="line">错误案例：</span><br><span class="line">[分段结果] vs [标准答案]</span><br><span class="line"></span><br><span class="line">请分析错误原因并提供改进的分段指导原则。</span><br></pre></td></tr></tbody></table></figure><p><strong>数学表示</strong>：<br>设 $\boldsymbol{G}<em>m$ 为第m轮的分段指导原则，更新公式为：<br>$$\boldsymbol{G}</em>{m+1} = \boldsymbol{G}_m - \eta \nabla \mathcal{L}(\boldsymbol{G}_m)$$</p><p>其中 $\nabla \mathcal{L}(\boldsymbol{G}_m)$ 为LLM隐式估计的分段损失梯度。</p><h4 id="2-3-3-增量分段算法"><a href="#2-3-3-增量分段算法" class="headerlink" title="2.3.3 增量分段算法"></a>2.3.3 增量分段算法</h4><p>对于新增的对话轮次，设计增量分段算法：</p><p><strong>算法流程</strong>：</p><ol><li>输入新轮次 $\mathbf{t}<em>{new}$ 和前一段落 $\mathbf{s}</em>{prev}$</li><li>判断是否应该合并：$binary = f_{judge}(\mathbf{t}<em>{new}, \mathbf{s}</em>{prev})$</li><li>如果合并：$\mathbf{s}<em>{prev} \leftarrow \mathbf{s}</em>{prev} \cup {\mathbf{t}_{new}}$</li><li>否则：创建新段落 $\mathbf{s}<em>{new} = {\mathbf{t}</em>{new}}$</li></ol><p><strong>判断提示</strong>：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">判断新的用户-机器人轮次是否应该与前一段落合并：</span><br><span class="line"></span><br><span class="line">新轮次：[新轮次内容]</span><br><span class="line">前一段落：[前一段落内容]</span><br><span class="line"></span><br><span class="line">如果属于同一主题，回答"Yes"，否则回答"No"。</span><br></pre></td></tr></tbody></table></figure><h3 id="2-4-压缩式内存去噪"><a href="#2-4-压缩式内存去噪" class="headerlink" title="2.4 压缩式内存去噪"></a>2.4 压缩式内存去噪</h3><h4 id="2-4-1-自然语言冗余性分析"><a href="#2-4-1-自然语言冗余性分析" class="headerlink" title="2.4.1 自然语言冗余性分析"></a>2.4.1 自然语言冗余性分析</h4><p><strong>理论基础</strong>：<br>根据Shannon信息论，自然语言具有高度冗余性，冗余率约为50-75%。这种冗余在人类交流中有助于错误纠正和理解，但在机器检索中构成噪声。</p><p><strong>冗余类型</strong>：</p><ol><li><strong>词汇冗余</strong>：同义词、重复表达</li><li><strong>语法冗余</strong>：冗余的语法结构</li><li><strong>语义冗余</strong>：重复的语义信息</li><li><strong>对话冗余</strong>：客套话、确认性回复</li></ol><h4 id="2-4-2-LLMLingua-2压缩原理"><a href="#2-4-2-LLMLingua-2压缩原理" class="headerlink" title="2.4.2 LLMLingua-2压缩原理"></a>2.4.2 LLMLingua-2压缩原理</h4><p><strong>算法核心</strong>：<br>LLMLingua-2基于token重要性评分进行压缩：</p><ol><li><p><strong>重要性评分</strong>：<br>$$s_i = f_{score}(x_i | x_{&lt;i}, x_{&gt;i})$$<br>其中 $x_i$ 为第i个token，$x_{&lt;i}$ 和 $x_{&gt;i}$ 为上下文</p></li><li><p><strong>动态压缩</strong>：<br>根据目标压缩率 $r$，保留top $(1-r) \times N$ 个重要token</p></li><li><p><strong>语义保持</strong>：<br>通过双向上下文建模确保关键语义信息不丢失</p></li></ol><p><strong>压缩效果分析</strong>：<br>实验表明，75%压缩率下：</p><ul><li>关键信息保留率 &gt; 95%</li><li>检索相关性提升 9.46分（GPT4Score）</li><li>计算效率提升 4倍</li></ul><h4 id="2-4-3-压缩对检索性能的影响"><a href="#2-4-3-压缩对检索性能的影响" class="headerlink" title="2.4.3 压缩对检索性能的影响"></a>2.4.3 压缩对检索性能的影响</h4><p><strong>相似性变化分析</strong>：<br>设 $sim(q, s)$ 为查询q与段落s的相似性</p><p>压缩前：$sim_{before}(q, s_{relevant})$，$sim_{before}(q, s_{irrelevant})$<br>压缩后：$sim_{after}(q, s’<em>{relevant})$，$sim</em>{after}(q, s’_{irrelevant})$</p><p>实验结果显示：</p><ul><li>$sim_{after}(q, s’<em>{relevant}) &gt; sim</em>{before}(q, s_{relevant})$ （相关段落相似性提升）</li><li>$sim_{after}(q, s’<em>{irrelevant}) &lt; sim</em>{before}(q, s_{irrelevant})$ （无关段落相似性降低）</li></ul><h3 id="2-5-多模态检索系统"><a href="#2-5-多模态检索系统" class="headerlink" title="2.5 多模态检索系统"></a>2.5 多模态检索系统</h3><h4 id="2-5-1-检索器选择与配置"><a href="#2-5-1-检索器选择与配置" class="headerlink" title="2.5.1 检索器选择与配置"></a>2.5.1 检索器选择与配置</h4><p><strong>BM25检索器</strong>：<br>$$BM25(q, d) = \sum_{i=1}^{|q|} IDF(q_i) \cdot \frac{tf(q_i, d) \cdot (k_1 + 1)}{tf(q_i, d) + k_1 \cdot (1 - b + b \cdot \frac{|d|}{avgdl})}$$</p><p>参数设置：$k_1 = 1.2$，$b = 0.75$</p><p><strong>MPNet检索器</strong>：<br>基于MPNet模型的密集检索：<br>$$score = \cos(\mathbf{e}_q, \mathbf{e}_d)$$<br>其中 $\mathbf{e}_q$ 和 $\mathbf{e}_d$ 分别为查询和文档的向量表示</p><h4 id="2-5-2-混合检索策略"><a href="#2-5-2-混合检索策略" class="headerlink" title="2.5.2 混合检索策略"></a>2.5.2 混合检索策略</h4><p>结合稀疏检索和密集检索的优势：<br>$$score_{hybrid} = \alpha \cdot score_{BM25} + (1-\alpha) \cdot score_{MPNet}$$</p><p>通过实验确定最优权重 $\alpha = 0.6$</p><h2 id="3-写在最后：一些思考"><a href="#3-写在最后：一些思考" class="headerlink" title="3. 写在最后：一些思考"></a>3. 写在最后：一些思考</h2><h3 id="3-1-SeCom给我们的启发"><a href="#3-1-SeCom给我们的启发" class="headerlink" title="3.1 SeCom给我们的启发"></a>3.1 SeCom给我们的启发</h3><p>研读这篇论文后，笔者有几点深刻的感悟：</p><p><strong>简单往往是最有效的</strong>：SeCom的核心思想其实很简单——分段+压缩，但正是这种简单的组合解决了复杂的问题。这提醒我们，在面对技术挑战时，有时候最朴素的想法反而是最有效的。</p><p><strong>理解问题比解决问题更重要</strong>：作者团队深入分析了内存粒度问题的本质，发现了段落级内存的最优性。这种对问题本质的深刻理解是技术创新的基础。</p><p>笔者认为未来可能的发展方向包括：</p><ul><li><strong>个性化分段策略</strong>：不同用户的对话模式不同，能否学习个性化的分段方式？</li><li><strong>实时优化机制</strong>：能否根据对话质量动态调整压缩率和分段策略？</li></ul><hr><h2 id="参考资源"><a href="#参考资源" class="headerlink" title="参考资源"></a>参考资源</h2><ul><li><strong>论文链接</strong>：<a href="https://www.arxiv.org/abs/2502.05589">SeCom: On Memory Construction and Retrieval for Personalized Conversational Agents (ICLR 2025)</a></li><li><strong>项目主页</strong>：<a href="https://llmlingua.com/secom.html">https://llmlingua.com/secom.html</a></li><li><strong>代码仓库</strong>：SeCom-main项目</li><li><strong>数据集</strong>：LOCOMO、Long-MT-Bench+、DialSeg711、TIAGE、SuperDialSeg</li></ul><p><em>本文基于Microsoft和清华大学联合研究团队在ICLR 2025发表的论文撰写，详细技术实现请参考原始论文和开源代码。</em> </p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;SeCom-重新定义对话AI的记忆管理&quot;&gt;&lt;a href=&quot;#SeCom-重新定义对话AI的记忆管理&quot; class=&quot;headerlink&quot; title=&quot;SeCom: 重新定义对话AI的记忆管理&quot;&gt;&lt;/a&gt;SeCom: 重新定义对话AI的记忆管理&lt;/h1&gt;&lt;h2</summary>
      
    
    
    
    <category term="NLP Insights" scheme="https://chenhuiyu.github.io/categories/NLP-Insights/"/>
    
    
    <category term="Conversational AI" scheme="https://chenhuiyu.github.io/tags/Conversational-AI/"/>
    
    <category term="Memory Management" scheme="https://chenhuiyu.github.io/tags/Memory-Management/"/>
    
    <category term="SeCom" scheme="https://chenhuiyu.github.io/tags/SeCom/"/>
    
    <category term="RAG" scheme="https://chenhuiyu.github.io/tags/RAG/"/>
    
  </entry>
  
  <entry>
    <title>SeCom: 重新定义对话AI的记忆管理</title>
    <link href="https://chenhuiyu.github.io/2025/06/24/NLP%20Insights/SeCom:%20%E9%87%8D%E6%96%B0%E5%AE%9A%E4%B9%89%E5%AF%B9%E8%AF%9DAI%E7%9A%84%E8%AE%B0%E5%BF%86%E7%AE%A1%E7%90%86/"/>
    <id>https://chenhuiyu.github.io/2025/06/24/NLP%20Insights/SeCom:%20%E9%87%8D%E6%96%B0%E5%AE%9A%E4%B9%89%E5%AF%B9%E8%AF%9DAI%E7%9A%84%E8%AE%B0%E5%BF%86%E7%AE%A1%E7%90%86/</id>
    <published>2025-06-24T08:00:00.000Z</published>
    <updated>2026-02-20T21:51:54.083Z</updated>
    
    <content type="html"><![CDATA[<h1 id="SeCom-重新定义对话AI的记忆管理"><a href="#SeCom-重新定义对话AI的记忆管理" class="headerlink" title="SeCom: 重新定义对话AI的记忆管理"></a>SeCom: 重新定义对话AI的记忆管理</h1><h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>最近笔者一直在研究对话AI中的内存管理问题，特别是长期对话场景下的记忆构建与检索技术。发现了一篇令人眼前一亮的ICLR 2025论文——<strong>《SeCom: On Memory Construction and Retrieval for Personalized Conversational Agents》</strong>，由Microsoft和清华大学的研究团队联合发表。</p><p>这篇论文提出的SeCom方法巧妙地解决了一个核心问题：<strong>如何在长期对话中有效管理和检索历史信息</strong>？今天想和大家分享一下这个方法的技术细节和创新点，希望能为从事相关研究的朋友们提供一些启发。</p><h2 id="1-为什么我们需要关注对话内存管理？"><a href="#1-为什么我们需要关注对话内存管理？" class="headerlink" title="1. 为什么我们需要关注对话内存管理？"></a>1. 为什么我们需要关注对话内存管理？</h2><h3 id="1-1-长期对话的现实挑战"><a href="#1-1-长期对话的现实挑战" class="headerlink" title="1.1 长期对话的现实挑战"></a>1.1 长期对话的现实挑战</h3><p>在与LLMs的日常交互中，相信大家都遇到过这样的困扰：当对话变得很长时，AI似乎”忘记”了之前讨论的内容，或者给出的回答与前面的上下文不够连贯。这背后反映的正是长期对话中的内存管理挑战。</p><p>随着大语言模型技术的成熟，基于LLM的对话代理已经深入到我们生活的方方面面。但是，当我们希望与AI进行真正的长期、个性化交互时——比如跨越数天、数周的项目讨论，现有的技术就显得力不从心了。</p><p>长期对话面临的主要技术挑战包括：</p><ul><li><strong>上下文长度限制</strong>：即使是支持长上下文的模型，在处理超长对话时也面临计算成本和性能下降的问题</li><li><strong>信息相关性</strong>：历史对话中可能包含大量与当前查询无关的信息</li><li><strong>语义连贯性</strong>：相关信息可能分散在多个不连续的对话轮次中</li><li><strong>个性化记忆</strong>：需要记住用户的偏好、习惯和历史交互模式</li></ul><h3 id="1-2-笔者对Memory管理领域的观察"><a href="#1-2-笔者对Memory管理领域的观察" class="headerlink" title="1.2 笔者对Memory管理领域的观察"></a>1.2 笔者对Memory管理领域的观察</h3><p>在深入研究这个领域的过程中，笔者发现对话内存管理其实是一个相当复杂的系统工程。它的核心目标听起来很简单：从历史对话中提取、存储和检索相关信息，以支持当前对话的生成。但实际实现起来，需要解决三个关键问题：</p><ol><li><strong>内存构建（Memory Construction）</strong>：如何将自然语言对话转换为结构化的内存单元？</li><li><strong>内存检索（Memory Retrieval）</strong>：面对海量历史信息，如何快速准确地找到相关内容？</li><li><strong>响应生成（Response Generation）</strong>：如何基于检索到的记忆生成连贯、个性化的回复？</li></ol><p>听起来是不是很像人类的记忆机制？确实如此，这也是为什么这个问题如此有趣和具有挑战性。</p><h4 id="1-2-2-现有方法的”三国演义”"><a href="#1-2-2-现有方法的”三国演义”" class="headerlink" title="1.2.2 现有方法的”三国演义”"></a>1.2.2 现有方法的”三国演义”</h4><p>在研究过程中，笔者发现现有的方法大致可以分为三大流派，每个都有自己的”哲学”：</p><p><strong>“全盘托出”派（基于完整历史）</strong>：</p><ul><li><strong>核心思想</strong>：既然不知道什么重要，那就全部给你！</li><li><strong>优势</strong>：信息完整，绝不遗漏</li><li><strong>问题</strong>：就像把整个图书馆搬给你找一本书，效率可想而知</li></ul><p><strong>“提纲挈领”派（基于摘要）</strong>：</p><ul><li><strong>核心思想</strong>：重要的信息浓缩成摘要就够了</li><li><strong>优势</strong>：信息压缩，计算高效</li><li><strong>问题</strong>：摘要过程中重要细节可能”意外失踪”</li></ul><p><strong>“精准打击”派（基于检索）</strong>：</p><ul><li><strong>代表方法</strong>：轮次级检索、会话级检索</li><li><strong>核心思想</strong>：需要什么就检索什么，按需取用</li><li><strong>优势</strong>：计算效率高，定位精确</li><li><strong>问题</strong>：关键在于如何确定检索的”粒度”——这正是SeCom要解决的核心问题！</li></ul><h4 id="1-2-3-检索增强生成（RAG）在对话中的应用"><a href="#1-2-3-检索增强生成（RAG）在对话中的应用" class="headerlink" title="1.2.3 检索增强生成（RAG）在对话中的应用"></a>1.2.3 检索增强生成（RAG）在对话中的应用</h4><p>检索增强生成技术在对话系统中的应用日益广泛，主要包括：</p><ul><li>**Dense Passage Retrieval (DPR)**：使用预训练的密集检索模型</li><li><strong>BM25</strong>：基于词频统计的稀疏检索方法</li><li><strong>Hybrid Retrieval</strong>：结合密集检索和稀疏检索的优势</li></ul><p>然而，现有RAG方法在对话场景中面临独特挑战：</p><ul><li><strong>分块策略（Chunking Strategy）</strong>：如何将对话分割为检索单元</li><li><strong>相关性判断</strong>：对话的相关性判断比文档检索更复杂</li><li><strong>时序依赖</strong>：对话具有强时序性，前后文关系重要</li></ul><h3 id="1-3-内存粒度问题的深层分析"><a href="#1-3-内存粒度问题的深层分析" class="headerlink" title="1.3 内存粒度问题的深层分析"></a>1.3 内存粒度问题的深层分析</h3><h4 id="1-3-1-轮次级内存的局限性"><a href="#1-3-1-轮次级内存的局限性" class="headerlink" title="1.3.1 轮次级内存的局限性"></a>1.3.1 轮次级内存的局限性</h4><p>轮次级内存将每个用户-代理交互（turn）作为独立的内存单元：</p><p><strong>数学表示</strong>：<br>设对话历史 $\mathcal{H} = {\mathbf{c}<em>i}</em>{i=1}^C$，其中每个会话 $\mathbf{c}<em>i = {\mathbf{t}<em>j}</em>{j=1}^{T_i}$<br>轮次级内存：$|\mathcal{M}| = \sum</em>{i=1}^C T_i$，每个 $\mathbf{m} \in \mathcal{M}$ 对应一个轮次 $\mathbf{t}$</p><p><strong>主要问题</strong>：</p><ul><li><strong>信息碎片化</strong>：相关信息分散在多个轮次中，单个轮次可能缺乏完整语义</li><li><strong>上下文缺失</strong>：轮次间的依赖关系丢失</li><li><strong>检索精度低</strong>：查询词汇可能不直接出现在相关轮次中</li></ul><p><strong>具体示例</strong>：<br>用户在第3轮询问”什么是机器学习”，第5轮询问”监督学习的例子”，第8轮询问”如何选择算法”。当用户在第10轮询问”之前提到的分类算法性能如何评估”时，轮次级检索可能无法找到完整的上下文。</p><h4 id="1-3-2-会话级内存的局限性"><a href="#1-3-2-会话级内存的局限性" class="headerlink" title="1.3.2 会话级内存的局限性"></a>1.3.2 会话级内存的局限性</h4><p>会话级内存将整个对话会话作为内存单元：</p><p><strong>数学表示</strong>：<br>会话级内存：$|\mathcal{M}| = C$，每个 $\mathbf{m} \in \mathcal{M}$ 对应一个会话 $\mathbf{c}$</p><p><strong>主要问题</strong>：</p><ul><li><strong>主题混杂</strong>：单个会话可能包含多个不相关主题</li><li><strong>噪声干扰</strong>：大量无关信息影响检索和生成质量</li><li><strong>检索粗糙</strong>：无法精确定位到具体相关内容</li></ul><p><strong>具体示例</strong>：<br>一个会话中用户讨论了机器学习、烹饪食谱、旅行计划和电影推荐。当查询机器学习相关问题时，检索到的会话包含大量无关的烹饪和旅行信息。</p><h4 id="1-3-3-摘要化方法的信息损失"><a href="#1-3-3-摘要化方法的信息损失" class="headerlink" title="1.3.3 摘要化方法的信息损失"></a>1.3.3 摘要化方法的信息损失</h4><p>摘要化方法通过压缩对话内容来减少信息量：</p><p><strong>主要问题</strong>：</p><ul><li><strong>细节丢失</strong>：摘要过程中重要细节可能被省略</li><li><strong>主观性</strong>：摘要质量依赖于模型的理解能力</li><li><strong>不可逆性</strong>：一旦信息被摘要，原始细节无法恢复</li></ul><h2 id="2-SeCom的设计"><a href="#2-SeCom的设计" class="headerlink" title="2. SeCom的设计"></a>2. SeCom的设计</h2><h3 id="2-1-核心发现"><a href="#2-1-核心发现" class="headerlink" title="2.1 核心发现"></a>2.1 核心发现</h3><p>SeCom（<strong>Se</strong>gmentation + <strong>Com</strong>pression）的两个核心发现：</p><p><strong>洞察一：对话天然具有”段落”结构</strong><br>就像我们写文章会分段一样，人类的对话其实也有天然的主题边界。比如在一次长对话中，我们可能先讨论工作项目，然后转到周末计划，再聊到最近看的电影。每个主题就是一个天然的”段落”。</p><p>传统方法要么把每句话当作独立单元（太碎片化），要么把整个对话当作一个整体（太粗糙），而SeCom找到了中间的最佳平衡点——<strong>段落级的语义单元</strong>。</p><p><strong>洞察二：自然语言充满”废话”</strong><br>这听起来有点刻薄，但确实如此。我们日常对话中充满了”嗯”、”那个”、”你知道的”这样的冗余表达，还有大量的重复、确认、客套话。这些在人际交流中很重要，但对机器检索来说就是噪声。</p><p>SeCom通过智能压缩，保留关键信息的同时去除这些”噪声”，让检索更加精准。</p><h3 id="2-2-系统设计"><a href="#2-2-系统设计" class="headerlink" title="2.2 系统设计"></a>2.2 系统设计</h3><p>SeCom的整体架构设计非常优雅，就像一条高效的流水线：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">历史对话 → [分段器] → 段落级内存单元 → [压缩器] → 去噪内存单元 → [检索器] → 相关上下文 → [生成器] → 最终回复</span><br></pre></td></tr></tbody></table></figure><p>用更直观的话来解释这个流程：</p><ol><li><strong>分段器</strong>：将杂乱的对话历史按主题”切块”</li><li><strong>压缩器</strong>：将每个”块”中的废话去掉，保留精华</li><li><strong>检索器</strong>：根据当前问题找到最相关的”块”</li><li><strong>生成器</strong>：基于相关信息生成回答</li></ol><p><strong>技术表示</strong>（没什么用，写给喜欢数学的朋友）：<br>设 $f_{\mathcal{I}}$ 为分段器，$f_{Comp}$ 为压缩器，$f_R$ 为检索器，$f_{LLM}$ 为生成器</p><p>完整流程：</p><ol><li>${\mathbf{s}<em>k}</em>{k=1}^K \leftarrow f_{\mathcal{I}}(\mathcal{H})$ （对话分段）</li><li>${\mathbf{m}<em>k}</em>{k=1}^K \leftarrow f_{Comp}({\mathbf{s}<em>k}</em>{k=1}^K)$ （压缩去噪）</li><li>${\mathbf{m}<em>n}</em>{n=1}^N \leftarrow f_R(u^*, {\mathbf{m}<em>k}</em>{k=1}^K, N)$ （内存检索）</li><li>$r^* = f_{LLM}(u^*, {\mathbf{m}<em>n}</em>{n=1}^N)$ （响应生成）</li></ol><h3 id="2-3-分段算法：教AI学会”断句”"><a href="#2-3-分段算法：教AI学会”断句”" class="headerlink" title="2.3 分段算法：教AI学会”断句”"></a>2.3 分段算法：教AI学会”断句”</h3><h4 id="2-3-1-零样本分段"><a href="#2-3-1-零样本分段" class="headerlink" title="2.3.1 零样本分段"></a>2.3.1 零样本分段</h4><p>如何让AI自动识别对话中的主题边界？传统方法需要大量标注数据训练专门的分段模型，而SeCom采用了一个非常聪明的”零样本”方法。</p><p><strong>核心思路</strong>：<br>既然GPT-4这样的大模型已经具备了强大的文本理解能力，为什么不直接让它来判断对话的主题边界呢？就像让一个文学老师来给文章分段一样。</p><p><strong>输入预处理</strong>：<br>将对话会话增强为结构化格式：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Turn j: </span><br><span class="line">[user]: u_j</span><br><span class="line">[agent]: r_j</span><br></pre></td></tr></tbody></table></figure><p><strong>分段提示设计</strong>：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">分析以下对话，识别主题边界，将对话分割为语义连贯的段落。</span><br><span class="line">每个段落应该：</span><br><span class="line">1. 围绕单一主题或相关主题</span><br><span class="line">2. 包含完整的交互序列</span><br><span class="line">3. 具有明确的开始和结束边界</span><br><span class="line"></span><br><span class="line">对话内容：</span><br><span class="line">[对话内容]</span><br><span class="line"></span><br><span class="line">请输出每个段落的起始和结束轮次编号。</span><br></pre></td></tr></tbody></table></figure><p><strong>优势</strong>：</p><ul><li>无需训练数据，适用于开放域对话</li><li>利用LLM的强大理解能力</li><li>可处理复杂的主题转换模式</li></ul><h4 id="2-3-2-基于反思的分段优化"><a href="#2-3-2-基于反思的分段优化" class="headerlink" title="2.3.2 基于反思的分段优化"></a>2.3.2 基于反思的分段优化</h4><p>当有少量标注数据时，采用反思机制优化分段效果：</p><p><strong>算法步骤</strong>：</p><ol><li><strong>初始分段</strong>：使用零样本方法对批量数据进行分段</li><li><strong>错误识别</strong>：基于WindowDiff指标选择top-K个分段错误最大的样本</li><li><strong>反思学习</strong>：让LLM分析分段错误，更新分段指导原则</li><li><strong>迭代优化</strong>：重复上述过程直到收敛</li></ol><p><strong>反思提示设计</strong>：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">分析以下分段错误，并更新分段指导原则：</span><br><span class="line"></span><br><span class="line">错误案例：</span><br><span class="line">[分段结果] vs [标准答案]</span><br><span class="line"></span><br><span class="line">请分析错误原因并提供改进的分段指导原则。</span><br></pre></td></tr></tbody></table></figure><p><strong>数学表示</strong>：<br>设 $\boldsymbol{G}<em>m$ 为第m轮的分段指导原则，更新公式为：<br>$$\boldsymbol{G}</em>{m+1} = \boldsymbol{G}_m - \eta \nabla \mathcal{L}(\boldsymbol{G}_m)$$</p><p>其中 $\nabla \mathcal{L}(\boldsymbol{G}_m)$ 为LLM隐式估计的分段损失梯度。</p><h4 id="2-3-3-增量分段算法"><a href="#2-3-3-增量分段算法" class="headerlink" title="2.3.3 增量分段算法"></a>2.3.3 增量分段算法</h4><p>对于新增的对话轮次，设计增量分段算法：</p><p><strong>算法流程</strong>：</p><ol><li>输入新轮次 $\mathbf{t}<em>{new}$ 和前一段落 $\mathbf{s}</em>{prev}$</li><li>判断是否应该合并：$binary = f_{judge}(\mathbf{t}<em>{new}, \mathbf{s}</em>{prev})$</li><li>如果合并：$\mathbf{s}<em>{prev} \leftarrow \mathbf{s}</em>{prev} \cup {\mathbf{t}_{new}}$</li><li>否则：创建新段落 $\mathbf{s}<em>{new} = {\mathbf{t}</em>{new}}$</li></ol><p><strong>判断提示</strong>：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">判断新的用户-机器人轮次是否应该与前一段落合并：</span><br><span class="line"></span><br><span class="line">新轮次：[新轮次内容]</span><br><span class="line">前一段落：[前一段落内容]</span><br><span class="line"></span><br><span class="line">如果属于同一主题，回答"Yes"，否则回答"No"。</span><br></pre></td></tr></tbody></table></figure><h3 id="2-4-压缩式内存去噪"><a href="#2-4-压缩式内存去噪" class="headerlink" title="2.4 压缩式内存去噪"></a>2.4 压缩式内存去噪</h3><h4 id="2-4-1-自然语言冗余性分析"><a href="#2-4-1-自然语言冗余性分析" class="headerlink" title="2.4.1 自然语言冗余性分析"></a>2.4.1 自然语言冗余性分析</h4><p><strong>理论基础</strong>：<br>根据Shannon信息论，自然语言具有高度冗余性，冗余率约为50-75%。这种冗余在人类交流中有助于错误纠正和理解，但在机器检索中构成噪声。</p><p><strong>冗余类型</strong>：</p><ol><li><strong>词汇冗余</strong>：同义词、重复表达</li><li><strong>语法冗余</strong>：冗余的语法结构</li><li><strong>语义冗余</strong>：重复的语义信息</li><li><strong>对话冗余</strong>：客套话、确认性回复</li></ol><h4 id="2-4-2-LLMLingua-2压缩原理"><a href="#2-4-2-LLMLingua-2压缩原理" class="headerlink" title="2.4.2 LLMLingua-2压缩原理"></a>2.4.2 LLMLingua-2压缩原理</h4><p><strong>算法核心</strong>：<br>LLMLingua-2基于token重要性评分进行压缩：</p><ol><li><p><strong>重要性评分</strong>：<br>$$s_i = f_{score}(x_i | x_{&lt;i}, x_{&gt;i})$$<br>其中 $x_i$ 为第i个token，$x_{&lt;i}$ 和 $x_{&gt;i}$ 为上下文</p></li><li><p><strong>动态压缩</strong>：<br>根据目标压缩率 $r$，保留top $(1-r) \times N$ 个重要token</p></li><li><p><strong>语义保持</strong>：<br>通过双向上下文建模确保关键语义信息不丢失</p></li></ol><p><strong>压缩效果分析</strong>：<br>实验表明，75%压缩率下：</p><ul><li>关键信息保留率 &gt; 95%</li><li>检索相关性提升 9.46分（GPT4Score）</li><li>计算效率提升 4倍</li></ul><h4 id="2-4-3-压缩对检索性能的影响"><a href="#2-4-3-压缩对检索性能的影响" class="headerlink" title="2.4.3 压缩对检索性能的影响"></a>2.4.3 压缩对检索性能的影响</h4><p><strong>相似性变化分析</strong>：<br>设 $sim(q, s)$ 为查询q与段落s的相似性</p><p>压缩前：$sim_{before}(q, s_{relevant})$，$sim_{before}(q, s_{irrelevant})$<br>压缩后：$sim_{after}(q, s’<em>{relevant})$，$sim</em>{after}(q, s’_{irrelevant})$</p><p>实验结果显示：</p><ul><li>$sim_{after}(q, s’<em>{relevant}) &gt; sim</em>{before}(q, s_{relevant})$ （相关段落相似性提升）</li><li>$sim_{after}(q, s’<em>{irrelevant}) &lt; sim</em>{before}(q, s_{irrelevant})$ （无关段落相似性降低）</li></ul><h3 id="2-5-多模态检索系统"><a href="#2-5-多模态检索系统" class="headerlink" title="2.5 多模态检索系统"></a>2.5 多模态检索系统</h3><h4 id="2-5-1-检索器选择与配置"><a href="#2-5-1-检索器选择与配置" class="headerlink" title="2.5.1 检索器选择与配置"></a>2.5.1 检索器选择与配置</h4><p><strong>BM25检索器</strong>：<br>$$BM25(q, d) = \sum_{i=1}^{|q|} IDF(q_i) \cdot \frac{tf(q_i, d) \cdot (k_1 + 1)}{tf(q_i, d) + k_1 \cdot (1 - b + b \cdot \frac{|d|}{avgdl})}$$</p><p>参数设置：$k_1 = 1.2$，$b = 0.75$</p><p><strong>MPNet检索器</strong>：<br>基于MPNet模型的密集检索：<br>$$score = \cos(\mathbf{e}_q, \mathbf{e}_d)$$<br>其中 $\mathbf{e}_q$ 和 $\mathbf{e}_d$ 分别为查询和文档的向量表示</p><h4 id="2-5-2-混合检索策略"><a href="#2-5-2-混合检索策略" class="headerlink" title="2.5.2 混合检索策略"></a>2.5.2 混合检索策略</h4><p>结合稀疏检索和密集检索的优势：<br>$$score_{hybrid} = \alpha \cdot score_{BM25} + (1-\alpha) \cdot score_{MPNet}$$</p><p>通过实验确定最优权重 $\alpha = 0.6$</p><h2 id="3-写在最后：一些思考"><a href="#3-写在最后：一些思考" class="headerlink" title="3. 写在最后：一些思考"></a>3. 写在最后：一些思考</h2><h3 id="3-1-SeCom给我们的启发"><a href="#3-1-SeCom给我们的启发" class="headerlink" title="3.1 SeCom给我们的启发"></a>3.1 SeCom给我们的启发</h3><p>研读这篇论文后，笔者有几点深刻的感悟：</p><p><strong>简单往往是最有效的</strong>：SeCom的核心思想其实很简单——分段+压缩，但正是这种简单的组合解决了复杂的问题。这提醒我们，在面对技术挑战时，有时候最朴素的想法反而是最有效的。</p><p><strong>理解问题比解决问题更重要</strong>：作者团队深入分析了内存粒度问题的本质，发现了段落级内存的最优性。这种对问题本质的深刻理解是技术创新的基础。</p><p>笔者认为未来可能的发展方向包括：</p><ul><li><strong>个性化分段策略</strong>：不同用户的对话模式不同，能否学习个性化的分段方式？</li><li><strong>实时优化机制</strong>：能否根据对话质量动态调整压缩率和分段策略？</li></ul><hr><h2 id="参考资源"><a href="#参考资源" class="headerlink" title="参考资源"></a>参考资源</h2><ul><li><strong>论文链接</strong>：<a href="https://www.arxiv.org/abs/2502.05589">SeCom: On Memory Construction and Retrieval for Personalized Conversational Agents (ICLR 2025)</a></li><li><strong>项目主页</strong>：<a href="https://llmlingua.com/secom.html">https://llmlingua.com/secom.html</a></li><li><strong>代码仓库</strong>：SeCom-main项目</li><li><strong>数据集</strong>：LOCOMO、Long-MT-Bench+、DialSeg711、TIAGE、SuperDialSeg</li></ul><p><em>本文基于Microsoft和清华大学联合研究团队在ICLR 2025发表的论文撰写，详细技术实现请参考原始论文和开源代码。</em> </p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;SeCom-重新定义对话AI的记忆管理&quot;&gt;&lt;a href=&quot;#SeCom-重新定义对话AI的记忆管理&quot; class=&quot;headerlink&quot; title=&quot;SeCom: 重新定义对话AI的记忆管理&quot;&gt;&lt;/a&gt;SeCom: 重新定义对话AI的记忆管理&lt;/h1&gt;&lt;h2</summary>
      
    
    
    
    <category term="NLP Insights" scheme="https://chenhuiyu.github.io/categories/NLP-Insights/"/>
    
    
    <category term="Conversational AI" scheme="https://chenhuiyu.github.io/tags/Conversational-AI/"/>
    
    <category term="Memory Management" scheme="https://chenhuiyu.github.io/tags/Memory-Management/"/>
    
    <category term="SeCom" scheme="https://chenhuiyu.github.io/tags/SeCom/"/>
    
    <category term="RAG" scheme="https://chenhuiyu.github.io/tags/RAG/"/>
    
  </entry>
  
  <entry>
    <title>Decoder-only与Encoder-only模型Padding策略的差异</title>
    <link href="https://chenhuiyu.github.io/2025/03/06/NLP%20Insights/Decoder%E6%A8%A1%E5%9E%8B%E5%92%8CEncoder%E6%A8%A1%E5%9E%8B%E5%9C%A8Padding%E4%B8%AD%E7%9A%84%E4%B8%8D%E5%90%8C.en/"/>
    <id>https://chenhuiyu.github.io/2025/03/06/NLP%20Insights/Decoder%E6%A8%A1%E5%9E%8B%E5%92%8CEncoder%E6%A8%A1%E5%9E%8B%E5%9C%A8Padding%E4%B8%AD%E7%9A%84%E4%B8%8D%E5%90%8C.en/</id>
    <published>2025-03-06T09:43:10.000Z</published>
    <updated>2026-02-20T21:47:32.626Z</updated>
    
    <content type="html"><![CDATA[<h2 id="📌-Padding-的含义"><a href="#📌-Padding-的含义" class="headerlink" title="📌 Padding 的含义"></a>📌 <strong>Padding 的含义</strong></h2><p>在大模型 (<strong>LLM</strong>) 中，<strong>padding</strong> 是用于将不同长度的序列调整为同一长度的方法，以便于批量 (<strong>batch</strong>) 处理。</p><p>例如：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">句子1: "I love NLP"</span><br><span class="line">句子2: "Padding is useful in LLM training"</span><br></pre></td></tr></tbody></table></figure><p>使用 <code>&lt;pad&gt;</code> token 进行对齐：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">"I love NLP &lt;pad&gt; &lt;pad&gt; &lt;pad&gt;"</span><br><span class="line">"Padding is useful in LLM training"</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="📌-Padding-位置的选择：Left-vs-Right"><a href="#📌-Padding-位置的选择：Left-vs-Right" class="headerlink" title="📌 Padding 位置的选择：Left vs Right"></a>📌 <strong>Padding 位置的选择：Left vs Right</strong></h2><p>Padding 有两种常见方式：</p><ul><li><p><strong>Right padding</strong>（右填充）：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">"I love NLP &lt;pad&gt; &lt;pad&gt;"</span><br></pre></td></tr></tbody></table></figure></li><li><p><strong>Left padding</strong>（左填充）：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">"&lt;pad&gt; &lt;pad&gt; I love NLP"</span><br></pre></td></tr></tbody></table></figure></li></ul><p>通常：</p><ul><li><strong>Decoder-only 模型</strong>（如 GPT, Llama）：采用 <strong>Left padding</strong></li><li><strong>Encoder-only 模型</strong>（如 BERT）：采用 <strong>Right padding</strong></li></ul><p>具体而言，Transformer 模型通常分为三类结构：</p><table><thead><tr><th>模型类型</th><th>代表模型</th><th>特征</th><th>常见用途</th></tr></thead><tbody><tr><td><strong>Encoder-only</strong></td><td><strong>BERT</strong>、RoBERTa、ALBERT、ELECTRA</td><td>双向注意力（Bidirectional Attention）</td><td>自然语言理解（NLU），如文本分类、序列标注</td></tr><tr><td><strong>Decoder-only</strong></td><td>GPT、GPT-2、GPT-3、GPT-4、LLaMA、Mistral</td><td>单向自回归注意力（Causal Attention）</td><td>文本生成、聊天、写作</td></tr><tr><td><strong>Encoder-Decoder</strong></td><td>Transformer原始论文中的模型、T5、BART、mT5、PEGASUS</td><td>Encoder为双向注意力，Decoder为单向自回归注意力</td><td>机器翻译、摘要生成、对话</td></tr></tbody></table><hr><h2 id="📌-为什么-Encoder-only-模型（如BERT）采用-Right-padding？"><a href="#📌-为什么-Encoder-only-模型（如BERT）采用-Right-padding？" class="headerlink" title="📌 为什么 Encoder-only 模型（如BERT）采用 Right padding？"></a>📌 为什么 Encoder-only 模型（如BERT）采用 Right padding？</h2><ul><li><strong>Encoder-only 模型</strong>（如 BERT）的核心目标是获得<strong>每个 token 的嵌入表示</strong>（Embedding representation）。</li><li>此类模型为<strong>双向注意力（Bidirectional Attention）</strong>，每个 token 可同时关注上下文，因此<strong>位置的轻微变化不会对结果造成严重干扰</strong>。</li><li>此外，encoder-only 模型中通常有特殊 token（如 <code>[CLS]</code>），位置相对稳定，用于句子分类或表示，因此采用 <strong>right padding</strong> 更自然，也更合理。</li></ul><p>示例说明：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[CLS] Hello I love NLP [SEP] &lt;pad&gt; &lt;pad&gt;</span><br></pre></td></tr></tbody></table></figure><ul><li>右填充后，<code>[CLS]</code> 和 <code>[SEP]</code> token 位置稳定，且便于模型专注于前面的有效信息。</li></ul><hr><h2 id="📌-为什么-Decoder-only-LLM-采用-Left-padding？"><a href="#📌-为什么-Decoder-only-LLM-采用-Left-padding？" class="headerlink" title="📌 为什么 Decoder-only LLM 采用 Left padding？"></a>📌 为什么 Decoder-only LLM 采用 Left padding？</h2><p>以 GPT 为代表的 <strong>Decoder-only 模型</strong> 是自回归（<strong>Autoregressive</strong>）模型，每个词的生成仅依赖于当前及之前的词，未来词不可见。因此：</p><ul><li><strong>位置编码的稳定性</strong>：<br>左填充确保真实 token 的相对位置稳定，模型生成新 token 时位置编码始终稳定于序列末尾。<ul><li>当采用<strong>绝对位置编码</strong>（Absolute Positional Encoding）时，每个 token（包括 <code>&lt;pad&gt;</code>）都有对应的位置编号。</li><li>对于左填充的 padding tokens，虽然它们占据了位置编号（如 1、2），但模型通过<strong>掩码机制</strong>忽略其对注意力和输出结果的影响。<br>示例：</li></ul></li></ul><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">位置编码: [ 1      2      3      4      5      6 ]</span><br><span class="line">Token:   [ &lt;pad&gt;, &lt;pad&gt;, Hello,  I,   love,  NLP ]</span><br><span class="line">掩码:     [  0,     0,     1,     1,     1,    1 ]</span><br></pre></td></tr></tbody></table></figure><ul><li>模型只关注掩码为 1 的有效 token，而忽略掩码为 0 的 padding tokens。</li><li><strong>注意力掩码（Attention Mask）</strong>：<br>左侧的 <code>&lt;pad&gt;</code> 会被<strong>注意力掩码（attention mask）忽略</strong>，从而避免 padding token 干扰有效 token 的位置编码和注意力计算。</li></ul><p>示例说明：</p><table><thead><tr><th>Token</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th></tr></thead><tbody><tr><td>Left</td><td><code>&lt;pad&gt;</code></td><td><code>&lt;pad&gt;</code></td><td>Hello</td><td>I</td><td>love</td><td>NLP</td></tr><tr><td>Right</td><td>Hello</td><td>I</td><td>love</td><td>NLP</td><td><code>&lt;pad&gt;</code></td><td><code>&lt;pad&gt;</code></td></tr></tbody></table><ul><li><strong>Left padding</strong> 下，最后有效 token 始终在同一位置（6）。</li><li><strong>Right padding</strong> 下，token 的位置随序列长度变化，影响位置编码的稳定性。</li></ul><hr><h2 id="📌-Padding-在训练与推理阶段的差异"><a href="#📌-Padding-在训练与推理阶段的差异" class="headerlink" title="📌 Padding 在训练与推理阶段的差异"></a>📌 <strong>Padding 在训练与推理阶段的差异</strong></h2><table><thead><tr><th>阶段 (Phase)</th><th>Padding 策略</th><th>原因</th></tr></thead><tbody><tr><td><strong>训练 (Training)</strong></td><td>批量处理时，Decoder-only 常用左填充；Encoder-only 模型则常用右填充</td><td>批量处理，加快计算效率</td></tr><tr><td><strong>推理 (Inference)</strong></td><td>通常单条序列，无需 padding；若需要批量推理，仍采用左填充</td><td>稳定位置编码</td></tr></tbody></table><hr><h2 id="📌-总结与关键要点（TL-DR）"><a href="#📌-总结与关键要点（TL-DR）" class="headerlink" title="📌 总结与关键要点（TL;DR）"></a>📌 <strong>总结与关键要点（TL;DR）</strong></h2><ul><li><strong>Padding</strong> 用于序列长度标准化。</li><li><strong>Decoder-only LLMs (GPT, Llama)</strong> 通常采用<strong>左填充（Left padding）</strong>，目的是<strong>稳定位置编码并避免未来信息泄漏</strong>；左侧 padding 会被掩码忽略，不干扰模型预测。</li><li><strong>Encoder-only 模型（如BERT系列）</strong>通常采用<strong>右填充（Right padding）</strong>，因为模型为双向注意力，且特殊token（如<code>[CLS]</code>）位置需要保持稳定。</li><li>位置编码中虽然 padding token 占位，但会被<strong>注意力掩码</strong>有效屏蔽，不影响模型的最终输出。</li></ul><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;📌-Padding-的含义&quot;&gt;&lt;a href=&quot;#📌-Padding-的含义&quot; class=&quot;headerlink&quot; title=&quot;📌 Padding 的含义&quot;&gt;&lt;/a&gt;📌 &lt;strong&gt;Padding 的含义&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;在大模型 </summary>
      
    
    
    
    <category term="NLP Insights" scheme="https://chenhuiyu.github.io/categories/NLP-Insights/"/>
    
    
    <category term="LLM" scheme="https://chenhuiyu.github.io/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>Decoder-only与Encoder-only模型Padding策略的差异</title>
    <link href="https://chenhuiyu.github.io/2025/03/06/NLP%20Insights/Decoder%E6%A8%A1%E5%9E%8B%E5%92%8CEncoder%E6%A8%A1%E5%9E%8B%E5%9C%A8Padding%E4%B8%AD%E7%9A%84%E4%B8%8D%E5%90%8C/"/>
    <id>https://chenhuiyu.github.io/2025/03/06/NLP%20Insights/Decoder%E6%A8%A1%E5%9E%8B%E5%92%8CEncoder%E6%A8%A1%E5%9E%8B%E5%9C%A8Padding%E4%B8%AD%E7%9A%84%E4%B8%8D%E5%90%8C/</id>
    <published>2025-03-06T09:43:10.000Z</published>
    <updated>2026-02-20T21:51:54.081Z</updated>
    
    <content type="html"><![CDATA[<h2 id="📌-Padding-的含义"><a href="#📌-Padding-的含义" class="headerlink" title="📌 Padding 的含义"></a>📌 <strong>Padding 的含义</strong></h2><p>在大模型 (<strong>LLM</strong>) 中，<strong>padding</strong> 是用于将不同长度的序列调整为同一长度的方法，以便于批量 (<strong>batch</strong>) 处理。</p><p>例如：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">句子1: "I love NLP"</span><br><span class="line">句子2: "Padding is useful in LLM training"</span><br></pre></td></tr></tbody></table></figure><p>使用 <code>&lt;pad&gt;</code> token 进行对齐：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">"I love NLP &lt;pad&gt; &lt;pad&gt; &lt;pad&gt;"</span><br><span class="line">"Padding is useful in LLM training"</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="📌-Padding-位置的选择：Left-vs-Right"><a href="#📌-Padding-位置的选择：Left-vs-Right" class="headerlink" title="📌 Padding 位置的选择：Left vs Right"></a>📌 <strong>Padding 位置的选择：Left vs Right</strong></h2><p>Padding 有两种常见方式：</p><ul><li><p><strong>Right padding</strong>（右填充）：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">"I love NLP &lt;pad&gt; &lt;pad&gt;"</span><br></pre></td></tr></tbody></table></figure></li><li><p><strong>Left padding</strong>（左填充）：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">"&lt;pad&gt; &lt;pad&gt; I love NLP"</span><br></pre></td></tr></tbody></table></figure></li></ul><p>通常：</p><ul><li><strong>Decoder-only 模型</strong>（如 GPT, Llama）：采用 <strong>Left padding</strong></li><li><strong>Encoder-only 模型</strong>（如 BERT）：采用 <strong>Right padding</strong></li></ul><p>具体而言，Transformer 模型通常分为三类结构：</p><table><thead><tr><th>模型类型</th><th>代表模型</th><th>特征</th><th>常见用途</th></tr></thead><tbody><tr><td><strong>Encoder-only</strong></td><td><strong>BERT</strong>、RoBERTa、ALBERT、ELECTRA</td><td>双向注意力（Bidirectional Attention）</td><td>自然语言理解（NLU），如文本分类、序列标注</td></tr><tr><td><strong>Decoder-only</strong></td><td>GPT、GPT-2、GPT-3、GPT-4、LLaMA、Mistral</td><td>单向自回归注意力（Causal Attention）</td><td>文本生成、聊天、写作</td></tr><tr><td><strong>Encoder-Decoder</strong></td><td>Transformer原始论文中的模型、T5、BART、mT5、PEGASUS</td><td>Encoder为双向注意力，Decoder为单向自回归注意力</td><td>机器翻译、摘要生成、对话</td></tr></tbody></table><hr><h2 id="📌-为什么-Encoder-only-模型（如BERT）采用-Right-padding？"><a href="#📌-为什么-Encoder-only-模型（如BERT）采用-Right-padding？" class="headerlink" title="📌 为什么 Encoder-only 模型（如BERT）采用 Right padding？"></a>📌 为什么 Encoder-only 模型（如BERT）采用 Right padding？</h2><ul><li><strong>Encoder-only 模型</strong>（如 BERT）的核心目标是获得<strong>每个 token 的嵌入表示</strong>（Embedding representation）。</li><li>此类模型为<strong>双向注意力（Bidirectional Attention）</strong>，每个 token 可同时关注上下文，因此<strong>位置的轻微变化不会对结果造成严重干扰</strong>。</li><li>此外，encoder-only 模型中通常有特殊 token（如 <code>[CLS]</code>），位置相对稳定，用于句子分类或表示，因此采用 <strong>right padding</strong> 更自然，也更合理。</li></ul><p>示例说明：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[CLS] Hello I love NLP [SEP] &lt;pad&gt; &lt;pad&gt;</span><br></pre></td></tr></tbody></table></figure><ul><li>右填充后，<code>[CLS]</code> 和 <code>[SEP]</code> token 位置稳定，且便于模型专注于前面的有效信息。</li></ul><hr><h2 id="📌-为什么-Decoder-only-LLM-采用-Left-padding？"><a href="#📌-为什么-Decoder-only-LLM-采用-Left-padding？" class="headerlink" title="📌 为什么 Decoder-only LLM 采用 Left padding？"></a>📌 为什么 Decoder-only LLM 采用 Left padding？</h2><p>以 GPT 为代表的 <strong>Decoder-only 模型</strong> 是自回归（<strong>Autoregressive</strong>）模型，每个词的生成仅依赖于当前及之前的词，未来词不可见。因此：</p><ul><li><strong>位置编码的稳定性</strong>：<br>左填充确保真实 token 的相对位置稳定，模型生成新 token 时位置编码始终稳定于序列末尾。<ul><li>当采用<strong>绝对位置编码</strong>（Absolute Positional Encoding）时，每个 token（包括 <code>&lt;pad&gt;</code>）都有对应的位置编号。</li><li>对于左填充的 padding tokens，虽然它们占据了位置编号（如 1、2），但模型通过<strong>掩码机制</strong>忽略其对注意力和输出结果的影响。<br>示例：</li></ul></li></ul><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">位置编码: [ 1      2      3      4      5      6 ]</span><br><span class="line">Token:   [ &lt;pad&gt;, &lt;pad&gt;, Hello,  I,   love,  NLP ]</span><br><span class="line">掩码:     [  0,     0,     1,     1,     1,    1 ]</span><br></pre></td></tr></tbody></table></figure><ul><li>模型只关注掩码为 1 的有效 token，而忽略掩码为 0 的 padding tokens。</li><li><strong>注意力掩码（Attention Mask）</strong>：<br>左侧的 <code>&lt;pad&gt;</code> 会被<strong>注意力掩码（attention mask）忽略</strong>，从而避免 padding token 干扰有效 token 的位置编码和注意力计算。</li></ul><p>示例说明：</p><table><thead><tr><th>Token</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th></tr></thead><tbody><tr><td>Left</td><td><code>&lt;pad&gt;</code></td><td><code>&lt;pad&gt;</code></td><td>Hello</td><td>I</td><td>love</td><td>NLP</td></tr><tr><td>Right</td><td>Hello</td><td>I</td><td>love</td><td>NLP</td><td><code>&lt;pad&gt;</code></td><td><code>&lt;pad&gt;</code></td></tr></tbody></table><ul><li><strong>Left padding</strong> 下，最后有效 token 始终在同一位置（6）。</li><li><strong>Right padding</strong> 下，token 的位置随序列长度变化，影响位置编码的稳定性。</li></ul><hr><h2 id="📌-Padding-在训练与推理阶段的差异"><a href="#📌-Padding-在训练与推理阶段的差异" class="headerlink" title="📌 Padding 在训练与推理阶段的差异"></a>📌 <strong>Padding 在训练与推理阶段的差异</strong></h2><table><thead><tr><th>阶段 (Phase)</th><th>Padding 策略</th><th>原因</th></tr></thead><tbody><tr><td><strong>训练 (Training)</strong></td><td>批量处理时，Decoder-only 常用左填充；Encoder-only 模型则常用右填充</td><td>批量处理，加快计算效率</td></tr><tr><td><strong>推理 (Inference)</strong></td><td>通常单条序列，无需 padding；若需要批量推理，仍采用左填充</td><td>稳定位置编码</td></tr></tbody></table><hr><h2 id="📌-总结与关键要点（TL-DR）"><a href="#📌-总结与关键要点（TL-DR）" class="headerlink" title="📌 总结与关键要点（TL;DR）"></a>📌 <strong>总结与关键要点（TL;DR）</strong></h2><ul><li><strong>Padding</strong> 用于序列长度标准化。</li><li><strong>Decoder-only LLMs (GPT, Llama)</strong> 通常采用<strong>左填充（Left padding）</strong>，目的是<strong>稳定位置编码并避免未来信息泄漏</strong>；左侧 padding 会被掩码忽略，不干扰模型预测。</li><li><strong>Encoder-only 模型（如BERT系列）</strong>通常采用<strong>右填充（Right padding）</strong>，因为模型为双向注意力，且特殊token（如<code>[CLS]</code>）位置需要保持稳定。</li><li>位置编码中虽然 padding token 占位，但会被<strong>注意力掩码</strong>有效屏蔽，不影响模型的最终输出。</li></ul><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;📌-Padding-的含义&quot;&gt;&lt;a href=&quot;#📌-Padding-的含义&quot; class=&quot;headerlink&quot; title=&quot;📌 Padding 的含义&quot;&gt;&lt;/a&gt;📌 &lt;strong&gt;Padding 的含义&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;在大模型 </summary>
      
    
    
    
    <category term="NLP Insights" scheme="https://chenhuiyu.github.io/categories/NLP-Insights/"/>
    
    
    <category term="LLM" scheme="https://chenhuiyu.github.io/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>Differences in Padding Strategies Between Decoder-only and Encoder-only Models</title>
    <link href="https://chenhuiyu.github.io/2025/03/06/NLP%20Insights/Differences%20in%20Padding%20Strategies%20Between%20Decoder-only%20and%20Encoder-only%20Models.en/"/>
    <id>https://chenhuiyu.github.io/2025/03/06/NLP%20Insights/Differences%20in%20Padding%20Strategies%20Between%20Decoder-only%20and%20Encoder-only%20Models.en/</id>
    <published>2025-03-06T09:43:10.000Z</published>
    <updated>2026-02-20T21:47:32.622Z</updated>
    
    <content type="html"><![CDATA[<h2 id="📌-What-is-Padding"><a href="#📌-What-is-Padding" class="headerlink" title="📌 What is Padding?"></a>📌 <strong>What is Padding?</strong></h2><p>In <strong>Large Language Models (LLMs)</strong>, <strong>padding</strong> is a method used to standardize sequence lengths for <strong>batch processing</strong>.</p><p>For example:</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Sentence 1: "I love NLP"</span><br><span class="line">Sentence 2: "Padding is useful in LLM training"</span><br></pre></td></tr></tbody></table></figure><p>Using the <code>&lt;pad&gt;</code> token for alignment:</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">"I love NLP &lt;pad&gt; &lt;pad&gt; &lt;pad&gt;"</span><br><span class="line">"Padding is useful in LLM training"</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="📌-Padding-Positioning-Left-vs-Right"><a href="#📌-Padding-Positioning-Left-vs-Right" class="headerlink" title="📌 Padding Positioning: Left vs Right"></a>📌 <strong>Padding Positioning: Left vs Right</strong></h2><p>There are two common padding strategies:</p><ul><li><p><strong>Right padding</strong>:</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">"I love NLP &lt;pad&gt; &lt;pad&gt;"</span><br></pre></td></tr></tbody></table></figure></li><li><p><strong>Left padding</strong>:</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">"&lt;pad&gt; &lt;pad&gt; I love NLP"</span><br></pre></td></tr></tbody></table></figure></li></ul><p>Typically:</p><ul><li><strong>Decoder-only models</strong> (e.g., GPT, Llama): Use <strong>Left padding</strong>.</li><li><strong>Encoder-only models</strong> (e.g., BERT): Use <strong>Right padding</strong>.</li></ul><p>Transformers can be categorized into three main architectures:</p><table><thead><tr><th>Model Type</th><th>Representative Models</th><th>Characteristics</th><th>Common Applications</th></tr></thead><tbody><tr><td><strong>Encoder-only</strong></td><td><strong>BERT</strong>, RoBERTa, ALBERT, ELECTRA</td><td><strong>Bidirectional attention</strong></td><td>NLP tasks like text classification, named entity recognition</td></tr><tr><td><strong>Decoder-only</strong></td><td>GPT, GPT-2, GPT-3, GPT-4, LLaMA, Mistral</td><td><strong>Causal attention (Autoregressive)</strong></td><td>Text generation, chatbots, writing assistance</td></tr><tr><td><strong>Encoder-Decoder</strong></td><td>Transformer (original), T5, BART, mT5, PEGASUS</td><td><strong>Encoder: bidirectional, Decoder: autoregressive</strong></td><td>Machine translation, summarization, dialogue systems</td></tr></tbody></table><hr><h2 id="📌-Why-Do-Encoder-only-Models-e-g-BERT-Use-Right-Padding"><a href="#📌-Why-Do-Encoder-only-Models-e-g-BERT-Use-Right-Padding" class="headerlink" title="📌 Why Do Encoder-only Models (e.g., BERT) Use Right Padding?"></a>📌 <strong>Why Do Encoder-only Models (e.g., BERT) Use Right Padding?</strong></h2><ul><li><strong>Encoder-only models</strong> (like BERT) aim to obtain <strong>representations for each token</strong>.</li><li>These models use <strong>bidirectional attention</strong>, meaning each token attends to <strong>both past and future tokens</strong>.</li><li><strong>Slight shifts in position do not significantly impact model performance</strong>.</li><li>Special tokens (e.g., <code>[CLS]</code>) in BERT maintain a <strong>fixed position</strong> for tasks like classification, making <strong>right padding more natural</strong>.</li></ul><p>Example:</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[CLS] Hello I love NLP [SEP] &lt;pad&gt; &lt;pad&gt;</span><br></pre></td></tr></tbody></table></figure><ul><li>Right padding keeps <code>[CLS]</code> and <code>[SEP]</code> in stable positions, allowing the model to focus on meaningful tokens.</li></ul><hr><h2 id="📌-Why-Do-Decoder-only-LLMs-Use-Left-Padding"><a href="#📌-Why-Do-Decoder-only-LLMs-Use-Left-Padding" class="headerlink" title="📌 Why Do Decoder-only LLMs Use Left Padding?"></a>📌 <strong>Why Do Decoder-only LLMs Use Left Padding?</strong></h2><p><strong>Decoder-only models</strong> (like GPT) are <strong>autoregressive</strong>, meaning each token is generated based only on <strong>previous tokens</strong>, and future tokens are <strong>masked</strong>.</p><ul><li><strong>Positional Encoding Stability</strong>:<br>Left padding ensures that meaningful tokens have a <strong>consistent relative position</strong>, preventing <strong>position encoding misalignment</strong>.<ul><li>When using <strong>absolute positional encoding</strong>, every token (including <code>&lt;pad&gt;</code>) gets a unique position index.</li><li>Padding tokens at the beginning <strong>do not affect the model’s attention mechanism</strong> due to <strong>masking</strong>.</li></ul></li></ul><p>Example:</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Position Index: [ 1      2      3      4      5      6 ]</span><br><span class="line">Token:         [ &lt;pad&gt;, &lt;pad&gt;, Hello,  I,   love,  NLP ]</span><br><span class="line">Mask:          [  0,     0,     1,     1,     1,    1 ]</span><br></pre></td></tr></tbody></table></figure><ul><li><p>The model <strong>only attends to tokens where the mask is 1</strong>, ignoring padding tokens.</p></li><li><p><strong>Attention Masking</strong>:<br>Left padding ensures that <code>&lt;pad&gt;</code> tokens <strong>do not interfere with token position encoding</strong>.</p></li></ul><p>Illustration:</p><table><thead><tr><th>Token</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th></tr></thead><tbody><tr><td>Left</td><td><code>&lt;pad&gt;</code></td><td><code>&lt;pad&gt;</code></td><td>Hello</td><td>I</td><td>love</td><td>NLP</td></tr><tr><td>Right</td><td>Hello</td><td>I</td><td>love</td><td>NLP</td><td><code>&lt;pad&gt;</code></td><td><code>&lt;pad&gt;</code></td></tr></tbody></table><ul><li><strong>With Left padding</strong>, the last valid token <strong>always remains in the same position</strong>.</li><li><strong>With Right padding</strong>, token positions shift, affecting positional encoding stability.</li></ul><hr><h2 id="📌-Padding-Differences-in-Training-vs-Inference"><a href="#📌-Padding-Differences-in-Training-vs-Inference" class="headerlink" title="📌 Padding Differences in Training vs Inference"></a>📌 <strong>Padding Differences in Training vs Inference</strong></h2><table><thead><tr><th>Phase</th><th>Padding Strategy</th><th>Reason</th></tr></thead><tbody><tr><td><strong>Training</strong></td><td>Left padding for decoder-only; Right padding for encoder-only</td><td>Optimized for batch processing efficiency</td></tr><tr><td><strong>Inference</strong></td><td>Typically, no padding for single sequences; Left padding for batched inference</td><td>Ensures stable positional encoding</td></tr></tbody></table><hr><h2 id="📌-Summary-amp-Key-Takeaways-TL-DR"><a href="#📌-Summary-amp-Key-Takeaways-TL-DR" class="headerlink" title="📌 Summary &amp; Key Takeaways (TL;DR)"></a>📌 <strong>Summary &amp; Key Takeaways (TL;DR)</strong></h2><ul><li><strong>Padding</strong> standardizes sequence lengths for batch processing.</li><li><strong>Decoder-only models (GPT, Llama)</strong> use <strong>Left padding</strong> to <strong>stabilize positional encoding and prevent future token leakage</strong>. Left padding tokens are masked out.</li><li><strong>Encoder-only models (BERT, RoBERTa)</strong> use <strong>Right padding</strong> since they employ <strong>bidirectional attention</strong> and rely on stable special token positions (e.g., <code>[CLS]</code>).</li><li>Although padding tokens occupy positions in <strong>positional encoding</strong>, <strong>attention masks</strong> effectively filter them out, ensuring they do not affect model predictions.</li></ul><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;📌-What-is-Padding&quot;&gt;&lt;a href=&quot;#📌-What-is-Padding&quot; class=&quot;headerlink&quot; title=&quot;📌 What is Padding?&quot;&gt;&lt;/a&gt;📌 &lt;strong&gt;What is Padding?&lt;/st</summary>
      
    
    
    
    <category term="NLP Insights" scheme="https://chenhuiyu.github.io/categories/NLP-Insights/"/>
    
    
    <category term="LLM" scheme="https://chenhuiyu.github.io/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>Differences in Padding Strategies Between Decoder-only and Encoder-only Models</title>
    <link href="https://chenhuiyu.github.io/2025/03/06/NLP%20Insights/Differences%20in%20Padding%20Strategies%20Between%20Decoder-only%20and%20Encoder-only%20Models/"/>
    <id>https://chenhuiyu.github.io/2025/03/06/NLP%20Insights/Differences%20in%20Padding%20Strategies%20Between%20Decoder-only%20and%20Encoder-only%20Models/</id>
    <published>2025-03-06T09:43:10.000Z</published>
    <updated>2026-02-20T21:51:54.079Z</updated>
    
    <content type="html"><![CDATA[<h2 id="📌-What-is-Padding"><a href="#📌-What-is-Padding" class="headerlink" title="📌 What is Padding?"></a>📌 <strong>What is Padding?</strong></h2><p>In <strong>Large Language Models (LLMs)</strong>, <strong>padding</strong> is a method used to standardize sequence lengths for <strong>batch processing</strong>.</p><p>For example:</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Sentence 1: "I love NLP"</span><br><span class="line">Sentence 2: "Padding is useful in LLM training"</span><br></pre></td></tr></tbody></table></figure><p>Using the <code>&lt;pad&gt;</code> token for alignment:</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">"I love NLP &lt;pad&gt; &lt;pad&gt; &lt;pad&gt;"</span><br><span class="line">"Padding is useful in LLM training"</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="📌-Padding-Positioning-Left-vs-Right"><a href="#📌-Padding-Positioning-Left-vs-Right" class="headerlink" title="📌 Padding Positioning: Left vs Right"></a>📌 <strong>Padding Positioning: Left vs Right</strong></h2><p>There are two common padding strategies:</p><ul><li><p><strong>Right padding</strong>:</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">"I love NLP &lt;pad&gt; &lt;pad&gt;"</span><br></pre></td></tr></tbody></table></figure></li><li><p><strong>Left padding</strong>:</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">"&lt;pad&gt; &lt;pad&gt; I love NLP"</span><br></pre></td></tr></tbody></table></figure></li></ul><p>Typically:</p><ul><li><strong>Decoder-only models</strong> (e.g., GPT, Llama): Use <strong>Left padding</strong>.</li><li><strong>Encoder-only models</strong> (e.g., BERT): Use <strong>Right padding</strong>.</li></ul><p>Transformers can be categorized into three main architectures:</p><table><thead><tr><th>Model Type</th><th>Representative Models</th><th>Characteristics</th><th>Common Applications</th></tr></thead><tbody><tr><td><strong>Encoder-only</strong></td><td><strong>BERT</strong>, RoBERTa, ALBERT, ELECTRA</td><td><strong>Bidirectional attention</strong></td><td>NLP tasks like text classification, named entity recognition</td></tr><tr><td><strong>Decoder-only</strong></td><td>GPT, GPT-2, GPT-3, GPT-4, LLaMA, Mistral</td><td><strong>Causal attention (Autoregressive)</strong></td><td>Text generation, chatbots, writing assistance</td></tr><tr><td><strong>Encoder-Decoder</strong></td><td>Transformer (original), T5, BART, mT5, PEGASUS</td><td><strong>Encoder: bidirectional, Decoder: autoregressive</strong></td><td>Machine translation, summarization, dialogue systems</td></tr></tbody></table><hr><h2 id="📌-Why-Do-Encoder-only-Models-e-g-BERT-Use-Right-Padding"><a href="#📌-Why-Do-Encoder-only-Models-e-g-BERT-Use-Right-Padding" class="headerlink" title="📌 Why Do Encoder-only Models (e.g., BERT) Use Right Padding?"></a>📌 <strong>Why Do Encoder-only Models (e.g., BERT) Use Right Padding?</strong></h2><ul><li><strong>Encoder-only models</strong> (like BERT) aim to obtain <strong>representations for each token</strong>.</li><li>These models use <strong>bidirectional attention</strong>, meaning each token attends to <strong>both past and future tokens</strong>.</li><li><strong>Slight shifts in position do not significantly impact model performance</strong>.</li><li>Special tokens (e.g., <code>[CLS]</code>) in BERT maintain a <strong>fixed position</strong> for tasks like classification, making <strong>right padding more natural</strong>.</li></ul><p>Example:</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[CLS] Hello I love NLP [SEP] &lt;pad&gt; &lt;pad&gt;</span><br></pre></td></tr></tbody></table></figure><ul><li>Right padding keeps <code>[CLS]</code> and <code>[SEP]</code> in stable positions, allowing the model to focus on meaningful tokens.</li></ul><hr><h2 id="📌-Why-Do-Decoder-only-LLMs-Use-Left-Padding"><a href="#📌-Why-Do-Decoder-only-LLMs-Use-Left-Padding" class="headerlink" title="📌 Why Do Decoder-only LLMs Use Left Padding?"></a>📌 <strong>Why Do Decoder-only LLMs Use Left Padding?</strong></h2><p><strong>Decoder-only models</strong> (like GPT) are <strong>autoregressive</strong>, meaning each token is generated based only on <strong>previous tokens</strong>, and future tokens are <strong>masked</strong>.</p><ul><li><strong>Positional Encoding Stability</strong>:<br>Left padding ensures that meaningful tokens have a <strong>consistent relative position</strong>, preventing <strong>position encoding misalignment</strong>.<ul><li>When using <strong>absolute positional encoding</strong>, every token (including <code>&lt;pad&gt;</code>) gets a unique position index.</li><li>Padding tokens at the beginning <strong>do not affect the model’s attention mechanism</strong> due to <strong>masking</strong>.</li></ul></li></ul><p>Example:</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Position Index: [ 1      2      3      4      5      6 ]</span><br><span class="line">Token:         [ &lt;pad&gt;, &lt;pad&gt;, Hello,  I,   love,  NLP ]</span><br><span class="line">Mask:          [  0,     0,     1,     1,     1,    1 ]</span><br></pre></td></tr></tbody></table></figure><ul><li><p>The model <strong>only attends to tokens where the mask is 1</strong>, ignoring padding tokens.</p></li><li><p><strong>Attention Masking</strong>:<br>Left padding ensures that <code>&lt;pad&gt;</code> tokens <strong>do not interfere with token position encoding</strong>.</p></li></ul><p>Illustration:</p><table><thead><tr><th>Token</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th></tr></thead><tbody><tr><td>Left</td><td><code>&lt;pad&gt;</code></td><td><code>&lt;pad&gt;</code></td><td>Hello</td><td>I</td><td>love</td><td>NLP</td></tr><tr><td>Right</td><td>Hello</td><td>I</td><td>love</td><td>NLP</td><td><code>&lt;pad&gt;</code></td><td><code>&lt;pad&gt;</code></td></tr></tbody></table><ul><li><strong>With Left padding</strong>, the last valid token <strong>always remains in the same position</strong>.</li><li><strong>With Right padding</strong>, token positions shift, affecting positional encoding stability.</li></ul><hr><h2 id="📌-Padding-Differences-in-Training-vs-Inference"><a href="#📌-Padding-Differences-in-Training-vs-Inference" class="headerlink" title="📌 Padding Differences in Training vs Inference"></a>📌 <strong>Padding Differences in Training vs Inference</strong></h2><table><thead><tr><th>Phase</th><th>Padding Strategy</th><th>Reason</th></tr></thead><tbody><tr><td><strong>Training</strong></td><td>Left padding for decoder-only; Right padding for encoder-only</td><td>Optimized for batch processing efficiency</td></tr><tr><td><strong>Inference</strong></td><td>Typically, no padding for single sequences; Left padding for batched inference</td><td>Ensures stable positional encoding</td></tr></tbody></table><hr><h2 id="📌-Summary-amp-Key-Takeaways-TL-DR"><a href="#📌-Summary-amp-Key-Takeaways-TL-DR" class="headerlink" title="📌 Summary &amp; Key Takeaways (TL;DR)"></a>📌 <strong>Summary &amp; Key Takeaways (TL;DR)</strong></h2><ul><li><strong>Padding</strong> standardizes sequence lengths for batch processing.</li><li><strong>Decoder-only models (GPT, Llama)</strong> use <strong>Left padding</strong> to <strong>stabilize positional encoding and prevent future token leakage</strong>. Left padding tokens are masked out.</li><li><strong>Encoder-only models (BERT, RoBERTa)</strong> use <strong>Right padding</strong> since they employ <strong>bidirectional attention</strong> and rely on stable special token positions (e.g., <code>[CLS]</code>).</li><li>Although padding tokens occupy positions in <strong>positional encoding</strong>, <strong>attention masks</strong> effectively filter them out, ensuring they do not affect model predictions.</li></ul><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;📌-What-is-Padding&quot;&gt;&lt;a href=&quot;#📌-What-is-Padding&quot; class=&quot;headerlink&quot; title=&quot;📌 What is Padding?&quot;&gt;&lt;/a&gt;📌 &lt;strong&gt;What is Padding?&lt;/st</summary>
      
    
    
    
    <category term="NLP Insights" scheme="https://chenhuiyu.github.io/categories/NLP-Insights/"/>
    
    
    <category term="LLM" scheme="https://chenhuiyu.github.io/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>MoE模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色</title>
    <link href="https://chenhuiyu.github.io/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2.en/"/>
    <id>https://chenhuiyu.github.io/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2.en/</id>
    <published>2025-02-11T03:50:29.000Z</published>
    <updated>2026-02-20T21:47:32.628Z</updated>
    
    <content type="html"><![CDATA[<h1 id="MoE-模型的的可视化指南：揭秘-MoE-在大型语言模型中的角色"><a href="#MoE-模型的的可视化指南：揭秘-MoE-在大型语言模型中的角色" class="headerlink" title="MoE 模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色"></a>MoE 模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色</h1><p><strong>原文地址</strong>：<a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts">A Visual Guide to Mixture of Experts (MoE)</a></p><p>📅 作者：Maarten Grootendorst</p><p>📆 日期：2024 年 10 月 7 日</p><hr><h1 id="探索语言模型：混合专家模型（MoE）可视化指南"><a href="#探索语言模型：混合专家模型（MoE）可视化指南" class="headerlink" title="探索语言模型：混合专家模型（MoE）可视化指南"></a>探索语言模型：混合专家模型（MoE）可视化指南</h1><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li><a href="#moe-%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%E6%8F%AD%E7%A7%98-moe-%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2">MoE 模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色</a></li><li><a href="#%E6%8E%A2%E7%B4%A2%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%B7%B7%E5%90%88%E4%B8%93%E5%AE%B6%E6%A8%A1%E5%9E%8Bmoe%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97">探索语言模型：混合专家模型（MoE）可视化指南</a><ul><li><a href="#%E7%9B%AE%E5%BD%95">目录</a></li><li><a href="#%E4%BB%80%E4%B9%88%E6%98%AF%E6%B7%B7%E5%90%88%E4%B8%93%E5%AE%B6moe%E6%A8%A1%E5%9E%8B">什么是混合专家（MoE）模型？</a></li><li><a href="#experts">Experts</a><ul><li><a href="#dense-layers">Dense Layers</a></li><li><a href="#sparse-layers">Sparse Layers</a></li><li><a href="#what-does-an-expert-learn">What does an Expert Learn?</a></li><li><a href="#%E4%B8%93%E5%AE%B6%E7%9A%84%E6%9E%B6%E6%9E%84architecture-of-experts">专家的架构（Architecture of Experts）</a></li></ul></li></ul></li></ul><p>当我们查看最新发布的大型语言模型（<strong>LLMs</strong>，Large Language Models）时，常常会在标题中看到 “<strong>MoE</strong>”。这个 “MoE” 代表什么？为什么这么多 LLM 都在使用它？</p><p>在这份可视化指南中，我们会通过 50 多个可视化图示，逐步探索这一关键组件：**Mixture of Experts (MoE)**。</p><p><strong>图示内容</strong>：在这张图中，可以看到一个典型 <strong>MoE</strong> 结构的两个主要组成部分：<strong>Experts</strong>（专家）和 <strong>Router</strong>（路由器或门控网络）。图中显示了一个 <strong>Router</strong>，以及下方并列的多个 <strong>Experts</strong>，表明在 <strong>LLM</strong> 架构中，MoE 会将输入根据需要路由到合适的专家。<br><strong>图 1 详细说明</strong>：</p><ol><li><strong>Router</strong>：决定将输入（例如 token）发送给哪一个或哪几个专家。</li><li><strong>Experts</strong>：若干个不同的子模型（通常是 <strong>FFNN</strong> 结构），每个专家可能在不同方面具有专长。</li><li><strong>工作流程</strong>：输入先通过 <strong>Router</strong>，再被分配到不同的专家进行处理，最后汇总结果。</li></ol><h2 id="什么是混合专家（MoE）模型？"><a href="#什么是混合专家（MoE）模型？" class="headerlink" title="什么是混合专家（MoE）模型？"></a>什么是混合专家（MoE）模型？</h2><p><strong>Mixture of Experts (MoE)</strong> 是一种技术，它使用许多不同的子模型（或“<strong>experts</strong>”）来提升大型语言模型的质量。</p><p>在 MoE 中，有两个主要组件：</p><ol><li><strong>Experts</strong><ul><li>每个 <strong>FFNN</strong> 层都不再是一个单独的网络，而是有一组“专家”可供选择。</li><li>这些“专家”通常也是 <strong>FFNN</strong>（Feedforward Neural Network）结构。</li></ul></li><li><strong>Router</strong> 或 <strong>gate network</strong><ul><li>负责决定哪些 <strong>tokens</strong> 被发送到哪些专家。</li></ul></li></ol><p>在一个带有 MoE 的 <strong>LLM</strong> 的每一层，我们都能看到（在某种程度上）有所专门化的专家：</p><p><strong>图示内容</strong>：展示了在 <strong>LLM</strong> 的每一层都可以拥有多个 <strong>Experts</strong>。它强调了这些专家在不同的上下文中能够处理不同的输入 token。<br><strong>图2详细说明</strong>：  </p><ol><li><strong>层结构</strong>：图中用不同的层级（Layer 1、Layer 2、Layer 3……）表示多层模型。  </li><li><strong>Experts</strong>：在每一层，都有若干个专家（Expert 1、Expert 2、Expert 3、Expert 4），这些专家并行存在。  </li><li><strong>目标</strong>：强调专家在特定上下文或特定输入时更具备“专业性”，从而被选中来处理该输入。</li></ol><p>尽管 MoE 并不会在特定领域（如心理学或生物学）上专门训练专家，但它们仍可能在词法或句法级别上形成一定的偏向：</p><ul><li><strong>MoE 专家可能学习到不同的语言特征</strong><ul><li><strong>Expert 1</strong> 处理<strong>标点符号</strong>（Punctuation）：如 <code>, . : &amp; - ?</code> 等。</li><li><strong>Expert 2</strong> 处理<strong>动词</strong>（Verbs）：如 <code>said, read, miss</code> 等。</li><li><strong>Expert 3</strong> 处理<strong>连接词</strong>（Conjunctions）：如 <code>the, and, if, not</code> 等。</li><li><strong>Expert 4</strong> 处理<strong>视觉描述词</strong>（Visual Descriptions）：如 <code>dark, outer, yellow</code> 等。</li></ul></li></ul><p>更具体地说，他们的专长是在特定上下文中处理特定的标记（tokens）。</p><hr><p><strong>Router (gate network)</strong> 选择最适合给定输入的专家或专家组合：</p><p><strong>图示内容</strong>：展示了 <strong>Router</strong> 如何在每一层根据输入选择合适的专家。图中高亮了被选中的专家，以及输入 token 的流动过程。<br><strong>图3详细说明</strong>：  </p><ol><li><strong>输入</strong>：图顶部的 Input 代表模型接收到的 token 或向量表示。  </li><li><strong>Router</strong>：位于网络结构中，起到决策作用。  </li><li><strong>专家选择</strong>：被选中的专家会接收输入，其余专家则不被激活。  </li><li><strong>输出</strong>：来自被激活专家的结果被汇总或继续流向下游层。</li></ol><p>需要注意的是，每个专家并不是整个 LLM，而是 <strong>LLM</strong> 架构中的一个子模型部分。</p><hr><h2 id="Experts"><a href="#Experts" class="headerlink" title="Experts"></a>Experts</h2><p>为了理解专家（<strong>Experts</strong>）是什么以及它们如何工作，我们先来看看 MoE 希望替代的东西：<strong>dense layers</strong>。</p><h3 id="Dense-Layers"><a href="#Dense-Layers" class="headerlink" title="Dense Layers"></a>Dense Layers</h3><p>所有的 <strong>Mixture of Experts (MoE)</strong> 都基于 LLM 中一个相对基础的功能：**Feedforward Neural Network (FFNN)**。</p><p>回忆一下，一个标准的 <strong>decoder-only Transformer</strong> 架构中，<strong>FFNN</strong> 通常是在 <strong>layer normalization</strong> 之后应用的：</p><p><strong>图示内容</strong>：展示了一个典型的 <strong>decoder</strong> 结构，每个 <strong>decoder block</strong> 包含 <strong>Masked Self-Attention</strong> 和 <strong>FFNN</strong>（中间会有 <strong>Layer Norm</strong>）。  </p><ol><li><strong>Position Embedding</strong>：在输入 token 之前或同时加入位置编码信息。  </li><li><strong>Decoder Block</strong>：包含 <strong>Masked Self-Attention</strong>、<strong>Layer Norm</strong> 和 <strong>FFNN</strong>。  </li><li><strong>FFNN</strong>：在图中用紫色方块表示，是该层对输入进一步变换以捕捉更复杂关系的关键组件。</li></ol><p><strong>FFNN</strong> 可以利用注意力机制产生的上下文信息，对其进行进一步的转换，以捕捉数据中更复杂的关系。</p><p>不过，为了学习这些复杂关系，<strong>FFNN</strong> 的规模会随之增长，通常会在输入上进行扩张（例如，中间层维度会变大）：</p><p><strong>图示内容</strong>：展示了一个 <strong>FFNN</strong> 的结构，输入先被映射到更高维度，然后再被映射回输出维度。  </p><ol><li><strong>输入维度</strong>：图中显示有 512 个输入单元。  </li><li><strong>隐藏层</strong>：通常会有 4 倍或更多的扩张（图中示例为 4 倍扩张到 2048 维）。  </li><li><strong>输出维度</strong>：再映射回 512 维的输出。</li></ol><h3 id="Sparse-Layers"><a href="#Sparse-Layers" class="headerlink" title="Sparse Layers"></a>Sparse Layers</h3><p>在传统的 Transformer 中，<strong>FFNN</strong> 称为 <strong>dense model</strong>，因为它的所有参数（权重和偏置）都会被激活。也就是说，模型的全部参数都参与计算输出。</p><p>如果我们仔细观察 <strong>dense model</strong>，可以看到输入会激活所有的参数：</p><p><strong>图示内容</strong>：展示了一个“密集”模型，输入层的每个神经元都与隐藏层所有神经元相连，隐藏层所有神经元又与输出层神经元相连。<br><strong>图6详细说明</strong>：  </p><ol><li><strong>全连接</strong>：图中所有节点都连接到下一层的所有节点，表示无稀疏性。  </li><li><strong>所有参数被激活</strong>：没有任何“闲置”或“未激活”的参数。</li></ol><p>与之对比，<strong>sparse models</strong>（稀疏模型）只激活一部分总参数，这与 <strong>Mixture of Experts</strong> 密切相关。</p><p>为了说明这一点，我们可以把 <strong>dense model</strong> 切分成多个部分（即专家，<strong>experts</strong>），重新训练它，并且在推理（inference）时只激活其中一部分：</p><p><strong>图示内容</strong>：将原本的密集模型分割成多个专家（Expert 1、Expert 2、Expert 3、Expert 4）。在推理阶段，只选择一部分专家进行激活。  </p><ol><li><strong>模型切分</strong>：原有的大网络被拆分成多个较小的“专家”。  </li><li><strong>稀疏激活</strong>：并不是所有专家都被激活，只有部分专家在某些输入下被激活。  </li><li><strong>好处</strong>：通过稀疏激活，可以在不显著增加计算成本的情况下，拥有更多的潜在参数容量。</li></ol><p>其核心思想是：在训练期间，每个专家学习不同的信息；在推理时，只用到与当前任务最相关的那些专家。</p><p>当我们提出一个问题时，就会选择最适合该任务的专家：</p><p><strong>图示内容</strong>：展示了一个示例：当输入是 “What is 1 + 1?” 这样的数字相关问题时，路由器只激活与数字相关的专家。  </p><ol><li><strong>输入</strong>：一个表示算术问题的句子或 token。  </li><li><strong>专家选择</strong>：只激活 “Numbers” 领域的专家。  </li><li><strong>输出</strong>：专家给出结果 “2”。</li></ol><h3 id="What-does-an-Expert-Learn"><a href="#What-does-an-Expert-Learn" class="headerlink" title="What does an Expert Learn?"></a>What does an Expert Learn?</h3><p>正如前面所提到的，专家（<strong>Experts</strong>）往往学习到比整个领域更细致的知识。有人会觉得称它们为“专家”可能会带来误解，但这是因为每个专家往往只专注于某些特定类型的输入特征或上下文。</p><p><strong>图示内容</strong>：展示了一个表格或对照，说明在某些情况下，不同的专家可能学习到不同的特征（比如标点符号、动词、数字等）。  </p><ol><li><strong>示例化专家</strong>：Punctuation、Conjunctions、Verbs、Numbers 等。  </li><li><strong>分层位置</strong>：不同专家可能出现在模型的不同层。  </li><li><strong>分配</strong>：某些 token 会路由到某些专家，以获得更有效的处理。</li></ol><p>在 <strong>decoder</strong> 模型中，专家之间可能没有那么明显的领域分工。然而，这并不意味着所有专家都完全相同。<br>在 <strong>Mixtral 8x7B</strong> 这篇论文中，有一个很好的示例：每个 token 会被标记为其首选专家，这些专家并不一定对应直观的语义领域，但在统计上表现出某些倾向。</p><p>这张可视化示例还展示了，experts（专家）更倾向于关注句法（syntax），而不是特定的领域（domain）。因此，虽然 decoder experts（解码器专家）似乎并没有明确的“专业领域（specialism）”，但它们似乎会在某些特定类型的 tokens（标记）上被持续地使用。</p><p>在[图1]中，展示了一段关于 MoELayer 的示例代码或可视化结果，色块区分了不同部分，强调了<strong>专家（experts）与路由器（router）</strong>之间的关系。通过色块可以看出：</p><ul><li>experts 列表（在代码中用 nn.ModuleList 表示）包含了多个子网络（即多个 FFNN，Feed-Forward Neural Network，前馈神经网络）。</li><li>gate（门控网络，也称 router）负责选择哪些专家会被激活。</li><li>整体上可以看到，这些专家通常关注到输入句子的句法层面，而非特定主题或领域。</li></ul><h3 id="专家的架构（Architecture-of-Experts）"><a href="#专家的架构（Architecture-of-Experts）" class="headerlink" title="专家的架构（Architecture of Experts）"></a>专家的架构（Architecture of Experts）</h3>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;MoE-模型的的可视化指南：揭秘-MoE-在大型语言模型中的角色&quot;&gt;&lt;a href=&quot;#MoE-模型的的可视化指南：揭秘-MoE-在大型语言模型中的角色&quot; class=&quot;headerlink&quot; title=&quot;MoE 模型的的可视化指南：揭秘 MoE 在大型语言模型</summary>
      
    
    
    
    <category term="NLP Insights" scheme="https://chenhuiyu.github.io/categories/NLP-Insights/"/>
    
    
    <category term="LLM" scheme="https://chenhuiyu.github.io/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>MoE模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色</title>
    <link href="https://chenhuiyu.github.io/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2/"/>
    <id>https://chenhuiyu.github.io/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2/</id>
    <published>2025-02-11T03:50:29.000Z</published>
    <updated>2026-02-20T21:51:54.083Z</updated>
    
    <content type="html"><![CDATA[<h1 id="MoE-模型的的可视化指南：揭秘-MoE-在大型语言模型中的角色"><a href="#MoE-模型的的可视化指南：揭秘-MoE-在大型语言模型中的角色" class="headerlink" title="MoE 模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色"></a>MoE 模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色</h1><p><strong>原文地址</strong>：<a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts">A Visual Guide to Mixture of Experts (MoE)</a></p><p>📅 作者：Maarten Grootendorst</p><p>📆 日期：2024 年 10 月 7 日</p><hr><h1 id="探索语言模型：混合专家模型（MoE）可视化指南"><a href="#探索语言模型：混合专家模型（MoE）可视化指南" class="headerlink" title="探索语言模型：混合专家模型（MoE）可视化指南"></a>探索语言模型：混合专家模型（MoE）可视化指南</h1><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li><a href="#moe-%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%E6%8F%AD%E7%A7%98-moe-%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2">MoE 模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色</a></li><li><a href="#%E6%8E%A2%E7%B4%A2%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%B7%B7%E5%90%88%E4%B8%93%E5%AE%B6%E6%A8%A1%E5%9E%8Bmoe%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97">探索语言模型：混合专家模型（MoE）可视化指南</a><ul><li><a href="#%E7%9B%AE%E5%BD%95">目录</a></li><li><a href="#%E4%BB%80%E4%B9%88%E6%98%AF%E6%B7%B7%E5%90%88%E4%B8%93%E5%AE%B6moe%E6%A8%A1%E5%9E%8B">什么是混合专家（MoE）模型？</a></li><li><a href="#experts">Experts</a><ul><li><a href="#dense-layers">Dense Layers</a></li><li><a href="#sparse-layers">Sparse Layers</a></li><li><a href="#what-does-an-expert-learn">What does an Expert Learn?</a></li><li><a href="#%E4%B8%93%E5%AE%B6%E7%9A%84%E6%9E%B6%E6%9E%84architecture-of-experts">专家的架构（Architecture of Experts）</a></li></ul></li></ul></li></ul><p>当我们查看最新发布的大型语言模型（<strong>LLMs</strong>，Large Language Models）时，常常会在标题中看到 “<strong>MoE</strong>”。这个 “MoE” 代表什么？为什么这么多 LLM 都在使用它？</p><p>在这份可视化指南中，我们会通过 50 多个可视化图示，逐步探索这一关键组件：**Mixture of Experts (MoE)**。</p><img src="/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2/20250224_145859.png" class=""><p><strong>图示内容</strong>：在这张图中，可以看到一个典型 <strong>MoE</strong> 结构的两个主要组成部分：<strong>Experts</strong>（专家）和 <strong>Router</strong>（路由器或门控网络）。图中显示了一个 <strong>Router</strong>，以及下方并列的多个 <strong>Experts</strong>，表明在 <strong>LLM</strong> 架构中，MoE 会将输入根据需要路由到合适的专家。<br><strong>图 1 详细说明</strong>：</p><ol><li><strong>Router</strong>：决定将输入（例如 token）发送给哪一个或哪几个专家。</li><li><strong>Experts</strong>：若干个不同的子模型（通常是 <strong>FFNN</strong> 结构），每个专家可能在不同方面具有专长。</li><li><strong>工作流程</strong>：输入先通过 <strong>Router</strong>，再被分配到不同的专家进行处理，最后汇总结果。</li></ol><h2 id="什么是混合专家（MoE）模型？"><a href="#什么是混合专家（MoE）模型？" class="headerlink" title="什么是混合专家（MoE）模型？"></a>什么是混合专家（MoE）模型？</h2><p><strong>Mixture of Experts (MoE)</strong> 是一种技术，它使用许多不同的子模型（或“<strong>experts</strong>”）来提升大型语言模型的质量。</p><p>在 MoE 中，有两个主要组件：</p><ol><li><strong>Experts</strong><ul><li>每个 <strong>FFNN</strong> 层都不再是一个单独的网络，而是有一组“专家”可供选择。</li><li>这些“专家”通常也是 <strong>FFNN</strong>（Feedforward Neural Network）结构。</li></ul></li><li><strong>Router</strong> 或 <strong>gate network</strong><ul><li>负责决定哪些 <strong>tokens</strong> 被发送到哪些专家。</li></ul></li></ol><p>在一个带有 MoE 的 <strong>LLM</strong> 的每一层，我们都能看到（在某种程度上）有所专门化的专家：</p><img src="/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2/20250224_150409.png" class=""><p><strong>图示内容</strong>：展示了在 <strong>LLM</strong> 的每一层都可以拥有多个 <strong>Experts</strong>。它强调了这些专家在不同的上下文中能够处理不同的输入 token。<br><strong>图2详细说明</strong>：  </p><ol><li><strong>层结构</strong>：图中用不同的层级（Layer 1、Layer 2、Layer 3……）表示多层模型。  </li><li><strong>Experts</strong>：在每一层，都有若干个专家（Expert 1、Expert 2、Expert 3、Expert 4），这些专家并行存在。  </li><li><strong>目标</strong>：强调专家在特定上下文或特定输入时更具备“专业性”，从而被选中来处理该输入。</li></ol><p>尽管 MoE 并不会在特定领域（如心理学或生物学）上专门训练专家，但它们仍可能在词法或句法级别上形成一定的偏向：</p><img src="/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2/20250224_153715.png" class=""><ul><li><strong>MoE 专家可能学习到不同的语言特征</strong><ul><li><strong>Expert 1</strong> 处理<strong>标点符号</strong>（Punctuation）：如 <code>, . : &amp; - ?</code> 等。</li><li><strong>Expert 2</strong> 处理<strong>动词</strong>（Verbs）：如 <code>said, read, miss</code> 等。</li><li><strong>Expert 3</strong> 处理<strong>连接词</strong>（Conjunctions）：如 <code>the, and, if, not</code> 等。</li><li><strong>Expert 4</strong> 处理<strong>视觉描述词</strong>（Visual Descriptions）：如 <code>dark, outer, yellow</code> 等。</li></ul></li></ul><p>更具体地说，他们的专长是在特定上下文中处理特定的标记（tokens）。</p><hr><p><strong>Router (gate network)</strong> 选择最适合给定输入的专家或专家组合：</p><img src="/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2/20250224_153924.png" class=""><p><strong>图示内容</strong>：展示了 <strong>Router</strong> 如何在每一层根据输入选择合适的专家。图中高亮了被选中的专家，以及输入 token 的流动过程。<br><strong>图3详细说明</strong>：  </p><ol><li><strong>输入</strong>：图顶部的 Input 代表模型接收到的 token 或向量表示。  </li><li><strong>Router</strong>：位于网络结构中，起到决策作用。  </li><li><strong>专家选择</strong>：被选中的专家会接收输入，其余专家则不被激活。  </li><li><strong>输出</strong>：来自被激活专家的结果被汇总或继续流向下游层。</li></ol><p>需要注意的是，每个专家并不是整个 LLM，而是 <strong>LLM</strong> 架构中的一个子模型部分。</p><hr><h2 id="Experts"><a href="#Experts" class="headerlink" title="Experts"></a>Experts</h2><p>为了理解专家（<strong>Experts</strong>）是什么以及它们如何工作，我们先来看看 MoE 希望替代的东西：<strong>dense layers</strong>。</p><h3 id="Dense-Layers"><a href="#Dense-Layers" class="headerlink" title="Dense Layers"></a>Dense Layers</h3><p>所有的 <strong>Mixture of Experts (MoE)</strong> 都基于 LLM 中一个相对基础的功能：**Feedforward Neural Network (FFNN)**。</p><p>回忆一下，一个标准的 <strong>decoder-only Transformer</strong> 架构中，<strong>FFNN</strong> 通常是在 <strong>layer normalization</strong> 之后应用的：</p><img src="/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2/20250224_154729.png" class=""><p><strong>图示内容</strong>：展示了一个典型的 <strong>decoder</strong> 结构，每个 <strong>decoder block</strong> 包含 <strong>Masked Self-Attention</strong> 和 <strong>FFNN</strong>（中间会有 <strong>Layer Norm</strong>）。  </p><ol><li><strong>Position Embedding</strong>：在输入 token 之前或同时加入位置编码信息。  </li><li><strong>Decoder Block</strong>：包含 <strong>Masked Self-Attention</strong>、<strong>Layer Norm</strong> 和 <strong>FFNN</strong>。  </li><li><strong>FFNN</strong>：在图中用紫色方块表示，是该层对输入进一步变换以捕捉更复杂关系的关键组件。</li></ol><p><strong>FFNN</strong> 可以利用注意力机制产生的上下文信息，对其进行进一步的转换，以捕捉数据中更复杂的关系。</p><p>不过，为了学习这些复杂关系，<strong>FFNN</strong> 的规模会随之增长，通常会在输入上进行扩张（例如，中间层维度会变大）：</p><img src="/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2/20250224_154923.png" class=""><p><strong>图示内容</strong>：展示了一个 <strong>FFNN</strong> 的结构，输入先被映射到更高维度，然后再被映射回输出维度。  </p><ol><li><strong>输入维度</strong>：图中显示有 512 个输入单元。  </li><li><strong>隐藏层</strong>：通常会有 4 倍或更多的扩张（图中示例为 4 倍扩张到 2048 维）。  </li><li><strong>输出维度</strong>：再映射回 512 维的输出。</li></ol><h3 id="Sparse-Layers"><a href="#Sparse-Layers" class="headerlink" title="Sparse Layers"></a>Sparse Layers</h3><p>在传统的 Transformer 中，<strong>FFNN</strong> 称为 <strong>dense model</strong>，因为它的所有参数（权重和偏置）都会被激活。也就是说，模型的全部参数都参与计算输出。</p><p>如果我们仔细观察 <strong>dense model</strong>，可以看到输入会激活所有的参数：</p><img src="/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2/20250224_163850.png" class=""><p><strong>图示内容</strong>：展示了一个“密集”模型，输入层的每个神经元都与隐藏层所有神经元相连，隐藏层所有神经元又与输出层神经元相连。<br><strong>图6详细说明</strong>：  </p><ol><li><strong>全连接</strong>：图中所有节点都连接到下一层的所有节点，表示无稀疏性。  </li><li><strong>所有参数被激活</strong>：没有任何“闲置”或“未激活”的参数。</li></ol><p>与之对比，<strong>sparse models</strong>（稀疏模型）只激活一部分总参数，这与 <strong>Mixture of Experts</strong> 密切相关。</p><p>为了说明这一点，我们可以把 <strong>dense model</strong> 切分成多个部分（即专家，<strong>experts</strong>），重新训练它，并且在推理（inference）时只激活其中一部分：</p><img src="/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2/20250224_164042.png" class=""><p><strong>图示内容</strong>：将原本的密集模型分割成多个专家（Expert 1、Expert 2、Expert 3、Expert 4）。在推理阶段，只选择一部分专家进行激活。  </p><ol><li><strong>模型切分</strong>：原有的大网络被拆分成多个较小的“专家”。  </li><li><strong>稀疏激活</strong>：并不是所有专家都被激活，只有部分专家在某些输入下被激活。  </li><li><strong>好处</strong>：通过稀疏激活，可以在不显著增加计算成本的情况下，拥有更多的潜在参数容量。</li></ol><p>其核心思想是：在训练期间，每个专家学习不同的信息；在推理时，只用到与当前任务最相关的那些专家。</p><p>当我们提出一个问题时，就会选择最适合该任务的专家：</p><img src="/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2/20250224_170127.png" class=""><p><strong>图示内容</strong>：展示了一个示例：当输入是 “What is 1 + 1?” 这样的数字相关问题时，路由器只激活与数字相关的专家。  </p><ol><li><strong>输入</strong>：一个表示算术问题的句子或 token。  </li><li><strong>专家选择</strong>：只激活 “Numbers” 领域的专家。  </li><li><strong>输出</strong>：专家给出结果 “2”。</li></ol><h3 id="What-does-an-Expert-Learn"><a href="#What-does-an-Expert-Learn" class="headerlink" title="What does an Expert Learn?"></a>What does an Expert Learn?</h3><p>正如前面所提到的，专家（<strong>Experts</strong>）往往学习到比整个领域更细致的知识。有人会觉得称它们为“专家”可能会带来误解，但这是因为每个专家往往只专注于某些特定类型的输入特征或上下文。</p><img src="/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2/20250224_170229.png" class=""><p><strong>图示内容</strong>：展示了一个表格或对照，说明在某些情况下，不同的专家可能学习到不同的特征（比如标点符号、动词、数字等）。  </p><ol><li><strong>示例化专家</strong>：Punctuation、Conjunctions、Verbs、Numbers 等。  </li><li><strong>分层位置</strong>：不同专家可能出现在模型的不同层。  </li><li><strong>分配</strong>：某些 token 会路由到某些专家，以获得更有效的处理。</li></ol><p>在 <strong>decoder</strong> 模型中，专家之间可能没有那么明显的领域分工。然而，这并不意味着所有专家都完全相同。<br>在 <strong>Mixtral 8x7B</strong> 这篇论文中，有一个很好的示例：每个 token 会被标记为其首选专家，这些专家并不一定对应直观的语义领域，但在统计上表现出某些倾向。</p><img src="/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2/20250224_182219.png" class=""><p>这张可视化示例还展示了，experts（专家）更倾向于关注句法（syntax），而不是特定的领域（domain）。因此，虽然 decoder experts（解码器专家）似乎并没有明确的“专业领域（specialism）”，但它们似乎会在某些特定类型的 tokens（标记）上被持续地使用。</p><p>在[图1]中，展示了一段关于 MoELayer 的示例代码或可视化结果，色块区分了不同部分，强调了<strong>专家（experts）与路由器（router）</strong>之间的关系。通过色块可以看出：</p><ul><li>experts 列表（在代码中用 nn.ModuleList 表示）包含了多个子网络（即多个 FFNN，Feed-Forward Neural Network，前馈神经网络）。</li><li>gate（门控网络，也称 router）负责选择哪些专家会被激活。</li><li>整体上可以看到，这些专家通常关注到输入句子的句法层面，而非特定主题或领域。</li></ul><h3 id="专家的架构（Architecture-of-Experts）"><a href="#专家的架构（Architecture-of-Experts）" class="headerlink" title="专家的架构（Architecture of Experts）"></a>专家的架构（Architecture of Experts）</h3>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;MoE-模型的的可视化指南：揭秘-MoE-在大型语言模型中的角色&quot;&gt;&lt;a href=&quot;#MoE-模型的的可视化指南：揭秘-MoE-在大型语言模型中的角色&quot; class=&quot;headerlink&quot; title=&quot;MoE 模型的的可视化指南：揭秘 MoE 在大型语言模型</summary>
      
    
    
    
    <category term="NLP Insights" scheme="https://chenhuiyu.github.io/categories/NLP-Insights/"/>
    
    
    <category term="LLM" scheme="https://chenhuiyu.github.io/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>推理 LLM 的可视化指南：探索推理时计算技术与 DeepSeek-R1</title>
    <link href="https://chenhuiyu.github.io/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1.en/"/>
    <id>https://chenhuiyu.github.io/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1.en/</id>
    <published>2025-02-11T03:50:29.000Z</published>
    <updated>2026-02-20T21:47:32.628Z</updated>
    
    <content type="html"><![CDATA[<h1 id="推理-LLM-的可视化指南：探索推理时计算技术与-DeepSeek-R1"><a href="#推理-LLM-的可视化指南：探索推理时计算技术与-DeepSeek-R1" class="headerlink" title="推理 LLM 的可视化指南：探索推理时计算技术与 DeepSeek-R1"></a>推理 LLM 的可视化指南：探索推理时计算技术与 DeepSeek-R1</h1><p><strong>原文地址</strong>：<a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-reasoning-llms">A Visual Guide to Reasoning LLMs</a></p><p>📅 作者：Maarten Grootendorst</p><p>📆 日期：2025 年 2 月 3 日</p><hr><h2 id="📌-引言"><a href="#📌-引言" class="headerlink" title="📌 引言"></a>📌 引言</h2><p>DeepSeek-R1、OpenAI o3-mini 和 Google Gemini 2.0 Flash Thinking 是如何通过“推理”框架将 <strong>LLM（大型语言模型, Large Language Models）</strong> 扩展到新高度的典型示例。</p><p>它们标志着从 <strong>扩展训练时计算（train-time compute）</strong> 到 <strong>扩展推理时计算（test-time compute）</strong> 的范式转变。</p><p>在本篇文章中，我们提供了 <strong>超过 40 张定制可视化图表</strong>，带你深入探索：</p><ul><li><strong>推理 LLM（Reasoning LLMs）</strong> 领域</li><li><strong>推理时计算（Test-Time Compute）</strong> 机制</li><li><strong>DeepSeek-R1</strong> 的核心思想</li></ul><p>我们将逐步介绍相关概念，帮助你建立对这一新范式的直觉理解。</p><hr><h2 id="📖-什么是推理-LLM？"><a href="#📖-什么是推理-LLM？" class="headerlink" title="📖 什么是推理 LLM？"></a>📖 什么是推理 LLM？</h2><p>与普通 <strong>LLM（Large Language Models，大型语言模型）</strong> 相比，<strong>推理 LLM</strong> 在回答问题之前，往往会将问题 <strong>分解为更小的步骤</strong>（通常称为 <strong>推理步骤（Reasoning Steps）</strong> 或 <strong>思考过程（Thought Process）</strong>）。</p><h3 id="🧠-“推理步骤”-或-“思考过程”-是什么？"><a href="#🧠-“推理步骤”-或-“思考过程”-是什么？" class="headerlink" title="🧠 “推理步骤” 或 “思考过程” 是什么？"></a>🧠 “推理步骤” 或 “思考过程” 是什么？</h3><p>尽管我们可以哲学化地探讨 LLM 是否真的能够像人类一样思考，但这些推理步骤实际上是将推理过程 分解为更小、更结构化的推断。<strong>推理 LLM 采用的是结构化推理方式</strong>，即：</p><ul><li><strong>普通 LLM</strong>：直接输出答案</li><li><strong>推理 LLM</strong>：通过系统性推理生成答案</li></ul><p>换句话说，推理 LLM 不是<strong>学习“回答什么”</strong>，而是<strong>学习“如何回答”</strong>！</p><p>要理解推理 LLM 的构建原理，我们首先需要探讨 <strong>训练时计算（Train-Time Compute）</strong> 和 <strong>推理时计算（Test-Time Compute）</strong> 之间的差异。</p><hr><h2 id="🔍-什么是训练时计算（Train-time-Compute）？"><a href="#🔍-什么是训练时计算（Train-time-Compute）？" class="headerlink" title="🔍 什么是训练时计算（Train-time Compute）？"></a>🔍 什么是训练时计算（Train-time Compute）？</h2><p>直到 2024 年年中，为了在 <strong>预训练（Pretraining）</strong> 期间提高 LLM 的性能，研究人员通常会扩大以下规模：</p><ul><li><strong>模型参数数量（# of Parameters）</strong></li><li><strong>数据集规模（# of Tokens）</strong></li><li><strong>计算量（# of FLOPs, Floating Point Operations）</strong></li></ul><p>这些合称为 <strong>训练时计算（Train-time Compute）</strong>，即 <strong>“AI 的化石燃料”</strong>，指的是：</p><blockquote><p><strong>预训练预算越大，最终得到的模型就越好。</strong></p><p>训练时计算（Train-Time Compute）包括<strong>训练（training）</strong>所需的计算，以及<strong>微调（fine-tuning）</strong>所需的计算。长期以来，一直是提高 LLM 性能的主要关注点。</p></blockquote><h3 id="🔢-规模定律（Scaling-Laws）"><a href="#🔢-规模定律（Scaling-Laws）" class="headerlink" title="🔢 规模定律（Scaling Laws）"></a>🔢 规模定律（Scaling Laws）</h3><p>在 <strong>LLM（大型语言模型）</strong> 研究领域，<strong>模型规模（Scale）</strong> 与 <strong>模型性能（Performance）</strong> 之间的关系被称为 <strong>规模定律（Scaling Laws）</strong>。这些定律通常用于描述 <strong>计算资源、数据规模和模型参数</strong> 如何影响模型的整体表现。</p><p>这些关系通常以 <strong>对数-对数（log-log）</strong> 方式呈现，并且在图表上通常显示为一条 <strong>近似直线</strong>，以突出计算量的巨大增长。</p><p>这张图片展示了<strong>不同坐标尺度（线性 vs. 对数）对计算资源（Compute）和模型性能（Performance）之间关系的影响</strong>，强调了大模型增长的幂律关系（Power Law）。</p><ul><li><p><strong>左图（普通线性尺度 - Normal Scale）</strong></p><ul><li>横轴（X 轴）：计算资源（Compute），<strong>线性刻度</strong>。</li><li>纵轴（Y 轴）：性能（Performance），<strong>线性刻度</strong>。</li><li>曲线显示<strong>递减收益（Diminishing Returns）</strong>，即：<strong>随着计算资源的增加，性能增长趋缓</strong>，但仍然在上升。</li></ul></li><li><p><strong>右图（对数-对数尺度 - Log-log Scale）</strong></p><ul><li>横轴（X 轴）：计算资源（Compute），<strong>对数刻度</strong>。</li><li>纵轴（Y 轴）：性能（Performance），<strong>对数刻度</strong>。</li><li>在对数-对数尺度下，原本弯曲的曲线变成<strong>一条直线</strong>，说明计算资源和性能之间呈<strong>幂律关系（Power Law Relationship）</strong>。</li></ul></li></ul><p>这些定律通常遵循 <strong>幂律（Power Laws）</strong>，即：</p><blockquote><p><strong>某个变量（如计算量）增加，会导致另一个变量（如性能）按一定比例变化。</strong></p></blockquote><p>最著名的 <strong>规模定律</strong> 包括：</p><ul><li><strong>Kaplan 规模定律</strong>（Kaplan Scaling Law）：当计算资源一定时，<strong>增加模型的参数规模比增加数据规模更有效</strong>。表明模型性能与参数量、计算量和训练数据（Tokens）之间存在幂律关系，即 更多参数、更多计算资源能提升性能（GPT-3 论文提出）。</li><li><strong>Chinchilla 规模定律</strong>（Chinchilla Scaling Law）：模型的大小和数据规模同样重要，二者需 <strong>同步扩展</strong> 才能实现最佳性能（DeepMind 提出）。</li></ul><p>这张图展示了<strong>大规模 AI 训练中的 Scaling Laws（缩放定律）</strong>，表明<strong>计算资源（Compute）、数据集规模（Dataset Size）和参数量（Parameters）</strong>对模型性能的影响。关键信息如下：</p><hr><p><strong>1. 纵轴（Y轴）：测试损失（Test Loss）</strong></p><ul><li><strong>目标是降低测试损失（Test Loss）</strong>，即提高模型的泛化性能。</li><li><strong>损失（L）越小，模型性能越好</strong>。</li></ul><p><strong>2. 横轴（X轴）：三种关键变量</strong></p><ul><li><p><strong>左图（Compute，计算资源）</strong>：</p><ul><li>X 轴是计算资源（PF-days, 非 embedding）。</li><li>计算资源越多，测试损失降低（性能提升）。</li><li>公式：<br>$$<br>L = \left( \frac{C_{\text{min}}}{2.3 \times 10^8} \right)^{-0.050}<br>$$</li><li><strong>体现计算资源的幂律关系</strong>：计算资源增加，损失减少，但收益递减（指数 -0.050）。</li></ul></li><li><p><strong>中图（Dataset Size，数据集规模）</strong>：</p><ul><li>X 轴是训练数据的 Token 数量。</li><li>数据规模越大，测试损失降低（性能提升）。</li><li>公式：<br>$$<br>L = \left( \frac{D}{5.4 \times 10^{13}} \right)^{-0.095}<br>$$</li><li><strong>数据规模对损失的影响较大</strong>（指数 -0.095）。</li></ul></li><li><p><strong>右图（Parameters，参数量）</strong>：</p><ul><li>X 轴是模型参数量（非 embedding）。</li><li>参数数量越大，测试损失降低（性能提升）。</li><li>公式：<br>$$<br>L = \left( \frac{N}{8.8 \times 10^{13}} \right)^{-0.076}<br>$$</li><li><strong>参数对损失的影响介于计算资源和数据规模之间</strong>（指数 -0.076）。</li></ul></li></ul><p>这些研究表明，<strong>模型规模、数据规模和计算资源必须协同扩展，才能最大化模型的性能</strong>。</p><ul><li><strong>计算资源增加 → 训练更强大模型</strong></li><li><strong>更多 Tokens → 更好泛化能力</strong></li><li><strong>参数增加 → 但需要与数据匹配，否则过拟合</strong></li></ul><p>Kaplan 规模定律认为，在 <strong>固定计算资源</strong> 的情况下，<strong>优先增加模型参数</strong> 通常比增加数据规模更有效。而 Chinchilla 规模定律则指出，<strong>模型参数和数据规模都应同步增长</strong>，以获得更优的模型性能。</p><p>然而，在 <strong>2024 年</strong>，研究人员发现，尽管计算资源、数据规模和模型参数 <strong>持续增长</strong>，但性能提升的 <strong>边际收益（Marginal Return）</strong> 却在 <strong>逐渐降低</strong>。</p><p>这引发了一个重要的问题：</p><p>❓ <strong>“我们是否已经遇到了 LLM 发展的瓶颈？”</strong></p><hr><h2 id="🚀-什么是推理时计算（Test-time-Compute）？"><a href="#🚀-什么是推理时计算（Test-time-Compute）？" class="headerlink" title="🚀 什么是推理时计算（Test-time Compute）？"></a>🚀 什么是推理时计算（Test-time Compute）？</h2><p>由于 <strong>训练时计算的成本极其昂贵</strong>，研究人员开始关注 <strong>推理时计算（Test-time Compute）</strong>，即：</p><blockquote><p><strong>让 LLM 在推理时“思考更长时间”</strong>，而非单纯依赖更大的模型和数据集。</p></blockquote><p>对于<strong>非推理模型</strong>，它们通常 <strong>直接输出答案</strong>：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Q: 8 + 5 = ?</span><br><span class="line">A: 13</span><br></pre></td></tr></tbody></table></figure><p>而<strong>推理模型</strong>则会 <strong>使用更多 token 进行推理</strong>，形成系统化的“思考”过程：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Q: 8 + 5 = ?</span><br><span class="line">A: 8 + 5 可拆解为 8 + 2 + 3 = 10 + 3 = 13</span><br></pre></td></tr></tbody></table></figure><p>LLM 需要消耗计算资源（如显存计算）来生成答案。然而，如果所有计算资源都用于直接生成答案，那将会是低效的！</p><p>相反，通过提前生成包含额外信息、关系和新思考的更多 token，模型可以在推理过程中分配更多计算资源以生成最终答案。</p><p>这张图片展示了 <strong>大语言模型（LLM）</strong> 在计算过程中如何分配 <strong>token</strong>（标记）来优化推理能力和最终的回答质量。核心思想是：<strong>如果计算资源（如 GPU/VRAM 计算量）全部用于直接生成答案，而没有用于思考，那么效率会受到影响</strong>。相反，增加 <strong>思考过程</strong>（即生成更多的中间 token），可以提高模型的 <strong>推理能力</strong>，从而提升 <strong>最终的回答质量</strong>。</p><p><strong>1. Token 的使用与计算量</strong></p><ul><li><strong>LLM 生成答案是按 token 逐步输出的</strong>，每个 token 都会占用计算资源。</li><li><strong>分配更多的 token 进行思考</strong>，意味着模型可以在得出最终答案之前有更多的推理步骤，从而提高正确率。</li></ul><p> <strong>2. 三种不同的计算方式</strong></p><ul><li><p><strong>场景 1（1 个 token：最少计算）</strong></p><ul><li>直接输出 <strong>“5”</strong> 作为答案。</li><li><strong>计算量最少</strong>，速度最快。</li><li><strong>如果问题较复杂，可能会出错</strong>，因为模型没有足够的计算时间来思考。</li></ul></li><li><p><strong>场景 2（6 个 token：中等计算）</strong></p><ul><li>模型生成一个简短的推理过程：<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Adding 3 and 2 gives 5</span><br></pre></td></tr></tbody></table></figure></li><li><strong>比第一种方法多了一些计算量</strong>，但仍然较为简洁。</li><li>这种方式适用于<strong>简单的数学运算或逻辑推理</strong>，但在更复杂的情况下仍可能出现错误。</li></ul></li><li><p><strong>场景 3（15 个 token：完整推理）</strong></p><ul><li>模型先进行详细的逐步推理：<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">3 + 1 = 4 , 4 + 1 = 5</span><br></pre></td></tr></tbody></table></figure>然后，模型再明确地总结：<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">the total is 5</span><br></pre></td></tr></tbody></table></figure></li><li><strong>推理过程更详细，占用的计算量最大</strong>。</li><li><strong>适用于需要多步推理的任务，如数学题、逻辑推理题等</strong>。</li></ul></li></ul><h3 id="🔢-规模定律（Scaling-Laws）-1"><a href="#🔢-规模定律（Scaling-Laws）-1" class="headerlink" title="🔢 规模定律（Scaling Laws）"></a>🔢 规模定律（Scaling Laws）</h3><p>相比于训练时计算，推理时计算的规模定律仍然较为新颖。值得注意的是，有两项研究揭示了推理时计算规模与训练时计算规模的关系。</p><p>首先，OpenAI 发表的一篇文章表明，推理时计算可能遵循与训练时计算相同的扩展趋势。</p><blockquote><p><strong>来自“学习如何推理的 LLM”一文的注释图</strong>：红色虚线显示了 OpenAI 提出的新范式可能是推理时计算。<br>这张图展示了 <strong>训练时间计算（train-time compute）和测试时间计算（test-time compute）</strong> 对模型 <strong>pass@1 准确率（accuracy）</strong> 的影响，具体来说，它强调了 <strong>测试时间计算可能比训练时间计算更有利于扩展模型性能</strong>。</p></blockquote><ol><li><p><strong>左图：训练时间计算 vs. 准确率</strong></p><ul><li><strong>X 轴（横轴）：训练时间计算（log scale，指数刻度）</strong>。</li><li><strong>Y 轴（纵轴）：pass@1 准确率</strong>（即模型在一次尝试中得到正确答案的概率）。</li><li><strong>黑色点</strong> 代表不同计算量下的模型表现，粉色虚线展示了大致的趋势。</li><li>可以看到，随着 <strong>训练计算量的增加，准确率逐渐提高</strong>，但增长趋势相对平稳。</li></ul></li><li><p><strong>右图：测试时间计算 vs. 准确率</strong></p><ul><li><strong>X 轴（横轴）：测试时间计算（log scale）</strong>。</li><li><strong>Y 轴（纵轴）：pass@1 准确率</strong>。</li><li>同样，黑色点代表不同计算量下的模型表现，粉色虚线展示了大致的趋势。</li><li>这里可以看到，随着 <strong>测试时计算量增加，模型的准确率增长更显著，甚至超过了训练计算量的效果</strong>。<br>因此，他们认为，推理时计算的扩展可能代表着新的研究范式。</li></ul></li></ol><p>其次，一篇名为《Scaling Scaling Laws with Board Games》的论文研究了 AlphaZero 在不同计算量下玩 Hex 游戏的表现。</p><blockquote><p><strong>来自“Scaling Scaling Laws with Board Games”一文的注释图</strong>：该图展示了他们如何构建不同规模的训练时计算和推理时计算。- <strong>AlphaZero</strong> 是 <strong>DeepMind</strong> 开发的一个 <strong>强化学习（Reinforcement Learning, RL）</strong> 训练的 AI。</p></blockquote><ul><li>该算法通过 <strong>自我对弈（self-play）</strong> 训练，无需人为规则输入，即可掌握<strong>围棋、国际象棋、将棋等游戏</strong>。</li><li>它结合了 <strong>神经网络预测</strong> 和 <strong>蒙特卡洛树搜索（MCTS, Monte Carlo Tree Search）</strong> 来进行决策。</li></ul><p>这张图片展示了 <strong>AlphaZero 算法</strong> 在<strong>训练阶段（train-time compute）和测试阶段（test-time compute）</strong>计算资源的不同应用。主要强调了：</p><ul><li><strong>训练时</strong>：依赖于<strong>更多参数和更长的训练时间</strong>来优化模型。</li><li><strong>测试时</strong>：依靠 <strong>更深入的树搜索（tree search）</strong> 来提升决策能力。</li></ul><blockquote><p>来自“Scaling Scaling Laws with Board Games”一文的注释图：该图展示了训练时计算与推理时计算之间的关系。<br>研究结果表明，训练时计算和推理时计算紧密相关。每条虚线表示达到特定 ELO 分数所需的最小计算量。<br><strong>1. 坐标轴含义</strong></p></blockquote><ul><li><strong>X 轴（横轴）：训练时计算量（Train-time Compute，FLOP-seconds）</strong></li><li><strong>Y 轴（纵轴）：推理时计算量（Test-time Compute，FLOP-seconds）</strong></li><li><strong>对数刻度（log scale）：计算量的增长呈指数级，而不是线性增长。</strong></li></ul><p><strong>2. 关键数据趋势</strong></p><ul><li>不同颜色的曲线分别表示<strong>不同的 ELO 分数水平</strong>（-1500、-1250、-1000、-750、-500、-250）。</li><li><strong>虚线和实线</strong>：<ul><li><strong>虚线</strong> 表示某个 ELO 分数下的最优计算边界。</li><li><strong>实线</strong> 代表实际数据趋势。</li></ul></li></ul><ol><li><p><strong>训练计算和推理计算可以互相替代</strong></p><ul><li><strong>如果推理计算量增加（左上区域）</strong>，那么所需的训练计算量减少。</li><li><strong>如果训练计算量增加（右下区域）</strong>，那么所需的推理计算量减少。</li><li><strong>两者呈现负相关关系</strong>。</li></ul></li><li><p><strong>低训练计算 vs. 高推理计算</strong></p><ul><li>在 <strong>训练计算较少</strong> 的情况下（如左侧的红色圈），模型仍然可以达到相同的 ELO 水平，但需要 <strong>在推理时增加计算量</strong>（如更深的搜索树、更长的思考路径）。</li></ul></li><li><p><strong>高训练计算 vs. 低推理计算</strong></p><ul><li>在 <strong>训练计算充足</strong> 的情况下（如右侧的红色圈），模型可以<strong>减少推理计算需求</strong>，即 <strong>即使使用较少的搜索深度，仍然能获得较高的性能</strong>。</li></ul></li><li><p><strong>公式解释</strong></p><ul><li>公式：<br>$$<br>\log_{10}(\text{test compute}) = -1.2 \cdot \log_{10}(\text{train compute}) + 0.004 \cdot \text{elo} + 29<br>$$</li><li>这说明：<ul><li><strong>训练计算（train compute）增加时，推理计算（test compute）减少（系数 -1.2）</strong>。</li><li><strong>更高的 ELO（更强的 AI）需要额外的计算（系数 0.004）</strong>。</li></ul></li></ul></li></ol><p>随着推理时计算扩展类似于训练时计算，研究范式正朝着“推理”模型利用更多推理时计算的方向发展。通过这种范式转变，这些“推理”模型不再单纯关注训练时计算（预训练和微调），而是平衡训练与推理。</p><p>推理时计算甚至可以随长度扩展：</p><p>这是我们在 DeepSeek-R1 研究中也将探讨的内容！</p><h3 id="📌-推理时计算的类别（Categories-of-Test-time-Compute）"><a href="#📌-推理时计算的类别（Categories-of-Test-time-Compute）" class="headerlink" title="📌 推理时计算的类别（Categories of Test-time Compute）"></a>📌 推理时计算的类别（Categories of Test-time Compute）</h3><p>推理模型（如 <strong>DeepSeek-R1</strong> 和 <strong>OpenAI o1</strong>）的成功表明，在推理过程中，除了简单地“思考更长时间”之外，还有更多的优化技术。</p><p>在本文中，我们将探讨 <strong>推理时计算（Test-time Compute）</strong> 的多种实现方式，包括：</p><ul><li><strong>链式思维（Chain-of-Thought）</strong></li><li><strong>答案修订（Revising Answers）</strong></li><li><strong>回溯推理（Backtracking）</strong></li><li><strong>多样性采样（Sampling）</strong></li><li><strong>其他方法</strong></li></ul><p>总体而言，推理时计算可归纳为以下 <strong>两大类别</strong>：</p><ol><li><p><strong>基于验证器的搜索（Search against Verifiers）</strong>  </p><ul><li>通过 <strong>采样多个答案</strong> 并 <strong>选择最佳答案</strong> 来优化推理。</li></ul></li><li><p><strong>修改提议分布（Modifying Proposal Distribution）</strong>  </p><ul><li>通过训练 <strong>“思考”过程</strong> 来提高推理能力。Proposal Distribution（提议分布，指在模型生成答案时，对不同可能答案的概率分布进行调整）</li></ul></li></ol><p>从本质上讲：</p><ul><li><strong>基于验证器的搜索</strong> 更关注 <strong>输出质量</strong>（Output-focused）。</li><li><strong>修改提议分布</strong> 关注 <strong>输入结构</strong>（Input-focused）。</li></ul><h3 id="🔍-两种主要验证器类型"><a href="#🔍-两种主要验证器类型" class="headerlink" title="🔍 两种主要验证器类型"></a>🔍 两种主要验证器类型</h3><p>为了更好地筛选和评估推理答案，我们引入了两种 <strong>验证器（Verifiers）</strong>：</p><ol><li><p><strong>结果奖励模型（Outcome Reward Models, ORM）</strong>  </p><ul><li>仅对最终答案进行评分，而不考虑推理过程。</li></ul></li><li><p><strong>过程奖励模型（Process Reward Models, PRM）</strong>  </p><ul><li>既评估最终答案，也对推理过程进行评分。</li></ul></li></ol><p>在接下来的部分，我们将详细探讨 <strong>如何将 ORM 和 PRM 应用于不同的验证方法</strong>！</p><p>顾名思义，<strong>结果奖励模型（Outcome Reward Model, ORM）</strong> 仅评估最终的答案质量，而不关注答案背后的推理过程：</p><ul><li>ORM 只看最终输出，而不关心模型是如何得出这个答案的。</li></ul><p>相比之下，<strong>过程奖励模型（Process Reward Model, PRM）</strong> 则会评估推理过程本身：</p><ul><li>PRM 既评估答案的正确性，也关注推理路径的合理性。</li></ul><h3 id="🧐-PRM-如何评估推理过程？"><a href="#🧐-PRM-如何评估推理过程？" class="headerlink" title="🧐 PRM 如何评估推理过程？"></a>🧐 PRM 如何评估推理过程？</h3><p>为了更清楚地说明推理步骤的重要性，让我们来看一个示例：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">问题：某个方程的解是多少？</span><br><span class="line"></span><br><span class="line">推理步骤 1：首先展开方程，得到 x = 3。</span><br><span class="line">推理步骤 2：错误地将 x = 3 改写为 x = 5。</span><br><span class="line">推理步骤 3：最终输出 x = 5。</span><br></pre></td></tr></tbody></table></figure><p>在上述示例中，虽然最终答案（x = 5）是错误的，但 ORM 仅评估最终输出，不会关注中间的错误推理。</p><p>或者在这个例子中，PRM 会发现 <strong>推理步骤 2 是错误的</strong>，并对此步骤给予低分，从而避免错误答案的出现。</p><hr><h3 id="🔍-ORM-vs-PRM-在推理中的应用"><a href="#🔍-ORM-vs-PRM-在推理中的应用" class="headerlink" title="🔍 ORM vs. PRM 在推理中的应用"></a>🔍 ORM vs. PRM 在推理中的应用</h3><p>现在你已经掌握了 <strong>结果奖励模型（ORM）</strong> 和 <strong>过程奖励模型（PRM）</strong> 之间的区别，我们接下来探讨如何将它们应用于各种 <strong>验证技术（Verification Techniques）</strong>。</p><h2 id="📌-基于验证器的搜索（Search-against-Verifiers）"><a href="#📌-基于验证器的搜索（Search-against-Verifiers）" class="headerlink" title="📌 基于验证器的搜索（Search against Verifiers）"></a>📌 基于验证器的搜索（Search against Verifiers）</h2><p>推理时计算的第一大类别是 <strong>基于验证器的搜索</strong>，它通常包含两个步骤：</p><ol><li><strong>生成多个推理过程和答案样本</strong></li><li><strong>使用验证器（奖励模型）对生成的输出进行评分</strong></li></ol><h3 id="🤖-验证器的作用"><a href="#🤖-验证器的作用" class="headerlink" title="🤖 验证器的作用"></a>🤖 验证器的作用</h3><p>验证器通常是一个大型语言模型（LLM），经过微调以评估结果（ORM）或过程（PRM）。 使用验证器的一个主要优势是，无需重新训练或微调用于回答问题的大型语言模型（LLM），仅通过评分机制选择最佳答案。</p><hr><h3 id="✅-多数投票法（Majority-Voting）"><a href="#✅-多数投票法（Majority-Voting）" class="headerlink" title="✅ 多数投票法（Majority Voting）"></a>✅ 多数投票法（Majority Voting）</h3><p>最简单的方法是 <strong>不使用奖励模型或验证器</strong>，而是执行 <strong>多数投票（Majority Voting）</strong>。</p><p>📌 <strong>方法：</strong> 让 LLM 生成多个答案，选择出现次数最多的答案作为最终答案。</p><p>📌 <strong>示例：</strong></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Q: 15 × 3 = ?</span><br><span class="line">A1: 45</span><br><span class="line">A2: 42</span><br><span class="line">A3: 45</span><br><span class="line">最终答案: 45（因其出现频率最高）</span><br></pre></td></tr></tbody></table></figure><p>这种方法也称为 <strong>自一致性（Self-Consistency）</strong>，强调 <strong>生成多个答案和推理步骤</strong> 的重要性。</p><hr><h3 id="🔢-Best-of-N-采样法（Best-of-N-Samples）"><a href="#🔢-Best-of-N-采样法（Best-of-N-Samples）" class="headerlink" title="🔢 Best-of-N 采样法（Best-of-N Samples）"></a>🔢 Best-of-N 采样法（Best-of-N Samples）</h3><p>Best-of-N 采样是第一个涉及验证器（Verifier）的方法，它的基本思想是生成 N 个样本答案，然后使用 奖励模型（Reward Model, RM） 对这些答案进行评分，并选择得分最高的答案。</p><p>📌 <strong>步骤：</strong></p><ol><li><strong>生成多个答案</strong>（使用较高或者不同的温度参数生成 N 个样本）。</li><li><strong>结果奖励模型（ORM, Outcome Reward Model）</strong>，每个答案都会通过 ORM 进行评分。选取得分最高的答案作为最终输出。📌 <strong>示例：</strong></li></ol><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Q: 8 + 5 = ?</span><br><span class="line">A1: 12 (得分 0.2)</span><br><span class="line">A2: 13 (得分 0.9)</span><br><span class="line">A3: 14 (得分 0.4)</span><br><span class="line">最终选择: A2（因其得分最高）</span><br></pre></td></tr></tbody></table></figure><h2 id="📌-进一步优化：-若使用-PRM，则不只评估答案，还评估整个推理过程。与仅评估最终答案的结果奖励模型（ORM）不同，过程奖励模型（PRM）会评估推理过程的质量。PRM-关注推理的每个步骤，确保推理过程合理、连贯，并最终选择总评分最高的候选答案。-加权-Best-of-N-采样（Weighted-Best-of-N-samples）-结合-ORM-和-PRM-两种验证方式，我们可以对所有候选答案进行加权评分，并选择总权重最高的答案。这种方法称为-加权-Best-of-N-采样（Weighted-Best-of-N-samples）：。"><a href="#📌-进一步优化：-若使用-PRM，则不只评估答案，还评估整个推理过程。与仅评估最终答案的结果奖励模型（ORM）不同，过程奖励模型（PRM）会评估推理过程的质量。PRM-关注推理的每个步骤，确保推理过程合理、连贯，并最终选择总评分最高的候选答案。-加权-Best-of-N-采样（Weighted-Best-of-N-samples）-结合-ORM-和-PRM-两种验证方式，我们可以对所有候选答案进行加权评分，并选择总权重最高的答案。这种方法称为-加权-Best-of-N-采样（Weighted-Best-of-N-samples）：。" class="headerlink" title="📌 进一步优化：- 若使用 PRM，则不只评估答案，还评估整个推理过程。与仅评估最终答案的结果奖励模型（ORM）不同，过程奖励模型（PRM）会评估推理过程的质量。PRM 关注推理的每个步骤，确保推理过程合理、连贯，并最终选择总评分最高的候选答案。- 加权 Best-of-N 采样（Weighted Best-of-N samples）:结合 ORM 和 PRM 两种验证方式，我们可以对所有候选答案进行加权评分，并选择总权重最高的答案。这种方法称为 加权 Best-of-N 采样（Weighted Best-of-N samples）：。"></a>📌 <strong>进一步优化：</strong><br>- 若使用 <strong>PRM</strong>，则不只评估答案，还评估整个推理过程。与仅评估最终答案的结果奖励模型（ORM）不同，过程奖励模型（PRM）会评估推理过程的质量。PRM 关注推理的每个步骤，确保推理过程合理、连贯，并最终选择总评分最高的候选答案。<br><br>- <strong>加权 Best-of-N 采样（Weighted Best-of-N samples）</strong>:结合 ORM 和 PRM 两种验证方式，我们可以对所有候选答案进行加权评分，并选择总权重最高的答案。这种方法称为 加权 Best-of-N 采样（Weighted Best-of-N samples）：。<br></h2><h3 id="🚀-使用过程奖励模型（PRM）的束搜索（Beam-Search）"><a href="#🚀-使用过程奖励模型（PRM）的束搜索（Beam-Search）" class="headerlink" title="🚀 使用过程奖励模型（PRM）的束搜索（Beam Search）"></a>🚀 使用过程奖励模型（PRM）的束搜索（Beam Search）</h3><p>在生成答案及其中间推理步骤的过程中，我们可以使用 <strong>束搜索（Beam Search）</strong> 进一步优化推理路径。</p><p>📌 <strong>束搜索的核心思想：</strong></p><ul><li>在推理过程中，生成多个可能的推理路径（称为“束”）。</li><li>使用 <strong>过程奖励模型（PRM, Process Reward Model）</strong> 对每条路径进行评分。</li><li>类似于 <strong>Tree of Thought</strong> 方法，始终保留得分最高的 <strong>前 3 条推理路径</strong>，并在推理过程中持续跟踪这些路径。</li><li>如果某条路径的得分较低（PRM 评分低），则提前停止该推理路径，以避免不必要的计算开销。</li></ul><p>📌 <strong>优化后的答案筛选方式：</strong><br>最终，生成的所有答案将使用 <strong>Best-of-N 采样</strong> 方法进行加权评分，确保选出最佳推理路径的最终答案。</p><p>🚀 <strong>优势：</strong></p><ul><li>避免计算资源浪费，快速淘汰低质量推理路径。</li><li>结合 PRM，可以确保模型的推理过程更连贯、更符合逻辑。</li><li>通过 Best-of-N 方法进一步优化答案质量，使最终答案更加可靠。</li></ul><hr><h3 id="🎲-蒙特卡洛树搜索（Monte-Carlo-Tree-Search-MCTS）"><a href="#🎲-蒙特卡洛树搜索（Monte-Carlo-Tree-Search-MCTS）" class="headerlink" title="🎲 蒙特卡洛树搜索（Monte Carlo Tree Search, MCTS）"></a>🎲 蒙特卡洛树搜索（Monte Carlo Tree Search, MCTS）</h3><p>蒙特卡洛树搜索（Monte Carlo Tree Search, <strong>MCTS</strong>）是一种常用于决策树搜索的算法，在 LLM 的推理优化中也可以采用该方法。MCTS 通过四个步骤来优化推理路径：<br>📌 <strong>主要步骤：</strong></p><ol><li><strong>选择（Selection）：</strong> 根据预定义的公式，从当前搜索树中选择一个叶节点 进行扩展。</li><li><strong>扩展（Expand）：</strong> 在所选叶节点的基础上 创建新的子节点，以探索更多可能的推理路径。</li><li><strong>模拟（Rollouts）：</strong> 通过随机生成新的推理路径，持续扩展节点，直到达到终点（即得到最终答案）。</li><li><strong>回溯（Backpropagation）：</strong> 根据最终输出结果 更新父节点的评分，从而优化未来的搜索决策。</li></ol><p>在大语言模型（LLM）的推理过程中，我们通常希望找到最佳的推理路径，使其最终生成的答案最优。但在这个过程中，需要在 <strong>探索（Exploration）</strong> 和 <strong>利用（Exploitation）</strong> 之间取得平衡：</p><ul><li><strong>利用（Exploitation）</strong>：选择当前看起来最优的路径，以利用已知的高质量推理步骤。</li><li><strong>探索（Exploration）</strong>：选择访问次数较少的路径，以发现可能更优的推理步骤。</li></ul><h4 id="选择分数（Selection-Score）"><a href="#选择分数（Selection-Score）" class="headerlink" title="选择分数（Selection Score）"></a>选择分数（Selection Score）</h4><p>在选择推理路径时，我们使用 <strong>选择分数（Selection Score）</strong> 计算每个推理步骤（即树的节点）的优先级，公式如下：</p><p>$$<br>\text{Selection Score} = \frac{\text{Total Node Reward}}{\text{Number of Node Visits}} + C \times \sqrt{\frac{\text{Number of Parent Node Visits}}{\text{Number of Node Visits}}}<br>$$</p><p>其中：</p><ul><li><p><strong>第一项</strong>：$$\frac{\text{Total Node Reward}}{\text{Number of Node Visits}}$$（利用项，Exploitation Term）</p><ul><li><strong>Total Node Reward</strong>：该节点累计获得的奖励值（表示其历史表现）。</li><li><strong>Number of Node Visits</strong>：该节点被访问的次数。</li><li>这项计算的是该节点的 <strong>平均奖励值</strong>，高奖励的节点会被优先选择。</li></ul></li><li><p><strong>第二项</strong>：$$C \times \sqrt{\frac{\text{Number of Parent Node Visits}}{\text{Number of Node Visits}}}$$（探索项，Exploration Term）</p><ul><li><strong># of Parent Node Visits</strong>：父节点被访问的次数。</li><li><strong># of Node Visits</strong>：当前节点被访问的次数。</li><li><strong>C</strong>：一个超参数，控制探索与利用的平衡。</li><li>这项鼓励探索访问次数较少的节点，以防止过早陷入局部最优解。</li></ul></li></ul><p>总结：</p><ul><li><strong>第一项（Exploitation Term）</strong> 让算法倾向于选择 <strong>历史表现较好的路径</strong>。</li><li><strong>第二项（Exploration Term）</strong> 让算法倾向于 <strong>探索访问较少的路径</strong>，避免陷入局部最优。</li><li><strong>参数 C</strong> 控制这两者的平衡。</li></ul><h4 id="2-选择（Selection）与扩展（Expand）"><a href="#2-选择（Selection）与扩展（Expand）" class="headerlink" title="2. 选择（Selection）与扩展（Expand）"></a><strong>2. 选择（Selection）与扩展（Expand）</strong></h4><p>这一阶段，我们使用 <strong>选择分数</strong> 来决定哪条推理路径值得继续扩展：</p><p><strong>（1）选择（Selection）</strong></p><ul><li><strong>输入：问题（Question）</strong></li><li><strong>LLM 生成多个推理步骤（Reasoning Steps）</strong><ul><li>例如，在图片中，LLM 生成了 3 个推理步骤：<ul><li><strong>Thought 1</strong>（评分 0.4）</li><li><strong>Thought 2</strong>（评分 0.2）</li><li><strong>Thought 3</strong>（评分 0.1）</li></ul></li></ul></li><li><strong>使用选择分数（Selection Score）选择最优路径</strong>（随机初始化）<ul><li>在示例中，评分最高的 <strong>Thought 1（0.4）</strong> 被选中。</li></ul></li></ul><p><strong>（2）扩展（Expand）</strong></p><ul><li><strong>在选中的推理路径上，生成新的推理步骤</strong></li><li>这些新推理步骤的初始值设为 0，表示它们还没有经过评估。</li></ul><p>这个过程类似于 <strong>MCTS 的拓展（Expansion）阶段</strong>，即：</p><ol><li>选择当前最优路径（使用 <strong>选择分数</strong>）。</li><li>在该路径下，扩展新的推理步骤（未评分的子节点）。</li></ol><h4 id="3-Rollouts（模拟）与-Backpropagation（反向传播）"><a href="#3-Rollouts（模拟）与-Backpropagation（反向传播）" class="headerlink" title="3. Rollouts（模拟）与 Backpropagation（反向传播）"></a><strong>3. Rollouts（模拟）与 Backpropagation（反向传播）</strong></h4><p>一旦扩展了推理步骤，我们需要继续探索，并利用 <strong>模拟（Rollouts）</strong> 和 <strong>反向传播（Backpropagation）</strong> 来优化整个搜索过程。</p><h3 id="（3）Rollouts（模拟）"><a href="#（3）Rollouts（模拟）" class="headerlink" title="（3）Rollouts（模拟）"></a><strong>（3）Rollouts（模拟）</strong></h3><ul><li>选定路径后，我们继续展开推理步骤，直到 <strong>生成最终答案</strong>。</li><li>这个过程类似于 <strong>在 MCTS 中随机模拟游戏到结束</strong>：<ul><li>我们从当前节点出发，进行一系列推理，直到模型生成最终的答案。</li><li>在图片中，我们沿着 Thought 1（0.4） 继续展开推理步骤。</li><li>这些推理步骤最终会 <strong>生成多个答案</strong>（图片中紫色框）。</li></ul></li></ul><h3 id="（4）Backpropagation（反向传播）"><a href="#（4）Backpropagation（反向传播）" class="headerlink" title="（4）Backpropagation（反向传播）"></a><strong>（4）Backpropagation（反向传播）</strong></h3><ul><li>通过对 <strong>最终答案</strong> 进行评分，我们可以更新前面所有参与推理的节点分数：<ul><li><strong>PRM（Process Reward Model）</strong>：对推理步骤本身进行评分，衡量其合理性。</li><li><strong>ORM（Output Reward Model）</strong>：对最终答案进行评分，衡量其正确性。</li><li>这些评分 <strong>向上传播</strong>，更新 <strong>所有经过的节点</strong> 的奖励值。</li></ul></li><li>例如：<ul><li>在图片中，最终答案的评分导致 <strong>Thought 1</strong> 的评分从 0.4 提高到 <strong>0.8</strong>。</li><li>进一步向上传播，使得 <strong>父节点的选择分数也随之更新</strong>。</li></ul></li></ul><p>这个过程保证了：</p><ul><li><strong>较好的推理路径会逐渐获得更高的分数</strong>，提高被选中的概率。</li><li><strong>较差的推理路径会被逐渐淘汰</strong>，避免浪费计算资源。</li></ul><hr><h2 id="📌-修改提议分布（Modifying-Proposal-Distribution）"><a href="#📌-修改提议分布（Modifying-Proposal-Distribution）" class="headerlink" title="📌 修改提议分布（Modifying Proposal Distribution）"></a>📌 修改提议分布（Modifying Proposal Distribution）</h2><p><strong>修改提议分布（Modifying Proposal Distribution）</strong></p><p>在大语言模型（LLM）的推理过程中，我们可以通过修改提议分布（Modifying Proposal Distribution）来优化模型的推理能力。这种方法的核心思想是：</p><ul><li><strong>不再单纯依赖模型搜索正确推理步骤</strong>（基于输出的优化），</li><li><strong>而是让模型主动生成更优的推理步骤</strong>（基于输入的优化）。</li></ul><p>换句话说，我们不是在输出结果后进行检验，而是直接修改模型在推理过程中如何选择 token，让它更倾向于选择能够引导推理的 token，而不是立即输出最终答案。修改了用于采样补全（completions）、思维（thoughts）或标记（tokens）的概率分布。这种方法可以让模型生成的答案更加准确、可解释，并且在面对复杂问题时更具有鲁棒性（robustness）。</p><p><strong>1. 直接选择最高概率 Token（Greedy 选择）</strong></p><p>在默认情况下，LLM 生成多个可能的 token 作为输出候选项，并根据其概率进行排序，最终选择最高概率的 token 进行输出。这种方法称为<strong>贪心选择（Greedy Selection）</strong>。</p><p>你可以想象，我们有一个问题（question）和一个用于采样 token 的概率分布（distribution）。常见的策略是选择得分最高的 token。</p><ul><li>例如，给定问题 <code>What is 3 + 2?</code>，LLM 可能会生成如下候选 token：<ul><li><code>5</code>（最高概率）</li><li><code>3</code></li><li><code>Adding</code></li><li><code>4</code></li><li><code>If</code></li></ul></li><li>在贪心策略下，模型会直接选择 <code>5</code> 作为最终答案，而不会进行推理。</li></ul><p>这种方法虽然快速，但存在如下问题：</p><ul><li><strong>缺乏推理能力</strong>：模型可能直接输出错误答案，因为它没有进行推理。</li><li><strong>可解释性差</strong>：对于复杂问题，用户无法理解模型是如何得出答案的。</li></ul><p><strong>2. 通过推理（Reasoning Before Answering）提高答案质量</strong></p><p>然而，请注意上图中有一些<strong>标记（tokens</strong>被标红。这些token更有可能引导模型进入一个合理的推理过程。虽然选择贪心（greedy）策略下得分最高的 token 不一定是错误的，但选择那些能引导模型进入推理过程的 token，通常会得到更好的答案。<br>让 LLM <strong>先进行推理，再给出答案</strong>，即：</p><ul><li>选择推理 token（如 <code>Adding</code>）</li><li>逐步生成推理过程，如：<ul><li><code>Adding → 3 and 2 gives → 5</code></li><li><code>If → 3 + 1 = 4, 4 + 1 = 5 → 5</code></li><li><code>The total is → 5</code></li></ul></li><li>通过推理链条逐步推导出 <code>5</code>，相比直接选择 <code>5</code>，这种方法更加可解释，并且能在复杂问题上表现更好。</li></ul><p><strong>3. 通过修改提议分布（Re-Ranking Token Probabilities）引导推理过程</strong></p><p>当我们<strong>修改提议分布（proposal distribution，即 token 的概率分布）</strong>时，实际上是在<strong>重新排序（re-rank）</strong>这个分布，使得“推理相关”的 token 被选中的概率更高。<br>在这种方法下，我们调整 LLM 的提议分布，使其更倾向于选择推理 token，而非直接选择答案：</p><ul><li>默认情况下，<code>5</code> 具有最高概率，而 <code>Adding</code>、<code>If</code> 等推理 token 的概率较低。</li><li>通过修改提议分布，我们提高 <code>Adding</code>、<code>If</code> 的概率，使模型倾向于进行推理。</li></ul><p><strong>4. 如何实现修改提议分布？</strong></p><p>主要有两种方式：</p><ol><li><strong>通过 Prompt Engineering</strong><ul><li>修改 Prompt，引导模型生成推理步骤。</li><li>例如：<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Q: What is 3 + 2?</span><br><span class="line">A: Let's think step by step.</span><br></pre></td></tr></tbody></table></figure></li></ul></li><li><strong>训练模型更倾向于推理</strong><ul><li>在微调过程中，提供更多具有推理链的训练数据，让模型习惯生成推理 token。</li></ul></li></ol><p><strong>总结</strong></p><ul><li><strong>贪心选择（Greedy Selection）</strong>：快速，但缺乏推理，可解释性差。</li><li><strong>推理后回答（Reasoning Before Answering）</strong>：提高答案质量和可解释性。</li><li><strong>修改提议分布（Modifying Proposal Distribution）</strong>：调整 token 选择的概率，使模型更倾向于选择推理 token，提高整体答案的合理性。</li></ul><p>这种方法在<strong>数学计算、逻辑推理、法律推理等任务</strong>上尤为重要，使得 LLM <strong>不仅能“答对”，还能“说明白”</strong>。</p><h3 id="Prompting"><a href="#Prompting" class="headerlink" title="Prompting"></a><strong>Prompting</strong></h3><p>随着我们使用 <strong>prompt engineering</strong>（提示工程）来改进输出，我们会通过更新提示（prompt）来尝试提升模型的表现。这个过程也可能推动模型去展示先前我们看到的一些<strong>reasoning</strong>（推理）过程。</p><p><strong>1. 改变 Proposal Distribution</strong></p><p>在更改 <strong>proposal distribution</strong>时，我们可以给模型提供示例（也叫做 <strong>in-context learning</strong>），让它在生成答案时模仿类似的推理风格。下面的图就展示了一个示例的情形：</p><blockquote><ul><li><strong>图示内容</strong>：左侧是一个简单的问题 “What is 3 + 2?”，模型内部用 “Thoughts” 表示隐藏的思考过程，比如：<ol><li>First, 3 and 1 gives 4.</li><li>Then, 4 and 1 gives 5.</li><li>I believe the answer is 5.</li></ol></li><li><strong>Answer</strong>（答案）：5</li><li>右侧用红色、蓝色等不同颜色的条形或方块表示推理过程的不同部分，示意有一部分属于隐藏的推理过程（红色），以及输出结果或若干中间步骤（蓝色）。</li></ul></blockquote><p>通过类似的示例，模型在推理时就可能模仿类似的格式来进行<strong>reasoning</strong>并给出最终答案。</p><p><strong>2. “Let’s think step-by-step” 的影响</strong></p><p>我们也可以通过在提示中直接使用 “Let’s think step-by-step” 来简化上述流程。这会改变模型的 <strong>proposal distribution</strong>，让 <strong>LLM</strong>（大型语言模型）倾向于在回答之前分步骤思考。如下图所示：</p><blockquote><ul><li><strong>图示内容</strong>：这里将提示换成 “Let’s think step-by-step”，问题仍然是 “What is 3 + 2?”。</li><li>模型产生更显式的推理过程（用红色块示意），再输出正确答案 5。</li><li>整个思路类似图1，但更加突出“分步骤思考”对最终答案生成的影响。</li></ul></blockquote><p>然而，这并不意味着模型本身已经内化了这种推理能力——它<strong>并没有从根本上学会</strong>去“反思”或“修正”错误。如果模型一开始的推理过程是错误的，那么在这种静态且线性的流程中，它往往会一直延续这个错误，而不是对自身推理进行修正。</p><hr><h3 id="STaR（Self-Taught-Reasoner）"><a href="#STaR（Self-Taught-Reasoner）" class="headerlink" title="STaR（Self-Taught Reasoner）"></a><strong>STaR（Self-Taught Reasoner）</strong></h3><p>除了通过 <strong>prompting</strong>（提示）让模型临时展示推理步骤，我们还可以让模型在训练中因为“产生正确推理步骤”而得到奖励，从而让它真正“学会”推理。这通常需要在<strong>大量带有推理过程的数据</strong>上进行训练，并结合 <strong>reinforcement learning</strong>（强化学习）来奖励特定的行为。</p><p>一个颇受争议（“much-debated”）的技术就是 <strong>STaR</strong>，即 <strong>Self-Taught Reasoner</strong>。它是让 <strong>LLM</strong> 生成自己的推理数据，再把这些数据用于对模型进行<strong>精调</strong>（<em>fine-tuning</em>）的过程。</p><p><strong>1. STaR 的流程概述</strong></p><ul><li>这幅图概括了 STaR 的工作原理：<ol><li><strong>Generate reasoning + answer</strong>：模型先针对输入问题生成一段 <strong>reasoning</strong>（推理）和一个 <strong>answer</strong>（答案）；<br>2a. 如果答案正确（Correct answer），则将 <strong>Question, Reasoning, Answer</strong> 作为训练样本添加到三元组数据集中（3a）；<br>  3b. 利用这些三元组数据进行 <strong>supervised fine-tuning</strong>（监督微调），让模型学会在类似情形下产出正确推理与答案。</li></ol></li></ul><p>如果模型给出了错误答案，则会触发另一条路径：</p><ul><li>当 (2b) 模型答案错误时，我们提供正确答案作为 <strong>hint</strong>（提示），并让模型去思考“为什么这个答案是正确的”；</li><li>也就是 <strong>Generate reasoning only</strong> (why this answer is correct?)；</li><li>得到的这段新的推理依旧会被加入到三元组数据中，然后再进行 <strong>supervised fine-tuning</strong>。</li></ul><p>这里的关键要点是，我们可以通过这种方法<strong>显式</strong>地训练模型“应该如何进行推理”，而不仅仅是让它临时地模仿推理过程。我们要对模型的推理方式进行<strong>监督</strong>（<em>supervised fine-tuning</em>），从而把我们想要的推理模式“灌输”给模型。</p><p><strong>2. 自动生成合成训练样本</strong></p><p>STaR 的整个流程非常有趣，因为它会<strong>自动生成合成训练样本</strong>（<em>synthetic training examples</em>）。这些样本不仅包含问题和答案，还包含一系列推理步骤，能够帮助模型更好地学习如何“思考”。在其他研究中（例如 <strong>DeepSeek R-1</strong>），我们可以利用这些合成样本来<strong>蒸馏</strong>（<em>distill</em>，意为“提炼和保留关键信息”）推理过程到其它模型上。也就是说，一个掌握了推理能力的模型可以帮助另一个模型更快地学会类似的推理。</p><hr><p><strong>重点：</strong></p><ul><li><strong>Prompting</strong>（提示）能够影响模型的输出风格和思维过程，比如使用 “Let’s think step-by-step” 让模型显式给出推理步骤，但并不保证模型自动纠正错误。</li><li><strong>STaR</strong>（<strong>Self-Taught Reasoner</strong>）等方法则通过<strong>生成推理数据、监督微调和奖励机制</strong>，帮助模型真正学会按照指定的推理方式去思考和回答问题。</li><li>无论是哪一种方法，都可以视为对 <strong>proposal distribution</strong> 的调节：要么是提示时临时<strong>nudge</strong>（引导），要么是从训练根源上进行调教，让模型内化这种推理过程。</li><li>利用 <strong>in-context learning</strong> 提供示例，能够让模型模仿推理风格。</li><li>用 <strong>reinforcement learning</strong> 或<strong>监督微调</strong>（<strong>supervised fine-tuning</strong>）可以使模型逐渐掌握我们期望的推理模式。</li><li><strong>STaR</strong> 方法会自动收集“正确推理”数据并进行训练，使得模型在后续回答中更可能产生正确且符合要求的推理步骤。</li></ul><hr><h2 id="DeepSeek-R1"><a href="#DeepSeek-R1" class="headerlink" title="DeepSeek-R1"></a>DeepSeek-R1</h2><hr><h3 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h3><p><strong>DeepSeek-R1</strong> 是一个在推理（reasoning）模型领域的重大版本，其权重已经开源。它直接与 OpenAI 的 <strong>o1</strong> 推理模型展开竞争，并在这一领域产生了重大影响。</p><p>DeepSeek 项目在将推理功能优雅地整合进其基础模型（<strong>DeepSeek-V3-Base</strong>）方面成就卓著，采用了多种技术来完成这一目标。</p><p>有趣的是，该项目在训练过程中并未依赖额外的验证器（verifier），而且并不是单纯地依靠监督微调（supervised fine-tuning）来提炼推理行为。相反，<strong>强化学习（Reinforcement Learning, RL）</strong> 在其中扮演了重要角色。</p><p>以下我们将一起探究他们是如何在模型中训练出推理行为的！</p><hr><h3 id="2-DeepSeek-R1-Zero：推理的关键探索"><a href="#2-DeepSeek-R1-Zero：推理的关键探索" class="headerlink" title="2. DeepSeek-R1 Zero：推理的关键探索"></a>2. DeepSeek-R1 Zero：推理的关键探索</h3><p>在通往 <strong>DeepSeek-R1</strong> 的道路上，有一个名为 <strong>DeepSeek-R1 Zero</strong> 的实验性模型为这次突破打下了基础。它从 <strong>DeepSeek-V3-Base</strong> 出发，完全不使用大规模监督微调来加入推理数据，而是只依靠 <strong>强化学习</strong> 来获得推理能力。</p><h4 id="训练过程与系统提示（Prompt）"><a href="#训练过程与系统提示（Prompt）" class="headerlink" title="训练过程与系统提示（Prompt）"></a>训练过程与系统提示（Prompt）</h4><p>在此过程中，他们首先准备了一个非常直接的提示（prompt），其形式类似于系统提示（system prompt），用来作为推理管线的一部分。下文即展示了相关提示。请注意，其中明确指出了推理过程要写在 <code>&lt;think&gt;</code> 标签内、答案要写在 <code>&lt;answer&gt;</code> 标签内，但没有进一步规定推理过程应如何具体呈现或组织。</p><p>在上图中，可以看到一个简化版的对话示例（System prompt 与 User prompt）以及模型如何将<strong>推理</strong>（reasoning）放在 <code>&lt;think&gt;</code> 标签内、将<strong>答案</strong>（answer）放在 <code>&lt;answer&gt;</code> 标签内。该图突出展示了在提示（prompt）中对模型的约束：</p><ul><li><em>“The assistant first thinks about the reasoning process in the mind and then provides the user with the answer.”</em></li><li>要求使用 <code>&lt;think&gt;</code> 进行推理，使用 <code>&lt;answer&gt;</code> 进行回答。</li></ul><p>这里并未提供关于“推理过程”格式的其他例子或模板——完全由模型自己在训练中摸索出要如何输出“Chain-of-Thought”式的推理文字。</p><h4 id="强化学习奖励"><a href="#强化学习奖励" class="headerlink" title="强化学习奖励"></a>强化学习奖励</h4><p>在训练中，采用了两个基于规则（rule-based）的奖励机制：</p><ol><li><strong>准确性奖励（Accuracy rewards）</strong><br>通过测试给出的答案是否正确来进行奖励。若模型输出的答案正确，就会增加奖励。</li><li><strong>格式奖励（Format rewards）</strong><br>奖励模型对 <code>&lt;think&gt;</code> 和 <code>&lt;answer&gt;</code> 标签的正确使用。</li></ol><p>他们所使用的强化学习算法名为 <strong>Group Relative Policy Optimization（GRPO）</strong>。此算法的直观想法在于：使所有导致正确或错误答案的决策更易或更难再次出现。这些决策可能包括模型生成的某些标记（token）序列，也可能包括推理步骤本身（即思考过程）。下文给出了这一训练阶段的示意图。</p><p>在图中，重点展示了在 RL（强化学习）过程中所使用的两类奖励：</p><ul><li>“is <code>&lt;think&gt;</code> used?” —— 为使用 <code>&lt;think&gt;</code> 标签而打分。</li><li>“is <code>&lt;answer&gt;</code> used?” —— 为使用 <code>&lt;answer&gt;</code> 标签而打分。</li></ul><p>除此之外，还有对答案<strong>正确性</strong>的奖励（accuracy reward）。图中箭头所示的循环代表了在训练中不断迭代更新模型，使之越来越倾向于正确的推理方式并合乎格式要求。</p><h4 id="自发推理行为的出现"><a href="#自发推理行为的出现" class="headerlink" title="自发推理行为的出现"></a>自发推理行为的出现</h4><p>值得一提的是，研究人员并没有向模型提供任何示例来告诉它 <code>&lt;think&gt;</code> 标签中的内容应该如何书写或展开。他们仅仅告诉模型：</p><blockquote><p>“It should use <code>&lt;think&gt;</code> tags, and nothing more!”</p></blockquote><p>通过对“Chain-of-Thought”相关行为进行<strong>间接奖励</strong>（即只要推理正确、使用正确格式，就鼓励输出更完整的推理内容），模型在训练中自发地学会了越写越长的推理过程，也更易产生正确答案。</p><p>上图呈现了模型在训练过程中输出的推理长度随训练步数增加而逐渐变长的趋势。纵轴是每个响应的平均长度，横轴是训练步数。可以看到，曲线整体是向上攀升的，这表明模型不断倾向于输出更长、更详细的思考内容（Chain-of-Thought），并因此获得更高奖励。这种做法将大部分计算消耗从训练阶段（train-time compute）转移到了推理阶段（test-time compute），也就是在推理时才生成更长的思考过程。</p><p>根据研究，他们发现通过这种训练策略，模型能够自行发现最优的 Chain-of-Thought 风格的思考方式，并展现出高级的推理能力，例如：<strong>自我反思（self-reflection）</strong> 和 <strong>自我验证（self-verification）</strong>。</p><p>不过，DeepSeek-R1 Zero 的模型输出仍存在一些问题，比如可读性欠佳，且有时会混用多种语言。为了在产品化或发布级别进一步完善，研究人员提出了另一个选项，也就是在正式版本中使用的 <strong>DeepSeek R1</strong>。</p><hr><h3 id="3-深入了解-DeepSeek-R1"><a href="#3-深入了解-DeepSeek-R1" class="headerlink" title="3. 深入了解 DeepSeek-R1"></a>3. 深入了解 DeepSeek-R1</h3><p>要构建 <strong>DeepSeek-R1</strong>，作者共进行了以下五个关键步骤：</p><ol><li><strong>冷启动（Cold Start）</strong></li><li><strong>以推理为导向的强化学习（Reasoning-oriented Reinforcement Learning）</strong></li><li><strong>拒绝采样（Rejection Sampling）</strong></li><li><strong>监督微调（Supervised Fine-Tuning）</strong></li><li><strong>在所有场景下进行强化学习（Reinforcement Learning for all Scenarios）</strong></li></ol><p>接下来我们依次展开说明。</p><hr><h4 id="第一步：冷启动"><a href="#第一步：冷启动" class="headerlink" title="第一步：冷启动"></a>第一步：冷启动</h4><p>在第一步中，研究人员先使用了一个约 5000 个tokens的高质量推理数据集对 <strong>DeepSeek-V3-Base</strong> 进行微调，以避免产生可读性不佳的<strong>冷启动问题（cold start problem）</strong>。这个微调步骤可以让模型的输出更加可读，不至于在一开始就产生混乱的推理文本。下文展示了这一过程的示意图。</p><hr><p>在图中可以看到：</p><ul><li>“DeepSeek-V3-Base” 通过<strong>监督微调（Supervised Fine-Tuning）</strong>的方式，引入了约 5000 条高质量推理样本。</li><li>这些样本包含了<strong>Reasoning</strong>（推理）和<strong>Answer</strong>（答案）两种部分。</li><li>该步骤目的是“防止冷启动”，即让模型在一开始就掌握基础的可读性推理。</li></ul><hr><h4 id="第二步：推理导向的强化学习"><a href="#第二步：推理导向的强化学习" class="headerlink" title="第二步：推理导向的强化学习"></a>第二步：推理导向的强化学习</h4><p>在得到一个初步微调后的模型后（上一步的成果），作者使用与 <strong>DeepSeek-V3-Zero</strong> 类似的强化学习流程对模型进行训练，但额外加入了<strong>目标语言一致性</strong>的奖励，以确保模型在推理和回答时不会混用多种语言。</p><p>除了之前提到的准确性（accuracy reward）和格式（format reward）等，还增加了<strong>语言奖励（language reward）</strong>来保证生成的语言风格或语言类型保持一致，不至于出现“中英文混杂”或“风格不稳”的现象。</p><ul><li><strong>Format reward</strong>：依旧关注 <code>&lt;think&gt;</code> 和 <code>&lt;answer&gt;</code> 的使用。</li><li><strong>Accuracy reward</strong>：检查答案是否正确，以及是否能通过相应的“单元测试”。</li><li><strong>Language reward</strong>：检查语言是否一致、通顺以及是否符合目标语言要求。</li></ul><p>这些奖励综合起来，通过强化学习（RL）循环使模型的推理和答案在可读性、准确度和语言风格方面逐渐优化。</p><hr><h4 id="第三步：拒绝采样"><a href="#第三步：拒绝采样" class="headerlink" title="第三步：拒绝采样"></a>第三步：拒绝采样</h4><p>在这一阶段，作者用<strong>第 2 步</strong>强化学习后得到的模型，来大规模生成<strong>合成推理数据</strong>，并配合 <strong>DeepSeek-V3-Base</strong> 模型来进行“评估”和“规则过滤”，最终产生约 60 万条高质量的推理样本可用于后续监督微调。同时，他们还另外生成了约 20 万条<strong>非推理样本</strong>，包含了写作、简单问答、自我认知、翻译等多种任务数据。下文总结了这一过程。</p><ul><li>左边展示了<strong>DeepSeek-V3-2</strong> 如何采样到大量<strong>Reasoning</strong>（推理）和<strong>Answer</strong>（答案），再利用基于规则的筛选和 <strong>DeepSeek-V3-Base</strong> 的判断（判断生成的内容质量），保留质量更好的推理数据（约 600,000 条）。</li><li>右边展示了<strong>非推理</strong>（non-reasoning）数据采样流程，来自 DeepSeek-V3-Base 所使用的一部分数据，总共约 200,000 条，这些数据主要涉及写作、事实性问答（factual QA）、自我认知、翻译等方面。</li></ul><p>由此，研究人员得到规模约 80 万条的“混合”数据，其中既有推理样本，也有非推理样本。</p><hr><h4 id="第四步：监督微调"><a href="#第四步：监督微调" class="headerlink" title="第四步：监督微调"></a>第四步：监督微调</h4><p>在得到上述 80 万条数据后，研究人员再次对 <strong>DeepSeek-V3-Base</strong> 进行监督微调，具体过程如下图所示。</p><ul><li>在图中，我们看到“DeepSeek-V3-Base”被用于执行<strong>监督微调（Supervised Fine-Tuning）</strong>，使用的正是前文所提到的 800,000 条<strong>高质量推理与非推理样本</strong>。</li><li>这一阶段使得模型在更大规模的数据基础上，学习到更广泛、更多样的推理形式和任务形式。</li></ul><hr><h4 id="第五步：在所有场景下的强化学习"><a href="#第五步：在所有场景下的强化学习" class="headerlink" title="第五步：在所有场景下的强化学习"></a>第五步：在所有场景下的强化学习</h4><p>在监督微调完成后，研究人员继续采用类似 <strong>DeepSeek-R1-Zero</strong> 的方法进行 <strong>RL（强化学习）</strong> 训练。但是，为了让模型更符合人类偏好，他们在这个阶段引入了更多的 <strong>“有益与无害”（helpfulness and harmlessness）</strong> 奖励信号，用来约束模型的回答。</p><p>同时，模型也被要求<strong>对推理过程进行总结（summarize）</strong>，以防止在最终输出时显示出过长、难以阅读的推理文本。这一步骤解决了前述提到的可读性问题。</p><ol><li><strong>Format reward（格式奖励）</strong>  <ul><li>是否正确使用 <code>&lt;think&gt;</code> 标签书写推理内容  </li><li>是否正确使用 <code>&lt;answer&gt;</code> 标签输出答案</li></ul></li><li><strong>Accuracy reward（准确性奖励）</strong>  <ul><li>测试输出是否能编译（“does it compile?”）  </li><li>是否能通过单元测试（“does it pass unit tests?”）</li></ul></li><li><strong>Preference rewards（偏好奖励）</strong>  <ul><li>关注 <strong>Helpfulness（有益）</strong>、<strong>Harmlessness（无害）</strong>、<strong>Human preference（人类偏好）</strong> 等  </li><li>由 RM（Reward Model） 模块来评估这些偏好指标</li></ul></li></ol><p>图中可以看到，<strong>Reasoning</strong>（推理）阶段和 <strong>Answer</strong>（答案）阶段需要分别用 <code>&lt;think&gt;</code> 和 <code>&lt;answer&gt;</code> 标签进行明确区分。同时，为了输出更为精简、可读的内容，模型也可能产生一个 <strong>Summary</strong>（总结）片段。强化学习的迭代过程会同时考虑多种奖励信号，从而不断更新模型并得到最终版本的 <strong>DeepSeek-R1</strong>。</p><p>上图中，“RM” 即 Reward Model，用于对偏好进行打分（如对话是否友善、是否符合伦理要求等），再把结果反馈给模型。</p><p>“<strong>And that’s it!<strong>”这意味着 <strong>DeepSeek-R1</strong> 实际上是 <strong>DeepSeek-V3-Base</strong> 经过监督微调（Supervised Fine-Tuning）和强化学习（RL）进一步优化而成。大量的工作都用于保证</strong>高质量数据</strong>的生成与使用，进而训练出这样一个具备强大推理能力的模型。</p><hr><h2 id="将推理知识从-DeepSeek-R1-蒸馏到其他模型"><a href="#将推理知识从-DeepSeek-R1-蒸馏到其他模型" class="headerlink" title="将推理知识从 DeepSeek-R1 蒸馏到其他模型"></a>将推理知识从 DeepSeek-R1 蒸馏到其他模型</h2><p><strong>DeepSeek-R1</strong> 拥有 <strong>6710 亿（671B）</strong> 参数。这一规模的模型在普通消费级硬件上运行存在较大难度。出于实用性考虑，作者们研究了如何将 <strong>DeepSeek-R1</strong> 的推理能力“蒸馏（distill）”到更小的模型（如 <strong>Qwen-32B</strong>）上，以便能在消费级硬件上部署和使用。</p><h3 id="蒸馏过程：Teacher-Student-框架"><a href="#蒸馏过程：Teacher-Student-框架" class="headerlink" title="蒸馏过程：Teacher-Student 框架"></a>蒸馏过程：Teacher-Student 框架</h3><p>在蒸馏过程中，<strong>DeepSeek-R1</strong> 作为教师模型（Teacher），而规模更小的模型（如 Qwen-32B）作为学生模型（Student）。二者面对相同的提示（prompt）时，分别会输出一组<strong>词元概率分布（token probability distribution）</strong>。训练时，学生模型会尽量学习并接近教师模型的输出分布。</p><ul><li>教师（DeepSeek-R1）给出自己的“proposal distribution”。例如在回答“What is 3 + 2?”时，教师模型可能倾向输出“Adding”“If”“5”“3”“4”等标记，并赋予各自不同的概率。  </li><li>学生（Qwen-32B）则会在训练中不断更新自己的概率分布，使之更接近教师的分布。</li></ul><blockquote><p><strong>额外解释</strong>：  </p><ol><li><strong>概率分布（proposal distribution）</strong>：语言模型在生成下一个词元（token）时，会输出对所有可能词元的概率估计。  </li><li><strong>蒸馏（distillation）</strong>：通过比较教师和学生的分布差异，学生会逐步调整自身参数，使其输出更接近教师模型的风格和推理倾向。</li></ol></blockquote><p>训练所使用的数据，正是之前提到的那 <strong>80 万条高质量样本</strong>——其中包含约 60 万推理样本和 20 万非推理样本。下图展示了这一数据流向： </p><ul><li>左侧的 <strong>Reasoning</strong>（推理）和 <strong>Answer</strong>（答案）数据，合计 80 万条。  </li><li>由 <strong>DeepSeek-R1</strong>（Teacher）生成或评估，得到对应的概率分布。  </li><li>学生模型 <strong>Qwen-32B</strong> 则根据教师的分布进行学习，最终得到一个蒸馏版本 <strong>DeepSeek-R1-Distill-Qwen-32B</strong>。</li></ul><p><img src="https://user-images.githubusercontent.com/your-image-url.png" alt="使用 80 万条高质量样本蒸馏的流程（图10）"></p><blockquote><p><strong>额外解释</strong>：  </p><ul><li>学生模型不仅仅学习了那 80 万条样本本身的输入-输出模式，也学习到 <strong>DeepSeek-R1</strong> 在面对这些数据时所“倾向”采用的推理策略和概率分布，从而在更小模型上复现类似的推理能力。  </li><li>“Distilled” 模型往往会在推理质量与计算资源之间找到更好的平衡：虽然可能在性能上略逊色于老师模型，但依然能在大多数常见任务上达到令人满意的结果，并且所需资源更低。</li></ul></blockquote><hr><h2 id="其他未成功的尝试"><a href="#其他未成功的尝试" class="headerlink" title="其他未成功的尝试"></a>其他未成功的尝试</h2><p>在研究过程中，DeepSeek 团队也曾尝试过 <strong>Process Reward Models（PRMs）</strong> 和 <strong>Monte Carlo Tree Search（MCTS）</strong> 等方法来注入推理能力，但结果并不理想：</p><ol><li><p><strong>使用 MCTS</strong>  </p><ul><li>面临的主要问题是搜索空间过于庞大，只能对节点展开进行严格限制。这样一来，效果就大打折扣。  </li><li>此外，精细化训练 Reward Model 也相当困难。</li></ul></li><li><p><strong>使用 PRMs 进行 Best-of-N 策略</strong>  </p><ul><li>如果不断重训练 Reward Model 以防止模型出现“投机取巧”（reward hacking）行为，会带来高昂的计算开销。</li></ul></li></ol><p>这些结果并不意味着这些技术无效，而是说明它们在当前大规模语言模型上的实践还有诸多限制与难点。<strong>DeepSeek-R1</strong> 之所以取得成功，更多依赖于<strong>强化学习 + 监督微调</strong>的组合，以及对大规模高质量数据的挖掘与利用。</p><hr><h2 id="总结与展望"><a href="#总结与展望" class="headerlink" title="总结与展望"></a>总结与展望</h2><p>至此，我们已经大致回顾了 <strong>DeepSeek-R1</strong> 的推理训练之旅。希望以上内容能够让你更好地理解：  </p><ul><li><strong>Test-time compute（推理时计算）</strong> 可以通过模型输出更长、更精细的思考过程（Chain-of-Thought）来取得更佳效果。  </li><li>大规模“<strong>先监督微调，再强化学习</strong>”的训练流程，以及<strong>蒸馏</strong>到更小模型的技术路线，也展现了在硬件资源和推理性能间取得平衡的方法。</li></ul><p>如前所述，<strong>DeepSeek-R1</strong> 引入了多种奖励机制，尤其是针对格式和人类偏好的奖励，来保证回答既正确又易读。“总结推理过程”（Summary）的做法也在很大程度上改善了纯文本Chain-of-Thought过长而导致的可读性问题。</p><hr><h2 id="更多资源"><a href="#更多资源" class="headerlink" title="更多资源"></a>更多资源</h2><p>如果你对 <strong>Large Language Models（LLMs）</strong> 中的推理话题感兴趣，以下资源值得参考：</p><ol><li><a href="https://newsletter.languagemodels.co/p/the-illustrated-deepseek-r1"><strong>The Illustrated DeepSeek-R1</strong></a>  <ul><li>Jay Alammar 制作的高质量可视化指南，详细介绍了 DeepSeek-R1 模型背后的原理与实现细节。</li></ul></li><li><a href="https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute"><strong>Hugging Face 的一篇博文</strong></a>  <ul><li>重点讨论了在推理阶段如何对计算量进行扩展，并给出了有趣的实验。</li></ul></li><li><a href="https://www.youtube.com/watch?v=6PEJ96k1kiw"><strong>视频 “Speculations on Test-Time Scaling”</strong></a>  <ul><li>深入探讨了在推理阶段进行各种计算扩展的常用技术细节。</li></ul></li></ol><p>此外，作者在文中也提到了一本关于大型语言模型的著作，内含更多可视化和实验结果，是想进一步研究推理 LLMs 的朋友可以深入阅读的好资料。</p><ul><li><strong>Official Website of the Book</strong>: <a href="https://www.llm-book.com/">llm-book.com</a>  </li><li><strong>Amazon 购买链接</strong>: <a href="https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961">Hands-On Large Language Models: Understanding, Building, and Optimizing LLMs</a>  </li><li><strong>GitHub 代码仓库</strong>: <a href="https://github.com/handsOnLLM/Hands-On-Large-Language-Models">handsOnLLM/Hands-On-Large-Language-Models</a></li></ul><hr><h3 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h3><p>感谢你阅读本篇关于 <strong>DeepSeek-R1</strong> 的介绍文档。通过对所有图片与文字内容的依次解读，以及对每个环节所涉及的关键技术进行了更多解释，我们希望让你对 <strong>DeepSeek-R1</strong> 的训练流程、蒸馏方法和未成功的尝试都有更加全面的了解。</p><p>在未来，随着硬件性能的提升与更成熟的训练技术出现，<strong>深度推理</strong>与<strong>模型蒸馏</strong>必将在更多实际应用场景中发挥巨大作用。让我们拭目以待！</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;推理-LLM-的可视化指南：探索推理时计算技术与-DeepSeek-R1&quot;&gt;&lt;a href=&quot;#推理-LLM-的可视化指南：探索推理时计算技术与-DeepSeek-R1&quot; class=&quot;headerlink&quot; title=&quot;推理 LLM 的可视化指南：探索推理时计</summary>
      
    
    
    
    <category term="NLP Insights" scheme="https://chenhuiyu.github.io/categories/NLP-Insights/"/>
    
    
    <category term="LLM" scheme="https://chenhuiyu.github.io/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>推理 LLM 的可视化指南：探索推理时计算技术与 DeepSeek-R1</title>
    <link href="https://chenhuiyu.github.io/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/"/>
    <id>https://chenhuiyu.github.io/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/</id>
    <published>2025-02-11T03:50:29.000Z</published>
    <updated>2026-02-20T21:51:54.082Z</updated>
    
    <content type="html"><![CDATA[<h1 id="推理-LLM-的可视化指南：探索推理时计算技术与-DeepSeek-R1"><a href="#推理-LLM-的可视化指南：探索推理时计算技术与-DeepSeek-R1" class="headerlink" title="推理 LLM 的可视化指南：探索推理时计算技术与 DeepSeek-R1"></a>推理 LLM 的可视化指南：探索推理时计算技术与 DeepSeek-R1</h1><p><strong>原文地址</strong>：<a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-reasoning-llms">A Visual Guide to Reasoning LLMs</a></p><p>📅 作者：Maarten Grootendorst</p><p>📆 日期：2025 年 2 月 3 日</p><hr><h2 id="📌-引言"><a href="#📌-引言" class="headerlink" title="📌 引言"></a>📌 引言</h2><p>DeepSeek-R1、OpenAI o3-mini 和 Google Gemini 2.0 Flash Thinking 是如何通过“推理”框架将 <strong>LLM（大型语言模型, Large Language Models）</strong> 扩展到新高度的典型示例。</p><p>它们标志着从 <strong>扩展训练时计算（train-time compute）</strong> 到 <strong>扩展推理时计算（test-time compute）</strong> 的范式转变。</p><p>在本篇文章中，我们提供了 <strong>超过 40 张定制可视化图表</strong>，带你深入探索：</p><ul><li><strong>推理 LLM（Reasoning LLMs）</strong> 领域</li><li><strong>推理时计算（Test-Time Compute）</strong> 机制</li><li><strong>DeepSeek-R1</strong> 的核心思想</li></ul><p>我们将逐步介绍相关概念，帮助你建立对这一新范式的直觉理解。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/i24pmg2.png" class=""><hr><h2 id="📖-什么是推理-LLM？"><a href="#📖-什么是推理-LLM？" class="headerlink" title="📖 什么是推理 LLM？"></a>📖 什么是推理 LLM？</h2><p>与普通 <strong>LLM（Large Language Models，大型语言模型）</strong> 相比，<strong>推理 LLM</strong> 在回答问题之前，往往会将问题 <strong>分解为更小的步骤</strong>（通常称为 <strong>推理步骤（Reasoning Steps）</strong> 或 <strong>思考过程（Thought Process）</strong>）。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_143007.png" class=""><h3 id="🧠-“推理步骤”-或-“思考过程”-是什么？"><a href="#🧠-“推理步骤”-或-“思考过程”-是什么？" class="headerlink" title="🧠 “推理步骤” 或 “思考过程” 是什么？"></a>🧠 “推理步骤” 或 “思考过程” 是什么？</h3><p>尽管我们可以哲学化地探讨 LLM 是否真的能够像人类一样思考，但这些推理步骤实际上是将推理过程 分解为更小、更结构化的推断。<strong>推理 LLM 采用的是结构化推理方式</strong>，即：</p><ul><li><strong>普通 LLM</strong>：直接输出答案</li><li><strong>推理 LLM</strong>：通过系统性推理生成答案</li></ul><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_143054.png" class=""><p>换句话说，推理 LLM 不是<strong>学习“回答什么”</strong>，而是<strong>学习“如何回答”</strong>！</p><p>要理解推理 LLM 的构建原理，我们首先需要探讨 <strong>训练时计算（Train-Time Compute）</strong> 和 <strong>推理时计算（Test-Time Compute）</strong> 之间的差异。</p><hr><h2 id="🔍-什么是训练时计算（Train-time-Compute）？"><a href="#🔍-什么是训练时计算（Train-time-Compute）？" class="headerlink" title="🔍 什么是训练时计算（Train-time Compute）？"></a>🔍 什么是训练时计算（Train-time Compute）？</h2><p>直到 2024 年年中，为了在 <strong>预训练（Pretraining）</strong> 期间提高 LLM 的性能，研究人员通常会扩大以下规模：</p><ul><li><strong>模型参数数量（# of Parameters）</strong></li><li><strong>数据集规模（# of Tokens）</strong></li><li><strong>计算量（# of FLOPs, Floating Point Operations）</strong></li></ul><p>这些合称为 <strong>训练时计算（Train-time Compute）</strong>，即 <strong>“AI 的化石燃料”</strong>，指的是：</p><blockquote><p><strong>预训练预算越大，最终得到的模型就越好。</strong></p><p>训练时计算（Train-Time Compute）包括<strong>训练（training）</strong>所需的计算，以及<strong>微调（fine-tuning）</strong>所需的计算。长期以来，一直是提高 LLM 性能的主要关注点。</p></blockquote><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_143927.png" class=""><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_144121.png" class=""><h3 id="🔢-规模定律（Scaling-Laws）"><a href="#🔢-规模定律（Scaling-Laws）" class="headerlink" title="🔢 规模定律（Scaling Laws）"></a>🔢 规模定律（Scaling Laws）</h3><p>在 <strong>LLM（大型语言模型）</strong> 研究领域，<strong>模型规模（Scale）</strong> 与 <strong>模型性能（Performance）</strong> 之间的关系被称为 <strong>规模定律（Scaling Laws）</strong>。这些定律通常用于描述 <strong>计算资源、数据规模和模型参数</strong> 如何影响模型的整体表现。</p><p>这些关系通常以 <strong>对数-对数（log-log）</strong> 方式呈现，并且在图表上通常显示为一条 <strong>近似直线</strong>，以突出计算量的巨大增长。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_144846.png" class=""><p>这张图片展示了<strong>不同坐标尺度（线性 vs. 对数）对计算资源（Compute）和模型性能（Performance）之间关系的影响</strong>，强调了大模型增长的幂律关系（Power Law）。</p><ul><li><p><strong>左图（普通线性尺度 - Normal Scale）</strong></p><ul><li>横轴（X 轴）：计算资源（Compute），<strong>线性刻度</strong>。</li><li>纵轴（Y 轴）：性能（Performance），<strong>线性刻度</strong>。</li><li>曲线显示<strong>递减收益（Diminishing Returns）</strong>，即：<strong>随着计算资源的增加，性能增长趋缓</strong>，但仍然在上升。</li></ul></li><li><p><strong>右图（对数-对数尺度 - Log-log Scale）</strong></p><ul><li>横轴（X 轴）：计算资源（Compute），<strong>对数刻度</strong>。</li><li>纵轴（Y 轴）：性能（Performance），<strong>对数刻度</strong>。</li><li>在对数-对数尺度下，原本弯曲的曲线变成<strong>一条直线</strong>，说明计算资源和性能之间呈<strong>幂律关系（Power Law Relationship）</strong>。</li></ul></li></ul><p>这些定律通常遵循 <strong>幂律（Power Laws）</strong>，即：</p><blockquote><p><strong>某个变量（如计算量）增加，会导致另一个变量（如性能）按一定比例变化。</strong></p></blockquote><p>最著名的 <strong>规模定律</strong> 包括：</p><ul><li><strong>Kaplan 规模定律</strong>（Kaplan Scaling Law）：当计算资源一定时，<strong>增加模型的参数规模比增加数据规模更有效</strong>。表明模型性能与参数量、计算量和训练数据（Tokens）之间存在幂律关系，即 更多参数、更多计算资源能提升性能（GPT-3 论文提出）。</li><li><strong>Chinchilla 规模定律</strong>（Chinchilla Scaling Law）：模型的大小和数据规模同样重要，二者需 <strong>同步扩展</strong> 才能实现最佳性能（DeepMind 提出）。</li></ul><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_145639.png" class=""><p>这张图展示了<strong>大规模 AI 训练中的 Scaling Laws（缩放定律）</strong>，表明<strong>计算资源（Compute）、数据集规模（Dataset Size）和参数量（Parameters）</strong>对模型性能的影响。关键信息如下：</p><hr><p><strong>1. 纵轴（Y轴）：测试损失（Test Loss）</strong></p><ul><li><strong>目标是降低测试损失（Test Loss）</strong>，即提高模型的泛化性能。</li><li><strong>损失（L）越小，模型性能越好</strong>。</li></ul><p><strong>2. 横轴（X轴）：三种关键变量</strong></p><ul><li><p><strong>左图（Compute，计算资源）</strong>：</p><ul><li>X 轴是计算资源（PF-days, 非 embedding）。</li><li>计算资源越多，测试损失降低（性能提升）。</li><li>公式：<br>$$<br>L = \left( \frac{C_{\text{min}}}{2.3 \times 10^8} \right)^{-0.050}<br>$$</li><li><strong>体现计算资源的幂律关系</strong>：计算资源增加，损失减少，但收益递减（指数 -0.050）。</li></ul></li><li><p><strong>中图（Dataset Size，数据集规模）</strong>：</p><ul><li>X 轴是训练数据的 Token 数量。</li><li>数据规模越大，测试损失降低（性能提升）。</li><li>公式：<br>$$<br>L = \left( \frac{D}{5.4 \times 10^{13}} \right)^{-0.095}<br>$$</li><li><strong>数据规模对损失的影响较大</strong>（指数 -0.095）。</li></ul></li><li><p><strong>右图（Parameters，参数量）</strong>：</p><ul><li>X 轴是模型参数量（非 embedding）。</li><li>参数数量越大，测试损失降低（性能提升）。</li><li>公式：<br>$$<br>L = \left( \frac{N}{8.8 \times 10^{13}} \right)^{-0.076}<br>$$</li><li><strong>参数对损失的影响介于计算资源和数据规模之间</strong>（指数 -0.076）。</li></ul></li></ul><p>这些研究表明，<strong>模型规模、数据规模和计算资源必须协同扩展，才能最大化模型的性能</strong>。</p><ul><li><strong>计算资源增加 → 训练更强大模型</strong></li><li><strong>更多 Tokens → 更好泛化能力</strong></li><li><strong>参数增加 → 但需要与数据匹配，否则过拟合</strong></li></ul><p>Kaplan 规模定律认为，在 <strong>固定计算资源</strong> 的情况下，<strong>优先增加模型参数</strong> 通常比增加数据规模更有效。而 Chinchilla 规模定律则指出，<strong>模型参数和数据规模都应同步增长</strong>，以获得更优的模型性能。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_145814.png" class=""><p>然而，在 <strong>2024 年</strong>，研究人员发现，尽管计算资源、数据规模和模型参数 <strong>持续增长</strong>，但性能提升的 <strong>边际收益（Marginal Return）</strong> 却在 <strong>逐渐降低</strong>。</p><p>这引发了一个重要的问题：</p><p>❓ <strong>“我们是否已经遇到了 LLM 发展的瓶颈？”</strong></p><hr><h2 id="🚀-什么是推理时计算（Test-time-Compute）？"><a href="#🚀-什么是推理时计算（Test-time-Compute）？" class="headerlink" title="🚀 什么是推理时计算（Test-time Compute）？"></a>🚀 什么是推理时计算（Test-time Compute）？</h2><p>由于 <strong>训练时计算的成本极其昂贵</strong>，研究人员开始关注 <strong>推理时计算（Test-time Compute）</strong>，即：</p><blockquote><p><strong>让 LLM 在推理时“思考更长时间”</strong>，而非单纯依赖更大的模型和数据集。</p></blockquote><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_145856.png" class=""><p>对于<strong>非推理模型</strong>，它们通常 <strong>直接输出答案</strong>：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Q: 8 + 5 = ?</span><br><span class="line">A: 13</span><br></pre></td></tr></tbody></table></figure><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_145936.png" class=""><p>而<strong>推理模型</strong>则会 <strong>使用更多 token 进行推理</strong>，形成系统化的“思考”过程：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Q: 8 + 5 = ?</span><br><span class="line">A: 8 + 5 可拆解为 8 + 2 + 3 = 10 + 3 = 13</span><br></pre></td></tr></tbody></table></figure><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_150033.png" class=""><p>LLM 需要消耗计算资源（如显存计算）来生成答案。然而，如果所有计算资源都用于直接生成答案，那将会是低效的！</p><p>相反，通过提前生成包含额外信息、关系和新思考的更多 token，模型可以在推理过程中分配更多计算资源以生成最终答案。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_151252.png" class=""><p>这张图片展示了 <strong>大语言模型（LLM）</strong> 在计算过程中如何分配 <strong>token</strong>（标记）来优化推理能力和最终的回答质量。核心思想是：<strong>如果计算资源（如 GPU/VRAM 计算量）全部用于直接生成答案，而没有用于思考，那么效率会受到影响</strong>。相反，增加 <strong>思考过程</strong>（即生成更多的中间 token），可以提高模型的 <strong>推理能力</strong>，从而提升 <strong>最终的回答质量</strong>。</p><p><strong>1. Token 的使用与计算量</strong></p><ul><li><strong>LLM 生成答案是按 token 逐步输出的</strong>，每个 token 都会占用计算资源。</li><li><strong>分配更多的 token 进行思考</strong>，意味着模型可以在得出最终答案之前有更多的推理步骤，从而提高正确率。</li></ul><p> <strong>2. 三种不同的计算方式</strong></p><ul><li><p><strong>场景 1（1 个 token：最少计算）</strong></p><ul><li>直接输出 <strong>“5”</strong> 作为答案。</li><li><strong>计算量最少</strong>，速度最快。</li><li><strong>如果问题较复杂，可能会出错</strong>，因为模型没有足够的计算时间来思考。</li></ul></li><li><p><strong>场景 2（6 个 token：中等计算）</strong></p><ul><li>模型生成一个简短的推理过程：<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Adding 3 and 2 gives 5</span><br></pre></td></tr></tbody></table></figure></li><li><strong>比第一种方法多了一些计算量</strong>，但仍然较为简洁。</li><li>这种方式适用于<strong>简单的数学运算或逻辑推理</strong>，但在更复杂的情况下仍可能出现错误。</li></ul></li><li><p><strong>场景 3（15 个 token：完整推理）</strong></p><ul><li>模型先进行详细的逐步推理：<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">3 + 1 = 4 , 4 + 1 = 5</span><br></pre></td></tr></tbody></table></figure>然后，模型再明确地总结：<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">the total is 5</span><br></pre></td></tr></tbody></table></figure></li><li><strong>推理过程更详细，占用的计算量最大</strong>。</li><li><strong>适用于需要多步推理的任务，如数学题、逻辑推理题等</strong>。</li></ul></li></ul><h3 id="🔢-规模定律（Scaling-Laws）-1"><a href="#🔢-规模定律（Scaling-Laws）-1" class="headerlink" title="🔢 规模定律（Scaling Laws）"></a>🔢 规模定律（Scaling Laws）</h3><p>相比于训练时计算，推理时计算的规模定律仍然较为新颖。值得注意的是，有两项研究揭示了推理时计算规模与训练时计算规模的关系。</p><p>首先，OpenAI 发表的一篇文章表明，推理时计算可能遵循与训练时计算相同的扩展趋势。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_151722.png" class=""><blockquote><p><strong>来自“学习如何推理的 LLM”一文的注释图</strong>：红色虚线显示了 OpenAI 提出的新范式可能是推理时计算。<br>这张图展示了 <strong>训练时间计算（train-time compute）和测试时间计算（test-time compute）</strong> 对模型 <strong>pass@1 准确率（accuracy）</strong> 的影响，具体来说，它强调了 <strong>测试时间计算可能比训练时间计算更有利于扩展模型性能</strong>。</p></blockquote><ol><li><p><strong>左图：训练时间计算 vs. 准确率</strong></p><ul><li><strong>X 轴（横轴）：训练时间计算（log scale，指数刻度）</strong>。</li><li><strong>Y 轴（纵轴）：pass@1 准确率</strong>（即模型在一次尝试中得到正确答案的概率）。</li><li><strong>黑色点</strong> 代表不同计算量下的模型表现，粉色虚线展示了大致的趋势。</li><li>可以看到，随着 <strong>训练计算量的增加，准确率逐渐提高</strong>，但增长趋势相对平稳。</li></ul></li><li><p><strong>右图：测试时间计算 vs. 准确率</strong></p><ul><li><strong>X 轴（横轴）：测试时间计算（log scale）</strong>。</li><li><strong>Y 轴（纵轴）：pass@1 准确率</strong>。</li><li>同样，黑色点代表不同计算量下的模型表现，粉色虚线展示了大致的趋势。</li><li>这里可以看到，随着 <strong>测试时计算量增加，模型的准确率增长更显著，甚至超过了训练计算量的效果</strong>。<br>因此，他们认为，推理时计算的扩展可能代表着新的研究范式。</li></ul></li></ol><p>其次，一篇名为《Scaling Scaling Laws with Board Games》的论文研究了 AlphaZero 在不同计算量下玩 Hex 游戏的表现。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_152050.png" class=""><blockquote><p><strong>来自“Scaling Scaling Laws with Board Games”一文的注释图</strong>：该图展示了他们如何构建不同规模的训练时计算和推理时计算。- <strong>AlphaZero</strong> 是 <strong>DeepMind</strong> 开发的一个 <strong>强化学习（Reinforcement Learning, RL）</strong> 训练的 AI。</p></blockquote><ul><li>该算法通过 <strong>自我对弈（self-play）</strong> 训练，无需人为规则输入，即可掌握<strong>围棋、国际象棋、将棋等游戏</strong>。</li><li>它结合了 <strong>神经网络预测</strong> 和 <strong>蒙特卡洛树搜索（MCTS, Monte Carlo Tree Search）</strong> 来进行决策。</li></ul><p>这张图片展示了 <strong>AlphaZero 算法</strong> 在<strong>训练阶段（train-time compute）和测试阶段（test-time compute）</strong>计算资源的不同应用。主要强调了：</p><ul><li><strong>训练时</strong>：依赖于<strong>更多参数和更长的训练时间</strong>来优化模型。</li><li><strong>测试时</strong>：依靠 <strong>更深入的树搜索（tree search）</strong> 来提升决策能力。</li></ul><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_152430.png" class=""><blockquote><p>来自“Scaling Scaling Laws with Board Games”一文的注释图：该图展示了训练时计算与推理时计算之间的关系。<br>研究结果表明，训练时计算和推理时计算紧密相关。每条虚线表示达到特定 ELO 分数所需的最小计算量。<br><strong>1. 坐标轴含义</strong></p></blockquote><ul><li><strong>X 轴（横轴）：训练时计算量（Train-time Compute，FLOP-seconds）</strong></li><li><strong>Y 轴（纵轴）：推理时计算量（Test-time Compute，FLOP-seconds）</strong></li><li><strong>对数刻度（log scale）：计算量的增长呈指数级，而不是线性增长。</strong></li></ul><p><strong>2. 关键数据趋势</strong></p><ul><li>不同颜色的曲线分别表示<strong>不同的 ELO 分数水平</strong>（-1500、-1250、-1000、-750、-500、-250）。</li><li><strong>虚线和实线</strong>：<ul><li><strong>虚线</strong> 表示某个 ELO 分数下的最优计算边界。</li><li><strong>实线</strong> 代表实际数据趋势。</li></ul></li></ul><ol><li><p><strong>训练计算和推理计算可以互相替代</strong></p><ul><li><strong>如果推理计算量增加（左上区域）</strong>，那么所需的训练计算量减少。</li><li><strong>如果训练计算量增加（右下区域）</strong>，那么所需的推理计算量减少。</li><li><strong>两者呈现负相关关系</strong>。</li></ul></li><li><p><strong>低训练计算 vs. 高推理计算</strong></p><ul><li>在 <strong>训练计算较少</strong> 的情况下（如左侧的红色圈），模型仍然可以达到相同的 ELO 水平，但需要 <strong>在推理时增加计算量</strong>（如更深的搜索树、更长的思考路径）。</li></ul></li><li><p><strong>高训练计算 vs. 低推理计算</strong></p><ul><li>在 <strong>训练计算充足</strong> 的情况下（如右侧的红色圈），模型可以<strong>减少推理计算需求</strong>，即 <strong>即使使用较少的搜索深度，仍然能获得较高的性能</strong>。</li></ul></li><li><p><strong>公式解释</strong></p><ul><li>公式：<br>$$<br>\log_{10}(\text{test compute}) = -1.2 \cdot \log_{10}(\text{train compute}) + 0.004 \cdot \text{elo} + 29<br>$$</li><li>这说明：<ul><li><strong>训练计算（train compute）增加时，推理计算（test compute）减少（系数 -1.2）</strong>。</li><li><strong>更高的 ELO（更强的 AI）需要额外的计算（系数 0.004）</strong>。</li></ul></li></ul></li></ol><p>随着推理时计算扩展类似于训练时计算，研究范式正朝着“推理”模型利用更多推理时计算的方向发展。通过这种范式转变，这些“推理”模型不再单纯关注训练时计算（预训练和微调），而是平衡训练与推理。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_152824.png" class=""><p>推理时计算甚至可以随长度扩展：</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_153922.png" class=""><p>这是我们在 DeepSeek-R1 研究中也将探讨的内容！</p><h3 id="📌-推理时计算的类别（Categories-of-Test-time-Compute）"><a href="#📌-推理时计算的类别（Categories-of-Test-time-Compute）" class="headerlink" title="📌 推理时计算的类别（Categories of Test-time Compute）"></a>📌 推理时计算的类别（Categories of Test-time Compute）</h3><p>推理模型（如 <strong>DeepSeek-R1</strong> 和 <strong>OpenAI o1</strong>）的成功表明，在推理过程中，除了简单地“思考更长时间”之外，还有更多的优化技术。</p><p>在本文中，我们将探讨 <strong>推理时计算（Test-time Compute）</strong> 的多种实现方式，包括：</p><ul><li><strong>链式思维（Chain-of-Thought）</strong></li><li><strong>答案修订（Revising Answers）</strong></li><li><strong>回溯推理（Backtracking）</strong></li><li><strong>多样性采样（Sampling）</strong></li><li><strong>其他方法</strong></li></ul><p>总体而言，推理时计算可归纳为以下 <strong>两大类别</strong>：</p><ol><li><p><strong>基于验证器的搜索（Search against Verifiers）</strong>  </p><ul><li>通过 <strong>采样多个答案</strong> 并 <strong>选择最佳答案</strong> 来优化推理。</li></ul></li><li><p><strong>修改提议分布（Modifying Proposal Distribution）</strong>  </p><ul><li>通过训练 <strong>“思考”过程</strong> 来提高推理能力。Proposal Distribution（提议分布，指在模型生成答案时，对不同可能答案的概率分布进行调整）</li></ul></li></ol><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_154456.png" class=""><p>从本质上讲：</p><ul><li><strong>基于验证器的搜索</strong> 更关注 <strong>输出质量</strong>（Output-focused）。</li><li><strong>修改提议分布</strong> 关注 <strong>输入结构</strong>（Input-focused）。</li></ul><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_154547.png" class=""><h3 id="🔍-两种主要验证器类型"><a href="#🔍-两种主要验证器类型" class="headerlink" title="🔍 两种主要验证器类型"></a>🔍 两种主要验证器类型</h3><p>为了更好地筛选和评估推理答案，我们引入了两种 <strong>验证器（Verifiers）</strong>：</p><ol><li><p><strong>结果奖励模型（Outcome Reward Models, ORM）</strong>  </p><ul><li>仅对最终答案进行评分，而不考虑推理过程。</li></ul></li><li><p><strong>过程奖励模型（Process Reward Models, PRM）</strong>  </p><ul><li>既评估最终答案，也对推理过程进行评分。</li></ul></li></ol><p>在接下来的部分，我们将详细探讨 <strong>如何将 ORM 和 PRM 应用于不同的验证方法</strong>！</p><p>顾名思义，<strong>结果奖励模型（Outcome Reward Model, ORM）</strong> 仅评估最终的答案质量，而不关注答案背后的推理过程：</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_160242.png" class=""><ul><li>ORM 只看最终输出，而不关心模型是如何得出这个答案的。</li></ul><p>相比之下，<strong>过程奖励模型（Process Reward Model, PRM）</strong> 则会评估推理过程本身：</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_160258.png" class=""><ul><li>PRM 既评估答案的正确性，也关注推理路径的合理性。</li></ul><h3 id="🧐-PRM-如何评估推理过程？"><a href="#🧐-PRM-如何评估推理过程？" class="headerlink" title="🧐 PRM 如何评估推理过程？"></a>🧐 PRM 如何评估推理过程？</h3><p>为了更清楚地说明推理步骤的重要性，让我们来看一个示例：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">问题：某个方程的解是多少？</span><br><span class="line"></span><br><span class="line">推理步骤 1：首先展开方程，得到 x = 3。</span><br><span class="line">推理步骤 2：错误地将 x = 3 改写为 x = 5。</span><br><span class="line">推理步骤 3：最终输出 x = 5。</span><br></pre></td></tr></tbody></table></figure><p>在上述示例中，虽然最终答案（x = 5）是错误的，但 ORM 仅评估最终输出，不会关注中间的错误推理。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_160332.png" class=""><p>或者在这个例子中，PRM 会发现 <strong>推理步骤 2 是错误的</strong>，并对此步骤给予低分，从而避免错误答案的出现。</p><hr><h3 id="🔍-ORM-vs-PRM-在推理中的应用"><a href="#🔍-ORM-vs-PRM-在推理中的应用" class="headerlink" title="🔍 ORM vs. PRM 在推理中的应用"></a>🔍 ORM vs. PRM 在推理中的应用</h3><p>现在你已经掌握了 <strong>结果奖励模型（ORM）</strong> 和 <strong>过程奖励模型（PRM）</strong> 之间的区别，我们接下来探讨如何将它们应用于各种 <strong>验证技术（Verification Techniques）</strong>。</p><h2 id="📌-基于验证器的搜索（Search-against-Verifiers）"><a href="#📌-基于验证器的搜索（Search-against-Verifiers）" class="headerlink" title="📌 基于验证器的搜索（Search against Verifiers）"></a>📌 基于验证器的搜索（Search against Verifiers）</h2><p>推理时计算的第一大类别是 <strong>基于验证器的搜索</strong>，它通常包含两个步骤：</p><ol><li><strong>生成多个推理过程和答案样本</strong></li><li><strong>使用验证器（奖励模型）对生成的输出进行评分</strong><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_160918.png" class=""></li></ol><h3 id="🤖-验证器的作用"><a href="#🤖-验证器的作用" class="headerlink" title="🤖 验证器的作用"></a>🤖 验证器的作用</h3><p>验证器通常是一个大型语言模型（LLM），经过微调以评估结果（ORM）或过程（PRM）。 使用验证器的一个主要优势是，无需重新训练或微调用于回答问题的大型语言模型（LLM），仅通过评分机制选择最佳答案。</p><hr><h3 id="✅-多数投票法（Majority-Voting）"><a href="#✅-多数投票法（Majority-Voting）" class="headerlink" title="✅ 多数投票法（Majority Voting）"></a>✅ 多数投票法（Majority Voting）</h3><p>最简单的方法是 <strong>不使用奖励模型或验证器</strong>，而是执行 <strong>多数投票（Majority Voting）</strong>。</p><p>📌 <strong>方法：</strong> 让 LLM 生成多个答案，选择出现次数最多的答案作为最终答案。</p><p>📌 <strong>示例：</strong></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Q: 15 × 3 = ?</span><br><span class="line">A1: 45</span><br><span class="line">A2: 42</span><br><span class="line">A3: 45</span><br><span class="line">最终答案: 45（因其出现频率最高）</span><br></pre></td></tr></tbody></table></figure><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_161249.png" class=""><p>这种方法也称为 <strong>自一致性（Self-Consistency）</strong>，强调 <strong>生成多个答案和推理步骤</strong> 的重要性。</p><hr><h3 id="🔢-Best-of-N-采样法（Best-of-N-Samples）"><a href="#🔢-Best-of-N-采样法（Best-of-N-Samples）" class="headerlink" title="🔢 Best-of-N 采样法（Best-of-N Samples）"></a>🔢 Best-of-N 采样法（Best-of-N Samples）</h3><p>Best-of-N 采样是第一个涉及验证器（Verifier）的方法，它的基本思想是生成 N 个样本答案，然后使用 奖励模型（Reward Model, RM） 对这些答案进行评分，并选择得分最高的答案。</p><p>📌 <strong>步骤：</strong></p><ol><li><strong>生成多个答案</strong>（使用较高或者不同的温度参数生成 N 个样本）。<img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_161825.png" class=""></li><li><strong>结果奖励模型（ORM, Outcome Reward Model）</strong>，每个答案都会通过 ORM 进行评分。选取得分最高的答案作为最终输出。<img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_161833.png" class="">📌 <strong>示例：</strong></li></ol><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Q: 8 + 5 = ?</span><br><span class="line">A1: 12 (得分 0.2)</span><br><span class="line">A2: 13 (得分 0.9)</span><br><span class="line">A3: 14 (得分 0.4)</span><br><span class="line">最终选择: A2（因其得分最高）</span><br></pre></td></tr></tbody></table></figure><h2 id="📌-进一步优化：-若使用-PRM，则不只评估答案，还评估整个推理过程。与仅评估最终答案的结果奖励模型（ORM）不同，过程奖励模型（PRM）会评估推理过程的质量。PRM-关注推理的每个步骤，确保推理过程合理、连贯，并最终选择总评分最高的候选答案。-加权-Best-of-N-采样（Weighted-Best-of-N-samples）-结合-ORM-和-PRM-两种验证方式，我们可以对所有候选答案进行加权评分，并选择总权重最高的答案。这种方法称为-加权-Best-of-N-采样（Weighted-Best-of-N-samples）：。"><a href="#📌-进一步优化：-若使用-PRM，则不只评估答案，还评估整个推理过程。与仅评估最终答案的结果奖励模型（ORM）不同，过程奖励模型（PRM）会评估推理过程的质量。PRM-关注推理的每个步骤，确保推理过程合理、连贯，并最终选择总评分最高的候选答案。-加权-Best-of-N-采样（Weighted-Best-of-N-samples）-结合-ORM-和-PRM-两种验证方式，我们可以对所有候选答案进行加权评分，并选择总权重最高的答案。这种方法称为-加权-Best-of-N-采样（Weighted-Best-of-N-samples）：。" class="headerlink" title="📌 进一步优化：- 若使用 PRM，则不只评估答案，还评估整个推理过程。与仅评估最终答案的结果奖励模型（ORM）不同，过程奖励模型（PRM）会评估推理过程的质量。PRM 关注推理的每个步骤，确保推理过程合理、连贯，并最终选择总评分最高的候选答案。- 加权 Best-of-N 采样（Weighted Best-of-N samples）:结合 ORM 和 PRM 两种验证方式，我们可以对所有候选答案进行加权评分，并选择总权重最高的答案。这种方法称为 加权 Best-of-N 采样（Weighted Best-of-N samples）：。"></a>📌 <strong>进一步优化：</strong><br>- 若使用 <strong>PRM</strong>，则不只评估答案，还评估整个推理过程。与仅评估最终答案的结果奖励模型（ORM）不同，过程奖励模型（PRM）会评估推理过程的质量。PRM 关注推理的每个步骤，确保推理过程合理、连贯，并最终选择总评分最高的候选答案。<br><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_161922.png" class=""><br>- <strong>加权 Best-of-N 采样（Weighted Best-of-N samples）</strong>:结合 ORM 和 PRM 两种验证方式，我们可以对所有候选答案进行加权评分，并选择总权重最高的答案。这种方法称为 加权 Best-of-N 采样（Weighted Best-of-N samples）：。<br><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_162139.png" class=""></h2><h3 id="🚀-使用过程奖励模型（PRM）的束搜索（Beam-Search）"><a href="#🚀-使用过程奖励模型（PRM）的束搜索（Beam-Search）" class="headerlink" title="🚀 使用过程奖励模型（PRM）的束搜索（Beam Search）"></a>🚀 使用过程奖励模型（PRM）的束搜索（Beam Search）</h3><p>在生成答案及其中间推理步骤的过程中，我们可以使用 <strong>束搜索（Beam Search）</strong> 进一步优化推理路径。</p><p>📌 <strong>束搜索的核心思想：</strong></p><ul><li>在推理过程中，生成多个可能的推理路径（称为“束”）。</li><li>使用 <strong>过程奖励模型（PRM, Process Reward Model）</strong> 对每条路径进行评分。</li><li>类似于 <strong>Tree of Thought</strong> 方法，始终保留得分最高的 <strong>前 3 条推理路径</strong>，并在推理过程中持续跟踪这些路径。</li><li>如果某条路径的得分较低（PRM 评分低），则提前停止该推理路径，以避免不必要的计算开销。</li></ul><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_162508.png" class=""><p>📌 <strong>优化后的答案筛选方式：</strong><br>最终，生成的所有答案将使用 <strong>Best-of-N 采样</strong> 方法进行加权评分，确保选出最佳推理路径的最终答案。</p><p>🚀 <strong>优势：</strong></p><ul><li>避免计算资源浪费，快速淘汰低质量推理路径。</li><li>结合 PRM，可以确保模型的推理过程更连贯、更符合逻辑。</li><li>通过 Best-of-N 方法进一步优化答案质量，使最终答案更加可靠。</li></ul><hr><h3 id="🎲-蒙特卡洛树搜索（Monte-Carlo-Tree-Search-MCTS）"><a href="#🎲-蒙特卡洛树搜索（Monte-Carlo-Tree-Search-MCTS）" class="headerlink" title="🎲 蒙特卡洛树搜索（Monte Carlo Tree Search, MCTS）"></a>🎲 蒙特卡洛树搜索（Monte Carlo Tree Search, MCTS）</h3><p>蒙特卡洛树搜索（Monte Carlo Tree Search, <strong>MCTS</strong>）是一种常用于决策树搜索的算法，在 LLM 的推理优化中也可以采用该方法。MCTS 通过四个步骤来优化推理路径：<br>📌 <strong>主要步骤：</strong></p><ol><li><strong>选择（Selection）：</strong> 根据预定义的公式，从当前搜索树中选择一个叶节点 进行扩展。</li><li><strong>扩展（Expand）：</strong> 在所选叶节点的基础上 创建新的子节点，以探索更多可能的推理路径。</li><li><strong>模拟（Rollouts）：</strong> 通过随机生成新的推理路径，持续扩展节点，直到达到终点（即得到最终答案）。</li><li><strong>回溯（Backpropagation）：</strong> 根据最终输出结果 更新父节点的评分，从而优化未来的搜索决策。</li></ol><p>在大语言模型（LLM）的推理过程中，我们通常希望找到最佳的推理路径，使其最终生成的答案最优。但在这个过程中，需要在 <strong>探索（Exploration）</strong> 和 <strong>利用（Exploitation）</strong> 之间取得平衡：</p><ul><li><strong>利用（Exploitation）</strong>：选择当前看起来最优的路径，以利用已知的高质量推理步骤。</li><li><strong>探索（Exploration）</strong>：选择访问次数较少的路径，以发现可能更优的推理步骤。</li></ul><h4 id="选择分数（Selection-Score）"><a href="#选择分数（Selection-Score）" class="headerlink" title="选择分数（Selection Score）"></a>选择分数（Selection Score）</h4><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_163149.png" class=""><p>在选择推理路径时，我们使用 <strong>选择分数（Selection Score）</strong> 计算每个推理步骤（即树的节点）的优先级，公式如下：</p><p>$$<br>\text{Selection Score} = \frac{\text{Total Node Reward}}{\text{Number of Node Visits}} + C \times \sqrt{\frac{\text{Number of Parent Node Visits}}{\text{Number of Node Visits}}}<br>$$</p><p>其中：</p><ul><li><p><strong>第一项</strong>：$$\frac{\text{Total Node Reward}}{\text{Number of Node Visits}}$$（利用项，Exploitation Term）</p><ul><li><strong>Total Node Reward</strong>：该节点累计获得的奖励值（表示其历史表现）。</li><li><strong>Number of Node Visits</strong>：该节点被访问的次数。</li><li>这项计算的是该节点的 <strong>平均奖励值</strong>，高奖励的节点会被优先选择。</li></ul></li><li><p><strong>第二项</strong>：$$C \times \sqrt{\frac{\text{Number of Parent Node Visits}}{\text{Number of Node Visits}}}$$（探索项，Exploration Term）</p><ul><li><strong># of Parent Node Visits</strong>：父节点被访问的次数。</li><li><strong># of Node Visits</strong>：当前节点被访问的次数。</li><li><strong>C</strong>：一个超参数，控制探索与利用的平衡。</li><li>这项鼓励探索访问次数较少的节点，以防止过早陷入局部最优解。</li></ul></li></ul><p>总结：</p><ul><li><strong>第一项（Exploitation Term）</strong> 让算法倾向于选择 <strong>历史表现较好的路径</strong>。</li><li><strong>第二项（Exploration Term）</strong> 让算法倾向于 <strong>探索访问较少的路径</strong>，避免陷入局部最优。</li><li><strong>参数 C</strong> 控制这两者的平衡。</li></ul><h4 id="2-选择（Selection）与扩展（Expand）"><a href="#2-选择（Selection）与扩展（Expand）" class="headerlink" title="2. 选择（Selection）与扩展（Expand）"></a><strong>2. 选择（Selection）与扩展（Expand）</strong></h4><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_164346.png" class=""><p>这一阶段，我们使用 <strong>选择分数</strong> 来决定哪条推理路径值得继续扩展：</p><p><strong>（1）选择（Selection）</strong></p><ul><li><strong>输入：问题（Question）</strong></li><li><strong>LLM 生成多个推理步骤（Reasoning Steps）</strong><ul><li>例如，在图片中，LLM 生成了 3 个推理步骤：<ul><li><strong>Thought 1</strong>（评分 0.4）</li><li><strong>Thought 2</strong>（评分 0.2）</li><li><strong>Thought 3</strong>（评分 0.1）</li></ul></li></ul></li><li><strong>使用选择分数（Selection Score）选择最优路径</strong>（随机初始化）<ul><li>在示例中，评分最高的 <strong>Thought 1（0.4）</strong> 被选中。</li></ul></li></ul><p><strong>（2）扩展（Expand）</strong></p><ul><li><strong>在选中的推理路径上，生成新的推理步骤</strong></li><li>这些新推理步骤的初始值设为 0，表示它们还没有经过评估。</li></ul><p>这个过程类似于 <strong>MCTS 的拓展（Expansion）阶段</strong>，即：</p><ol><li>选择当前最优路径（使用 <strong>选择分数</strong>）。</li><li>在该路径下，扩展新的推理步骤（未评分的子节点）。</li></ol><h4 id="3-Rollouts（模拟）与-Backpropagation（反向传播）"><a href="#3-Rollouts（模拟）与-Backpropagation（反向传播）" class="headerlink" title="3. Rollouts（模拟）与 Backpropagation（反向传播）"></a><strong>3. Rollouts（模拟）与 Backpropagation（反向传播）</strong></h4><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_165052.png" class=""><p>一旦扩展了推理步骤，我们需要继续探索，并利用 <strong>模拟（Rollouts）</strong> 和 <strong>反向传播（Backpropagation）</strong> 来优化整个搜索过程。</p><h3 id="（3）Rollouts（模拟）"><a href="#（3）Rollouts（模拟）" class="headerlink" title="（3）Rollouts（模拟）"></a><strong>（3）Rollouts（模拟）</strong></h3><ul><li>选定路径后，我们继续展开推理步骤，直到 <strong>生成最终答案</strong>。</li><li>这个过程类似于 <strong>在 MCTS 中随机模拟游戏到结束</strong>：<ul><li>我们从当前节点出发，进行一系列推理，直到模型生成最终的答案。</li><li>在图片中，我们沿着 Thought 1（0.4） 继续展开推理步骤。</li><li>这些推理步骤最终会 <strong>生成多个答案</strong>（图片中紫色框）。</li></ul></li></ul><h3 id="（4）Backpropagation（反向传播）"><a href="#（4）Backpropagation（反向传播）" class="headerlink" title="（4）Backpropagation（反向传播）"></a><strong>（4）Backpropagation（反向传播）</strong></h3><ul><li>通过对 <strong>最终答案</strong> 进行评分，我们可以更新前面所有参与推理的节点分数：<ul><li><strong>PRM（Process Reward Model）</strong>：对推理步骤本身进行评分，衡量其合理性。</li><li><strong>ORM（Output Reward Model）</strong>：对最终答案进行评分，衡量其正确性。</li><li>这些评分 <strong>向上传播</strong>，更新 <strong>所有经过的节点</strong> 的奖励值。</li></ul></li><li>例如：<ul><li>在图片中，最终答案的评分导致 <strong>Thought 1</strong> 的评分从 0.4 提高到 <strong>0.8</strong>。</li><li>进一步向上传播，使得 <strong>父节点的选择分数也随之更新</strong>。</li></ul></li></ul><p>这个过程保证了：</p><ul><li><strong>较好的推理路径会逐渐获得更高的分数</strong>，提高被选中的概率。</li><li><strong>较差的推理路径会被逐渐淘汰</strong>，避免浪费计算资源。</li></ul><hr><h2 id="📌-修改提议分布（Modifying-Proposal-Distribution）"><a href="#📌-修改提议分布（Modifying-Proposal-Distribution）" class="headerlink" title="📌 修改提议分布（Modifying Proposal Distribution）"></a>📌 修改提议分布（Modifying Proposal Distribution）</h2><p><strong>修改提议分布（Modifying Proposal Distribution）</strong></p><p>在大语言模型（LLM）的推理过程中，我们可以通过修改提议分布（Modifying Proposal Distribution）来优化模型的推理能力。这种方法的核心思想是：</p><ul><li><strong>不再单纯依赖模型搜索正确推理步骤</strong>（基于输出的优化），</li><li><strong>而是让模型主动生成更优的推理步骤</strong>（基于输入的优化）。</li></ul><p>换句话说，我们不是在输出结果后进行检验，而是直接修改模型在推理过程中如何选择 token，让它更倾向于选择能够引导推理的 token，而不是立即输出最终答案。修改了用于采样补全（completions）、思维（thoughts）或标记（tokens）的概率分布。这种方法可以让模型生成的答案更加准确、可解释，并且在面对复杂问题时更具有鲁棒性（robustness）。</p><p><strong>1. 直接选择最高概率 Token（Greedy 选择）</strong></p><p>在默认情况下，LLM 生成多个可能的 token 作为输出候选项，并根据其概率进行排序，最终选择最高概率的 token 进行输出。这种方法称为<strong>贪心选择（Greedy Selection）</strong>。</p><p>你可以想象，我们有一个问题（question）和一个用于采样 token 的概率分布（distribution）。常见的策略是选择得分最高的 token。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_170357.png" class=""><ul><li>例如，给定问题 <code>What is 3 + 2?</code>，LLM 可能会生成如下候选 token：<ul><li><code>5</code>（最高概率）</li><li><code>3</code></li><li><code>Adding</code></li><li><code>4</code></li><li><code>If</code></li></ul></li><li>在贪心策略下，模型会直接选择 <code>5</code> 作为最终答案，而不会进行推理。</li></ul><p>这种方法虽然快速，但存在如下问题：</p><ul><li><strong>缺乏推理能力</strong>：模型可能直接输出错误答案，因为它没有进行推理。</li><li><strong>可解释性差</strong>：对于复杂问题，用户无法理解模型是如何得出答案的。</li></ul><p><strong>2. 通过推理（Reasoning Before Answering）提高答案质量</strong></p><p>然而，请注意上图中有一些<strong>标记（tokens</strong>被标红。这些token更有可能引导模型进入一个合理的推理过程。虽然选择贪心（greedy）策略下得分最高的 token 不一定是错误的，但选择那些能引导模型进入推理过程的 token，通常会得到更好的答案。<br>让 LLM <strong>先进行推理，再给出答案</strong>，即：</p><ul><li>选择推理 token（如 <code>Adding</code>）</li><li>逐步生成推理过程，如：<ul><li><code>Adding → 3 and 2 gives → 5</code></li><li><code>If → 3 + 1 = 4, 4 + 1 = 5 → 5</code></li><li><code>The total is → 5</code></li></ul></li><li>通过推理链条逐步推导出 <code>5</code>，相比直接选择 <code>5</code>，这种方法更加可解释，并且能在复杂问题上表现更好。</li></ul><p><strong>3. 通过修改提议分布（Re-Ranking Token Probabilities）引导推理过程</strong></p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_171002.png" class=""><p>当我们<strong>修改提议分布（proposal distribution，即 token 的概率分布）</strong>时，实际上是在<strong>重新排序（re-rank）</strong>这个分布，使得“推理相关”的 token 被选中的概率更高。<br>在这种方法下，我们调整 LLM 的提议分布，使其更倾向于选择推理 token，而非直接选择答案：</p><ul><li>默认情况下，<code>5</code> 具有最高概率，而 <code>Adding</code>、<code>If</code> 等推理 token 的概率较低。</li><li>通过修改提议分布，我们提高 <code>Adding</code>、<code>If</code> 的概率，使模型倾向于进行推理。</li></ul><p><strong>4. 如何实现修改提议分布？</strong></p><p>主要有两种方式：</p><ol><li><strong>通过 Prompt Engineering</strong><ul><li>修改 Prompt，引导模型生成推理步骤。</li><li>例如：<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Q: What is 3 + 2?</span><br><span class="line">A: Let's think step by step.</span><br></pre></td></tr></tbody></table></figure></li></ul></li><li><strong>训练模型更倾向于推理</strong><ul><li>在微调过程中，提供更多具有推理链的训练数据，让模型习惯生成推理 token。</li></ul></li></ol><p><strong>总结</strong></p><ul><li><strong>贪心选择（Greedy Selection）</strong>：快速，但缺乏推理，可解释性差。</li><li><strong>推理后回答（Reasoning Before Answering）</strong>：提高答案质量和可解释性。</li><li><strong>修改提议分布（Modifying Proposal Distribution）</strong>：调整 token 选择的概率，使模型更倾向于选择推理 token，提高整体答案的合理性。</li></ul><p>这种方法在<strong>数学计算、逻辑推理、法律推理等任务</strong>上尤为重要，使得 LLM <strong>不仅能“答对”，还能“说明白”</strong>。</p><h3 id="Prompting"><a href="#Prompting" class="headerlink" title="Prompting"></a><strong>Prompting</strong></h3><p>随着我们使用 <strong>prompt engineering</strong>（提示工程）来改进输出，我们会通过更新提示（prompt）来尝试提升模型的表现。这个过程也可能推动模型去展示先前我们看到的一些<strong>reasoning</strong>（推理）过程。</p><p><strong>1. 改变 Proposal Distribution</strong></p><p>在更改 <strong>proposal distribution</strong>时，我们可以给模型提供示例（也叫做 <strong>in-context learning</strong>），让它在生成答案时模仿类似的推理风格。下面的图就展示了一个示例的情形：</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_172130.png" class=""><blockquote><ul><li><strong>图示内容</strong>：左侧是一个简单的问题 “What is 3 + 2?”，模型内部用 “Thoughts” 表示隐藏的思考过程，比如：<ol><li>First, 3 and 1 gives 4.</li><li>Then, 4 and 1 gives 5.</li><li>I believe the answer is 5.</li></ol></li><li><strong>Answer</strong>（答案）：5</li><li>右侧用红色、蓝色等不同颜色的条形或方块表示推理过程的不同部分，示意有一部分属于隐藏的推理过程（红色），以及输出结果或若干中间步骤（蓝色）。</li></ul></blockquote><p>通过类似的示例，模型在推理时就可能模仿类似的格式来进行<strong>reasoning</strong>并给出最终答案。</p><p><strong>2. “Let’s think step-by-step” 的影响</strong></p><p>我们也可以通过在提示中直接使用 “Let’s think step-by-step” 来简化上述流程。这会改变模型的 <strong>proposal distribution</strong>，让 <strong>LLM</strong>（大型语言模型）倾向于在回答之前分步骤思考。如下图所示：</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_172232.png" class=""><blockquote><ul><li><strong>图示内容</strong>：这里将提示换成 “Let’s think step-by-step”，问题仍然是 “What is 3 + 2?”。</li><li>模型产生更显式的推理过程（用红色块示意），再输出正确答案 5。</li><li>整个思路类似图1，但更加突出“分步骤思考”对最终答案生成的影响。</li></ul></blockquote><p>然而，这并不意味着模型本身已经内化了这种推理能力——它<strong>并没有从根本上学会</strong>去“反思”或“修正”错误。如果模型一开始的推理过程是错误的，那么在这种静态且线性的流程中，它往往会一直延续这个错误，而不是对自身推理进行修正。</p><hr><h3 id="STaR（Self-Taught-Reasoner）"><a href="#STaR（Self-Taught-Reasoner）" class="headerlink" title="STaR（Self-Taught Reasoner）"></a><strong>STaR（Self-Taught Reasoner）</strong></h3><p>除了通过 <strong>prompting</strong>（提示）让模型临时展示推理步骤，我们还可以让模型在训练中因为“产生正确推理步骤”而得到奖励，从而让它真正“学会”推理。这通常需要在<strong>大量带有推理过程的数据</strong>上进行训练，并结合 <strong>reinforcement learning</strong>（强化学习）来奖励特定的行为。</p><p>一个颇受争议（“much-debated”）的技术就是 <strong>STaR</strong>，即 <strong>Self-Taught Reasoner</strong>。它是让 <strong>LLM</strong> 生成自己的推理数据，再把这些数据用于对模型进行<strong>精调</strong>（<em>fine-tuning</em>）的过程。</p><p><strong>1. STaR 的流程概述</strong></p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_172455.png" class=""><ul><li>这幅图概括了 STaR 的工作原理：<ol><li><strong>Generate reasoning + answer</strong>：模型先针对输入问题生成一段 <strong>reasoning</strong>（推理）和一个 <strong>answer</strong>（答案）；<br>2a. 如果答案正确（Correct answer），则将 <strong>Question, Reasoning, Answer</strong> 作为训练样本添加到三元组数据集中（3a）；<br>  3b. 利用这些三元组数据进行 <strong>supervised fine-tuning</strong>（监督微调），让模型学会在类似情形下产出正确推理与答案。</li></ol></li></ul><p>如果模型给出了错误答案，则会触发另一条路径：</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_172608.png" class=""><ul><li>当 (2b) 模型答案错误时，我们提供正确答案作为 <strong>hint</strong>（提示），并让模型去思考“为什么这个答案是正确的”；</li><li>也就是 <strong>Generate reasoning only</strong> (why this answer is correct?)；</li><li>得到的这段新的推理依旧会被加入到三元组数据中，然后再进行 <strong>supervised fine-tuning</strong>。</li></ul><p>这里的关键要点是，我们可以通过这种方法<strong>显式</strong>地训练模型“应该如何进行推理”，而不仅仅是让它临时地模仿推理过程。我们要对模型的推理方式进行<strong>监督</strong>（<em>supervised fine-tuning</em>），从而把我们想要的推理模式“灌输”给模型。</p><p><strong>2. 自动生成合成训练样本</strong></p><p>STaR 的整个流程非常有趣，因为它会<strong>自动生成合成训练样本</strong>（<em>synthetic training examples</em>）。这些样本不仅包含问题和答案，还包含一系列推理步骤，能够帮助模型更好地学习如何“思考”。在其他研究中（例如 <strong>DeepSeek R-1</strong>），我们可以利用这些合成样本来<strong>蒸馏</strong>（<em>distill</em>，意为“提炼和保留关键信息”）推理过程到其它模型上。也就是说，一个掌握了推理能力的模型可以帮助另一个模型更快地学会类似的推理。</p><hr><p><strong>重点：</strong></p><ul><li><strong>Prompting</strong>（提示）能够影响模型的输出风格和思维过程，比如使用 “Let’s think step-by-step” 让模型显式给出推理步骤，但并不保证模型自动纠正错误。</li><li><strong>STaR</strong>（<strong>Self-Taught Reasoner</strong>）等方法则通过<strong>生成推理数据、监督微调和奖励机制</strong>，帮助模型真正学会按照指定的推理方式去思考和回答问题。</li><li>无论是哪一种方法，都可以视为对 <strong>proposal distribution</strong> 的调节：要么是提示时临时<strong>nudge</strong>（引导），要么是从训练根源上进行调教，让模型内化这种推理过程。</li><li>利用 <strong>in-context learning</strong> 提供示例，能够让模型模仿推理风格。</li><li>用 <strong>reinforcement learning</strong> 或<strong>监督微调</strong>（<strong>supervised fine-tuning</strong>）可以使模型逐渐掌握我们期望的推理模式。</li><li><strong>STaR</strong> 方法会自动收集“正确推理”数据并进行训练，使得模型在后续回答中更可能产生正确且符合要求的推理步骤。</li></ul><hr><h2 id="DeepSeek-R1"><a href="#DeepSeek-R1" class="headerlink" title="DeepSeek-R1"></a>DeepSeek-R1</h2><hr><h3 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h3><p><strong>DeepSeek-R1</strong> 是一个在推理（reasoning）模型领域的重大版本，其权重已经开源。它直接与 OpenAI 的 <strong>o1</strong> 推理模型展开竞争，并在这一领域产生了重大影响。</p><p>DeepSeek 项目在将推理功能优雅地整合进其基础模型（<strong>DeepSeek-V3-Base</strong>）方面成就卓著，采用了多种技术来完成这一目标。</p><p>有趣的是，该项目在训练过程中并未依赖额外的验证器（verifier），而且并不是单纯地依靠监督微调（supervised fine-tuning）来提炼推理行为。相反，<strong>强化学习（Reinforcement Learning, RL）</strong> 在其中扮演了重要角色。</p><p>以下我们将一起探究他们是如何在模型中训练出推理行为的！</p><hr><h3 id="2-DeepSeek-R1-Zero：推理的关键探索"><a href="#2-DeepSeek-R1-Zero：推理的关键探索" class="headerlink" title="2. DeepSeek-R1 Zero：推理的关键探索"></a>2. DeepSeek-R1 Zero：推理的关键探索</h3><p>在通往 <strong>DeepSeek-R1</strong> 的道路上，有一个名为 <strong>DeepSeek-R1 Zero</strong> 的实验性模型为这次突破打下了基础。它从 <strong>DeepSeek-V3-Base</strong> 出发，完全不使用大规模监督微调来加入推理数据，而是只依靠 <strong>强化学习</strong> 来获得推理能力。</p><h4 id="训练过程与系统提示（Prompt）"><a href="#训练过程与系统提示（Prompt）" class="headerlink" title="训练过程与系统提示（Prompt）"></a>训练过程与系统提示（Prompt）</h4><p>在此过程中，他们首先准备了一个非常直接的提示（prompt），其形式类似于系统提示（system prompt），用来作为推理管线的一部分。下文即展示了相关提示。请注意，其中明确指出了推理过程要写在 <code>&lt;think&gt;</code> 标签内、答案要写在 <code>&lt;answer&gt;</code> 标签内，但没有进一步规定推理过程应如何具体呈现或组织。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_183150.png" class=""><p>在上图中，可以看到一个简化版的对话示例（System prompt 与 User prompt）以及模型如何将<strong>推理</strong>（reasoning）放在 <code>&lt;think&gt;</code> 标签内、将<strong>答案</strong>（answer）放在 <code>&lt;answer&gt;</code> 标签内。该图突出展示了在提示（prompt）中对模型的约束：</p><ul><li><em>“The assistant first thinks about the reasoning process in the mind and then provides the user with the answer.”</em></li><li>要求使用 <code>&lt;think&gt;</code> 进行推理，使用 <code>&lt;answer&gt;</code> 进行回答。</li></ul><p>这里并未提供关于“推理过程”格式的其他例子或模板——完全由模型自己在训练中摸索出要如何输出“Chain-of-Thought”式的推理文字。</p><h4 id="强化学习奖励"><a href="#强化学习奖励" class="headerlink" title="强化学习奖励"></a>强化学习奖励</h4><p>在训练中，采用了两个基于规则（rule-based）的奖励机制：</p><ol><li><strong>准确性奖励（Accuracy rewards）</strong><br>通过测试给出的答案是否正确来进行奖励。若模型输出的答案正确，就会增加奖励。</li><li><strong>格式奖励（Format rewards）</strong><br>奖励模型对 <code>&lt;think&gt;</code> 和 <code>&lt;answer&gt;</code> 标签的正确使用。</li></ol><p>他们所使用的强化学习算法名为 <strong>Group Relative Policy Optimization（GRPO）</strong>。此算法的直观想法在于：使所有导致正确或错误答案的决策更易或更难再次出现。这些决策可能包括模型生成的某些标记（token）序列，也可能包括推理步骤本身（即思考过程）。下文给出了这一训练阶段的示意图。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_183402.png" class=""><p>在图中，重点展示了在 RL（强化学习）过程中所使用的两类奖励：</p><ul><li>“is <code>&lt;think&gt;</code> used?” —— 为使用 <code>&lt;think&gt;</code> 标签而打分。</li><li>“is <code>&lt;answer&gt;</code> used?” —— 为使用 <code>&lt;answer&gt;</code> 标签而打分。</li></ul><p>除此之外，还有对答案<strong>正确性</strong>的奖励（accuracy reward）。图中箭头所示的循环代表了在训练中不断迭代更新模型，使之越来越倾向于正确的推理方式并合乎格式要求。</p><h4 id="自发推理行为的出现"><a href="#自发推理行为的出现" class="headerlink" title="自发推理行为的出现"></a>自发推理行为的出现</h4><p>值得一提的是，研究人员并没有向模型提供任何示例来告诉它 <code>&lt;think&gt;</code> 标签中的内容应该如何书写或展开。他们仅仅告诉模型：</p><blockquote><p>“It should use <code>&lt;think&gt;</code> tags, and nothing more!”</p></blockquote><p>通过对“Chain-of-Thought”相关行为进行<strong>间接奖励</strong>（即只要推理正确、使用正确格式，就鼓励输出更完整的推理内容），模型在训练中自发地学会了越写越长的推理过程，也更易产生正确答案。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_183551.png" class=""><p>上图呈现了模型在训练过程中输出的推理长度随训练步数增加而逐渐变长的趋势。纵轴是每个响应的平均长度，横轴是训练步数。可以看到，曲线整体是向上攀升的，这表明模型不断倾向于输出更长、更详细的思考内容（Chain-of-Thought），并因此获得更高奖励。这种做法将大部分计算消耗从训练阶段（train-time compute）转移到了推理阶段（test-time compute），也就是在推理时才生成更长的思考过程。</p><p>根据研究，他们发现通过这种训练策略，模型能够自行发现最优的 Chain-of-Thought 风格的思考方式，并展现出高级的推理能力，例如：<strong>自我反思（self-reflection）</strong> 和 <strong>自我验证（self-verification）</strong>。</p><p>不过，DeepSeek-R1 Zero 的模型输出仍存在一些问题，比如可读性欠佳，且有时会混用多种语言。为了在产品化或发布级别进一步完善，研究人员提出了另一个选项，也就是在正式版本中使用的 <strong>DeepSeek R1</strong>。</p><hr><h3 id="3-深入了解-DeepSeek-R1"><a href="#3-深入了解-DeepSeek-R1" class="headerlink" title="3. 深入了解 DeepSeek-R1"></a>3. 深入了解 DeepSeek-R1</h3><p>要构建 <strong>DeepSeek-R1</strong>，作者共进行了以下五个关键步骤：</p><ol><li><strong>冷启动（Cold Start）</strong></li><li><strong>以推理为导向的强化学习（Reasoning-oriented Reinforcement Learning）</strong></li><li><strong>拒绝采样（Rejection Sampling）</strong></li><li><strong>监督微调（Supervised Fine-Tuning）</strong></li><li><strong>在所有场景下进行强化学习（Reinforcement Learning for all Scenarios）</strong></li></ol><p>接下来我们依次展开说明。</p><hr><h4 id="第一步：冷启动"><a href="#第一步：冷启动" class="headerlink" title="第一步：冷启动"></a>第一步：冷启动</h4><p>在第一步中，研究人员先使用了一个约 5000 个tokens的高质量推理数据集对 <strong>DeepSeek-V3-Base</strong> 进行微调，以避免产生可读性不佳的<strong>冷启动问题（cold start problem）</strong>。这个微调步骤可以让模型的输出更加可读，不至于在一开始就产生混乱的推理文本。下文展示了这一过程的示意图。</p><hr><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_184058.png" class=""><p>在图中可以看到：</p><ul><li>“DeepSeek-V3-Base” 通过<strong>监督微调（Supervised Fine-Tuning）</strong>的方式，引入了约 5000 条高质量推理样本。</li><li>这些样本包含了<strong>Reasoning</strong>（推理）和<strong>Answer</strong>（答案）两种部分。</li><li>该步骤目的是“防止冷启动”，即让模型在一开始就掌握基础的可读性推理。</li></ul><hr><h4 id="第二步：推理导向的强化学习"><a href="#第二步：推理导向的强化学习" class="headerlink" title="第二步：推理导向的强化学习"></a>第二步：推理导向的强化学习</h4><p>在得到一个初步微调后的模型后（上一步的成果），作者使用与 <strong>DeepSeek-V3-Zero</strong> 类似的强化学习流程对模型进行训练，但额外加入了<strong>目标语言一致性</strong>的奖励，以确保模型在推理和回答时不会混用多种语言。</p><p>除了之前提到的准确性（accuracy reward）和格式（format reward）等，还增加了<strong>语言奖励（language reward）</strong>来保证生成的语言风格或语言类型保持一致，不至于出现“中英文混杂”或“风格不稳”的现象。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_184229.png" class=""><ul><li><strong>Format reward</strong>：依旧关注 <code>&lt;think&gt;</code> 和 <code>&lt;answer&gt;</code> 的使用。</li><li><strong>Accuracy reward</strong>：检查答案是否正确，以及是否能通过相应的“单元测试”。</li><li><strong>Language reward</strong>：检查语言是否一致、通顺以及是否符合目标语言要求。</li></ul><p>这些奖励综合起来，通过强化学习（RL）循环使模型的推理和答案在可读性、准确度和语言风格方面逐渐优化。</p><hr><h4 id="第三步：拒绝采样"><a href="#第三步：拒绝采样" class="headerlink" title="第三步：拒绝采样"></a>第三步：拒绝采样</h4><p>在这一阶段，作者用<strong>第 2 步</strong>强化学习后得到的模型，来大规模生成<strong>合成推理数据</strong>，并配合 <strong>DeepSeek-V3-Base</strong> 模型来进行“评估”和“规则过滤”，最终产生约 60 万条高质量的推理样本可用于后续监督微调。同时，他们还另外生成了约 20 万条<strong>非推理样本</strong>，包含了写作、简单问答、自我认知、翻译等多种任务数据。下文总结了这一过程。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_184407.png" class=""><ul><li>左边展示了<strong>DeepSeek-V3-2</strong> 如何采样到大量<strong>Reasoning</strong>（推理）和<strong>Answer</strong>（答案），再利用基于规则的筛选和 <strong>DeepSeek-V3-Base</strong> 的判断（判断生成的内容质量），保留质量更好的推理数据（约 600,000 条）。</li><li>右边展示了<strong>非推理</strong>（non-reasoning）数据采样流程，来自 DeepSeek-V3-Base 所使用的一部分数据，总共约 200,000 条，这些数据主要涉及写作、事实性问答（factual QA）、自我认知、翻译等方面。</li></ul><p>由此，研究人员得到规模约 80 万条的“混合”数据，其中既有推理样本，也有非推理样本。</p><hr><h4 id="第四步：监督微调"><a href="#第四步：监督微调" class="headerlink" title="第四步：监督微调"></a>第四步：监督微调</h4><p>在得到上述 80 万条数据后，研究人员再次对 <strong>DeepSeek-V3-Base</strong> 进行监督微调，具体过程如下图所示。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_184526.png" class=""><ul><li>在图中，我们看到“DeepSeek-V3-Base”被用于执行<strong>监督微调（Supervised Fine-Tuning）</strong>，使用的正是前文所提到的 800,000 条<strong>高质量推理与非推理样本</strong>。</li><li>这一阶段使得模型在更大规模的数据基础上，学习到更广泛、更多样的推理形式和任务形式。</li></ul><hr><h4 id="第五步：在所有场景下的强化学习"><a href="#第五步：在所有场景下的强化学习" class="headerlink" title="第五步：在所有场景下的强化学习"></a>第五步：在所有场景下的强化学习</h4><p>在监督微调完成后，研究人员继续采用类似 <strong>DeepSeek-R1-Zero</strong> 的方法进行 <strong>RL（强化学习）</strong> 训练。但是，为了让模型更符合人类偏好，他们在这个阶段引入了更多的 <strong>“有益与无害”（helpfulness and harmlessness）</strong> 奖励信号，用来约束模型的回答。</p><p>同时，模型也被要求<strong>对推理过程进行总结（summarize）</strong>，以防止在最终输出时显示出过长、难以阅读的推理文本。这一步骤解决了前述提到的可读性问题。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_184907.png" class=""><ol><li><strong>Format reward（格式奖励）</strong>  <ul><li>是否正确使用 <code>&lt;think&gt;</code> 标签书写推理内容  </li><li>是否正确使用 <code>&lt;answer&gt;</code> 标签输出答案</li></ul></li><li><strong>Accuracy reward（准确性奖励）</strong>  <ul><li>测试输出是否能编译（“does it compile?”）  </li><li>是否能通过单元测试（“does it pass unit tests?”）</li></ul></li><li><strong>Preference rewards（偏好奖励）</strong>  <ul><li>关注 <strong>Helpfulness（有益）</strong>、<strong>Harmlessness（无害）</strong>、<strong>Human preference（人类偏好）</strong> 等  </li><li>由 RM（Reward Model） 模块来评估这些偏好指标</li></ul></li></ol><p>图中可以看到，<strong>Reasoning</strong>（推理）阶段和 <strong>Answer</strong>（答案）阶段需要分别用 <code>&lt;think&gt;</code> 和 <code>&lt;answer&gt;</code> 标签进行明确区分。同时，为了输出更为精简、可读的内容，模型也可能产生一个 <strong>Summary</strong>（总结）片段。强化学习的迭代过程会同时考虑多种奖励信号，从而不断更新模型并得到最终版本的 <strong>DeepSeek-R1</strong>。</p><p>上图中，“RM” 即 Reward Model，用于对偏好进行打分（如对话是否友善、是否符合伦理要求等），再把结果反馈给模型。</p><p>“<strong>And that’s it!<strong>”这意味着 <strong>DeepSeek-R1</strong> 实际上是 <strong>DeepSeek-V3-Base</strong> 经过监督微调（Supervised Fine-Tuning）和强化学习（RL）进一步优化而成。大量的工作都用于保证</strong>高质量数据</strong>的生成与使用，进而训练出这样一个具备强大推理能力的模型。</p><hr><h2 id="将推理知识从-DeepSeek-R1-蒸馏到其他模型"><a href="#将推理知识从-DeepSeek-R1-蒸馏到其他模型" class="headerlink" title="将推理知识从 DeepSeek-R1 蒸馏到其他模型"></a>将推理知识从 DeepSeek-R1 蒸馏到其他模型</h2><p><strong>DeepSeek-R1</strong> 拥有 <strong>6710 亿（671B）</strong> 参数。这一规模的模型在普通消费级硬件上运行存在较大难度。出于实用性考虑，作者们研究了如何将 <strong>DeepSeek-R1</strong> 的推理能力“蒸馏（distill）”到更小的模型（如 <strong>Qwen-32B</strong>）上，以便能在消费级硬件上部署和使用。</p><h3 id="蒸馏过程：Teacher-Student-框架"><a href="#蒸馏过程：Teacher-Student-框架" class="headerlink" title="蒸馏过程：Teacher-Student 框架"></a>蒸馏过程：Teacher-Student 框架</h3><p>在蒸馏过程中，<strong>DeepSeek-R1</strong> 作为教师模型（Teacher），而规模更小的模型（如 Qwen-32B）作为学生模型（Student）。二者面对相同的提示（prompt）时，分别会输出一组<strong>词元概率分布（token probability distribution）</strong>。训练时，学生模型会尽量学习并接近教师模型的输出分布。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_185245.png" class=""><ul><li>教师（DeepSeek-R1）给出自己的“proposal distribution”。例如在回答“What is 3 + 2?”时，教师模型可能倾向输出“Adding”“If”“5”“3”“4”等标记，并赋予各自不同的概率。  </li><li>学生（Qwen-32B）则会在训练中不断更新自己的概率分布，使之更接近教师的分布。</li></ul><blockquote><p><strong>额外解释</strong>：  </p><ol><li><strong>概率分布（proposal distribution）</strong>：语言模型在生成下一个词元（token）时，会输出对所有可能词元的概率估计。  </li><li><strong>蒸馏（distillation）</strong>：通过比较教师和学生的分布差异，学生会逐步调整自身参数，使其输出更接近教师模型的风格和推理倾向。</li></ol></blockquote><p>训练所使用的数据，正是之前提到的那 <strong>80 万条高质量样本</strong>——其中包含约 60 万推理样本和 20 万非推理样本。下图展示了这一数据流向： </p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_185323.png" class=""><ul><li>左侧的 <strong>Reasoning</strong>（推理）和 <strong>Answer</strong>（答案）数据，合计 80 万条。  </li><li>由 <strong>DeepSeek-R1</strong>（Teacher）生成或评估，得到对应的概率分布。  </li><li>学生模型 <strong>Qwen-32B</strong> 则根据教师的分布进行学习，最终得到一个蒸馏版本 <strong>DeepSeek-R1-Distill-Qwen-32B</strong>。</li></ul><p><img src="https://user-images.githubusercontent.com/your-image-url.png" alt="使用 80 万条高质量样本蒸馏的流程（图10）"></p><blockquote><p><strong>额外解释</strong>：  </p><ul><li>学生模型不仅仅学习了那 80 万条样本本身的输入-输出模式，也学习到 <strong>DeepSeek-R1</strong> 在面对这些数据时所“倾向”采用的推理策略和概率分布，从而在更小模型上复现类似的推理能力。  </li><li>“Distilled” 模型往往会在推理质量与计算资源之间找到更好的平衡：虽然可能在性能上略逊色于老师模型，但依然能在大多数常见任务上达到令人满意的结果，并且所需资源更低。</li></ul></blockquote><hr><h2 id="其他未成功的尝试"><a href="#其他未成功的尝试" class="headerlink" title="其他未成功的尝试"></a>其他未成功的尝试</h2><p>在研究过程中，DeepSeek 团队也曾尝试过 <strong>Process Reward Models（PRMs）</strong> 和 <strong>Monte Carlo Tree Search（MCTS）</strong> 等方法来注入推理能力，但结果并不理想：</p><ol><li><p><strong>使用 MCTS</strong>  </p><ul><li>面临的主要问题是搜索空间过于庞大，只能对节点展开进行严格限制。这样一来，效果就大打折扣。  </li><li>此外，精细化训练 Reward Model 也相当困难。</li></ul></li><li><p><strong>使用 PRMs 进行 Best-of-N 策略</strong>  </p><ul><li>如果不断重训练 Reward Model 以防止模型出现“投机取巧”（reward hacking）行为，会带来高昂的计算开销。</li></ul></li></ol><p>这些结果并不意味着这些技术无效，而是说明它们在当前大规模语言模型上的实践还有诸多限制与难点。<strong>DeepSeek-R1</strong> 之所以取得成功，更多依赖于<strong>强化学习 + 监督微调</strong>的组合，以及对大规模高质量数据的挖掘与利用。</p><hr><h2 id="总结与展望"><a href="#总结与展望" class="headerlink" title="总结与展望"></a>总结与展望</h2><p>至此，我们已经大致回顾了 <strong>DeepSeek-R1</strong> 的推理训练之旅。希望以上内容能够让你更好地理解：  </p><ul><li><strong>Test-time compute（推理时计算）</strong> 可以通过模型输出更长、更精细的思考过程（Chain-of-Thought）来取得更佳效果。  </li><li>大规模“<strong>先监督微调，再强化学习</strong>”的训练流程，以及<strong>蒸馏</strong>到更小模型的技术路线，也展现了在硬件资源和推理性能间取得平衡的方法。</li></ul><p>如前所述，<strong>DeepSeek-R1</strong> 引入了多种奖励机制，尤其是针对格式和人类偏好的奖励，来保证回答既正确又易读。“总结推理过程”（Summary）的做法也在很大程度上改善了纯文本Chain-of-Thought过长而导致的可读性问题。</p><hr><h2 id="更多资源"><a href="#更多资源" class="headerlink" title="更多资源"></a>更多资源</h2><p>如果你对 <strong>Large Language Models（LLMs）</strong> 中的推理话题感兴趣，以下资源值得参考：</p><ol><li><a href="https://newsletter.languagemodels.co/p/the-illustrated-deepseek-r1"><strong>The Illustrated DeepSeek-R1</strong></a>  <ul><li>Jay Alammar 制作的高质量可视化指南，详细介绍了 DeepSeek-R1 模型背后的原理与实现细节。</li></ul></li><li><a href="https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute"><strong>Hugging Face 的一篇博文</strong></a>  <ul><li>重点讨论了在推理阶段如何对计算量进行扩展，并给出了有趣的实验。</li></ul></li><li><a href="https://www.youtube.com/watch?v=6PEJ96k1kiw"><strong>视频 “Speculations on Test-Time Scaling”</strong></a>  <ul><li>深入探讨了在推理阶段进行各种计算扩展的常用技术细节。</li></ul></li></ol><p>此外，作者在文中也提到了一本关于大型语言模型的著作，内含更多可视化和实验结果，是想进一步研究推理 LLMs 的朋友可以深入阅读的好资料。</p><ul><li><strong>Official Website of the Book</strong>: <a href="https://www.llm-book.com/">llm-book.com</a>  </li><li><strong>Amazon 购买链接</strong>: <a href="https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961">Hands-On Large Language Models: Understanding, Building, and Optimizing LLMs</a>  </li><li><strong>GitHub 代码仓库</strong>: <a href="https://github.com/handsOnLLM/Hands-On-Large-Language-Models">handsOnLLM/Hands-On-Large-Language-Models</a></li></ul><hr><h3 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h3><p>感谢你阅读本篇关于 <strong>DeepSeek-R1</strong> 的介绍文档。通过对所有图片与文字内容的依次解读，以及对每个环节所涉及的关键技术进行了更多解释，我们希望让你对 <strong>DeepSeek-R1</strong> 的训练流程、蒸馏方法和未成功的尝试都有更加全面的了解。</p><p>在未来，随着硬件性能的提升与更成熟的训练技术出现，<strong>深度推理</strong>与<strong>模型蒸馏</strong>必将在更多实际应用场景中发挥巨大作用。让我们拭目以待！</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;推理-LLM-的可视化指南：探索推理时计算技术与-DeepSeek-R1&quot;&gt;&lt;a href=&quot;#推理-LLM-的可视化指南：探索推理时计算技术与-DeepSeek-R1&quot; class=&quot;headerlink&quot; title=&quot;推理 LLM 的可视化指南：探索推理时计</summary>
      
    
    
    
    <category term="NLP Insights" scheme="https://chenhuiyu.github.io/categories/NLP-Insights/"/>
    
    
    <category term="LLM" scheme="https://chenhuiyu.github.io/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>追逐与倒影</title>
    <link href="https://chenhuiyu.github.io/2024/12/11/Life%20Reflections/%E8%BF%BD%E9%80%90%E4%B8%8E%E5%80%92%E5%BD%B1.en/"/>
    <id>https://chenhuiyu.github.io/2024/12/11/Life%20Reflections/%E8%BF%BD%E9%80%90%E4%B8%8E%E5%80%92%E5%BD%B1.en/</id>
    <published>2024-12-10T18:29:06.000Z</published>
    <updated>2026-02-20T21:47:32.616Z</updated>
    
    <content type="html"><![CDATA[<h3 id="追逐与倒影"><a href="#追逐与倒影" class="headerlink" title="追逐与倒影"></a>追逐与倒影</h3><p>在清晨的第一缕光洒下之前，世间一切尚未显形。光与影的边界模糊，彷佛可以交叠，又彷佛注定分离。人们常说，朝阳是希望的象征，可它升起时，必将抛下一地影子。光和影之间，究竟是追逐还是相伴？这样的思考让我想起一则古老的寓言：一匹马在沙漠中追逐远方的绿洲，却不知道那不过是海市蜃楼，它每前进一步，绿洲也随之远去。</p><p>有时我们追寻的目标，如同沙漠中的绿洲一般，它并非虚无，但也不完全真实。它是一种存在于心中的映像，一个无法企及的彼岸。无论我们怎样靠近，那份距离似乎总是恒定，甚至在我们伸手触碰的一刹那，它便如烟雾般消散。是目标变了，还是我们的执念让它愈加模糊？</p><p>镜中的倒影也是如此。当你站在镜前凝视自己时，你看见的那个“你”，究竟是谁？是一个忠实的再现，还是一场温柔的欺骗？镜中的倒影总会回应你的动作，可是你永远无法拥抱它，甚至连碰触都无法做到。这种触不可及的关系，既令人惋惜，又教人思索。倘若生命中许多事物都像这面镜子，是否意味着我们注定只能遥望，却无法真正拥有？</p><p>“人类最大的悲剧在于，他们注定要追求那些不可得之物。”起初，我对这句话嗤之以鼻。世界这么大，怎么可能所有的追求都是徒劳？然而，当经历了一些无法言说的感受后，我逐渐明白，这种“不可得”并非指绝对的失败，而是一种与目标间无法消弭的间隙。它可能是时间的错位，也可能是空间的疏离，甚至可能只是心境的不同。</p><p>也许，正是这种无法彻底握住的感觉，赋予了追逐的过程以意义。倘若一切触手可得，生命是否会因少了些许遗憾而失去色彩？光因为有影子才得以分明，爱因为不可得才显得深刻。我们在遗憾中体会到希望，在距离中发现自我，这是否是一种隐秘的平衡？</p><p>夜幕降临时，清晨的影子会被黑暗吞没，但它并未真正消失，只是换了一种形态，成为心底一束无法熄灭的微光。这光指引着我们，虽不可捉摸，却又真实存在。或许，我们无需执着于是否能得到，而是学会在追逐的过程中，接受光与影交织的美。</p><p>世间的一切，皆是倒影。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;追逐与倒影&quot;&gt;&lt;a href=&quot;#追逐与倒影&quot; class=&quot;headerlink&quot; title=&quot;追逐与倒影&quot;&gt;&lt;/a&gt;追逐与倒影&lt;/h3&gt;&lt;p&gt;在清晨的第一缕光洒下之前，世间一切尚未显形。光与影的边界模糊，彷佛可以交叠，又彷佛注定分离。人们常说，朝阳是希望的象</summary>
      
    
    
    
    <category term="Life Reflections" scheme="https://chenhuiyu.github.io/categories/Life-Reflections/"/>
    
    
    <category term="杂谈" scheme="https://chenhuiyu.github.io/tags/%E6%9D%82%E8%B0%88/"/>
    
  </entry>
  
  <entry>
    <title>追逐与倒影</title>
    <link href="https://chenhuiyu.github.io/2024/12/11/Life%20Reflections/%E8%BF%BD%E9%80%90%E4%B8%8E%E5%80%92%E5%BD%B1/"/>
    <id>https://chenhuiyu.github.io/2024/12/11/Life%20Reflections/%E8%BF%BD%E9%80%90%E4%B8%8E%E5%80%92%E5%BD%B1/</id>
    <published>2024-12-10T18:29:06.000Z</published>
    <updated>2026-02-20T21:51:54.076Z</updated>
    
    <content type="html"><![CDATA[<h3 id="追逐与倒影"><a href="#追逐与倒影" class="headerlink" title="追逐与倒影"></a>追逐与倒影</h3><p>在清晨的第一缕光洒下之前，世间一切尚未显形。光与影的边界模糊，彷佛可以交叠，又彷佛注定分离。人们常说，朝阳是希望的象征，可它升起时，必将抛下一地影子。光和影之间，究竟是追逐还是相伴？这样的思考让我想起一则古老的寓言：一匹马在沙漠中追逐远方的绿洲，却不知道那不过是海市蜃楼，它每前进一步，绿洲也随之远去。</p><p>有时我们追寻的目标，如同沙漠中的绿洲一般，它并非虚无，但也不完全真实。它是一种存在于心中的映像，一个无法企及的彼岸。无论我们怎样靠近，那份距离似乎总是恒定，甚至在我们伸手触碰的一刹那，它便如烟雾般消散。是目标变了，还是我们的执念让它愈加模糊？</p><p>镜中的倒影也是如此。当你站在镜前凝视自己时，你看见的那个“你”，究竟是谁？是一个忠实的再现，还是一场温柔的欺骗？镜中的倒影总会回应你的动作，可是你永远无法拥抱它，甚至连碰触都无法做到。这种触不可及的关系，既令人惋惜，又教人思索。倘若生命中许多事物都像这面镜子，是否意味着我们注定只能遥望，却无法真正拥有？</p><p>“人类最大的悲剧在于，他们注定要追求那些不可得之物。”起初，我对这句话嗤之以鼻。世界这么大，怎么可能所有的追求都是徒劳？然而，当经历了一些无法言说的感受后，我逐渐明白，这种“不可得”并非指绝对的失败，而是一种与目标间无法消弭的间隙。它可能是时间的错位，也可能是空间的疏离，甚至可能只是心境的不同。</p><p>也许，正是这种无法彻底握住的感觉，赋予了追逐的过程以意义。倘若一切触手可得，生命是否会因少了些许遗憾而失去色彩？光因为有影子才得以分明，爱因为不可得才显得深刻。我们在遗憾中体会到希望，在距离中发现自我，这是否是一种隐秘的平衡？</p><p>夜幕降临时，清晨的影子会被黑暗吞没，但它并未真正消失，只是换了一种形态，成为心底一束无法熄灭的微光。这光指引着我们，虽不可捉摸，却又真实存在。或许，我们无需执着于是否能得到，而是学会在追逐的过程中，接受光与影交织的美。</p><p>世间的一切，皆是倒影。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;追逐与倒影&quot;&gt;&lt;a href=&quot;#追逐与倒影&quot; class=&quot;headerlink&quot; title=&quot;追逐与倒影&quot;&gt;&lt;/a&gt;追逐与倒影&lt;/h3&gt;&lt;p&gt;在清晨的第一缕光洒下之前，世间一切尚未显形。光与影的边界模糊，彷佛可以交叠，又彷佛注定分离。人们常说，朝阳是希望的象</summary>
      
    
    
    
    <category term="Life Reflections" scheme="https://chenhuiyu.github.io/categories/Life-Reflections/"/>
    
    
    <category term="杂谈" scheme="https://chenhuiyu.github.io/tags/%E6%9D%82%E8%B0%88/"/>
    
  </entry>
  
  <entry>
    <title>基于生成的大语言模型（LLM）评估：从生成到判断的机遇与挑战</title>
    <link href="https://chenhuiyu.github.io/2024/12/06/NLP%20Insights/%E5%9F%BA%E4%BA%8E%E7%94%9F%E6%88%90%E7%9A%84%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%88LLM%EF%BC%89%E8%AF%84%E4%BC%B0%EF%BC%9A%E4%BB%8E%E7%94%9F%E6%88%90%E5%88%B0%E5%88%A4%E6%96%AD%E7%9A%84%E6%9C%BA%E9%81%87%E4%B8%8E%E6%8C%91%E6%88%98.en/"/>
    <id>https://chenhuiyu.github.io/2024/12/06/NLP%20Insights/%E5%9F%BA%E4%BA%8E%E7%94%9F%E6%88%90%E7%9A%84%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%88LLM%EF%BC%89%E8%AF%84%E4%BC%B0%EF%BC%9A%E4%BB%8E%E7%94%9F%E6%88%90%E5%88%B0%E5%88%A4%E6%96%AD%E7%9A%84%E6%9C%BA%E9%81%87%E4%B8%8E%E6%8C%91%E6%88%98.en/</id>
    <published>2024-12-06T06:34:18.000Z</published>
    <updated>2026-02-20T21:47:32.626Z</updated>
    
    <content type="html"><![CDATA[<hr><h1 id="基于生成的大语言模型（LLM）评估：从生成到判断的机遇与挑战"><a href="#基于生成的大语言模型（LLM）评估：从生成到判断的机遇与挑战" class="headerlink" title="基于生成的大语言模型（LLM）评估：从生成到判断的机遇与挑战"></a>基于生成的大语言模型（LLM）评估：从生成到判断的机遇与挑战</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>人工智能（AI）与自然语言处理（NLP）领域中的评估任务长期面临挑战。传统的评估方法（如基于匹配或嵌入的技术）在判断复杂属性时效果有限。近期大语言模型（LLM）的发展催生了“LLM-as-a-Judge”范式，利用LLM对任务进行评分、排序或选择。本论文对LLM评估方法进行了全面综述，包括其定义、分类框架、评估基准，以及未来的研究方向。</p><hr><h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><h3 id="1-1-背景"><a href="#1-1-背景" class="headerlink" title="1.1 背景"></a>1.1 背景</h3><p>评估是机器学习和NLP的核心问题之一，传统评估方法如BLEU和ROUGE通常基于文本重叠，缺乏对复杂场景的适用性。随着深度学习和LLM的发展（如GPT-4），研究者提出了“LLM-as-a-Judge”模式，以解决传统评估的局限。</p><h3 id="1-2-研究问题"><a href="#1-2-研究问题" class="headerlink" title="1.2 研究问题"></a>1.2 研究问题</h3><p>本论文旨在探讨以下问题：</p><ul><li><strong>评估内容：LLM评估什么？</strong></li><li><strong>评估方法：如何进行评估？</strong></li><li><strong>应用场景：LLM在哪里评估？</strong></li></ul><hr><h2 id="2-预备知识"><a href="#2-预备知识" class="headerlink" title="2. 预备知识"></a>2. 预备知识</h2><h3 id="2-1-输入格式"><a href="#2-1-输入格式" class="headerlink" title="2.1 输入格式"></a>2.1 输入格式</h3><p>评估输入可分为：</p><ul><li><strong>点对点（Point-Wise）</strong>：单个样本评估。</li><li><strong>对/列表评估（Pair/List-Wise）</strong>：多个样本的比较评估。</li></ul><h3 id="2-2-输出格式"><a href="#2-2-输出格式" class="headerlink" title="2.2 输出格式"></a>2.2 输出格式</h3><p>评估输出包括：</p><ul><li><strong>评分（Score）</strong>：对样本进行量化评分。</li><li><strong>排序（Ranking）</strong>：根据优劣排序。</li><li><strong>选择（Selection）</strong>：从多个候选中选取最佳方案。</li></ul><hr><h2 id="3-评估属性"><a href="#3-评估属性" class="headerlink" title="3. 评估属性"></a>3. 评估属性</h2><h3 id="3-1-有用性（Helpfulness）"><a href="#3-1-有用性（Helpfulness）" class="headerlink" title="3.1 有用性（Helpfulness）"></a>3.1 有用性（Helpfulness）</h3><p>LLM通过指导用户任务和生成反馈，对响应的有用性进行评估。这在AI对齐（Alignment）中尤为重要。</p><h3 id="3-2-无害性（Harmlessness）"><a href="#3-2-无害性（Harmlessness）" class="headerlink" title="3.2 无害性（Harmlessness）"></a>3.2 无害性（Harmlessness）</h3><p>评估文本的无害性是生成安全内容的关键。LLM可辅助数据标注或直接评估潜在的有害内容。</p><h3 id="3-3-可靠性（Reliability）"><a href="#3-3-可靠性（Reliability）" class="headerlink" title="3.3 可靠性（Reliability）"></a>3.3 可靠性（Reliability）</h3><p>LLM可检测事实性和一致性。例如，通过生成辅助证据或进行对话级别的可靠性评估。</p><h3 id="3-4-相关性（Relevance）"><a href="#3-4-相关性（Relevance）" class="headerlink" title="3.4 相关性（Relevance）"></a>3.4 相关性（Relevance）</h3><p>LLM可评估生成或检索内容的相关性，适用于会话、检索增强生成（RAG）等场景。</p><h3 id="3-5-可行性（Feasibility）"><a href="#3-5-可行性（Feasibility）" class="headerlink" title="3.5 可行性（Feasibility）"></a>3.5 可行性（Feasibility）</h3><p>在复杂任务中，LLM可对候选步骤或行动进行可行性判断，从而优化决策路径。</p><h3 id="3-6-总体质量（Overall-Quality）"><a href="#3-6-总体质量（Overall-Quality）" class="headerlink" title="3.6 总体质量（Overall Quality）"></a>3.6 总体质量（Overall Quality）</h3><p>LLM通过多维度评分生成整体评价，适用于生成任务的综合比较。</p><hr><h3 id="4-方法论"><a href="#4-方法论" class="headerlink" title="4. 方法论"></a>4. 方法论</h3><h4 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h4><p>方法论部分主要探讨如何优化LLM作为评估者（LLM-as-a-Judge）的能力，从调优和提示技术两个方面进行阐述：</p><ol><li><strong>调优技术</strong>：通过监督微调（SFT）和偏好学习等方法，利用人工标注数据或合成反馈来增强LLM的判断能力。</li><li><strong>提示技术</strong>：设计高效的提示策略（如操作交换、规则增强、多代理协作等）以提升LLM在推理和评估过程中的准确性和可靠性。</li></ol><hr><h4 id="4-1-调优技术"><a href="#4-1-调优技术" class="headerlink" title="4.1 调优技术"></a>4.1 调优技术</h4><h5 id="数据来源"><a href="#数据来源" class="headerlink" title="数据来源"></a>数据来源</h5><h6 id="1-人工标注数据"><a href="#1-人工标注数据" class="headerlink" title="1. 人工标注数据"></a>1. <strong>人工标注数据</strong></h6><p>人工标注数据提供了高质量的训练样本，帮助LLM学习人类偏好。以下是核心研究及其创新点：</p><ol><li><p><strong>PandaLM</strong>【Wang et al., 2024h】：</p><ul><li>PandaLM项目收集了多样化的人工标注数据集，涵盖指令生成任务的300,000个样本。</li><li>作者通过整合多种数据源（如开放领域问答和对话生成）来增强模型的泛化能力。</li><li>该研究的关键创新在于引入了标准化的标注流程，以确保数据质量与一致性。</li><li>此外，PandaLM强调多语言支持，通过跨文化的数据标注提高模型的适用性。</li><li>最终，PandaLM被证明在多个评估任务上表现优异，其输出与人工评估高度相关。</li></ul></li><li><p><strong>AspectInstruct</strong>【Liu et al., 2024a】：</p><ul><li>该研究首次提出了一个针对多维度评估的指令调优数据集，涵盖65个任务和27个评估维度。</li><li>数据集中包含对话生成、摘要和数据到文本转换等复杂任务的多方面评分。</li><li>作者设计了独特的任务分割机制，使模型能够根据上下文理解并优先评估特定维度。</li><li>研究的亮点在于数据集的多样性和全面性，为多任务评估提供了新的基准。</li><li>最终，该数据集显著提升了LLM在不同评估场景中的多维度理解和评估能力。</li></ul></li></ol><h6 id="2-合成数据"><a href="#2-合成数据" class="headerlink" title="2. 合成数据"></a>2. <strong>合成数据</strong></h6><p>合成数据通过LLM生成训练样本，减少了对人工标注的依赖，同时扩展了数据覆盖范围。以下是核心研究及其创新点：</p><ol><li><p><strong>JudgeLM</strong>【Zhu et al., 2023】：</p><ul><li>研究者利用GPT-4生成包含任务种子、生成答案及相关评估的高质量数据集。</li><li>数据集中包含10万个样本，覆盖了指令生成任务的多种场景。</li><li>核心创新点在于引入了生成任务种子的方法，确保生成数据的多样性和针对性。</li><li>作者还设计了一种基于偏好学习的优化方法，以提高LLM对细粒度任务的判断能力。</li><li>研究表明，经过这种优化后的JudgeLM在多个基准测试中超越了传统方法。</li></ul></li><li><p><strong>Meta-Rewarding</strong>【Wu et al., 2024】：</p><ul><li>提出了一种新颖的“元奖励”（Meta-Rewarding）方法，通过LLM自我评估生成的判断信号增强训练效果。</li><li>该方法要求模型在生成答案后对自己的输出进行评分，从而生成偏好数据。</li><li>创新点在于采用策略模型作为评估者，显著提高了数据生成效率和质量。</li><li>此外，该研究通过逐步改进的偏好数据训练LLM，提高了其评估任务的鲁棒性。</li><li>最终，Meta-Rewarding展示了LLM自我增强能力的潜力，成为偏好学习领域的重要进展。</li></ul></li></ol><h5 id="调优方法"><a href="#调优方法" class="headerlink" title="调优方法"></a>调优方法</h5><h6 id="1-监督微调（SFT）"><a href="#1-监督微调（SFT）" class="headerlink" title="1. 监督微调（SFT）"></a>1. <strong>监督微调（SFT）</strong></h6><p>监督微调通过使用人工标注或合成数据，让LLM从示例中学习判断标准。以下是核心研究及其创新点：</p><ol><li><p><strong>FLAMe</strong>【Vu et al., 2024】：</p><ul><li>该研究提出了Foundational Large Autorater Models (FLAMe)，利用超过500万个样本进行大规模多任务监督微调。</li><li>FLAMe在多任务数据中引入了统一的评价标准，提高了模型在多样化任务中的评估能力。</li><li>创新点在于采用多任务学习框架，将多个评估维度集成到一个模型中。</li><li>作者还设计了任务分层训练策略，使模型能够逐步掌握复杂的评估任务。</li><li>实验结果表明，FLAMe在多个生成任务上的表现优于传统评估指标。</li></ul></li><li><p><strong>JSFT</strong>【Lee et al., 2024】：</p><ul><li>提出了Judge-augmented Supervised Fine-Tuning（JSFT）方法，通过扩展偏好学习数据增强微调效果。</li><li>数据集中包含点对点和对比评估任务，以全面覆盖多种评估场景。</li><li>创新点在于引入了多阶段训练策略，结合监督学习和偏好学习优化模型性能。</li><li>此外，研究者设计了简化提示机制，显著提高了模型处理复杂输入的能力。</li><li>JSFT的实验结果显示，其生成的评估结果在多个基准上超过了现有方法。</li></ul></li></ol><h6 id="2-偏好学习"><a href="#2-偏好学习" class="headerlink" title="2. 偏好学习"></a>2. <strong>偏好学习</strong></h6><p>偏好学习通过优化LLM的比较和排序能力，适用于复杂评估任务。以下是核心研究及其创新点：</p><ol><li><p><strong>HALU-J</strong>【Wang et al., 2024a】：</p><ul><li>提出了一种基于批评的偏好学习方法，专注于选择相关证据并生成详细批评。</li><li>创新点在于设计了多证据选择机制，提高了LLM的可靠性评估能力。</li><li>该方法通过Directed Preference Optimization（DPO）进行优化，使模型能够更准确地判断任务间的优劣。</li><li>HALU-J还结合了上下文推理，扩展了偏好学习的应用场景。</li><li>实验表明，HALU-J显著提升了复杂任务的评估准确性，尤其是在事实性和逻辑性判断上。</li></ul></li><li><p><strong>Self-Taught Evaluators</strong>【Wang et al., 2024f】：</p><ul><li>该研究提出了一种自学习的评估者方法，利用被扰乱的指令生成低质量数据作为偏好学习的负样本。</li><li>自学习方法通过自动生成的次优响应，提供了丰富的训练数据。</li><li>创新点在于通过动态调整偏好信号，提升了模型的适应性和通用性。</li><li>作者还设计了基于多轮交互的学习策略，使模型能够在动态环境中自我优化。</li><li>实验结果显示，Self-Taught Evaluators在多个开放式生成任务中表现优异。</li></ul></li></ol><h3 id="4-2-提示技术"><a href="#4-2-提示技术" class="headerlink" title="4.2 提示技术"></a>4.2 提示技术</h3><h4 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h4><p>提示技术（Prompting）通过设计高效的提示策略和推理流程优化LLM的评估能力。这部分探讨如何在推理阶段利用提示技术提升判断精度，减少偏差，并增强模型的评估鲁棒性。主要方法包括操作交换、规则增强、多代理协作、演示、多轮交互以及比较加速。</p><hr><h4 id="4-2-1-操作交换（Swapping-Operation）"><a href="#4-2-1-操作交换（Swapping-Operation）" class="headerlink" title="4.2.1 操作交换（Swapping Operation）"></a>4.2.1 操作交换（Swapping Operation）</h4><h5 id="概述-2"><a href="#概述-2" class="headerlink" title="概述"></a>概述</h5><p>操作交换技术通过更改候选项顺序减少评估的偏置性，确保LLM对输入顺序不敏感，从而提高评估的公平性和可靠性。</p><h6 id="1-MT-Bench【Zheng-et-al-2023】："><a href="#1-MT-Bench【Zheng-et-al-2023】：" class="headerlink" title="1. MT-Bench【Zheng et al., 2023】："></a>1. <strong>MT-Bench</strong>【Zheng et al., 2023】：</h6><ul><li>本研究首次系统性地提出操作交换技术，通过多轮评估减少LLM的顺序敏感性。</li><li>创新点在于引入“对称性检查”机制：将候选项顺序互换，若评分结果一致，则标记为稳定，否则标记为不稳定。</li><li>作者发现操作交换能够有效减少由于位置偏差导致的错误判断。</li><li>该技术应用于多任务评估中，尤其是在复杂生成任务的排序中表现突出。</li><li>MT-Bench为后续的LLM评估技术提供了一个重要的公平性基准。</li></ul><h6 id="2-Starling【Zhu-et-al-2024a】："><a href="#2-Starling【Zhu-et-al-2024a】：" class="headerlink" title="2. Starling【Zhu et al., 2024a】："></a>2. <strong>Starling</strong>【Zhu et al., 2024a】：</h6><ul><li>提出一种类似链式推理（Chain-of-Thought, CoT）的提示技术，通过全面评估所有候选项的两两关系，再总结为最终排序。</li><li>创新点在于强制模型生成所有可能的对比结果，确保评估全面且无偏。</li><li>作者还设计了一种交叉验证机制，进一步提高评估稳定性。</li><li>实验显示，这种方法显著减少了位置偏差带来的误差，特别是在排序任务中表现优异。</li><li>Starling验证了链式思维结合操作交换技术的潜力，尤其在复杂对比任务中的效果显著。</li></ul><hr><h4 id="4-2-2-规则增强（Rule-Augmentation）"><a href="#4-2-2-规则增强（Rule-Augmentation）" class="headerlink" title="4.2.2 规则增强（Rule Augmentation）"></a>4.2.2 规则增强（Rule Augmentation）</h4><h5 id="概述-3"><a href="#概述-3" class="headerlink" title="概述"></a>概述</h5><p>规则增强技术通过在提示中嵌入明确的原则、标准或参考内容，使模型能够更加系统地评估任务，从而提升评估的准确性和一致性。</p><h6 id="1-Constitutional-AI【Bai-et-al-2022】："><a href="#1-Constitutional-AI【Bai-et-al-2022】：" class="headerlink" title="1. Constitutional AI【Bai et al., 2022】："></a>1. <strong>Constitutional AI</strong>【Bai et al., 2022】：</h6><ul><li>本研究引入了“原则驱动”的规则增强方法，利用帮助性、无害性和诚实性等标准指导模型评估。</li><li>创新点在于为每个评估维度定义详细的评分标准，并通过原则约束生成内容。</li><li>作者采用多层提示设计，使LLM能够逐步推理并给出最终评估。</li><li>实验表明，这种方法显著提升了模型在复杂场景中的判断一致性。</li><li>Constitutional AI成为后续研究的重要基石，为基于规则的评估技术奠定了基础。</li></ul><h6 id="2-OAIF【Guo-et-al-2024】："><a href="#2-OAIF【Guo-et-al-2024】：" class="headerlink" title="2. OAIF【Guo et al., 2024】："></a>2. <strong>OAIF</strong>【Guo et al., 2024】：</h6><ul><li>提出了在线AI反馈（Online AI Feedback, OAIF）框架，通过实时原则指导提升模型评估的灵活性。</li><li>核心创新点在于动态调整评估规则，使模型能够适应多变的任务需求。</li><li>OAIF引入了细粒度的多维评分策略，为每个候选项生成独立的评估报告。</li><li>作者验证了这种方法在实时决策中的潜力，尤其在对话和生成任务中表现突出。</li><li>OAIF展现了规则增强的实时适应能力，为实时评估任务提供了新方向。</li></ul><hr><h4 id="4-2-3-多代理协作（Multi-agent-Collaboration）"><a href="#4-2-3-多代理协作（Multi-agent-Collaboration）" class="headerlink" title="4.2.3 多代理协作（Multi-agent Collaboration）"></a>4.2.3 多代理协作（Multi-agent Collaboration）</h4><h5 id="概述-4"><a href="#概述-4" class="headerlink" title="概述"></a>概述</h5><p>多代理协作通过组合多个LLM的评估结果，减少单一模型的偏差，提高评估的准确性和鲁棒性。这种方法强调模型之间的角色分工和合作。</p><h6 id="1-Peer-Rank-PR-【Li-et-al-2023】："><a href="#1-Peer-Rank-PR-【Li-et-al-2023】：" class="headerlink" title="1. **Peer Rank (PR)**【Li et al., 2023】："></a>1. **Peer Rank (PR)**【Li et al., 2023】：</h6><ul><li>提出了同行排名算法，整合多个LLM的对比偏好生成最终排序。</li><li>创新点在于设计了“加权投票”机制，根据模型之间的评分一致性调整权重。</li><li>该研究还探讨了代理间的协作效率和鲁棒性，提出了优化协作路径的方法。</li><li>PR的实验结果显示，其生成的评估结果在排序准确性上优于传统单模型方法。</li><li>该研究为多模型协作技术奠定了理论基础，是后续研究的重要参考。</li></ul><h6 id="2-Cascaded-Selective-Evaluation【Jung-et-al-2024】："><a href="#2-Cascaded-Selective-Evaluation【Jung-et-al-2024】：" class="headerlink" title="2. Cascaded Selective Evaluation【Jung et al., 2024】："></a>2. <strong>Cascaded Selective Evaluation</strong>【Jung et al., 2024】：</h6><ul><li>设计了级联选择评估框架，首先由较弱的模型进行初步评估，仅在需要时调用更强大的模型。</li><li>创新点在于通过分级策略优化计算成本，同时确保评估结果的高质量。</li><li>作者提出了一种交叉验证机制，结合多个代理的结果生成最终判断。</li><li>研究表明，这种级联策略在复杂任务中表现出显著的资源效率提升。</li><li>Cascaded Selective Evaluation展示了多代理协作在资源有限情况下的潜力。</li></ul><hr><h4 id="4-2-4-演示（Demonstration）"><a href="#4-2-4-演示（Demonstration）" class="headerlink" title="4.2.4 演示（Demonstration）"></a>4.2.4 演示（Demonstration）</h4><h5 id="概述-5"><a href="#概述-5" class="headerlink" title="概述"></a>概述</h5><p>演示技术利用具体的示例作为提示，帮助LLM学习评估标准。这种方法通过少量高质量样例显著提高模型的评估能力。</p><h6 id="1-ALLURE【Hasanbeig-et-al-2023】："><a href="#1-ALLURE【Hasanbeig-et-al-2023】：" class="headerlink" title="1. ALLURE【Hasanbeig et al., 2023】："></a>1. <strong>ALLURE</strong>【Hasanbeig et al., 2023】：</h6><ul><li>提出了迭代演示技术，通过在提示中加入显著偏差的示例提高模型的鲁棒性。</li><li>创新点在于采用动态演示方法，逐步更新提示以适应不同的评估任务。</li><li>研究表明，这种方法在低资源场景中表现出色，尤其是在新任务的适应性上有显著提升。</li><li>作者还探讨了如何选择代表性样例以最大化演示效果。</li><li>ALLURE验证了高质量演示样例在提升评估能力方面的重要性。</li></ul><h6 id="2-ICE【Jain-et-al-2023b】："><a href="#2-ICE【Jain-et-al-2023b】：" class="headerlink" title="2. ICE【Jain et al., 2023b】："></a>2. <strong>ICE</strong>【Jain et al., 2023b】：</h6><ul><li>提出了交互式多维评估框架，通过少量上下文示例指导LLM评估。</li><li>创新点在于将评估任务分解为多个独立维度，每个维度都有针对性的示例支持。</li><li>研究表明，ICE框架显著减少了模型在多维任务中的评估偏差。</li><li>实验结果显示，其生成的评估结果在与人工评价的一致性上达到高水平。</li><li>ICE为多维度评估任务的提示设计提供了新思路。</li></ul><hr><h4 id="4-2-5-多轮交互（Multi-turn-Interaction）"><a href="#4-2-5-多轮交互（Multi-turn-Interaction）" class="headerlink" title="4.2.5 多轮交互（Multi-turn Interaction）"></a>4.2.5 多轮交互（Multi-turn Interaction）</h4><h5 id="概述-6"><a href="#概述-6" class="headerlink" title="概述"></a>概述</h5><p>多轮交互通过动态调整提示和上下文信息，为LLM提供更全面的评估依据，适用于需要多步推理的复杂任务。</p><h6 id="1-KIEval【Yu-et-al-2024】："><a href="#1-KIEval【Yu-et-al-2024】：" class="headerlink" title="1. KIEval【Yu et al., 2024】："></a>1. <strong>KIEval</strong>【Yu et al., 2024】：</h6><ul><li>提出了知识交互式评估框架，通过动态问答生成丰富的上下文信息。</li><li>创新点在于引入了“交互者”角色，模拟用户和模型之间的动态交互。</li><li>作者设计了一种鲁棒性检测机制，避免因上下文污染导致的错误评估。</li><li>研究表明，KIEval在复杂任务中的表现优于传统静态评估方法。</li><li>此框架适用于多维度评估，特别是在需要动态调整上下文的场景中。</li></ul><h6 id="2-Auto-Arena【Zhao-et-al-2024c】："><a href="#2-Auto-Arena【Zhao-et-al-2024c】：" class="headerlink" title="2. Auto-Arena【Zhao et al., 2024c】："></a>2. <strong>Auto-Arena</strong>【Zhao et al., 2024c】：</h6><ul><li>设计了一种多轮辩论框架，允许多个模型围绕特定任务进行交互讨论。</li><li>创新点在于结合多轮问答和动态评分机制，从不同角度对候选答案进行评估。</li><li>研究表明，这种方法能够揭示候选答案间的深层次差异。</li><li>作者还探讨了如何通过动态调整辩论内容提高评估效率。</li><li>Auto-Arena展示了多轮交互在复杂评估任务中的潜力。</li></ul><hr><h4 id="4-2-6-比较加速（Comparison-Acceleration）"><a href="#4-2-6-比较加速（Comparison-Acceleration）" class="headerlink" title="4.2.6 比较加速（Comparison Acceleration）"></a>4.2.6 比较加速（Comparison Acceleration）</h4><h5 id="概述-7"><a href="#概述-7" class="headerlink" title="概述"></a>概述</h5><p>比较加速技术通过优化比较流程，减少多候选排序任务的计算成本，提高评估效率。</p><h6 id="1-Ranked-Pairing【Zhai-et-al-2024】："><a href="#1-Ranked-Pairing【Zhai-et-al-2024】：" class="headerlink" title="1. Ranked Pairing【Zhai et al., 2024】："></a>1. <strong>Ranked Pairing</strong>【Zhai et al., 2024】：</h6><ul><li>提出了一种基于基线比较的排序方法，通过对所有候选项与基线进行比较确定优劣。</li><li>创新点在于避免传统两两比较的高计算开销，显著提高了评估效率。</li><li>作者还设计了一种自适应比较策略，进一步优化排序性能。</li><li>研究表明，Ranked Pairing在大规模排序任务中表现出极高的效率。</li><li>此方法特别适用于需要快速生成排序结果的场景。</li></ul><h6 id="2-Tournament-based-Comparison【Lee-et-al-2024】："><a href="#2-Tournament-based-Comparison【Lee-et-al-2024】：" class="headerlink" title="2. Tournament-based Comparison【Lee et al., 2024】："></a>2. <strong>Tournament-based Comparison</strong>【Lee et al., 2024】：</h6><ul><li>采用锦标赛式的比较方法，构建树状结构逐层筛选最佳</li></ul><p>候选。</p><ul><li>创新点在于结合拒绝采样和多轮比较，减少了低质量候选的影响。</li><li>作者探讨了不同树结构设计对评估效率和准确性的影响。</li><li>实验结果显示，该方法在多候选任务中显著提高了计算效率。</li><li>Tournament-based Comparison展示了基于结构化比较的潜在优势。</li></ul><hr><h3 id="5-应用场景"><a href="#5-应用场景" class="headerlink" title="5. 应用场景"></a>5. 应用场景</h3><h4 id="概述-8"><a href="#概述-8" class="headerlink" title="概述"></a>概述</h4><p>LLM-as-a-Judge的应用场景已从最初的生成任务评估扩展到多个领域，包括评估、对齐（Alignment）、检索和推理（Reasoning）。这一部分系统性地介绍这些应用场景，讨论每种应用的具体任务和代表性研究。</p><hr><h4 id="5-1-评估"><a href="#5-1-评估" class="headerlink" title="5.1 评估"></a>5.1 评估</h4><h5 id="概述-9"><a href="#概述-9" class="headerlink" title="概述"></a>概述</h5><p>LLM-as-a-Judge最初的核心应用是评估任务，包括开放式生成任务（如对话生成、摘要生成）、推理任务，以及其他新兴任务。通过LLM评估，能够更精准地捕捉复杂生成任务中的质量、相关性及逻辑性等维度。</p><h6 id="1-MD-Judge【Li-et-al-2024f】："><a href="#1-MD-Judge【Li-et-al-2024f】：" class="headerlink" title="1. MD-Judge【Li et al., 2024f】："></a>1. <strong>MD-Judge</strong>【Li et al., 2024f】：</h6><ul><li>提出了专门针对安全性相关问答的评估框架，用于检测LLM在生成敏感内容时的可靠性。</li><li>创新点在于设计了多维度的安全性评估标准，包括潜在伤害性、道德风险以及语言误导性。</li><li>作者通过对比多个LLM的评估能力，验证了MD-Judge框架的鲁棒性。</li><li>此框架在评估复杂场景（如恶意问题）的生成效果方面表现突出。</li><li>MD-Judge为生成模型的安全性评估提供了一个新的基准。</li></ul><h6 id="2-Chan框架【Chan-et-al-2023】："><a href="#2-Chan框架【Chan-et-al-2023】：" class="headerlink" title="2. Chan框架【Chan et al., 2023】："></a>2. <strong>Chan框架</strong>【Chan et al., 2023】：</h6><ul><li>提出了一个多代理辩论框架，通过让多个LLM角色分别生成答案并彼此评估，提升生成任务的评估质量。</li><li>创新点在于设计了角色分工机制，不同模型在辩论中扮演不同的立场，从多角度评估候选答案。</li><li>研究表明，该框架能够显著提升评估结果的细粒度和多样性。</li><li>作者还探讨了模型间的交互如何影响评估的一致性和公平性。</li><li>Chan框架在开放式文本生成任务中的应用表明，模型之间的协作能够显著改进评估质量。</li></ul><h6 id="3-ICE【Jain-et-al-2023b】："><a href="#3-ICE【Jain-et-al-2023b】：" class="headerlink" title="3. ICE【Jain et al., 2023b】："></a>3. <strong>ICE</strong>【Jain et al., 2023b】：</h6><ul><li>提出了交互式多维评估框架，通过少量上下文示例指导LLM评估。</li><li>创新点在于将评估任务分解为多个独立维度，每个维度都有针对性的示例支持。</li><li>研究表明，ICE框架显著减少了模型在多维任务中的评估偏差。</li><li>实验结果显示，其生成的评估结果在与人工评价的一致性上达到高水平。</li><li>ICE为多维度评估任务的提示设计提供了新思路。</li></ul><hr><h4 id="5-2-对齐（Alignment）"><a href="#5-2-对齐（Alignment）" class="headerlink" title="5.2 对齐（Alignment）"></a>5.2 对齐（Alignment）</h4><h5 id="概述-10"><a href="#概述-10" class="headerlink" title="概述"></a>概述</h5><p>对齐任务的目标是通过训练或微调使LLM的生成内容更符合人类的价值观和偏好。LLM-as-a-Judge被广泛用于生成对齐数据和评估对齐效果。</p><h6 id="1-Constitutional-AI【Bai-et-al-2022】：-1"><a href="#1-Constitutional-AI【Bai-et-al-2022】：-1" class="headerlink" title="1. Constitutional AI【Bai et al., 2022】："></a>1. <strong>Constitutional AI</strong>【Bai et al., 2022】：</h6><ul><li>提出了基于原则对齐的框架，通过定义帮助性、无害性和诚实性等原则，优化生成模型的输出。</li><li>创新点在于将原则融入奖励建模过程，利用LLM生成的偏好信号构建对齐数据集。</li><li>作者通过多轮实验验证了这种基于规则的对齐方法对生成质量的显著提升。</li><li>此框架适用于各种生成任务，尤其在减少有害输出方面效果显著。</li><li>Constitutional AI的成功展示了基于规则的对齐方法的潜力。</li></ul><h6 id="2-DIRECT-RLAIF【Lee-et-al-2023】："><a href="#2-DIRECT-RLAIF【Lee-et-al-2023】：" class="headerlink" title="2. DIRECT-RLAIF【Lee et al., 2023】："></a>2. <strong>DIRECT-RLAIF</strong>【Lee et al., 2023】：</h6><ul><li>提出了一种直接强化学习对齐反馈（DIRECT-RLAIF）方法，通过较大的LLM生成偏好信号指导较小模型。</li><li>核心创新点在于利用较强的LLM模型作为动态评估者，避免传统奖励模型中存在的“奖励陈旧性”问题。</li><li>作者验证了这种方法在对齐生成任务中的有效性，特别是在开放式对话中的显著改进。</li><li>DIRECT-RLAIF为更高效的对齐方法提供了理论基础。</li><li>研究结果表明，这种方法可以在较少人工干预的情况下生成符合人类偏好的内容。</li></ul><h6 id="3-OAIF【Guo-et-al-2024】："><a href="#3-OAIF【Guo-et-al-2024】：" class="headerlink" title="3. OAIF【Guo et al., 2024】："></a>3. <strong>OAIF</strong>【Guo et al., 2024】：</h6><ul><li>提出了在线AI反馈（Online AI Feedback, OAIF）框架，通过实时原则指导提升模型评估的灵活性。</li><li>核心创新点在于动态调整评估规则，使模型能够适应多变的任务需求。</li><li>OAIF引入了细粒度的多维评分策略，为每个候选项生成独立的评估报告。</li><li>作者验证了这种方法在实时决策中的潜力，尤其在对话和生成任务中表现突出。</li><li>OAIF展现了规则增强的实时适应能力，为实时评估任务提供了新方向。</li></ul><hr><h4 id="5-3-检索（Retrieval）"><a href="#5-3-检索（Retrieval）" class="headerlink" title="5.3 检索（Retrieval）"></a>5.3 检索（Retrieval）</h4><h5 id="概述-11"><a href="#概述-11" class="headerlink" title="概述"></a>概述</h5><p>在检索场景中，LLM-as-a-Judge主要用于提升文档排序的精度和检索增强生成（RAG）的效果。通过更高效的排序算法，LLM能够在传统检索和复杂生成任务中提供更高质量的相关性评估。</p><h6 id="1-Ranked-Pairing【Zhai-et-al-2024】：-1"><a href="#1-Ranked-Pairing【Zhai-et-al-2024】：-1" class="headerlink" title="1. Ranked Pairing【Zhai et al., 2024】："></a>1. <strong>Ranked Pairing</strong>【Zhai et al., 2024】：</h6><ul><li>提出了一种基于基线比较的排序方法，通过对所有候选项与基线进行比较确定优劣。</li><li>创新点在于避免传统两两比较的高计算开销，显著提高了评估效率。</li><li>作者还设计了一种自适应比较策略，进一步优化排序性能。</li><li>研究表明，Ranked Pairing在大规模排序任务中表现出极高的效率。</li><li>此方法特别适用于需要快速生成排序结果的场景。</li></ul><h6 id="2-LLM-Eval【Lin-and-Chen-2023a】："><a href="#2-LLM-Eval【Lin-and-Chen-2023a】：" class="headerlink" title="2. LLM-Eval【Lin and Chen, 2023a】："></a>2. <strong>LLM-Eval</strong>【Lin and Chen, 2023a】：</h6><ul><li>提出了在对话生成中的相关性评估框架，利用LLM替代人工标注。</li><li>创新点在于设计了结合上下文和生成内容的提示技术，确保评估更加精确。</li><li>作者通过对比实验验证了LLM在会话相关性评估中的潜力，结果与人工标注高度一致。</li><li>此框架显著减少了评估成本，同时提升了效率。</li><li>LLM-Eval在对话生成任务中的应用表明，模型在生成评估中的角色日益重要。</li></ul><h6 id="3-ToT-Tree-of-Thought-【Yao-et-al-2023a】："><a href="#3-ToT-Tree-of-Thought-【Yao-et-al-2023a】：" class="headerlink" title="3. **ToT (Tree of Thought)**【Yao et al., 2023a】："></a>3. **ToT (Tree of Thought)**【Yao et al., 2023a】：</h6><ul><li>提出了通过树状结构增强推理能力的方法，并结合LLM进行评估。</li><li>创新点在于引入了状态评估模块，通过逐步筛选最优推理路径提升检索和生成任务的精度。</li><li>研究表明，ToT框架显著提升了复杂任务的解决能力，尤其在多步推理和决策中表现优异。</li><li>作者还提出了评估路径的动态调整机制，使LLM能够更灵活地应对多样化任务。</li><li>ToT验证了结构化评估框架在复杂任务中的有效性。</li></ul><hr><h4 id="5-4-推理（Reasoning）"><a href="#5-4-推理（Reasoning）" class="headerlink" title="5.4 推理（Reasoning）"></a>5.4 推理（Reasoning）</h4><h5 id="概述-12"><a href="#概述-12" class="headerlink" title="概述"></a>概述</h5><p>推理任务的核心是评估LLM的中间推理过程和最终答案的正确性。LLM-as-a-Judge在数学推理、时间推理和复杂逻辑推理任务中展示了显著的评估能力。</p><h6 id="1-HALU-J【Wang-et-al-2024a】："><a href="#1-HALU-J【Wang-et-al-2024a】：" class="headerlink" title="1. HALU-J【Wang et al., 2024a】："></a>1. <strong>HALU-J</strong>【Wang et al., 2024a】：</h6><ul><li>提出了一种基于批评的偏好学习方法，专注于选择相关证据并生成详细批评。</li><li>创新点在于设计了多证据选择机制，提高了LLM的可靠性评估能力。</li><li>该方法通过Directed Preference Optimization（DPO）进行优化，使模型能够更准确地判断任务间的优劣。</li><li>HALU-J还结合了上下文推理，扩展了偏好学习的应用场景。</li><li>实验表明，HALU-J显著提升了复杂任务的评估准确性，尤其是在事实性和逻辑性判断上。</li></ul><h6 id="2-KIEval【Yu-et-al-2024】："><a href="#2-KIEval【Yu-et-al-2024】：" class="headerlink" title="2. KIEval【Yu et al., 2024】："></a>2. <strong>KIEval</strong>【Yu et al., 2024】：</h6><ul><li>提出了知识交互式评估框架，通过动态问答生成丰富的上下文信息。</li><li>创新点在于引入了“交互者”角色，模拟用户和模型之间的动态交互。</li><li>作者设计了一种鲁棒性检测机制，避免因上下文污染导致的错误评估。</li><li>研究表明，KIEval在复杂任务中的表现优于传统静态评估方法。</li><li>此框架适用于多维度评估，特别是在需要动态调整上下文的场景中。</li></ul><hr><h3 id="6-评估基准"><a href="#6-评估基准" class="headerlink" title="6. 评估基准"></a>6. 评估基准</h3><h4 id="概述-13"><a href="#概述-13" class="headerlink" title="概述"></a>概述</h4><p>评估基准是验证LLM-as-a-Judge能力的重要工具。本节整理并介绍当前用于不同评估维度的基准，包括有用性、无害性、可靠性等方面的具体框架和其核心思想。这些基准覆盖了从对话生成到复杂任务推理的广泛应用场景，为后续研究提供了关键数据支持。</p><hr><h5 id="6-1-综合评估基准"><a href="#6-1-综合评估基准" class="headerlink" title="6.1 综合评估基准"></a>6.1 综合评估基准</h5><h6 id="1-SORRY-Bench【Xie-et-al-2024a】："><a href="#1-SORRY-Bench【Xie-et-al-2024a】：" class="headerlink" title="1. SORRY-Bench【Xie et al., 2024a】："></a>1. <strong>SORRY-Bench</strong>【Xie et al., 2024a】：</h6><ul><li>设计了一个专注于安全性和无害性评估的综合基准，重点测试LLM对潜在有害内容的拒绝能力。</li><li>创新点在于提供了一个多模型对比框架，包括开源和专有LLM的表现分析。</li><li>基准数据集涵盖多种潜在危险场景，如政治敏感内容和虚假信息生成。</li><li>作者还引入了动态拒绝率作为衡量指标，展示了不同模型在拒绝任务中的细粒度表现。</li><li>实验表明，小型LLM经过微调后可以在安全性评估中达到与大型模型相当的水平。</li></ul><h6 id="2-HalluJudge【Luo-et-al-2024】："><a href="#2-HalluJudge【Luo-et-al-2024】：" class="headerlink" title="2. HalluJudge【Luo et al., 2024】："></a>2. <strong>HalluJudge</strong>【Luo et al., 2024】：</h6><ul><li>提出了一个专门用于对话级事实性评估的基准，涵盖大规模对话数据集。</li><li>核心创新在于设计了一种细粒度的事实性评分机制，通过引入上下文验证生成内容的准确性。</li><li>数据集中包括多种类型的事实性错误，如数据遗漏、模糊表述和直接虚假信息。</li><li>HalluJudge还整合了自动化和人工评估方法，提高了基准的覆盖面和可靠性。</li><li>实验结果表明，HalluJudge能够显著提高LLM在对话场景中的事实性检测能力。</li></ul><hr><h5 id="6-2-专用领域评估基准"><a href="#6-2-专用领域评估基准" class="headerlink" title="6.2 专用领域评估基准"></a>6.2 专用领域评估基准</h5><h6 id="1-FaithScore【Jing-et-al-2024】："><a href="#1-FaithScore【Jing-et-al-2024】：" class="headerlink" title="1. FaithScore【Jing et al., 2024】："></a>1. <strong>FaithScore</strong>【Jing et al., 2024】：</h6><ul><li>FaithScore是第一个跨模态的可靠性评估框架，适用于文本和图像生成任务。</li><li>创新点在于设计了多模态评估方法，结合语言和视觉信号来验证生成内容的真实性。</li><li>数据集覆盖了从事实描述到跨模态推理的多个任务，测试了模型的全局一致性和细节准确性。</li><li>FaithScore还引入了多阶段评分机制，逐步分解任务以提高评估的精细化程度。</li><li>实验显示，FaithScore在多模态生成任务中的评估结果与人工评分高度一致。</li></ul><h6 id="2-GEMBA【Kocmi-and-Federmann-2023】："><a href="#2-GEMBA【Kocmi-and-Federmann-2023】：" class="headerlink" title="2. GEMBA【Kocmi and Federmann, 2023】："></a>2. <strong>GEMBA</strong>【Kocmi and Federmann, 2023】：</h6><ul><li>GEMBA基准专注于机器翻译和文本摘要任务的整体质量评估。</li><li>核心创新点在于结合BLEU等传统指标和LLM生成的综合评分，提供更全面的评估结果。</li><li>数据集中包含多种语言和领域的真实文本，覆盖多样化的任务需求。</li><li>作者设计了一种动态反馈机制，允许LLM在评估过程中进行自适应调整。</li><li>GEMBA基准的引入显著推动了机器翻译和摘要任务中LLM-as-a-Judge的应用。</li></ul><h6 id="3-Just-Eval【Lin-et-al-2023】："><a href="#3-Just-Eval【Lin-et-al-2023】：" class="headerlink" title="3. Just-Eval【Lin et al., 2023】："></a>3. <strong>Just-Eval</strong>【Lin et al., 2023】：</h6><ul><li>提出了一个基于生成内容有用性和无害性的综合基准，适用于广泛的开放式任务。</li><li>创新点在于为不同任务设计了定制化的评估标准，并结合多维评分系统生成最终评价。</li><li>数据集中涵盖了对话、问答和复杂推理等任务，验证了基准的通用性。</li><li>作者还分析了模型在不同任务和领域上的表现，提供了详细的对比结果。</li><li>Just-Eval的应用表明，评估框架需要结合任务特点进行优化，才能最大化评估的准确性。</li></ul><hr><h4 id="6-3-动态评估基准"><a href="#6-3-动态评估基准" class="headerlink" title="6.3 动态评估基准"></a>6.3 动态评估基准</h4><h6 id="1-RevisEval【Zhang-et-al-2024e】："><a href="#1-RevisEval【Zhang-et-al-2024e】：" class="headerlink" title="1. RevisEval【Zhang et al., 2024e】："></a>1. <strong>RevisEval</strong>【Zhang et al., 2024e】：</h6><ul><li>RevisEval通过引入动态自我修正机制，让LLM在生成评估之前对输出进行多次调整。</li><li>核心创新在于结合LLM的自我纠错能力，将最终输出用于多维度评估。</li><li>数据集中覆盖了对话生成、摘要和复杂推理任务，验证了基准的动态适应能力。</li><li>RevisEval引入了多轮反馈机制，允许模型在评估过程中迭代改进。</li><li>实验结果表明，动态评估能够显著提升复杂任务中评估的精确性和稳定性。</li></ul><h6 id="2-Meta-ranking【Liu-et-al-2024c】："><a href="#2-Meta-ranking【Liu-et-al-2024c】：" class="headerlink" title="2. Meta-ranking【Liu et al., 2024c】："></a>2. <strong>Meta-ranking</strong>【Liu et al., 2024c】：</h6><ul><li>Meta-ranking框架通过弱模型生成初步排序，再由强模型进行最终评估。</li><li>创新点在于使用多阶段的排名方法，提高评估效率并降低计算开销。</li><li>数据集中包含了多种任务类型，并通过实验验证了Meta-ranking的通用性。</li><li>该框架特别适用于大规模排序任务，显著减少了评估时间。</li><li>Meta-ranking展示了弱模型和强模型协作评估的潜力，是多模型评估的新方向。</li></ul><hr><h3 id="7-挑战与未来方向"><a href="#7-挑战与未来方向" class="headerlink" title="7. 挑战与未来方向"></a>7. 挑战与未来方向</h3><h4 id="概述-14"><a href="#概述-14" class="headerlink" title="概述"></a>概述</h4><p>尽管LLM-as-a-Judge在评估任务中展现了强大能力，但依然面临着多方面的挑战。主要问题包括评估偏差与脆弱性、动态与复杂任务中的适应性，以及人机协同评估的潜力。本节探讨这些挑战并提出未来的研究方向。</p><hr><h5 id="7-1-偏差与脆弱性"><a href="#7-1-偏差与脆弱性" class="headerlink" title="7.1 偏差与脆弱性"></a>7.1 偏差与脆弱性</h5><h6 id="1-OffsetBias【Park-et-al-2024】："><a href="#1-OffsetBias【Park-et-al-2024】：" class="headerlink" title="1. OffsetBias【Park et al., 2024】："></a>1. <strong>OffsetBias</strong>【Park et al., 2024】：</h6><ul><li>OffsetBias通过设计一个去偏优化框架，减少LLM在评估任务中的位置偏差和内容偏见。</li><li>创新点在于使用合成数据生成“坏”样本，通过训练模型识别并修正偏差。</li><li>作者提出了一种多维度的去偏学习机制，确保评估在不同场景下的一致性。</li><li>研究表明，OffsetBias能够显著降低模型在生成任务中的不公平表现。</li><li>此方法为减少LLM评估中的偏差问题提供了重要方向。</li></ul><h6 id="2-SORRY-Bench【Xie-et-al-2024a】："><a href="#2-SORRY-Bench【Xie-et-al-2024a】：" class="headerlink" title="2. SORRY-Bench【Xie et al., 2024a】："></a>2. <strong>SORRY-Bench</strong>【Xie et al., 2024a】：</h6><ul><li>进一步研究了模型在拒绝有害内容时可能出现的误拒绝问题。</li><li>创新点在于结合动态评分机制和拒绝数据集，分析模型在多种任务中的拒绝倾向。</li><li>作者指出，小型模型在特定场景中可能比大型模型更高效。</li><li>实验结果表明，SORRY-Bench能够帮助识别并减轻评估偏差。</li><li>此基准成为探讨评估脆弱性的一个重要工具。</li></ul><hr><h5 id="7-2-动态与复杂评估"><a href="#7-2-动态与复杂评估" class="headerlink" title="7.2 动态与复杂评估"></a>7.2 动态与复杂评估</h5><h6 id="1-Tree-of-Thought-ToT-【Yao-et-al-2023a】："><a href="#1-Tree-of-Thought-ToT-【Yao-et-al-2023a】：" class="headerlink" title="1. **Tree of Thought (ToT)**【Yao et al., 2023a】："></a>1. **Tree of Thought (ToT)**【Yao et al., 2023a】：</h6><ul><li>ToT通过树状结构优化复杂任务的多步推理和评估。</li><li>创新点在于结合动态状态评估机制，使评估更加适应复杂多变的任务需求。</li><li>数据集中覆盖了需要多步推理的复杂任务，如问答和决策优化。</li><li>实验表明，ToT框架显著提升了复杂任务的解决能力和评估准确性。</li><li>该研究为动态评估提供了新的理论和实践支持。</li></ul><h6 id="2-RAIN【Li-et-al-2024】："><a href="#2-RAIN【Li-et-al-2024】：" class="headerlink" title="2. RAIN【Li et al., 2024】："></a>2. <strong>RAIN</strong>【Li et al., 2024】：</h6><ul><li>RAIN提出了可回溯的自回归推理机制，让LLM能够在评估过程中动态修正错误。</li><li>创新点在于结合自我评估和多轮推理机制，确保最终输出的高质量。</li><li>作者还设计了一种动态调整机制，使模型能够适应不同任务的变化。</li><li>实验显示，RAIN在复杂任务中的评估能力优于传统静态方法。</li><li>此框架展示了动态评估在复杂场景中的潜力。</li></ul><hr><h5 id="7-3-自我评估与人机协同"><a href="#7-3-自我评估与人机协同" class="headerlink" title="7.3 自我评估与人机协同"></a>7.3 自我评估与人机协同</h5><h6 id="1-Self-Taught-Evaluators【Wang-et-al-2024f】："><a href="#1-Self-Taught-Evaluators【Wang-et-al-2024f】：" class="headerlink" title="1. Self-Taught Evaluators【Wang et al., 2024f】："></a>1. <strong>Self-Taught Evaluators</strong>【Wang et al., 2024f】：</h6><ul><li>提出了一种自我学习框架，模型通过生成低质量数据对自身进行动态优化。</li><li>创新点在于引入了一种动态评估机制，让模型能够逐步提升自身评估能力。</li><li>数据集中包括了多种类型的任务，为自我评估提供了广泛支持。</li><li>Self-Taught Evaluators展示了模型在无需人工干预情况下的自我提升能力。</li><li>此框架为自动化评估任务提供了新思路。</li></ul><h6 id="2-Meta-Rewarding【Wu-et-al-2024】："><a href="#2-Meta-Rewarding【Wu-et-al-2024】：" class="headerlink" title="2. Meta-Rewarding【Wu et al., 2024】："></a>2. <strong>Meta-Rewarding</strong>【Wu et al., 2024】：</h6><ul><li>Meta-Rewarding通过将LLM的自评估信号作为偏好数据，用于进一步优化模型。</li><li>创新点在于结合策略模型自我反馈，增强模型的自适应能力。</li><li>作者还探讨了如何动态调整评估策略以提高鲁棒性。</li><li>实验表明，Meta-Rewarding能够显著提升复杂任务中的评估效果。</li><li>该研究展示了人机协同评估的潜在优势。</li></ul><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;hr&gt;
&lt;h1 id=&quot;基于生成的大语言模型（LLM）评估：从生成到判断的机遇与挑战&quot;&gt;&lt;a href=&quot;#基于生成的大语言模型（LLM）评估：从生成到判断的机遇与挑战&quot; class=&quot;headerlink&quot; title=&quot;基于生成的大语言模型（LLM）评估：从生成到判断的机遇</summary>
      
    
    
    
    <category term="NLP Insights" scheme="https://chenhuiyu.github.io/categories/NLP-Insights/"/>
    
    
    <category term="Onnx" scheme="https://chenhuiyu.github.io/tags/Onnx/"/>
    
    <category term="Deployment" scheme="https://chenhuiyu.github.io/tags/Deployment/"/>
    
  </entry>
  
  <entry>
    <title>基于生成的大语言模型（LLM）评估：从生成到判断的机遇与挑战</title>
    <link href="https://chenhuiyu.github.io/2024/12/06/NLP%20Insights/%E5%9F%BA%E4%BA%8E%E7%94%9F%E6%88%90%E7%9A%84%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%88LLM%EF%BC%89%E8%AF%84%E4%BC%B0%EF%BC%9A%E4%BB%8E%E7%94%9F%E6%88%90%E5%88%B0%E5%88%A4%E6%96%AD%E7%9A%84%E6%9C%BA%E9%81%87%E4%B8%8E%E6%8C%91%E6%88%98/"/>
    <id>https://chenhuiyu.github.io/2024/12/06/NLP%20Insights/%E5%9F%BA%E4%BA%8E%E7%94%9F%E6%88%90%E7%9A%84%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%88LLM%EF%BC%89%E8%AF%84%E4%BC%B0%EF%BC%9A%E4%BB%8E%E7%94%9F%E6%88%90%E5%88%B0%E5%88%A4%E6%96%AD%E7%9A%84%E6%9C%BA%E9%81%87%E4%B8%8E%E6%8C%91%E6%88%98/</id>
    <published>2024-12-06T06:34:18.000Z</published>
    <updated>2026-02-20T21:51:54.082Z</updated>
    
    <content type="html"><![CDATA[<hr><h1 id="基于生成的大语言模型（LLM）评估：从生成到判断的机遇与挑战"><a href="#基于生成的大语言模型（LLM）评估：从生成到判断的机遇与挑战" class="headerlink" title="基于生成的大语言模型（LLM）评估：从生成到判断的机遇与挑战"></a>基于生成的大语言模型（LLM）评估：从生成到判断的机遇与挑战</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>人工智能（AI）与自然语言处理（NLP）领域中的评估任务长期面临挑战。传统的评估方法（如基于匹配或嵌入的技术）在判断复杂属性时效果有限。近期大语言模型（LLM）的发展催生了“LLM-as-a-Judge”范式，利用LLM对任务进行评分、排序或选择。本论文对LLM评估方法进行了全面综述，包括其定义、分类框架、评估基准，以及未来的研究方向。</p><hr><h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><h3 id="1-1-背景"><a href="#1-1-背景" class="headerlink" title="1.1 背景"></a>1.1 背景</h3><p>评估是机器学习和NLP的核心问题之一，传统评估方法如BLEU和ROUGE通常基于文本重叠，缺乏对复杂场景的适用性。随着深度学习和LLM的发展（如GPT-4），研究者提出了“LLM-as-a-Judge”模式，以解决传统评估的局限。</p><h3 id="1-2-研究问题"><a href="#1-2-研究问题" class="headerlink" title="1.2 研究问题"></a>1.2 研究问题</h3><p>本论文旨在探讨以下问题：</p><ul><li><strong>评估内容：LLM评估什么？</strong></li><li><strong>评估方法：如何进行评估？</strong></li><li><strong>应用场景：LLM在哪里评估？</strong></li></ul><hr><h2 id="2-预备知识"><a href="#2-预备知识" class="headerlink" title="2. 预备知识"></a>2. 预备知识</h2><h3 id="2-1-输入格式"><a href="#2-1-输入格式" class="headerlink" title="2.1 输入格式"></a>2.1 输入格式</h3><p>评估输入可分为：</p><ul><li><strong>点对点（Point-Wise）</strong>：单个样本评估。</li><li><strong>对/列表评估（Pair/List-Wise）</strong>：多个样本的比较评估。</li></ul><h3 id="2-2-输出格式"><a href="#2-2-输出格式" class="headerlink" title="2.2 输出格式"></a>2.2 输出格式</h3><p>评估输出包括：</p><ul><li><strong>评分（Score）</strong>：对样本进行量化评分。</li><li><strong>排序（Ranking）</strong>：根据优劣排序。</li><li><strong>选择（Selection）</strong>：从多个候选中选取最佳方案。</li></ul><hr><h2 id="3-评估属性"><a href="#3-评估属性" class="headerlink" title="3. 评估属性"></a>3. 评估属性</h2><h3 id="3-1-有用性（Helpfulness）"><a href="#3-1-有用性（Helpfulness）" class="headerlink" title="3.1 有用性（Helpfulness）"></a>3.1 有用性（Helpfulness）</h3><p>LLM通过指导用户任务和生成反馈，对响应的有用性进行评估。这在AI对齐（Alignment）中尤为重要。</p><h3 id="3-2-无害性（Harmlessness）"><a href="#3-2-无害性（Harmlessness）" class="headerlink" title="3.2 无害性（Harmlessness）"></a>3.2 无害性（Harmlessness）</h3><p>评估文本的无害性是生成安全内容的关键。LLM可辅助数据标注或直接评估潜在的有害内容。</p><h3 id="3-3-可靠性（Reliability）"><a href="#3-3-可靠性（Reliability）" class="headerlink" title="3.3 可靠性（Reliability）"></a>3.3 可靠性（Reliability）</h3><p>LLM可检测事实性和一致性。例如，通过生成辅助证据或进行对话级别的可靠性评估。</p><h3 id="3-4-相关性（Relevance）"><a href="#3-4-相关性（Relevance）" class="headerlink" title="3.4 相关性（Relevance）"></a>3.4 相关性（Relevance）</h3><p>LLM可评估生成或检索内容的相关性，适用于会话、检索增强生成（RAG）等场景。</p><h3 id="3-5-可行性（Feasibility）"><a href="#3-5-可行性（Feasibility）" class="headerlink" title="3.5 可行性（Feasibility）"></a>3.5 可行性（Feasibility）</h3><p>在复杂任务中，LLM可对候选步骤或行动进行可行性判断，从而优化决策路径。</p><h3 id="3-6-总体质量（Overall-Quality）"><a href="#3-6-总体质量（Overall-Quality）" class="headerlink" title="3.6 总体质量（Overall Quality）"></a>3.6 总体质量（Overall Quality）</h3><p>LLM通过多维度评分生成整体评价，适用于生成任务的综合比较。</p><hr><h3 id="4-方法论"><a href="#4-方法论" class="headerlink" title="4. 方法论"></a>4. 方法论</h3><h4 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h4><p>方法论部分主要探讨如何优化LLM作为评估者（LLM-as-a-Judge）的能力，从调优和提示技术两个方面进行阐述：</p><ol><li><strong>调优技术</strong>：通过监督微调（SFT）和偏好学习等方法，利用人工标注数据或合成反馈来增强LLM的判断能力。</li><li><strong>提示技术</strong>：设计高效的提示策略（如操作交换、规则增强、多代理协作等）以提升LLM在推理和评估过程中的准确性和可靠性。</li></ol><hr><h4 id="4-1-调优技术"><a href="#4-1-调优技术" class="headerlink" title="4.1 调优技术"></a>4.1 调优技术</h4><h5 id="数据来源"><a href="#数据来源" class="headerlink" title="数据来源"></a>数据来源</h5><h6 id="1-人工标注数据"><a href="#1-人工标注数据" class="headerlink" title="1. 人工标注数据"></a>1. <strong>人工标注数据</strong></h6><p>人工标注数据提供了高质量的训练样本，帮助LLM学习人类偏好。以下是核心研究及其创新点：</p><ol><li><p><strong>PandaLM</strong>【Wang et al., 2024h】：</p><ul><li>PandaLM项目收集了多样化的人工标注数据集，涵盖指令生成任务的300,000个样本。</li><li>作者通过整合多种数据源（如开放领域问答和对话生成）来增强模型的泛化能力。</li><li>该研究的关键创新在于引入了标准化的标注流程，以确保数据质量与一致性。</li><li>此外，PandaLM强调多语言支持，通过跨文化的数据标注提高模型的适用性。</li><li>最终，PandaLM被证明在多个评估任务上表现优异，其输出与人工评估高度相关。</li></ul></li><li><p><strong>AspectInstruct</strong>【Liu et al., 2024a】：</p><ul><li>该研究首次提出了一个针对多维度评估的指令调优数据集，涵盖65个任务和27个评估维度。</li><li>数据集中包含对话生成、摘要和数据到文本转换等复杂任务的多方面评分。</li><li>作者设计了独特的任务分割机制，使模型能够根据上下文理解并优先评估特定维度。</li><li>研究的亮点在于数据集的多样性和全面性，为多任务评估提供了新的基准。</li><li>最终，该数据集显著提升了LLM在不同评估场景中的多维度理解和评估能力。</li></ul></li></ol><h6 id="2-合成数据"><a href="#2-合成数据" class="headerlink" title="2. 合成数据"></a>2. <strong>合成数据</strong></h6><p>合成数据通过LLM生成训练样本，减少了对人工标注的依赖，同时扩展了数据覆盖范围。以下是核心研究及其创新点：</p><ol><li><p><strong>JudgeLM</strong>【Zhu et al., 2023】：</p><ul><li>研究者利用GPT-4生成包含任务种子、生成答案及相关评估的高质量数据集。</li><li>数据集中包含10万个样本，覆盖了指令生成任务的多种场景。</li><li>核心创新点在于引入了生成任务种子的方法，确保生成数据的多样性和针对性。</li><li>作者还设计了一种基于偏好学习的优化方法，以提高LLM对细粒度任务的判断能力。</li><li>研究表明，经过这种优化后的JudgeLM在多个基准测试中超越了传统方法。</li></ul></li><li><p><strong>Meta-Rewarding</strong>【Wu et al., 2024】：</p><ul><li>提出了一种新颖的“元奖励”（Meta-Rewarding）方法，通过LLM自我评估生成的判断信号增强训练效果。</li><li>该方法要求模型在生成答案后对自己的输出进行评分，从而生成偏好数据。</li><li>创新点在于采用策略模型作为评估者，显著提高了数据生成效率和质量。</li><li>此外，该研究通过逐步改进的偏好数据训练LLM，提高了其评估任务的鲁棒性。</li><li>最终，Meta-Rewarding展示了LLM自我增强能力的潜力，成为偏好学习领域的重要进展。</li></ul></li></ol><h5 id="调优方法"><a href="#调优方法" class="headerlink" title="调优方法"></a>调优方法</h5><h6 id="1-监督微调（SFT）"><a href="#1-监督微调（SFT）" class="headerlink" title="1. 监督微调（SFT）"></a>1. <strong>监督微调（SFT）</strong></h6><p>监督微调通过使用人工标注或合成数据，让LLM从示例中学习判断标准。以下是核心研究及其创新点：</p><ol><li><p><strong>FLAMe</strong>【Vu et al., 2024】：</p><ul><li>该研究提出了Foundational Large Autorater Models (FLAMe)，利用超过500万个样本进行大规模多任务监督微调。</li><li>FLAMe在多任务数据中引入了统一的评价标准，提高了模型在多样化任务中的评估能力。</li><li>创新点在于采用多任务学习框架，将多个评估维度集成到一个模型中。</li><li>作者还设计了任务分层训练策略，使模型能够逐步掌握复杂的评估任务。</li><li>实验结果表明，FLAMe在多个生成任务上的表现优于传统评估指标。</li></ul></li><li><p><strong>JSFT</strong>【Lee et al., 2024】：</p><ul><li>提出了Judge-augmented Supervised Fine-Tuning（JSFT）方法，通过扩展偏好学习数据增强微调效果。</li><li>数据集中包含点对点和对比评估任务，以全面覆盖多种评估场景。</li><li>创新点在于引入了多阶段训练策略，结合监督学习和偏好学习优化模型性能。</li><li>此外，研究者设计了简化提示机制，显著提高了模型处理复杂输入的能力。</li><li>JSFT的实验结果显示，其生成的评估结果在多个基准上超过了现有方法。</li></ul></li></ol><h6 id="2-偏好学习"><a href="#2-偏好学习" class="headerlink" title="2. 偏好学习"></a>2. <strong>偏好学习</strong></h6><p>偏好学习通过优化LLM的比较和排序能力，适用于复杂评估任务。以下是核心研究及其创新点：</p><ol><li><p><strong>HALU-J</strong>【Wang et al., 2024a】：</p><ul><li>提出了一种基于批评的偏好学习方法，专注于选择相关证据并生成详细批评。</li><li>创新点在于设计了多证据选择机制，提高了LLM的可靠性评估能力。</li><li>该方法通过Directed Preference Optimization（DPO）进行优化，使模型能够更准确地判断任务间的优劣。</li><li>HALU-J还结合了上下文推理，扩展了偏好学习的应用场景。</li><li>实验表明，HALU-J显著提升了复杂任务的评估准确性，尤其是在事实性和逻辑性判断上。</li></ul></li><li><p><strong>Self-Taught Evaluators</strong>【Wang et al., 2024f】：</p><ul><li>该研究提出了一种自学习的评估者方法，利用被扰乱的指令生成低质量数据作为偏好学习的负样本。</li><li>自学习方法通过自动生成的次优响应，提供了丰富的训练数据。</li><li>创新点在于通过动态调整偏好信号，提升了模型的适应性和通用性。</li><li>作者还设计了基于多轮交互的学习策略，使模型能够在动态环境中自我优化。</li><li>实验结果显示，Self-Taught Evaluators在多个开放式生成任务中表现优异。</li></ul></li></ol><h3 id="4-2-提示技术"><a href="#4-2-提示技术" class="headerlink" title="4.2 提示技术"></a>4.2 提示技术</h3><h4 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h4><p>提示技术（Prompting）通过设计高效的提示策略和推理流程优化LLM的评估能力。这部分探讨如何在推理阶段利用提示技术提升判断精度，减少偏差，并增强模型的评估鲁棒性。主要方法包括操作交换、规则增强、多代理协作、演示、多轮交互以及比较加速。</p><hr><h4 id="4-2-1-操作交换（Swapping-Operation）"><a href="#4-2-1-操作交换（Swapping-Operation）" class="headerlink" title="4.2.1 操作交换（Swapping Operation）"></a>4.2.1 操作交换（Swapping Operation）</h4><h5 id="概述-2"><a href="#概述-2" class="headerlink" title="概述"></a>概述</h5><p>操作交换技术通过更改候选项顺序减少评估的偏置性，确保LLM对输入顺序不敏感，从而提高评估的公平性和可靠性。</p><h6 id="1-MT-Bench【Zheng-et-al-2023】："><a href="#1-MT-Bench【Zheng-et-al-2023】：" class="headerlink" title="1. MT-Bench【Zheng et al., 2023】："></a>1. <strong>MT-Bench</strong>【Zheng et al., 2023】：</h6><ul><li>本研究首次系统性地提出操作交换技术，通过多轮评估减少LLM的顺序敏感性。</li><li>创新点在于引入“对称性检查”机制：将候选项顺序互换，若评分结果一致，则标记为稳定，否则标记为不稳定。</li><li>作者发现操作交换能够有效减少由于位置偏差导致的错误判断。</li><li>该技术应用于多任务评估中，尤其是在复杂生成任务的排序中表现突出。</li><li>MT-Bench为后续的LLM评估技术提供了一个重要的公平性基准。</li></ul><h6 id="2-Starling【Zhu-et-al-2024a】："><a href="#2-Starling【Zhu-et-al-2024a】：" class="headerlink" title="2. Starling【Zhu et al., 2024a】："></a>2. <strong>Starling</strong>【Zhu et al., 2024a】：</h6><ul><li>提出一种类似链式推理（Chain-of-Thought, CoT）的提示技术，通过全面评估所有候选项的两两关系，再总结为最终排序。</li><li>创新点在于强制模型生成所有可能的对比结果，确保评估全面且无偏。</li><li>作者还设计了一种交叉验证机制，进一步提高评估稳定性。</li><li>实验显示，这种方法显著减少了位置偏差带来的误差，特别是在排序任务中表现优异。</li><li>Starling验证了链式思维结合操作交换技术的潜力，尤其在复杂对比任务中的效果显著。</li></ul><hr><h4 id="4-2-2-规则增强（Rule-Augmentation）"><a href="#4-2-2-规则增强（Rule-Augmentation）" class="headerlink" title="4.2.2 规则增强（Rule Augmentation）"></a>4.2.2 规则增强（Rule Augmentation）</h4><h5 id="概述-3"><a href="#概述-3" class="headerlink" title="概述"></a>概述</h5><p>规则增强技术通过在提示中嵌入明确的原则、标准或参考内容，使模型能够更加系统地评估任务，从而提升评估的准确性和一致性。</p><h6 id="1-Constitutional-AI【Bai-et-al-2022】："><a href="#1-Constitutional-AI【Bai-et-al-2022】：" class="headerlink" title="1. Constitutional AI【Bai et al., 2022】："></a>1. <strong>Constitutional AI</strong>【Bai et al., 2022】：</h6><ul><li>本研究引入了“原则驱动”的规则增强方法，利用帮助性、无害性和诚实性等标准指导模型评估。</li><li>创新点在于为每个评估维度定义详细的评分标准，并通过原则约束生成内容。</li><li>作者采用多层提示设计，使LLM能够逐步推理并给出最终评估。</li><li>实验表明，这种方法显著提升了模型在复杂场景中的判断一致性。</li><li>Constitutional AI成为后续研究的重要基石，为基于规则的评估技术奠定了基础。</li></ul><h6 id="2-OAIF【Guo-et-al-2024】："><a href="#2-OAIF【Guo-et-al-2024】：" class="headerlink" title="2. OAIF【Guo et al., 2024】："></a>2. <strong>OAIF</strong>【Guo et al., 2024】：</h6><ul><li>提出了在线AI反馈（Online AI Feedback, OAIF）框架，通过实时原则指导提升模型评估的灵活性。</li><li>核心创新点在于动态调整评估规则，使模型能够适应多变的任务需求。</li><li>OAIF引入了细粒度的多维评分策略，为每个候选项生成独立的评估报告。</li><li>作者验证了这种方法在实时决策中的潜力，尤其在对话和生成任务中表现突出。</li><li>OAIF展现了规则增强的实时适应能力，为实时评估任务提供了新方向。</li></ul><hr><h4 id="4-2-3-多代理协作（Multi-agent-Collaboration）"><a href="#4-2-3-多代理协作（Multi-agent-Collaboration）" class="headerlink" title="4.2.3 多代理协作（Multi-agent Collaboration）"></a>4.2.3 多代理协作（Multi-agent Collaboration）</h4><h5 id="概述-4"><a href="#概述-4" class="headerlink" title="概述"></a>概述</h5><p>多代理协作通过组合多个LLM的评估结果，减少单一模型的偏差，提高评估的准确性和鲁棒性。这种方法强调模型之间的角色分工和合作。</p><h6 id="1-Peer-Rank-PR-【Li-et-al-2023】："><a href="#1-Peer-Rank-PR-【Li-et-al-2023】：" class="headerlink" title="1. **Peer Rank (PR)**【Li et al., 2023】："></a>1. **Peer Rank (PR)**【Li et al., 2023】：</h6><ul><li>提出了同行排名算法，整合多个LLM的对比偏好生成最终排序。</li><li>创新点在于设计了“加权投票”机制，根据模型之间的评分一致性调整权重。</li><li>该研究还探讨了代理间的协作效率和鲁棒性，提出了优化协作路径的方法。</li><li>PR的实验结果显示，其生成的评估结果在排序准确性上优于传统单模型方法。</li><li>该研究为多模型协作技术奠定了理论基础，是后续研究的重要参考。</li></ul><h6 id="2-Cascaded-Selective-Evaluation【Jung-et-al-2024】："><a href="#2-Cascaded-Selective-Evaluation【Jung-et-al-2024】：" class="headerlink" title="2. Cascaded Selective Evaluation【Jung et al., 2024】："></a>2. <strong>Cascaded Selective Evaluation</strong>【Jung et al., 2024】：</h6><ul><li>设计了级联选择评估框架，首先由较弱的模型进行初步评估，仅在需要时调用更强大的模型。</li><li>创新点在于通过分级策略优化计算成本，同时确保评估结果的高质量。</li><li>作者提出了一种交叉验证机制，结合多个代理的结果生成最终判断。</li><li>研究表明，这种级联策略在复杂任务中表现出显著的资源效率提升。</li><li>Cascaded Selective Evaluation展示了多代理协作在资源有限情况下的潜力。</li></ul><hr><h4 id="4-2-4-演示（Demonstration）"><a href="#4-2-4-演示（Demonstration）" class="headerlink" title="4.2.4 演示（Demonstration）"></a>4.2.4 演示（Demonstration）</h4><h5 id="概述-5"><a href="#概述-5" class="headerlink" title="概述"></a>概述</h5><p>演示技术利用具体的示例作为提示，帮助LLM学习评估标准。这种方法通过少量高质量样例显著提高模型的评估能力。</p><h6 id="1-ALLURE【Hasanbeig-et-al-2023】："><a href="#1-ALLURE【Hasanbeig-et-al-2023】：" class="headerlink" title="1. ALLURE【Hasanbeig et al., 2023】："></a>1. <strong>ALLURE</strong>【Hasanbeig et al., 2023】：</h6><ul><li>提出了迭代演示技术，通过在提示中加入显著偏差的示例提高模型的鲁棒性。</li><li>创新点在于采用动态演示方法，逐步更新提示以适应不同的评估任务。</li><li>研究表明，这种方法在低资源场景中表现出色，尤其是在新任务的适应性上有显著提升。</li><li>作者还探讨了如何选择代表性样例以最大化演示效果。</li><li>ALLURE验证了高质量演示样例在提升评估能力方面的重要性。</li></ul><h6 id="2-ICE【Jain-et-al-2023b】："><a href="#2-ICE【Jain-et-al-2023b】：" class="headerlink" title="2. ICE【Jain et al., 2023b】："></a>2. <strong>ICE</strong>【Jain et al., 2023b】：</h6><ul><li>提出了交互式多维评估框架，通过少量上下文示例指导LLM评估。</li><li>创新点在于将评估任务分解为多个独立维度，每个维度都有针对性的示例支持。</li><li>研究表明，ICE框架显著减少了模型在多维任务中的评估偏差。</li><li>实验结果显示，其生成的评估结果在与人工评价的一致性上达到高水平。</li><li>ICE为多维度评估任务的提示设计提供了新思路。</li></ul><hr><h4 id="4-2-5-多轮交互（Multi-turn-Interaction）"><a href="#4-2-5-多轮交互（Multi-turn-Interaction）" class="headerlink" title="4.2.5 多轮交互（Multi-turn Interaction）"></a>4.2.5 多轮交互（Multi-turn Interaction）</h4><h5 id="概述-6"><a href="#概述-6" class="headerlink" title="概述"></a>概述</h5><p>多轮交互通过动态调整提示和上下文信息，为LLM提供更全面的评估依据，适用于需要多步推理的复杂任务。</p><h6 id="1-KIEval【Yu-et-al-2024】："><a href="#1-KIEval【Yu-et-al-2024】：" class="headerlink" title="1. KIEval【Yu et al., 2024】："></a>1. <strong>KIEval</strong>【Yu et al., 2024】：</h6><ul><li>提出了知识交互式评估框架，通过动态问答生成丰富的上下文信息。</li><li>创新点在于引入了“交互者”角色，模拟用户和模型之间的动态交互。</li><li>作者设计了一种鲁棒性检测机制，避免因上下文污染导致的错误评估。</li><li>研究表明，KIEval在复杂任务中的表现优于传统静态评估方法。</li><li>此框架适用于多维度评估，特别是在需要动态调整上下文的场景中。</li></ul><h6 id="2-Auto-Arena【Zhao-et-al-2024c】："><a href="#2-Auto-Arena【Zhao-et-al-2024c】：" class="headerlink" title="2. Auto-Arena【Zhao et al., 2024c】："></a>2. <strong>Auto-Arena</strong>【Zhao et al., 2024c】：</h6><ul><li>设计了一种多轮辩论框架，允许多个模型围绕特定任务进行交互讨论。</li><li>创新点在于结合多轮问答和动态评分机制，从不同角度对候选答案进行评估。</li><li>研究表明，这种方法能够揭示候选答案间的深层次差异。</li><li>作者还探讨了如何通过动态调整辩论内容提高评估效率。</li><li>Auto-Arena展示了多轮交互在复杂评估任务中的潜力。</li></ul><hr><h4 id="4-2-6-比较加速（Comparison-Acceleration）"><a href="#4-2-6-比较加速（Comparison-Acceleration）" class="headerlink" title="4.2.6 比较加速（Comparison Acceleration）"></a>4.2.6 比较加速（Comparison Acceleration）</h4><h5 id="概述-7"><a href="#概述-7" class="headerlink" title="概述"></a>概述</h5><p>比较加速技术通过优化比较流程，减少多候选排序任务的计算成本，提高评估效率。</p><h6 id="1-Ranked-Pairing【Zhai-et-al-2024】："><a href="#1-Ranked-Pairing【Zhai-et-al-2024】：" class="headerlink" title="1. Ranked Pairing【Zhai et al., 2024】："></a>1. <strong>Ranked Pairing</strong>【Zhai et al., 2024】：</h6><ul><li>提出了一种基于基线比较的排序方法，通过对所有候选项与基线进行比较确定优劣。</li><li>创新点在于避免传统两两比较的高计算开销，显著提高了评估效率。</li><li>作者还设计了一种自适应比较策略，进一步优化排序性能。</li><li>研究表明，Ranked Pairing在大规模排序任务中表现出极高的效率。</li><li>此方法特别适用于需要快速生成排序结果的场景。</li></ul><h6 id="2-Tournament-based-Comparison【Lee-et-al-2024】："><a href="#2-Tournament-based-Comparison【Lee-et-al-2024】：" class="headerlink" title="2. Tournament-based Comparison【Lee et al., 2024】："></a>2. <strong>Tournament-based Comparison</strong>【Lee et al., 2024】：</h6><ul><li>采用锦标赛式的比较方法，构建树状结构逐层筛选最佳</li></ul><p>候选。</p><ul><li>创新点在于结合拒绝采样和多轮比较，减少了低质量候选的影响。</li><li>作者探讨了不同树结构设计对评估效率和准确性的影响。</li><li>实验结果显示，该方法在多候选任务中显著提高了计算效率。</li><li>Tournament-based Comparison展示了基于结构化比较的潜在优势。</li></ul><hr><h3 id="5-应用场景"><a href="#5-应用场景" class="headerlink" title="5. 应用场景"></a>5. 应用场景</h3><h4 id="概述-8"><a href="#概述-8" class="headerlink" title="概述"></a>概述</h4><p>LLM-as-a-Judge的应用场景已从最初的生成任务评估扩展到多个领域，包括评估、对齐（Alignment）、检索和推理（Reasoning）。这一部分系统性地介绍这些应用场景，讨论每种应用的具体任务和代表性研究。</p><hr><h4 id="5-1-评估"><a href="#5-1-评估" class="headerlink" title="5.1 评估"></a>5.1 评估</h4><h5 id="概述-9"><a href="#概述-9" class="headerlink" title="概述"></a>概述</h5><p>LLM-as-a-Judge最初的核心应用是评估任务，包括开放式生成任务（如对话生成、摘要生成）、推理任务，以及其他新兴任务。通过LLM评估，能够更精准地捕捉复杂生成任务中的质量、相关性及逻辑性等维度。</p><h6 id="1-MD-Judge【Li-et-al-2024f】："><a href="#1-MD-Judge【Li-et-al-2024f】：" class="headerlink" title="1. MD-Judge【Li et al., 2024f】："></a>1. <strong>MD-Judge</strong>【Li et al., 2024f】：</h6><ul><li>提出了专门针对安全性相关问答的评估框架，用于检测LLM在生成敏感内容时的可靠性。</li><li>创新点在于设计了多维度的安全性评估标准，包括潜在伤害性、道德风险以及语言误导性。</li><li>作者通过对比多个LLM的评估能力，验证了MD-Judge框架的鲁棒性。</li><li>此框架在评估复杂场景（如恶意问题）的生成效果方面表现突出。</li><li>MD-Judge为生成模型的安全性评估提供了一个新的基准。</li></ul><h6 id="2-Chan框架【Chan-et-al-2023】："><a href="#2-Chan框架【Chan-et-al-2023】：" class="headerlink" title="2. Chan框架【Chan et al., 2023】："></a>2. <strong>Chan框架</strong>【Chan et al., 2023】：</h6><ul><li>提出了一个多代理辩论框架，通过让多个LLM角色分别生成答案并彼此评估，提升生成任务的评估质量。</li><li>创新点在于设计了角色分工机制，不同模型在辩论中扮演不同的立场，从多角度评估候选答案。</li><li>研究表明，该框架能够显著提升评估结果的细粒度和多样性。</li><li>作者还探讨了模型间的交互如何影响评估的一致性和公平性。</li><li>Chan框架在开放式文本生成任务中的应用表明，模型之间的协作能够显著改进评估质量。</li></ul><h6 id="3-ICE【Jain-et-al-2023b】："><a href="#3-ICE【Jain-et-al-2023b】：" class="headerlink" title="3. ICE【Jain et al., 2023b】："></a>3. <strong>ICE</strong>【Jain et al., 2023b】：</h6><ul><li>提出了交互式多维评估框架，通过少量上下文示例指导LLM评估。</li><li>创新点在于将评估任务分解为多个独立维度，每个维度都有针对性的示例支持。</li><li>研究表明，ICE框架显著减少了模型在多维任务中的评估偏差。</li><li>实验结果显示，其生成的评估结果在与人工评价的一致性上达到高水平。</li><li>ICE为多维度评估任务的提示设计提供了新思路。</li></ul><hr><h4 id="5-2-对齐（Alignment）"><a href="#5-2-对齐（Alignment）" class="headerlink" title="5.2 对齐（Alignment）"></a>5.2 对齐（Alignment）</h4><h5 id="概述-10"><a href="#概述-10" class="headerlink" title="概述"></a>概述</h5><p>对齐任务的目标是通过训练或微调使LLM的生成内容更符合人类的价值观和偏好。LLM-as-a-Judge被广泛用于生成对齐数据和评估对齐效果。</p><h6 id="1-Constitutional-AI【Bai-et-al-2022】：-1"><a href="#1-Constitutional-AI【Bai-et-al-2022】：-1" class="headerlink" title="1. Constitutional AI【Bai et al., 2022】："></a>1. <strong>Constitutional AI</strong>【Bai et al., 2022】：</h6><ul><li>提出了基于原则对齐的框架，通过定义帮助性、无害性和诚实性等原则，优化生成模型的输出。</li><li>创新点在于将原则融入奖励建模过程，利用LLM生成的偏好信号构建对齐数据集。</li><li>作者通过多轮实验验证了这种基于规则的对齐方法对生成质量的显著提升。</li><li>此框架适用于各种生成任务，尤其在减少有害输出方面效果显著。</li><li>Constitutional AI的成功展示了基于规则的对齐方法的潜力。</li></ul><h6 id="2-DIRECT-RLAIF【Lee-et-al-2023】："><a href="#2-DIRECT-RLAIF【Lee-et-al-2023】：" class="headerlink" title="2. DIRECT-RLAIF【Lee et al., 2023】："></a>2. <strong>DIRECT-RLAIF</strong>【Lee et al., 2023】：</h6><ul><li>提出了一种直接强化学习对齐反馈（DIRECT-RLAIF）方法，通过较大的LLM生成偏好信号指导较小模型。</li><li>核心创新点在于利用较强的LLM模型作为动态评估者，避免传统奖励模型中存在的“奖励陈旧性”问题。</li><li>作者验证了这种方法在对齐生成任务中的有效性，特别是在开放式对话中的显著改进。</li><li>DIRECT-RLAIF为更高效的对齐方法提供了理论基础。</li><li>研究结果表明，这种方法可以在较少人工干预的情况下生成符合人类偏好的内容。</li></ul><h6 id="3-OAIF【Guo-et-al-2024】："><a href="#3-OAIF【Guo-et-al-2024】：" class="headerlink" title="3. OAIF【Guo et al., 2024】："></a>3. <strong>OAIF</strong>【Guo et al., 2024】：</h6><ul><li>提出了在线AI反馈（Online AI Feedback, OAIF）框架，通过实时原则指导提升模型评估的灵活性。</li><li>核心创新点在于动态调整评估规则，使模型能够适应多变的任务需求。</li><li>OAIF引入了细粒度的多维评分策略，为每个候选项生成独立的评估报告。</li><li>作者验证了这种方法在实时决策中的潜力，尤其在对话和生成任务中表现突出。</li><li>OAIF展现了规则增强的实时适应能力，为实时评估任务提供了新方向。</li></ul><hr><h4 id="5-3-检索（Retrieval）"><a href="#5-3-检索（Retrieval）" class="headerlink" title="5.3 检索（Retrieval）"></a>5.3 检索（Retrieval）</h4><h5 id="概述-11"><a href="#概述-11" class="headerlink" title="概述"></a>概述</h5><p>在检索场景中，LLM-as-a-Judge主要用于提升文档排序的精度和检索增强生成（RAG）的效果。通过更高效的排序算法，LLM能够在传统检索和复杂生成任务中提供更高质量的相关性评估。</p><h6 id="1-Ranked-Pairing【Zhai-et-al-2024】：-1"><a href="#1-Ranked-Pairing【Zhai-et-al-2024】：-1" class="headerlink" title="1. Ranked Pairing【Zhai et al., 2024】："></a>1. <strong>Ranked Pairing</strong>【Zhai et al., 2024】：</h6><ul><li>提出了一种基于基线比较的排序方法，通过对所有候选项与基线进行比较确定优劣。</li><li>创新点在于避免传统两两比较的高计算开销，显著提高了评估效率。</li><li>作者还设计了一种自适应比较策略，进一步优化排序性能。</li><li>研究表明，Ranked Pairing在大规模排序任务中表现出极高的效率。</li><li>此方法特别适用于需要快速生成排序结果的场景。</li></ul><h6 id="2-LLM-Eval【Lin-and-Chen-2023a】："><a href="#2-LLM-Eval【Lin-and-Chen-2023a】：" class="headerlink" title="2. LLM-Eval【Lin and Chen, 2023a】："></a>2. <strong>LLM-Eval</strong>【Lin and Chen, 2023a】：</h6><ul><li>提出了在对话生成中的相关性评估框架，利用LLM替代人工标注。</li><li>创新点在于设计了结合上下文和生成内容的提示技术，确保评估更加精确。</li><li>作者通过对比实验验证了LLM在会话相关性评估中的潜力，结果与人工标注高度一致。</li><li>此框架显著减少了评估成本，同时提升了效率。</li><li>LLM-Eval在对话生成任务中的应用表明，模型在生成评估中的角色日益重要。</li></ul><h6 id="3-ToT-Tree-of-Thought-【Yao-et-al-2023a】："><a href="#3-ToT-Tree-of-Thought-【Yao-et-al-2023a】：" class="headerlink" title="3. **ToT (Tree of Thought)**【Yao et al., 2023a】："></a>3. **ToT (Tree of Thought)**【Yao et al., 2023a】：</h6><ul><li>提出了通过树状结构增强推理能力的方法，并结合LLM进行评估。</li><li>创新点在于引入了状态评估模块，通过逐步筛选最优推理路径提升检索和生成任务的精度。</li><li>研究表明，ToT框架显著提升了复杂任务的解决能力，尤其在多步推理和决策中表现优异。</li><li>作者还提出了评估路径的动态调整机制，使LLM能够更灵活地应对多样化任务。</li><li>ToT验证了结构化评估框架在复杂任务中的有效性。</li></ul><hr><h4 id="5-4-推理（Reasoning）"><a href="#5-4-推理（Reasoning）" class="headerlink" title="5.4 推理（Reasoning）"></a>5.4 推理（Reasoning）</h4><h5 id="概述-12"><a href="#概述-12" class="headerlink" title="概述"></a>概述</h5><p>推理任务的核心是评估LLM的中间推理过程和最终答案的正确性。LLM-as-a-Judge在数学推理、时间推理和复杂逻辑推理任务中展示了显著的评估能力。</p><h6 id="1-HALU-J【Wang-et-al-2024a】："><a href="#1-HALU-J【Wang-et-al-2024a】：" class="headerlink" title="1. HALU-J【Wang et al., 2024a】："></a>1. <strong>HALU-J</strong>【Wang et al., 2024a】：</h6><ul><li>提出了一种基于批评的偏好学习方法，专注于选择相关证据并生成详细批评。</li><li>创新点在于设计了多证据选择机制，提高了LLM的可靠性评估能力。</li><li>该方法通过Directed Preference Optimization（DPO）进行优化，使模型能够更准确地判断任务间的优劣。</li><li>HALU-J还结合了上下文推理，扩展了偏好学习的应用场景。</li><li>实验表明，HALU-J显著提升了复杂任务的评估准确性，尤其是在事实性和逻辑性判断上。</li></ul><h6 id="2-KIEval【Yu-et-al-2024】："><a href="#2-KIEval【Yu-et-al-2024】：" class="headerlink" title="2. KIEval【Yu et al., 2024】："></a>2. <strong>KIEval</strong>【Yu et al., 2024】：</h6><ul><li>提出了知识交互式评估框架，通过动态问答生成丰富的上下文信息。</li><li>创新点在于引入了“交互者”角色，模拟用户和模型之间的动态交互。</li><li>作者设计了一种鲁棒性检测机制，避免因上下文污染导致的错误评估。</li><li>研究表明，KIEval在复杂任务中的表现优于传统静态评估方法。</li><li>此框架适用于多维度评估，特别是在需要动态调整上下文的场景中。</li></ul><hr><h3 id="6-评估基准"><a href="#6-评估基准" class="headerlink" title="6. 评估基准"></a>6. 评估基准</h3><h4 id="概述-13"><a href="#概述-13" class="headerlink" title="概述"></a>概述</h4><p>评估基准是验证LLM-as-a-Judge能力的重要工具。本节整理并介绍当前用于不同评估维度的基准，包括有用性、无害性、可靠性等方面的具体框架和其核心思想。这些基准覆盖了从对话生成到复杂任务推理的广泛应用场景，为后续研究提供了关键数据支持。</p><hr><h5 id="6-1-综合评估基准"><a href="#6-1-综合评估基准" class="headerlink" title="6.1 综合评估基准"></a>6.1 综合评估基准</h5><h6 id="1-SORRY-Bench【Xie-et-al-2024a】："><a href="#1-SORRY-Bench【Xie-et-al-2024a】：" class="headerlink" title="1. SORRY-Bench【Xie et al., 2024a】："></a>1. <strong>SORRY-Bench</strong>【Xie et al., 2024a】：</h6><ul><li>设计了一个专注于安全性和无害性评估的综合基准，重点测试LLM对潜在有害内容的拒绝能力。</li><li>创新点在于提供了一个多模型对比框架，包括开源和专有LLM的表现分析。</li><li>基准数据集涵盖多种潜在危险场景，如政治敏感内容和虚假信息生成。</li><li>作者还引入了动态拒绝率作为衡量指标，展示了不同模型在拒绝任务中的细粒度表现。</li><li>实验表明，小型LLM经过微调后可以在安全性评估中达到与大型模型相当的水平。</li></ul><h6 id="2-HalluJudge【Luo-et-al-2024】："><a href="#2-HalluJudge【Luo-et-al-2024】：" class="headerlink" title="2. HalluJudge【Luo et al., 2024】："></a>2. <strong>HalluJudge</strong>【Luo et al., 2024】：</h6><ul><li>提出了一个专门用于对话级事实性评估的基准，涵盖大规模对话数据集。</li><li>核心创新在于设计了一种细粒度的事实性评分机制，通过引入上下文验证生成内容的准确性。</li><li>数据集中包括多种类型的事实性错误，如数据遗漏、模糊表述和直接虚假信息。</li><li>HalluJudge还整合了自动化和人工评估方法，提高了基准的覆盖面和可靠性。</li><li>实验结果表明，HalluJudge能够显著提高LLM在对话场景中的事实性检测能力。</li></ul><hr><h5 id="6-2-专用领域评估基准"><a href="#6-2-专用领域评估基准" class="headerlink" title="6.2 专用领域评估基准"></a>6.2 专用领域评估基准</h5><h6 id="1-FaithScore【Jing-et-al-2024】："><a href="#1-FaithScore【Jing-et-al-2024】：" class="headerlink" title="1. FaithScore【Jing et al., 2024】："></a>1. <strong>FaithScore</strong>【Jing et al., 2024】：</h6><ul><li>FaithScore是第一个跨模态的可靠性评估框架，适用于文本和图像生成任务。</li><li>创新点在于设计了多模态评估方法，结合语言和视觉信号来验证生成内容的真实性。</li><li>数据集覆盖了从事实描述到跨模态推理的多个任务，测试了模型的全局一致性和细节准确性。</li><li>FaithScore还引入了多阶段评分机制，逐步分解任务以提高评估的精细化程度。</li><li>实验显示，FaithScore在多模态生成任务中的评估结果与人工评分高度一致。</li></ul><h6 id="2-GEMBA【Kocmi-and-Federmann-2023】："><a href="#2-GEMBA【Kocmi-and-Federmann-2023】：" class="headerlink" title="2. GEMBA【Kocmi and Federmann, 2023】："></a>2. <strong>GEMBA</strong>【Kocmi and Federmann, 2023】：</h6><ul><li>GEMBA基准专注于机器翻译和文本摘要任务的整体质量评估。</li><li>核心创新点在于结合BLEU等传统指标和LLM生成的综合评分，提供更全面的评估结果。</li><li>数据集中包含多种语言和领域的真实文本，覆盖多样化的任务需求。</li><li>作者设计了一种动态反馈机制，允许LLM在评估过程中进行自适应调整。</li><li>GEMBA基准的引入显著推动了机器翻译和摘要任务中LLM-as-a-Judge的应用。</li></ul><h6 id="3-Just-Eval【Lin-et-al-2023】："><a href="#3-Just-Eval【Lin-et-al-2023】：" class="headerlink" title="3. Just-Eval【Lin et al., 2023】："></a>3. <strong>Just-Eval</strong>【Lin et al., 2023】：</h6><ul><li>提出了一个基于生成内容有用性和无害性的综合基准，适用于广泛的开放式任务。</li><li>创新点在于为不同任务设计了定制化的评估标准，并结合多维评分系统生成最终评价。</li><li>数据集中涵盖了对话、问答和复杂推理等任务，验证了基准的通用性。</li><li>作者还分析了模型在不同任务和领域上的表现，提供了详细的对比结果。</li><li>Just-Eval的应用表明，评估框架需要结合任务特点进行优化，才能最大化评估的准确性。</li></ul><hr><h4 id="6-3-动态评估基准"><a href="#6-3-动态评估基准" class="headerlink" title="6.3 动态评估基准"></a>6.3 动态评估基准</h4><h6 id="1-RevisEval【Zhang-et-al-2024e】："><a href="#1-RevisEval【Zhang-et-al-2024e】：" class="headerlink" title="1. RevisEval【Zhang et al., 2024e】："></a>1. <strong>RevisEval</strong>【Zhang et al., 2024e】：</h6><ul><li>RevisEval通过引入动态自我修正机制，让LLM在生成评估之前对输出进行多次调整。</li><li>核心创新在于结合LLM的自我纠错能力，将最终输出用于多维度评估。</li><li>数据集中覆盖了对话生成、摘要和复杂推理任务，验证了基准的动态适应能力。</li><li>RevisEval引入了多轮反馈机制，允许模型在评估过程中迭代改进。</li><li>实验结果表明，动态评估能够显著提升复杂任务中评估的精确性和稳定性。</li></ul><h6 id="2-Meta-ranking【Liu-et-al-2024c】："><a href="#2-Meta-ranking【Liu-et-al-2024c】：" class="headerlink" title="2. Meta-ranking【Liu et al., 2024c】："></a>2. <strong>Meta-ranking</strong>【Liu et al., 2024c】：</h6><ul><li>Meta-ranking框架通过弱模型生成初步排序，再由强模型进行最终评估。</li><li>创新点在于使用多阶段的排名方法，提高评估效率并降低计算开销。</li><li>数据集中包含了多种任务类型，并通过实验验证了Meta-ranking的通用性。</li><li>该框架特别适用于大规模排序任务，显著减少了评估时间。</li><li>Meta-ranking展示了弱模型和强模型协作评估的潜力，是多模型评估的新方向。</li></ul><hr><h3 id="7-挑战与未来方向"><a href="#7-挑战与未来方向" class="headerlink" title="7. 挑战与未来方向"></a>7. 挑战与未来方向</h3><h4 id="概述-14"><a href="#概述-14" class="headerlink" title="概述"></a>概述</h4><p>尽管LLM-as-a-Judge在评估任务中展现了强大能力，但依然面临着多方面的挑战。主要问题包括评估偏差与脆弱性、动态与复杂任务中的适应性，以及人机协同评估的潜力。本节探讨这些挑战并提出未来的研究方向。</p><hr><h5 id="7-1-偏差与脆弱性"><a href="#7-1-偏差与脆弱性" class="headerlink" title="7.1 偏差与脆弱性"></a>7.1 偏差与脆弱性</h5><h6 id="1-OffsetBias【Park-et-al-2024】："><a href="#1-OffsetBias【Park-et-al-2024】：" class="headerlink" title="1. OffsetBias【Park et al., 2024】："></a>1. <strong>OffsetBias</strong>【Park et al., 2024】：</h6><ul><li>OffsetBias通过设计一个去偏优化框架，减少LLM在评估任务中的位置偏差和内容偏见。</li><li>创新点在于使用合成数据生成“坏”样本，通过训练模型识别并修正偏差。</li><li>作者提出了一种多维度的去偏学习机制，确保评估在不同场景下的一致性。</li><li>研究表明，OffsetBias能够显著降低模型在生成任务中的不公平表现。</li><li>此方法为减少LLM评估中的偏差问题提供了重要方向。</li></ul><h6 id="2-SORRY-Bench【Xie-et-al-2024a】："><a href="#2-SORRY-Bench【Xie-et-al-2024a】：" class="headerlink" title="2. SORRY-Bench【Xie et al., 2024a】："></a>2. <strong>SORRY-Bench</strong>【Xie et al., 2024a】：</h6><ul><li>进一步研究了模型在拒绝有害内容时可能出现的误拒绝问题。</li><li>创新点在于结合动态评分机制和拒绝数据集，分析模型在多种任务中的拒绝倾向。</li><li>作者指出，小型模型在特定场景中可能比大型模型更高效。</li><li>实验结果表明，SORRY-Bench能够帮助识别并减轻评估偏差。</li><li>此基准成为探讨评估脆弱性的一个重要工具。</li></ul><hr><h5 id="7-2-动态与复杂评估"><a href="#7-2-动态与复杂评估" class="headerlink" title="7.2 动态与复杂评估"></a>7.2 动态与复杂评估</h5><h6 id="1-Tree-of-Thought-ToT-【Yao-et-al-2023a】："><a href="#1-Tree-of-Thought-ToT-【Yao-et-al-2023a】：" class="headerlink" title="1. **Tree of Thought (ToT)**【Yao et al., 2023a】："></a>1. **Tree of Thought (ToT)**【Yao et al., 2023a】：</h6><ul><li>ToT通过树状结构优化复杂任务的多步推理和评估。</li><li>创新点在于结合动态状态评估机制，使评估更加适应复杂多变的任务需求。</li><li>数据集中覆盖了需要多步推理的复杂任务，如问答和决策优化。</li><li>实验表明，ToT框架显著提升了复杂任务的解决能力和评估准确性。</li><li>该研究为动态评估提供了新的理论和实践支持。</li></ul><h6 id="2-RAIN【Li-et-al-2024】："><a href="#2-RAIN【Li-et-al-2024】：" class="headerlink" title="2. RAIN【Li et al., 2024】："></a>2. <strong>RAIN</strong>【Li et al., 2024】：</h6><ul><li>RAIN提出了可回溯的自回归推理机制，让LLM能够在评估过程中动态修正错误。</li><li>创新点在于结合自我评估和多轮推理机制，确保最终输出的高质量。</li><li>作者还设计了一种动态调整机制，使模型能够适应不同任务的变化。</li><li>实验显示，RAIN在复杂任务中的评估能力优于传统静态方法。</li><li>此框架展示了动态评估在复杂场景中的潜力。</li></ul><hr><h5 id="7-3-自我评估与人机协同"><a href="#7-3-自我评估与人机协同" class="headerlink" title="7.3 自我评估与人机协同"></a>7.3 自我评估与人机协同</h5><h6 id="1-Self-Taught-Evaluators【Wang-et-al-2024f】："><a href="#1-Self-Taught-Evaluators【Wang-et-al-2024f】：" class="headerlink" title="1. Self-Taught Evaluators【Wang et al., 2024f】："></a>1. <strong>Self-Taught Evaluators</strong>【Wang et al., 2024f】：</h6><ul><li>提出了一种自我学习框架，模型通过生成低质量数据对自身进行动态优化。</li><li>创新点在于引入了一种动态评估机制，让模型能够逐步提升自身评估能力。</li><li>数据集中包括了多种类型的任务，为自我评估提供了广泛支持。</li><li>Self-Taught Evaluators展示了模型在无需人工干预情况下的自我提升能力。</li><li>此框架为自动化评估任务提供了新思路。</li></ul><h6 id="2-Meta-Rewarding【Wu-et-al-2024】："><a href="#2-Meta-Rewarding【Wu-et-al-2024】：" class="headerlink" title="2. Meta-Rewarding【Wu et al., 2024】："></a>2. <strong>Meta-Rewarding</strong>【Wu et al., 2024】：</h6><ul><li>Meta-Rewarding通过将LLM的自评估信号作为偏好数据，用于进一步优化模型。</li><li>创新点在于结合策略模型自我反馈，增强模型的自适应能力。</li><li>作者还探讨了如何动态调整评估策略以提高鲁棒性。</li><li>实验表明，Meta-Rewarding能够显著提升复杂任务中的评估效果。</li><li>该研究展示了人机协同评估的潜在优势。</li></ul><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;hr&gt;
&lt;h1 id=&quot;基于生成的大语言模型（LLM）评估：从生成到判断的机遇与挑战&quot;&gt;&lt;a href=&quot;#基于生成的大语言模型（LLM）评估：从生成到判断的机遇与挑战&quot; class=&quot;headerlink&quot; title=&quot;基于生成的大语言模型（LLM）评估：从生成到判断的机遇</summary>
      
    
    
    
    <category term="NLP Insights" scheme="https://chenhuiyu.github.io/categories/NLP-Insights/"/>
    
    
    <category term="Onnx" scheme="https://chenhuiyu.github.io/tags/Onnx/"/>
    
    <category term="Deployment" scheme="https://chenhuiyu.github.io/tags/Deployment/"/>
    
  </entry>
  
  <entry>
    <title>Reflections on Identity and Subjectivity</title>
    <link href="https://chenhuiyu.github.io/2024/12/03/Life%20Reflections/Reflections%20on%20Identity%20and%20Subjectivity.en/"/>
    <id>https://chenhuiyu.github.io/2024/12/03/Life%20Reflections/Reflections%20on%20Identity%20and%20Subjectivity.en/</id>
    <published>2024-12-03T06:11:06.000Z</published>
    <updated>2026-02-20T21:47:32.617Z</updated>
    
    <content type="html"><![CDATA[<h1 id="PR-Application-Rejected-Reflections-on-Identity-and-Subjectivity"><a href="#PR-Application-Rejected-Reflections-on-Identity-and-Subjectivity" class="headerlink" title="PR Application Rejected: Reflections on Identity and Subjectivity"></a>PR Application Rejected: Reflections on Identity and Subjectivity</h1><p>When I received the news of my PR application being rejected, after a brief moment of shock, what arose within me was not merely frustration but a peculiar sense of “existential dilemma.” On the surface, it seemed like just an administrative outcome, yet it profoundly mirrored the multiple tensions between the structure of contemporary global mobility and the construction of subjectivity.</p><ul><li>Amid the tension between globalization and national sovereignty, is it even possible to affirm an individual’s identity?</li><li>Does the rejection of a PR application symbolically exclude an individual from a collective sense of belonging?</li></ul><hr><h2 id="PR-Application-From-the-Fantasy-of-Rights-to-the-Maze-of-Identity"><a href="#PR-Application-From-the-Fantasy-of-Rights-to-the-Maze-of-Identity" class="headerlink" title="PR Application: From the Fantasy of Rights to the Maze of Identity"></a>PR Application: From the Fantasy of Rights to the Maze of Identity</h2><p>Within the theoretical framework of Anthony Giddens’ <em>Modernity and Self-Identity</em>, applying for PR is not merely a pursuit of residency rights but a symbolic quest for identity stability and future possibilities. However, in the context of globalization, this pursuit often falls into what Derrida describes as the structure of <em>différance</em>: the realization of rights is perpetually deferred, and the confirmation of identity remains suspended.</p><p>In this context, rejection is tantamount to a form of <strong>symbolic violence</strong>. It not only disrupts my plans for the future but also shatters the illusion of subjectivity I held within this domain.</p><hr><h2 id="Subjectivity-vs-Institutional-Discipline"><a href="#Subjectivity-vs-Institutional-Discipline" class="headerlink" title="Subjectivity vs. Institutional Discipline"></a>Subjectivity vs. Institutional Discipline</h2><p>Bourdieu’s field theory reveals the distribution of power in social practices, and the practice of PR applications is a concrete field where power disciplines individuals. Rejection is not merely an administrative outcome but an invisible disciplining of the subject, hinting at the imbalance of power between individuals and institutions in the era of platform capitalism.</p><p>Through Foucault’s lens of discipline, this process not only constrains individuals’ <strong>physical mobility</strong> but also profoundly affects the <strong>emotional and mental freedom</strong> of individuals.</p><hr><h2 id="From-Loss-to-Reflection"><a href="#From-Loss-to-Reflection" class="headerlink" title="From Loss to Reflection"></a>From Loss to Reflection</h2><p>In a sense, rejection is not an end but an opportunity for <strong>reconstruction</strong>. Bauman’s concept of <em>liquid modernity</em> might help me interpret this failure: in a constantly fluid world, fixed identities and stable senses of belonging are scarce resources. Perhaps I need to redefine my position amid this loss and find my own meaning in the fragments of grand narratives.</p><p>As Žižek puts it: <strong>“True freedom is not about getting what you want but about confronting the trauma of reality.”</strong> The failure of my PR application may not be the end of identity but a challenge to how I reconstruct subjectivity in the face of uncertainty.</p><p>Thus, this is not an ending but a dialectical transformation: in the moment of shattered stability, perhaps lies the beginning of transcending grand narratives and rediscovering the meaning of one’s existence.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;PR-Application-Rejected-Reflections-on-Identity-and-Subjectivity&quot;&gt;&lt;a href=&quot;#PR-Application-Rejected-Reflections-on-Identity-and-Subj</summary>
      
    
    
    
    <category term="Life Reflections" scheme="https://chenhuiyu.github.io/categories/Life-Reflections/"/>
    
    
    <category term="Living in Singapore" scheme="https://chenhuiyu.github.io/tags/Living-in-Singapore/"/>
    
  </entry>
  
  <entry>
    <title>Reflections on Identity and Subjectivity</title>
    <link href="https://chenhuiyu.github.io/2024/12/03/Life%20Reflections/Reflections%20on%20Identity%20and%20Subjectivity/"/>
    <id>https://chenhuiyu.github.io/2024/12/03/Life%20Reflections/Reflections%20on%20Identity%20and%20Subjectivity/</id>
    <published>2024-12-03T06:11:06.000Z</published>
    <updated>2026-02-20T21:51:54.076Z</updated>
    
    <content type="html"><![CDATA[<h1 id="PR-Application-Rejected-Reflections-on-Identity-and-Subjectivity"><a href="#PR-Application-Rejected-Reflections-on-Identity-and-Subjectivity" class="headerlink" title="PR Application Rejected: Reflections on Identity and Subjectivity"></a>PR Application Rejected: Reflections on Identity and Subjectivity</h1><p>When I received the news of my PR application being rejected, after a brief moment of shock, what arose within me was not merely frustration but a peculiar sense of “existential dilemma.” On the surface, it seemed like just an administrative outcome, yet it profoundly mirrored the multiple tensions between the structure of contemporary global mobility and the construction of subjectivity.</p><ul><li>Amid the tension between globalization and national sovereignty, is it even possible to affirm an individual’s identity?</li><li>Does the rejection of a PR application symbolically exclude an individual from a collective sense of belonging?</li></ul><hr><h2 id="PR-Application-From-the-Fantasy-of-Rights-to-the-Maze-of-Identity"><a href="#PR-Application-From-the-Fantasy-of-Rights-to-the-Maze-of-Identity" class="headerlink" title="PR Application: From the Fantasy of Rights to the Maze of Identity"></a>PR Application: From the Fantasy of Rights to the Maze of Identity</h2><p>Within the theoretical framework of Anthony Giddens’ <em>Modernity and Self-Identity</em>, applying for PR is not merely a pursuit of residency rights but a symbolic quest for identity stability and future possibilities. However, in the context of globalization, this pursuit often falls into what Derrida describes as the structure of <em>différance</em>: the realization of rights is perpetually deferred, and the confirmation of identity remains suspended.</p><p>In this context, rejection is tantamount to a form of <strong>symbolic violence</strong>. It not only disrupts my plans for the future but also shatters the illusion of subjectivity I held within this domain.</p><hr><h2 id="Subjectivity-vs-Institutional-Discipline"><a href="#Subjectivity-vs-Institutional-Discipline" class="headerlink" title="Subjectivity vs. Institutional Discipline"></a>Subjectivity vs. Institutional Discipline</h2><p>Bourdieu’s field theory reveals the distribution of power in social practices, and the practice of PR applications is a concrete field where power disciplines individuals. Rejection is not merely an administrative outcome but an invisible disciplining of the subject, hinting at the imbalance of power between individuals and institutions in the era of platform capitalism.</p><p>Through Foucault’s lens of discipline, this process not only constrains individuals’ <strong>physical mobility</strong> but also profoundly affects the <strong>emotional and mental freedom</strong> of individuals.</p><hr><h2 id="From-Loss-to-Reflection"><a href="#From-Loss-to-Reflection" class="headerlink" title="From Loss to Reflection"></a>From Loss to Reflection</h2><p>In a sense, rejection is not an end but an opportunity for <strong>reconstruction</strong>. Bauman’s concept of <em>liquid modernity</em> might help me interpret this failure: in a constantly fluid world, fixed identities and stable senses of belonging are scarce resources. Perhaps I need to redefine my position amid this loss and find my own meaning in the fragments of grand narratives.</p><p>As Žižek puts it: <strong>“True freedom is not about getting what you want but about confronting the trauma of reality.”</strong> The failure of my PR application may not be the end of identity but a challenge to how I reconstruct subjectivity in the face of uncertainty.</p><p>Thus, this is not an ending but a dialectical transformation: in the moment of shattered stability, perhaps lies the beginning of transcending grand narratives and rediscovering the meaning of one’s existence.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;PR-Application-Rejected-Reflections-on-Identity-and-Subjectivity&quot;&gt;&lt;a href=&quot;#PR-Application-Rejected-Reflections-on-Identity-and-Subj</summary>
      
    
    
    
    <category term="Life Reflections" scheme="https://chenhuiyu.github.io/categories/Life-Reflections/"/>
    
    
    <category term="Living in Singapore" scheme="https://chenhuiyu.github.io/tags/Living-in-Singapore/"/>
    
  </entry>
  
</feed>
