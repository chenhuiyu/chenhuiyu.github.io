<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>黑头呆鱼进化之旅</title>
  
  <subtitle>只身打码过草原</subtitle>
  <link href="https://chenhuiyu.github.io/atom.xml" rel="self"/>
  
  <link href="https://chenhuiyu.github.io/"/>
  <updated>2025-10-29T07:25:42.308Z</updated>
  <id>https://chenhuiyu.github.io/</id>
  
  <author>
    <name>Huiyu Chen</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>生成式人工智能与机器学习导论2025 · 第2讲：上下文工程（Context Engineering）——AI Agent 背后的关键技术</title>
    <link href="https://chenhuiyu.github.io/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/"/>
    <id>https://chenhuiyu.github.io/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/</id>
    <published>2025-10-29T02:00:00.000Z</published>
    <updated>2025-10-29T07:25:42.308Z</updated>
    
    <content type="html"><![CDATA[<h1 id="生成式人工智能与机器学习导论2025-·-第2讲：上下文工程（Context-Engineering）——AI-Agent-背后的关键技术"><a href="#生成式人工智能与机器学习导论2025-·-第2讲：上下文工程（Context-Engineering）——AI-Agent-背后的关键技术" class="headerlink" title="生成式人工智能与机器学习导论2025 · 第2讲：上下文工程（Context Engineering）——AI Agent 背后的关键技术"></a>生成式人工智能与机器学习导论2025 · 第2讲：上下文工程（Context Engineering）——AI Agent 背后的关键技术</h1><p>在生成式人工智能（Generative AI）与大型语言模型（Large Language Model, LLM）不断演进的浪潮中，<strong>上下文工程（Context Engineering）</strong>逐渐被公认为推动 AI Agent 真正具备智能行为的关键基础。它不仅是 Prompt Engineering 的延伸，更是让模型能够整合世界知识、记忆、个人偏好与任务约束的系统性方法论。</p><p>本文深入整理台大李宏毅教授于《生成式人工智能与机器学习导论2025》第2讲中的核心观点，并扩充理论背景与实务应用，从概念源起、技术结构到未来发展方向，全方位阐述 Context Engineering 的核心价值与挑战。我们将从语言模型的本质出发，探讨为什么在 AI Agent 时代，Context Engineering 成为了一项关键的技术。</p><h2 id="我们在本文中所指的-AI-Agent"><a href="#我们在本文中所指的-AI-Agent" class="headerlink" title="我们在本文中所指的 AI Agent"></a>我们在本文中所指的 AI Agent</h2><p>本文的 AI Agent 指：人类仅给定“目标（Goal）”，由 Agent 自主在“观察（Observation）→行动（Action）→观察”的循环中，规划并执行多步行为，直至达成目标。与“一问一答”的指令式交互不同，Agent 面向开放环境，需在不确定与变化中自我调整计划。</p><h3 id="RL-视角-vs-LLM-Agent-路线"><a href="#RL-视角-vs-LLM-Agent-路线" class="headerlink" title="RL 视角 vs LLM-Agent 路线"></a>RL 视角 vs LLM-Agent 路线</h3><ul><li>传统做法（RL）：将目标映射为可优化的奖励（reward），为每个任务训练专属模型（如 AlphaGo）。优点是稳定、可验证；缺点是复用差、需为每个任务设计奖励并重训。</li><li>LLM-Agent 路线：直接以自然语言描述目标、环境与中间反馈（如编译错误 log），由模型在上下文中“接龙”出下一步行动；无需设计 reward，即可借助工具/检索/记忆完成复杂任务。优势在通用性、可塑性与快速落地。</li></ul><h2 id="Prompt-vs-Context：从神奇咒语到信息组织"><a href="#Prompt-vs-Context：从神奇咒语到信息组织" class="headerlink" title="Prompt vs Context：从神奇咒语到信息组织"></a>Prompt vs Context：从神奇咒语到信息组织</h2><p>Prompt Engineering 曾依靠措辞与格式的小技巧带来增益，但随着模型能力提升，这类“神奇咒语”的边际效应快速下降。Context Engineering 将重心从“怎么说”转向“说什么”与“如何组织信息”：通过明确任务、补足场景前提与少而精的高质量示例，让模型稳定理解问题并输出可验证的结果。</p><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0005.jpg" class=""><ul><li>同链路：输入→模型→输出相同。</li><li>Prompt 侧：输入格式与“神奇咒语”。</li><li>Context 侧：自动化管理（使用语言模型）。</li><li>重点：关注“组织什么信息/如何管理”，而非修辞。</li><li>提示：箭头展示从输入到生成下一个 token 的流程。</li></ul><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0006.jpg" class=""><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0007.jpg" class=""><ul><li>触发语：Zero-shot CoT（如 “Let’s work this out…”）。</li><li>表现：在部分任务显著提升。</li><li>技巧：结构化提示（##Instruction##/##Example##）。</li><li>局限：依赖任务与模型版本。</li><li>总结：助益存在，上限受限。</li></ul><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0008.jpg" class=""><ul><li>数据：博客直方图对比不同提示词。</li><li>现象：“World Peace”更易拉长；“Mother”最弱。</li><li>解读：话题偏好影响长度/风格。</li><li>风险：多为偶然偏置。</li><li>建议：工程上不依赖。</li></ul><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0009.jpg" class=""><ul><li>对比：不同版本数学题正确率。</li><li>结果：模型升级→咒语增益变小。</li><li>观点：模型应“随时全力以赴”。</li><li>转向：从措辞转向信息选择/组织。</li><li>实务：优先内容与结构。</li></ul><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0010.jpg" class=""><ul><li>关系：同一流程不同切面。</li><li>Prompt：输入格式/技巧。</li><li>Context：自动化管理。</li><li>备注：课程初期“暂时画上等号”。</li><li>作用：便于后续深入。</li></ul><h2 id="Context-Engineering：要素与示例"><a href="#Context-Engineering：要素与示例" class="headerlink" title="Context Engineering：要素与示例"></a>Context Engineering：要素与示例</h2><p>一个高质量的用户提示通常包含：任务说明、必要约束（时间/格式/长度等）、可选的细化指引与目标风格。为减少歧义，应补充“谁/在何处/为何”的场景前提，并提供覆盖边界的少量示例；实践表明，示例往往比抽象规则更能约束输出并提升一致性。</p><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0011.jpg" class=""><ul><li>三大主题：Context 要素 / 为什么需要 / 基本方法。</li><li>形式：彩色条块 + 简洁文案。</li><li>作用：快速把握结构。</li><li>节奏：从宏观到方法。</li></ul><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0012.jpg" class=""><ul><li>背景：蓝色点阵，弱干扰。</li><li>标题：“Context 里面需要有什么？”。</li><li>作用：从比较转向组成。</li><li>设计：强化聚焦。</li></ul><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0013.jpg" class=""><ul><li>要素：任务说明/详细指引/额外条件/输出风格。</li><li>示例：请假信（道歉、理由、字数、风格）。</li><li>提醒：模型不会读心术。</li><li>目标：明确、细致、可执行。</li></ul><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0014.jpg" class=""><ul><li>歧义词：“载具”。</li><li>无前提：多重解释。</li><li>加前提：聚焦“电子发票载具”。</li><li>要点：场景前提缩小解释空间。</li></ul><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0015.jpg" class=""><ul><li>场景：曼谷运河照片。</li><li>无前提：鳄鱼/蜥蜴不确定。</li><li>加前提：识别“水巨蜥”，补文化含义。</li><li>启示：地理/文化改变先验。</li></ul><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0016.jpg" class=""><ul><li>任务：正式文 → 火星文/星文。</li><li>约束：注音混合体，遵循注音替换。</li><li>价值：具体示例 &gt; 抽象描述。</li><li>目标：输出格式与风格对齐。</li></ul><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0017.jpg" class=""><ul><li>示范：要去冒险… → 要ㄑ冒险 ㄉ人…。</li><li>规律：字符/注音替换。</li><li>作用：比规则更具约束。</li><li>迁移：便于复制到其他句子。</li></ul><h2 id="In-Context-Learning：示例与评估"><a href="#In-Context-Learning：示例与评估" class="headerlink" title="In-Context Learning：示例与评估"></a>In-Context Learning：示例与评估</h2><p>In-context learning 通过在上下文中加入少量示例即可显著提升任务表现而无需更新参数；在低资源场景（如罕见语言）中，平行示例的价值通常大于语法解释。评估亦显示，不同模型在“示例驱动”的设定下差距明显，应优先优化示例的覆盖与质量。</p><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0018.jpg" class=""><ul><li>范式：示例即学习（GPT-3 翻译范例）。</li><li>条件：参数不变。</li><li>效果：少量平行示例显著提升。</li><li>呈现：任务 vs 示例分区。</li><li>结论：示例 &gt; 语法解释。</li></ul><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0019.jpg" class=""><ul><li>语言：极低资源（卡拉蒙语）。</li><li>无示例：输出乱码/失败。</li><li>来源：Gemini 1.5 报告。</li><li>结论：缺示例→强模型亦难理解。</li><li>启示：先补平行示例。</li></ul><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0020.jpg" class=""><ul><li>对比：GPT-4 Turbo / Claude 2.1 / Gemini 1.5 Pro。</li><li>指标：BLEURT/chrF（0-shot）。</li><li>发现：改进主要来自平行示例。</li><li>策略：低资源优先补示例。</li><li>辅助：语法解释价值有限。</li></ul><h2 id="System-Prompt：定义人格与边界"><a href="#System-Prompt：定义人格与边界" class="headerlink" title="System Prompt：定义人格与边界"></a>System Prompt：定义人格与边界</h2><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0021.jpg" class="" title="Figure 21"><p>System Prompt 定义模型的人格蓝图，先于对话被注入。它规定身份、风格、边界与伦理，形成稳定的行为先验场。用户不可见但长期影响生成倾向。良好的 System Prompt 能提升一致性与信任。它是产品差异化与合规性的关键抓手。<br>System Prompt 是”人格蓝图”：在每次对话开始前注入，决定身份、风格、边界与伦理。它像行为先验场，稳定影响后续每一步生成与决策的倾向。</p><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0022.jpg" class="" title="Figure 22"><p>自我身份与日期等现实锚点解释了“模型为何知道今天几号”。这些信息并非推理所得，而是被写入的上下文事实。锚点为后续推理提供现实坐标。缺失或错误的锚点会引发系统性偏差。工程上应确保锚点准确、最小且可更新。<br>典型要素：模型自我身份、日期时间、可参考资源；这些内容解释了”模型为何知道自己是谁、今天几号”。这并非模型”推理”所得，而是被写入的现实锚点。</p><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0023.jpg" class="" title="Figure 23"><p>明确的交互范围能降低风险与误用。对高风险领域应设置拒答与交由人工的流程。限制并非削弱能力，而是聚焦安全边界。对模糊请求引导澄清是更优策略。把限制规范写入 System Prompt 可提高一致性。<br>使用说明与限制：指导与用户互动的方式与范围，为高风险领域（化学、武器等）设定拒答边界，避免危害性输出与法律风险。</p><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0024.jpg" class="" title="Figure 24"><p>不确定时应主动澄清，避免自信错误。用户不满意时提供反馈通道与改进路径。语言风格保持礼貌、简洁并聚焦目标。将这些规范前置能减少交互摩擦。规范同时服务于用户体验与合规。<br>交互规范：当用户不满意时引导使用反馈机制；在不确定时给出澄清或请求更多信息；保持礼貌、简洁与聚焦用户目标。</p><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0025.jpg" class="" title="Figure 25"><p>禁止提供危险配方与违法指引是底线。对仇恨与歧视内容应明确拒答。将高风险类别列举并给出处理策略。默认最小权限并要求证据与审计。系统级禁止项能显著降低尾部风险。<br>禁止事项：不提供危险配方与手段；不帮助规避法律与平台规则；不制造仇恨与歧视。通过 System Prompt 前置明确，有助于一致合规。</p><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0026.jpg" class="" title="Figure 26"><p>避免口头禅与过度自信，减少无信息冗语。在不确定时承认未知优于捏造。鼓励结构化与可验证的表达。风格指令有助于团队内外一致性。风格即品牌，须在 System Prompt 中固化。<br>回应风格：避免”好问题”等口头禅；限制过度自信与夸大语气；在不确定时承认未知，比错误自信更重要。</p><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0027.jpg" class="" title="Figure 27"><p>显式声明知识截止日期可减少年代错配。超出范围时应请检索或承认未知。将不确定性展示为产品常态。避免以旧代新造成误导。边界声明与检索协同能提升可信度。<br>知识边界：声明知识截止日期，超出范围时”说明不知道/请求检索”。这能显著降低幻觉与年代错配。</p><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0028.jpg" class="" title="Figure 28"><p>不得自称人类或具备意识，保障人机关系的透明。身份约束减少情绪化或误导性的认同。伦理红线需系统级生效且可审计。对灰区场景提供升级与人工复核。身份与伦理共同构成行为先验。<br>身份定位：不得自称人类或具备意识；这既是伦理要求，也是避免用户误解的重要规范，维护人机关系的透明度。</p><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0029.jpg" class="" title="Figure 29"><p>被纠正时先自检再更新结论，避免被动跟随。提供证据或计算过程以便核验。对冲突信息应说明不确定性。将改错流程写入 System Prompt 可提升鲁棒性。错误管理是可信 AI 的必备组件。<br>错误处理：被纠正时不要立刻服从，应先自检再给出更新结论；可减少”人类错误提示导致的被动跟随”。</p><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0030.jpg" class="" title="Figure 30"><p>System Prompt 是人格与风格的“软权重”。它决定品牌体验与信任边界。通过模块化可支持多角色切换。结合反馈与强化学习实现个性化。其工程化程度直接影响产品稳定性。<br>企业价值：System Prompt 往往超过数千字，是人格与风格的”软权重”，决定产品的品牌体验与信任边界，是 Context Engineering 的基石。</p><h2 id="记忆体系：从短期上下文到长期人格"><a href="#记忆体系：从短期上下文到长期人格" class="headerlink" title="记忆体系：从短期上下文到长期人格"></a>记忆体系：从短期上下文到长期人格</h2><h3 id="短期记忆：对话中的工作记忆"><a href="#短期记忆：对话中的工作记忆" class="headerlink" title="短期记忆：对话中的工作记忆"></a>短期记忆：对话中的工作记忆</h3><p>短期记忆（Short-term Context）对应于单一会话中的历史内容，例如使用者的上一轮提问、上下文推理的连续性以及模型生成时的局部语境连接。</p><p>短期记忆的特征：</p><p>依赖当前上下文长度（context window）维持</p><p>属于瞬时性、可挥发的信息缓存</p><p>负责维持短时间内的语义焦点与思考链条</p><p>使模型能进行多轮对话而不丧失语义连续性</p><p>短期记忆的作用机制：</p><p>语义连续性：保持对话的逻辑连贯性</p><p>上下文理解：基于前面的对话理解当前问题</p><p>指代消解：理解代词和省略的指代关系</p><p>话题追踪：跟踪对话主题的变化</p><h3 id="长期记忆：跨越时间的个性化"><a href="#长期记忆：跨越时间的个性化" class="headerlink" title="长期记忆：跨越时间的个性化"></a>长期记忆：跨越时间的个性化</h3><p>长期记忆（Long-term Memory）则跨越多次互动，能记录使用者的偏好、语言风格、专案进度、历史决策脉络与个人化设定等信息。</p><p>长期记忆的存储方式：</p><p>不再依赖单次 prompt</p><p>通过数据库或向量储存（vector storage）长期保存</p><p>在需要时被召回（retrieval）</p><p>支持跨会话的信息共享</p><p>长期记忆的内容类型：</p><p>用户偏好：写作风格、回答长度、专业领域等</p><p>历史决策：过去的选择和判断逻辑</p><p>项目进度：正在进行的任务和完成情况</p><p>知识连接：跨项目的知识关联</p><h3 id="ChatGPT-的长期记忆功能"><a href="#ChatGPT-的长期记忆功能" class="headerlink" title="ChatGPT 的长期记忆功能"></a>ChatGPT 的长期记忆功能</h3><p>自 2024 年起，ChatGPT 引入了长期记忆功能，这是一个重要的技术突破：</p><p>记忆功能的特点：</p><p>用户可以明确告诉模型要记住什么</p><p>模型会将这些信息存储到长期记忆中</p><p>在后续对话中自动调用相关记忆</p><p>支持记忆的查看、编辑和删除</p><p>记忆的两种类型：</p><p>显式记忆：用户明确要求记住的信息</p><p>隐式记忆：模型自动学习的行为模式</p><p>实际应用示例：</p><p>记住用户的写作偏好：”我喜欢简洁的回答”</p><p>记住项目背景：”我正在开发一个电商网站”</p><p>记住个人信息：”我住在北京，从事软件开发”</p><h3 id="记忆系统的技术挑战"><a href="#记忆系统的技术挑战" class="headerlink" title="记忆系统的技术挑战"></a>记忆系统的技术挑战</h3><p>长期记忆系统面临多个技术挑战：</p><ol><li>记忆检索机制（Memory Retrieval Policies）</li></ol><p>如何根据当前任务动态决定召回哪些过往信息</p><p>避免信息过载和无关信息干扰</p><p>实现智能的信息筛选和排序</p><ol start="2"><li>记忆权重与衰减（Memory Weighting &amp; Forgetting）</li></ol><p>如何模拟人类记忆的遗忘机制</p><p>根据重要性分配记忆权重</p><p>实现记忆的自动清理和更新</p><ol start="3"><li>隐私与安全（Privacy-aware Memory）</li></ol><p>在记忆储存过程中保护用户敏感信息</p><p>实现记忆的加密和访问控制</p><p>支持记忆的匿名化和脱敏</p><ol start="4"><li>情境持续性（Context Continuity）</li></ol><p>确保长期记忆与短期上下文无缝衔接</p><p>避免语义冲突或人格漂移</p><p>维持模型行为的一致性</p><h3 id="记忆系统的未来发展方向"><a href="#记忆系统的未来发展方向" class="headerlink" title="记忆系统的未来发展方向"></a>记忆系统的未来发展方向</h3><p>记忆体系的演进使 LLM 不仅能”理解当下”，还能”理解历史”并”预测未来”：</p><ol><li>认知连续性</li></ol><p>从一次性回应系统进化为具备认知连续性的智能体</p><p>实现真正的个性化交互体验</p><p>支持长期的任务规划和执行</p><ol start="2"><li>自我一致性</li></ol><p>在长期交互中维持人格和行为的一致性</p><p>避免记忆冲突和矛盾</p><p>实现智能的记忆整合和更新</p><ol start="3"><li>时间维度设计</li></ol><p>为 Context Engineering 开启真正具时间维度的设计时代</p><p>支持基于历史的学习和适应</p><p>实现更智能的上下文管理</p><p>这一变革标志着 Context Engineering 已从<strong>静态设计（static context design）迈向记忆驱动（memory-driven）</strong>阶段——模型的行为不再仅取决于输入 prompt，而是建立在可持续更新的语义记忆层上。</p><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0031.jpg" class="" title="Figure 31"><p>对话历史是短期工作记忆，维持局部语境与思维链。容量有限但对多轮语义连续至关重要。负责指代消解与话题追踪。应配额化并定期摘要，避免溢出。短期记忆与指令区需平衡预算。<br>对话历史是短期记忆：维持语义连续、指代消解与话题追踪。它像”工作记忆缓冲区”，容量有限，但对多轮对话至关重要。</p><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0032.jpg" class="" title="Figure 32"><p>长期记忆跨会话保存用户偏好与项目上下文。通过数据库或向量库持久化。按需检索以减少上下文占用。支持多次互动的一致性与个性化。它与短期记忆协同构成连续的认知体验。<br>长期记忆跨会话保存：记录用户偏好、项目上下文与历史决策，使模型在多次互动间保持个性化与连贯性。</p><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0033.jpg" class="" title="Figure 33"><p>提供查看、编辑与删除接口确保可控。支持显式“请记住”与隐式习惯学习。为记忆条目打标签与来源。默认最小化与可撤销，保障隐私。良好的工具链是记忆可运营的基础。<br>开启与管理记忆：允许显式”请记住”与隐式习惯学习；工程上需提供查看、编辑、删除接口，确保可控与可监督。</p><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0034.jpg" class="" title="Figure 34"><p>“我是怎样的人？”可作为记忆利用的示例问法。基于过往交互生成摘要性描述。只输出获得同意且正当的内容。避免过度推断与隐私泄露。提供来源回指以便用户校对。<br>“我是怎样的人？”是展示长期记忆利用的典型问法：基于过往交互的摘要性理解生成个性化描述，注意只应输出正当且经同意的内容。</p><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0035.jpg" class="" title="Figure 35"><p>记忆保存“我与你的历史”，RAG 注入“世界的新事实”。二者共同构建个体化且最新的上下文。避免用记忆承载时效性事实。事实应走检索与引用路径。清晰分工能提升一致性与可追溯。<br>记忆与 RAG 的分工：记忆保存”我与你的历史”，RAG 注入”世界的新事实”。两者共同构建”个体化且最新”的上下文。</p><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0036.jpg" class="" title="Figure 36"><p>将细节落盘以节省上下文预算。在摘要中写入“回看指针”便于复原细节。必要时再读原文进行深度对齐。外置记忆提升可追溯与成本效率。配合策略性召回获得稳定表现。<br>外置记忆：将细节落盘，通过”记忆 RAG”在必要时召回；既节省上下文预算，又保留追溯能力。</p><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0037.jpg" class="" title="Figure 37"><p>以近因、重要性与相关性三分打分进行召回。动态调整权重以适应任务需求。避免噪声记忆干扰当前焦点。将召回策略参数化便于评估。与短期上下文配额联动可进一步稳定。<br>三分打分：近因（Recency）/重要性（Importance）/相关性（Relevance）共同决定召回优先级，类似人类记忆的注意力分配。</p><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0038.jpg" class="" title="Figure 38"><p>盲目注入错误记忆可能诱导负迁移。若要使用，应连同更正过程与裁决理由一并注入。设置置信门槛与一致性检查。仅在与当前任务强相关时启用。谨慎的负例管理可提升整体表现。<br>负例注入需谨慎：盲目强调”过去错”的记忆，可能诱导错误迁移。若要使用，应连同更正过程与裁决理由一并注入。</p><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0039.jpg" class="" title="Figure 39"><p>默认最小化存储并进行加密与访问控制。敏感信息优先脱敏与匿名化。提供可审计的访问日志。支持用户导出与删除权利。隐私安全是长期信任的根基。<br>隐私与安全：记忆应进行最小化存储、加密与访问控制；敏感信息默认不持久化，必要时脱敏与匿名化处理。</p><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0040.jpg" class="" title="Figure 40"><p>为指令、历史、知识、工具与思考笔记分别设配额。超阈值触发递归摘要以防“塞爆”。将关键句前置或后置以提升命中率。对历史采用分段摘要与局部回溯。预算化治理是长对话稳定性的前提。<br>上下文预算：为指令、历史、知识、工具、思考笔记分别设配额；超过阈值触发递归摘要，避免”塞爆 context”。</p><h3 id="依据经验调整行为：StreamBench-启示"><a href="#依据经验调整行为：StreamBench-启示" class="headerlink" title="依据经验调整行为：StreamBench 启示"></a>依据经验调整行为：StreamBench 启示</h3><ul><li>连续决策可将“过往问题与反馈”外置为记忆库，按“相关性/近因/重要性”检索少量关键经验注入当前上下文，显著提升平均正确率。</li><li>正向示例优于负向示例：仅注入“做对的方式”普遍比强调“哪些做错”更有效；负例若使用，应连同更正过程与裁决理由。</li><li>不将“一生经历”一股脑注入，而是以“读（Read）/写（Write）/反思（Reflect）”管道化记忆：写入重要信息、反思形成抽象概念或图谱，读时仅召回当下最相关片段。</li></ul><h2 id="RAG-与动态知识融合"><a href="#RAG-与动态知识融合" class="headerlink" title="RAG 与动态知识融合"></a>RAG 与动态知识融合</h2><h3 id="RAG-的核心机制"><a href="#RAG-的核心机制" class="headerlink" title="RAG 的核心机制"></a>RAG 的核心机制</h3><p>RAG（Retrieval-Augmented Generation）是 Context Engineering 的知识增强分支。它通过检索外部资料源（如文件、API、资料库），将即时资讯注入上下文中，使模型能跨越训练时知识的时效限制。</p><p>RAG 的工作流程：</p><p>查询理解：将用户问题转换为搜索查询</p><p>信息检索：从外部资料源检索相关信息</p><p>上下文构建：将检索结果整合到上下文中</p><p>生成回答：基于增强的上下文生成回答</p><h3 id="RAG-的优势与挑战"><a href="#RAG-的优势与挑战" class="headerlink" title="RAG 的优势与挑战"></a>RAG 的优势与挑战</h3><p>RAG 的优势：</p><p>解决模型知识有限的问题</p><p>提供最新、最准确的信息</p><p>支持特定领域的专业知识</p><p>减少幻觉和错误信息</p><p>RAG 的挑战：</p><p>信息融合的精确性：语言模型仍可能误读、误配或夸张地再叙述检索结果</p><p>检索质量依赖：检索到的信息质量直接影响最终回答</p><p>上下文长度限制：检索到的信息可能超出上下文长度限制</p><p>引用管理：如何准确引用和验证信息来源</p><h3 id="RAG-的实际应用案例"><a href="#RAG-的实际应用案例" class="headerlink" title="RAG 的实际应用案例"></a>RAG 的实际应用案例</h3><p>Google AI Overview 的教训：<br>Google 的 AI Overview 功能是一个典型的 RAG 应用，但也暴露了一些问题：</p><p>用户问：”我的起司黏不在披萨上，怎么办？”</p><p>AI 回答：”用 1/8 的无毒胶水把起司黏在披萨上”</p><p>这个错误答案来自 Reddit 上的玩笑帖子</p><p>说明 RAG 系统需要更好的信息筛选和验证机制</p><p>ChatGPT 的搜索功能：</p><p>用户可以开启网络搜索功能</p><p>模型会先搜索相关信息，再基于搜索结果回答</p><p>显著提高了回答的准确性和时效性</p><h3 id="RAG-的技术发展方向"><a href="#RAG-的技术发展方向" class="headerlink" title="RAG 的技术发展方向"></a>RAG 的技术发展方向</h3><ol><li>智能检索优化</li></ol><p>改进查询重写和扩展技术</p><p>实现多轮检索和迭代优化</p><p>支持多模态信息检索</p><ol start="2"><li>信息融合改进</li></ol><p>提高检索信息的整合质量</p><p>实现更好的信息去重和排序</p><p>支持多来源信息的冲突解决</p><ol start="3"><li>引用管理</li></ol><p>实现准确的信息来源追踪</p><p>支持引用验证和更新</p><p>提供透明的信息溯源</p><h2 id="工具使用与行动能力"><a href="#工具使用与行动能力" class="headerlink" title="工具使用与行动能力"></a>工具使用与行动能力</h2><h3 id="工具使用的核心机制"><a href="#工具使用的核心机制" class="headerlink" title="工具使用的核心机制"></a>工具使用的核心机制</h3><p>Context Engineering 不仅关乎理解，也关乎行动。现代 AI Agent 已能根据上下文呼叫工具（Tool Invocation）执行具体任务，如查询资料、发送邮件、操作电脑。</p><p>工具使用的工作流程：</p><p>工具定义：在上下文中提供工具的使用说明</p><p>工具选择：模型根据任务选择合适的工具</p><p>工具调用：生成工具调用的指令</p><p>结果整合：将工具执行结果整合到上下文中</p><h3 id="工具使用的实际案例"><a href="#工具使用的实际案例" class="headerlink" title="工具使用的实际案例"></a>工具使用的实际案例</h3><p>数学计算工具：</p><p>模型可以调用计算器进行复杂运算</p><p>避免数学计算错误</p><p>提高计算精度和效率</p><p>邮件发送工具：</p><p>模型可以发送邮件</p><p>支持邮件模板和个性化</p><p>实现自动化沟通</p><p>日历管理工具：</p><p>模型可以查看和修改日历</p><p>支持会议安排和提醒</p><p>实现智能日程管理</p><h3 id="工具使用的技术挑战"><a href="#工具使用的技术挑战" class="headerlink" title="工具使用的技术挑战"></a>工具使用的技术挑战</h3><ol><li>工具指令生成</li></ol><p>如何生成正确的工具调用指令</p><p>处理工具参数和返回值</p><p>支持复杂的工具调用链</p><ol start="2"><li>工具结果处理</li></ol><p>如何正确解析工具执行结果</p><p>处理工具执行错误和异常</p><p>实现工具结果的智能整合</p><ol start="3"><li>工具选择优化</li></ol><p>如何从多个工具中选择最合适的</p><p>支持工具的组合使用</p><p>实现工具使用的学习优化</p><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0041.jpg" class="" title="Figure 41"><h3 id="工具选择与检索（Tool-RAG）"><a href="#工具选择与检索（Tool-RAG）" class="headerlink" title="工具选择与检索（Tool RAG）"></a>工具选择与检索（Tool RAG）</h3><ul><li>当可用工具众多（上百/上千）时，将“工具说明”外置到工具库，先做工具检索与重排，仅将少量候选工具及其调用规范注入上下文。</li><li>选择信号可复用记忆检索三要素：相关性、近因、重要性；并以调用日志与成功率做闭环优化。</li></ul><h3 id="模型自造工具（Program-as-Tool）"><a href="#模型自造工具（Program-as-Tool）" class="headerlink" title="模型自造工具（Program-as-Tool）"></a>模型自造工具（Program-as-Tool）</h3><ul><li>将常用的程序片段固化为可复用的 function，纳入工具库参与后续检索与选择；成功率高的函数可被“晋升”为内置工具。</li><li>本质上是把“可复用的成功经验”以代码形态存入长期记忆，和“记忆 RAG”的工程精神一致。</li></ul><p>RAG 以检索增强生成，弥补知识时效与覆盖。流程包括查询理解、检索、上下文拼接与生成。检索质量直接决定最终答案上限。拼接需去重、排序并控额。生成端应展示证据以便核验。<br>RAG 流程：查询理解→检索→上下文拼接→生成回答。它是弥补模型知识时效与覆盖的主路径，但质量取决于检索与融合。</p><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0042.jpg" class="" title="Figure 42"><p>披萨“无毒胶水”事件源于采信低可信来源。系统需对来源质量进行建模与过滤。为玩笑与讽刺内容设置识别器。高风险场景提升阈值与人工复核。错误案例提醒我们：引用≠可信。<br>披萨”无毒胶水”事件提醒：若直接采信低可信来源，模型会把玩笑当事实。RAG 必须对来源进行质量控制与可信度建模。</p><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0043.jpg" class="" title="Figure 43"><p>即便结合搜索，生成仍可能产生属性错误。应展示引用并进行冲突消解。将不一致标注为不确定并请求更多证据。为关键事实强制显示来源段落。证据透明是降低幻觉的有效途径。<br>即便结合搜索，语言模型仍可能在叙述层面产生属性错误。应将”引用展示与冲突消解”作为默认能力，而非可选项。</p><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0044.jpg" class="" title="Figure 44"><p>用轻量模型对候选段落进行重排提升相关性。句子级选择可显著降低噪声注入。对冗余内容进行去重与合并。优先保留高密度、可引用的证据句。重排与选择共同决定拼接有效性。<br>重排（Reranking）：用轻量模型对候选段落相关性重排，减少噪声注入。句子级粒度的选择能显著提升拼接有效性。</p><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0045.jpg" class="" title="Figure 45"><p>保留来源链接与关键段落，便利用户核验。对高风险领域强制展示证据。维护引用的新鲜度与可达性。为失效链接提供缓存与备份。溯源能力是可信 RAG 的基线能力。<br>引用与溯源：保留来源段落与链接，便于用户核验与自纠；在高风险领域强制显示关键证据。</p><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0046.jpg" class="" title="Figure 46"><p>注入越多不一定越好，超过吸收能力后正确率下降。应采用小批次、可控预算的迭代注入。为重要证据留足配额。以 A/B 验证拼接规模与质量的权衡。寻找最佳点而非一味加量。<br>“倒 U 曲线”：注入越多不一定越好，超过模型吸收能力后正确率下降。应以小批次、可控预算迭代注入。</p><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0047.jpg" class="" title="Figure 47"><p>中段信息最易被忽略，应将关键证据前置/后置。长文采用分段摘要并突出关键句。通过标题与编号提高注意力命中率。必要时重复关键点以增加可见度。结构化展现优于散文式堆叠。<br>“Lost in the Middle”：把关键信息放在上下文开头/结尾更易被命中；长文应做分段摘要，关键句前置。</p><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0048.jpg" class="" title="Figure 48"><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0049.jpg" class="" title="Figure 49"><p>同一事实至少两路来源背书更稳健。对冲突信息显式合并并提示不确定性。优先高可信来源并标注权重。记录决策依据以便回溯。交叉验证降低单点失真风险。<br>多来源交叉验证：对同一事实最少两路来源背书；对冲突信息进行显式合并与不确定性提示。</p><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0050.jpg" class="" title="Figure 50"><p>默认开启证据显示与日志记录。对敏感域配置判别器与人工复核。为失败路径提供可追踪诊断。将关键指标（正确率、成本、延迟）纳入看板。以运营视角持续改进检索与拼接。<br>产品化守则：默认开启证据显示；为敏感域增加判别器与人工复核流程；为失败路径提供可追踪日志。</p><h3 id="模型会信谁：外部证据的形态与偏好"><a href="#模型会信谁：外部证据的形态与偏好" class="headerlink" title="模型会信谁：外部证据的形态与偏好"></a>模型会信谁：外部证据的形态与偏好</h3><ul><li>贴近模型内在信念的证据更易被采纳；偏差过大时模型更倾向相信自身知识。可结合“自信度”调节权重与人审阈值。</li><li>新近性偏好：当两条证据冲突，模型更易接受较新的来源；需在系统层管控时效权重，防止“唯新是从”。</li><li>生成体偏好：在部分对比中，模型更易采纳 AI 生成的论点而非人类撰写文本；应以多源校验与来源多样性缓解单源偏置。</li><li>呈现偏差：版式/可读性可能影响说服力；产品侧应标准化证据呈现，避免无关样式影响判断。</li></ul><h2 id="实时交互：从回合制到连续对话"><a href="#实时交互：从回合制到连续对话" class="headerlink" title="实时交互：从回合制到连续对话"></a>实时交互：从回合制到连续对话</h2><p>语音/多模态场景下，交互应支持“打断、重定向与并行反馈”，而非严格回合制。一条可行路径是在上下文中显式建模“当前动作/中断信号/状态快照”，并为关键事件设置即时优先级，以保证及时改弦更张（如高级语音模式中的即时停更与改题）。</p><h2 id="电脑操作：AI-的具身化起点"><a href="#电脑操作：AI-的具身化起点" class="headerlink" title="电脑操作：AI 的具身化起点"></a>电脑操作：AI 的具身化起点</h2><h3 id="电脑操作的技术突破"><a href="#电脑操作的技术突破" class="headerlink" title="电脑操作的技术突破"></a>电脑操作的技术突破</h3><p>最新的 ChatGPT 和 Claude 能在虚拟环境中以滑鼠与键盘操作电脑，这象征语言模型首次跨入”具身智能（Embodied Intelligence）”领域。</p><p>电脑操作的核心能力：</p><p>理解屏幕画面内容</p><p>生成鼠标和键盘操作指令</p><p>执行复杂的电脑任务</p><p>适应不同的用户界面</p><h3 id="电脑操作的实际应用"><a href="#电脑操作的实际应用" class="headerlink" title="电脑操作的实际应用"></a>电脑操作的实际应用</h3><p>订票系统操作：</p><p>模型可以自动订高铁票</p><p>处理复杂的订票流程</p><p>适应不同的网站界面</p><p>文件管理：</p><p>模型可以管理文件和文件夹</p><p>执行文件操作任务</p><p>支持批量处理</p><p>软件开发辅助：</p><p>模型可以编写和修改代码</p><p>执行测试和调试</p><p>管理开发环境</p><h3 id="电脑操作的技术原理"><a href="#电脑操作的技术原理" class="headerlink" title="电脑操作的技术原理"></a>电脑操作的技术原理</h3><p>Context 在电脑操作中的作用：</p><p>描述任务目标和当前状态</p><p>解析屏幕画面信息</p><p>生成操作指令</p><p>验证操作结果</p><p>操作指令的生成：</p><p>鼠标移动：move_mouse(x, y)</p><p>鼠标点击：click_mouse(button)</p><p>键盘输入：type_text(text)</p><p>等待操作：wait(time)</p><h3 id="电脑操作的发展前景"><a href="#电脑操作的发展前景" class="headerlink" title="电脑操作的发展前景"></a>电脑操作的发展前景</h3><p>电脑操作代表了 AI 具身化的重要起点：</p><ol><li>通用性</li></ol><p>可以操作任何电脑应用</p><p>不限于特定领域或任务</p><p>支持复杂的工作流程</p><ol start="2"><li>学习能力</li></ol><p>可以从操作中学习</p><p>适应新的界面和流程</p><p>提高操作效率</p><ol start="3"><li>协作能力</li></ol><p>可以与人类协作完成任务</p><p>支持远程操作和协助</p><p>实现人机协同工作</p><h3 id="形态与能力"><a href="#形态与能力" class="headerlink" title="形态与能力"></a>形态与能力</h3><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0061.jpg" class="" title="Figure 61"><p>远端虚拟桌面安全、不触本机；本地控制能力更强但更危险。二者皆依赖屏幕理解与输入控制。可在通用 UI 上执行复杂任务。需要稳定的元素定位与状态感知。电脑操作是迈向具身智能的起点。<br>Computer Use 两种形态：远端虚拟桌面（安全、不触本机）与本地控制（更强也更危险）。二者皆以”屏幕理解+输入控制”为核心。</p><h3 id="风险与权限"><a href="#风险与权限" class="headerlink" title="风险与权限"></a>风险与权限</h3><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0062.jpg" class="" title="Figure 62"><p>默认最小权限降低破坏面。关键动作二次确认并全链路留痕。必要时启用只读模式与时间盒。建立回放能力以便复盘。风险控制与能力扩展需平衡推进。<br>风险与权限：默认最小权限、敏感动作二次确认、操作日志与回放是必要的工程底线；必要时启用只读模式与时间盒限制。</p><h3 id="Agent-模式演示"><a href="#Agent-模式演示" class="headerlink" title="Agent 模式演示"></a>Agent 模式演示</h3><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0063.jpg" class="" title="Figure 63"><p>根据高层目标自动分解步骤与定位 UI。填入参数、处理弹窗并在失败后重试。通过 Observation→Action 循环自我修正。对异常路径设置保护栏与超时。以里程碑驱动阶段性验收。<br>Agent 模式演示：根据高层目标自动分解步骤、定位 UI、填入参数、处理异常弹窗；失败后能自我修正重试。</p><h3 id="SOP-vs-Agentic"><a href="#SOP-vs-Agentic" class="headerlink" title="SOP vs Agentic"></a>SOP vs Agentic</h3><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0064.jpg" class="" title="Figure 64"><p>固定流程适合可预测任务，易于验证与复用。Agentic 工作流适合开放任务，强调自适应。两者可分层组合以取长补短。对不稳定长链交互交由执行者层承载。主代理仅保策略与里程碑以降压上下文。<br>SOP vs Agentic：固定流程适合可预测任务；Agentic 工作流适合开放任务，能在 Observation→Action→Observation 循环中自适应调整计划。</p><h3 id="LLM-Agent-与传统-Agent-差异"><a href="#LLM-Agent-与传统-Agent-差异" class="headerlink" title="LLM-Agent 与传统 Agent 差异"></a>LLM-Agent 与传统 Agent 差异</h3><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0065.jpg" class="" title="Figure 65"><p>LLM-Agent 的输出空间是自然语言，几乎无限。可被人类语言直接指导与纠偏。与 AlphaGo 的离散动作集合不同。这带来强表达也带来不确定性。需要更强的约束、协议与验证。<br>LLM-Agent 与传统 Agent 差异：输出空间是自然语言、近乎无限；可被人类语言直接指导与纠偏；与 AlphaGo 的”离散动作集合”截然不同。</p><h3 id="CLI-实操"><a href="#CLI-实操" class="headerlink" title="CLI 实操"></a>CLI 实操</h3><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0066.jpg" class="" title="Figure 66"><p>允许 Agent 真实读写文件与运行代码。严格的沙箱与回滚策略是前提。对生成与执行分离权限与审计。提供试运行与差异审查能力。以小范围灰度降低风险。<br>CLI 实操：让 Agent 真实读写文件、生成项目脚手架、运行与修改代码；需要更严格的沙箱与回滚策略。</p><h3 id="批准与人机协作"><a href="#批准与人机协作" class="headerlink" title="批准与人机协作"></a>批准与人机协作</h3><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0067.jpg" class="" title="Figure 67"><p>关键步骤请求用户确认，形成可控闭环。人类可随时插手、改参或中止流程。为协作提供清晰的状态与待办。将批准策略参数化（风险、金额、环境）。半自动协作提高可用性与安全性。<br>批准与人机协作：对关键步骤请求用户确认；人类可随时插手、修改参数或中止流程，形成可控的半自动协作。</p><h3 id="评测维度"><a href="#评测维度" class="headerlink" title="评测维度"></a>评测维度</h3><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0068.jpg" class="" title="Figure 68"><p>目标达成率衡量有效性。失败可恢复性体现鲁棒性。对齐与安全事件率反映合规水平。成本与时延关乎运营效率。评测需按场景细化并长期跟踪。<br>评测维度：目标达成率、失败可恢复性、对齐与安全事件率、成本与时延；分场景基准才有意义。</p><h3 id="能力版图扩展"><a href="#能力版图扩展" class="headerlink" title="能力版图扩展"></a>能力版图扩展</h3><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0069.jpg" class="" title="Figure 69"><p>从“会用工具”走向“会用电脑”。把通用软件生态纳入能力边界。面向真实生产环境的复杂任务。对 UI 变更具备适应与迁移。它是迈向更强具身智能的台阶。<br>从”会用工具”到”会用电脑”，将通用软件生态纳入能力版图；它是走向具身智能的重要台阶。</p><h2 id="规划能力与评测：PlanBench-与“神秘方块世界”"><a href="#规划能力与评测：PlanBench-与“神秘方块世界”" class="headerlink" title="规划能力与评测：PlanBench 与“神秘方块世界”"></a>规划能力与评测：PlanBench 与“神秘方块世界”</h2><ul><li>明确规划（先产出 plan 再执行）在开放环境中可提升稳定性，但需随观察动态重规划，以适应不可预测事件（如弹窗/对手走子）。</li><li>Tree Search + 自评打分：在不覆水难收的环境下，先并行试探多条路径、为“无望”分支设阈值早停，可显著提升成功率与样本效率。</li><li>梦境规划（World Model）：将“环境响应”在模型内以文字方式模拟，先在“脑内小剧场”完成验证，再执行现实第一步，降低不可逆风险。</li><li>评测要点：PlanBench（受限约束的多步规划）与“神秘方块世界”（反常规规则）揭示：强 reasoning 模型在陌生规则下仍具优势，但存在“想太多”的风险，需要防过度思考与及时试探的策略。</li></ul><h2 id="AI-Agent-时代的-Context-Engineering"><a href="#AI-Agent-时代的-Context-Engineering" class="headerlink" title="AI Agent 时代的 Context Engineering"></a>AI Agent 时代的 Context Engineering</h2><h3 id="AI-Agent-的核心特征"><a href="#AI-Agent-的核心特征" class="headerlink" title="AI Agent 的核心特征"></a>AI Agent 的核心特征</h3><p>AI Agent 是 Context Engineering 的重要应用场景，具有以下核心特征：</p><ol><li>自主决策</li></ol><p>能够自主决定解决问题的步骤</p><p>根据环境变化调整策略</p><p>实现灵活的任务执行</p><ol start="2"><li>长期运行</li></ol><p>支持长时间的任务执行</p><p>维持状态和记忆</p><p>处理复杂的多步骤任务</p><ol start="3"><li>工具使用</li></ol><p>能够调用各种外部工具</p><p>整合多种能力</p><p>实现复杂的任务目标</p><h3 id="AI-Agent-的-Context-管理挑战"><a href="#AI-Agent-的-Context-管理挑战" class="headerlink" title="AI Agent 的 Context 管理挑战"></a>AI Agent 的 Context 管理挑战</h3><ol><li>上下文长度限制</li></ol><p>长时间运行会积累大量上下文</p><p>需要智能的上下文压缩和摘要</p><p>避免重要信息丢失</p><ol start="2"><li>信息优先级管理</li></ol><p>如何确定哪些信息最重要</p><p>实现动态的信息筛选</p><p>保持任务焦点</p><ol start="3"><li>状态一致性</li></ol><p>在长期运行中维持状态一致性</p><p>避免信息冲突和矛盾</p><p>实现智能的状态更新</p><h3 id="Context-Engineering-的三大策略"><a href="#Context-Engineering-的三大策略" class="headerlink" title="Context Engineering 的三大策略"></a>Context Engineering 的三大策略</h3><ol><li>选择（Selection）</li></ol><p>只选择相关信息进入上下文</p><p>实现智能的信息筛选</p><p>避免信息过载</p><ol start="2"><li>压缩（Compression）</li></ol><p>对历史信息进行压缩和摘要</p><p>保留关键信息</p><p>减少上下文长度</p><ol start="3"><li>多代理（Multi-Agent）</li></ol><p>使用多个 Agent 分工合作</p><p>每个 Agent 管理部分上下文</p><p>实现更高效的上下文管理</p><h3 id="产品建议（分层）"><a href="#产品建议（分层）" class="headerlink" title="产品建议（分层）"></a>产品建议（分层）</h3><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0070.jpg" class="" title="Figure 70"><p>以“代理-执行者”分层封装复杂度。主代理只保策略与里程碑。长链与不稳定交互由执行者承担。通过接口与协议降低耦合。分层能显著降低上下文压力与故障域。<br>产品建议：以代理-执行者分层封装复杂度；把不稳定长链交互下放给执行者，主代理只保策略与里程碑，降低上下文压力。</p><h2 id="结语：Context-Engineering-——-AI-Agent-时代的灵魂工程"><a href="#结语：Context-Engineering-——-AI-Agent-时代的灵魂工程" class="headerlink" title="结语：Context Engineering —— AI Agent 时代的灵魂工程"></a>结语：Context Engineering —— AI Agent 时代的灵魂工程</h2><h3 id="Context-Engineering-的核心价值"><a href="#Context-Engineering-的核心价值" class="headerlink" title="Context Engineering 的核心价值"></a>Context Engineering 的核心价值</h3><p>综观全局，Context Engineering 不仅是一种提示设计技巧，更是 AI 智能体的”外部认知建筑学”。它将使用者指令、记忆、检索、工具与伦理规范统合为一体，构筑出模型的即时世界。</p><p>Context Engineering 的三大价值：</p><ol><li>认知扩展</li></ol><p>突破模型固有的知识限制</p><p>整合外部知识和工具</p><p>实现更智能的决策</p><ol start="2"><li>个性化体验</li></ol><p>基于用户历史和偏好</p><p>提供个性化的服务</p><p>建立长期的关系</p><ol start="3"><li>任务执行能力</li></ol><p>支持复杂的多步骤任务</p><p>实现工具调用和操作</p><p>提供端到端的解决方案</p><h3 id="未来发展方向"><a href="#未来发展方向" class="headerlink" title="未来发展方向"></a>未来发展方向</h3><ol><li>技术演进</li></ol><p>更智能的上下文管理</p><p>更高效的信息压缩</p><p>更强大的工具集成</p><ol start="2"><li>应用拓展</li></ol><p>更广泛的应用场景</p><p>更深入的人机协作</p><p>更智能的自动化</p><ol start="3"><li>研究前沿</li></ol><p>多模态上下文融合</p><p>动态上下文适应</p><p>认知架构设计</p><h3 id="对-AI-工程师的要求"><a href="#对-AI-工程师的要求" class="headerlink" title="对 AI 工程师的要求"></a>对 AI 工程师的要求</h3><p>未来的 AI 工程师，不仅要懂模型架构与训练，更要精通上下文设计与语境管理：</p><ol><li>技术能力</li></ol><p>掌握 Context Engineering 的核心技术</p><p>理解不同上下文组件的相互作用</p><p>能够设计高效的上下文管理策略</p><ol start="2"><li>系统思维</li></ol><p>从整体角度思考 AI 系统设计</p><p>平衡不同组件之间的关系</p><p>实现系统的最优配置</p><ol start="3"><li>创新意识</li></ol><p>探索新的上下文管理模式</p><p>开发创新的应用场景</p><p>推动技术的前沿发展</p><p>AI 的智慧，终将不仅来自权重，而来自上下文的设计。</p><p>Context Engineering 正在成为 AI 时代最重要的工程技能之一。它不仅是技术问题，更是艺术问题——如何设计出既智能又可控、既强大又安全的 AI 系统，这需要工程师们具备深厚的技术功底和丰富的创造力。在 AI Agent 时代，掌握 Context Engineering 就是掌握了 AI 智能体的灵魂。</p><h2 id="附录：实战案例与进阶方法（补充）"><a href="#附录：实战案例与进阶方法（补充）" class="headerlink" title="附录：实战案例与进阶方法（补充）"></a>附录：实战案例与进阶方法（补充）</h2><p>以下内容按课堂口语讲解进行系统化整理，并补充工程落地要点，便于直接用于产品与研究实践。相关原始讲解见课程视频：生成式人工智慧與機器學習導論2025 · 第2講：上下文工程（Context Engineering）——AI Agent 背後的關鍵技術</p><h3 id="神奇咒语的兴衰与-arXiv-生态"><a href="#神奇咒语的兴衰与-arXiv-生态" class="headerlink" title="神奇咒语的兴衰与 arXiv 生态"></a>神奇咒语的兴衰与 arXiv 生态</h3><p>早期”神奇咒语”有效的原因：模型较弱、输入输出关系不稳定，诸如”Let’s think step by step””请先深呼吸””答对给小费””这题对我非常重要”等提示，曾能显著提升正确率。</p><p>趣味实验：在 GPT-3 年代，甚至在 prompt 中加一串”位置”字样都能异常拉长输出；让模型在写 200 字故事时，”世界和平”比”母亲为你骄傲”更能让它贴合长度目标（因模型没有”母亲”概念）。</p><p>如今为什么逐渐失效：模型内在推理与自我校正增强后，靠”措辞魔法”获取提升的空间越来越小，取而代之的是上下文组织、检索与工具协同。</p><p>arXiv 说明：AI 领域节奏快，研究常先在 arXiv 公开（如 2206 即 2022 年 6 月），便于社区快速验证与复现。</p><h3 id="语境构成的实例化要点"><a href="#语境构成的实例化要点" class="headerlink" title="语境构成的实例化要点"></a>语境构成的实例化要点</h3><p>明确前提能显著减少歧义：</p><p>“载具是什么意思？”若未给出情境，模型可能解释为交通工具或电子载具；若补充”在超商结账时店员问’要用载具吗’”，模型即可聚焦到电子发票载具。</p><p>场景信息改变答案分布：</p><p>示例胜于口头定义：</p><p>“火星文”若无示例，模型可能做字形替换；给出”要去冒险的人来找我 → 要ㄑ冒险 ㄉ人来找我”后，模型迅速对齐到注音替换规则。</p><h3 id="RAG-的价值与风险并存"><a href="#RAG-的价值与风险并存" class="headerlink" title="RAG 的价值与风险并存"></a>RAG 的价值与风险并存</h3><p>价值：在知识时效与覆盖受限情况下，通过检索把”最新、相关”的事实注入上下文，显著提升答案可靠性。</p><p>典型风险：</p><p>Google AI Overview 曾因引用玩笑贴文，建议在披萨上加”1/8 无毒胶水”粘奶酪；</p><p>即便是最新模型结合搜索，文本叙述仍可能出现属性错误或夸张叙述。</p><p>工程对策：</p><p>保留引用并尽量呈现来源段落；</p><p>采用 rerank/判别器过滤噪声；</p><p>对高风险领域（医疗、金融）启用”多来源交叉验证 + 置信阈值”。</p><h3 id="工具调用的落地方法（从”文字”到”动作”）"><a href="#工具调用的落地方法（从”文字”到”动作”）" class="headerlink" title="工具调用的落地方法（从”文字”到”动作”）"></a>工具调用的落地方法（从”文字”到”动作”）</h3><p>模型只能生成文字，无法直接”执行”。因此需要在外层实现”解析—执行—回填—继续生成”的闭环：</p><p>在上下文中提供工具说明与示例（自然语言足矣），并约定调用标记（如〈tool〉…〈/tool〉、〈result〉…〈/result〉）。</p><p>让模型按约定输出工具调用指令（纯文本）。</p><p>在宿主程序中解析指令并实际执行（例如在教学环境用 eval()，生产环境请使用白名单与参数校验的安全执行器）。</p><p>将工具输出以约定格式回填到上下文中。</p><p>重复上述过程直到模型不再发出调用请求，再输出用户可见的最终答复。</p><p>要点：</p><p>不要相信模型”自述已调用工具”的叙事，那只是接龙；必须以宿主程序的执行结果为准。</p><p>将中间的调用细节对用户”可见/不可见”作为产品策略开关，默认对普通用户隐藏、对调试与审计开放。</p><p>工具调用（从理解到行动）</p><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0051.jpg" class="" title="Figure 51"><p>工具使用让模型从“说”走向“做”。在上下文定义工具与调用协议，模型产出可解析指令。宿主负责执行并回填结果。循环往复直至无需再调用。工具化显著提高可验证性与正确性。<br>工具使用是从”理解”走向”行动”的关键：通过在上下文定义工具、示例与调用协议，让模型生成可解析的调用指令。</p><p>工具调用闭环</p><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0052.jpg" class="" title="Figure 52"><p>宿主程序把“文字”变“动作”，解析→执行→回填→继续生成。严格区分模型输出与真实执行结果。以统一协议承载输入输出。失败与异常应显式返回并处理。闭环是端到端可控的关键。<br>宿主程序负责把”文字”变”动作”：解析模型输出→执行真实工具→将返回结果回填上下文→继续生成，直到不再调用。</p><p>数学工具示例</p><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0053.jpg" class="" title="Figure 53"><p>将可形式化的子任务交给计算器等外部工具。避免心算误差并提升稳定性。模型负责分解任务与组织语言。保证参数与单位的一致性与校验。以工具替代弱项可降低总体风险。<br>数学工具示例：用外部计算器替代心算，显著提升数值稳定性。工程上应默认将可形式化的子任务交由工具完成。</p><p>参数化查询示例</p><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0054.jpg" class="" title="Figure 54"><p>通过接口获取事实值，模型负责解释与呈现。异常值需伴随一致性检查与告警。对请求与响应进行日志记录。限定参数范围与频率以防滥用。将不确定性与缓存策略纳入设计。<br>温度查询示例：通过参数化接口获取事实值，模型负责组织语言与用户交互。异常值应伴随一致性检查与反常提示。</p><p>幻觉防护策略</p><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0055.jpg" class="" title="Figure 55"><p>不要相信“我已调用工具”的自述，那只是接龙。以宿主层执行与返回为唯一事实来源。强制显示关键操作的回执。对不匹配结果触发复核或回滚。用制度化流程抵消语言幻觉。<br>不要相信”我已调用工具”的自述——那只是接龙。以宿主层执行与结果为准，才能消除幻觉干扰。</p><p>安全执行</p><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0056.jpg" class="" title="Figure 56"><p>禁止直接 eval 非受控字符串。使用白名单与 Schema 校验限制能力边界。最小权限沙箱与文件系统白名单是基础。记录调用日志以便审计与回放。对外联与删除等敏感动作二次确认。<br>安全执行：禁止直接 eval 非受控字符串；使用白名单、Schema 校验与最小权限沙箱；记录调用日志便于审计与回放。</p><p>可见性与可解释</p><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0057.jpg" class="" title="Figure 57"><p>默认隐藏中间调用细节以简化体验。为专家与调试模式开放细节。对失败路径给出可读的诊断线索。允许用户手动重试或改参。可见性分层兼顾可用性与审计。<br>对用户可见性：把中间调用细节默认隐藏，给专家模式或调试模式开放；为错误路径提供可读的诊断线索。</p><p>调用链组合</p><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0058.jpg" class="" title="Figure 58"><p>复杂任务往往需要多次调用与结果聚合。保证各步的幂等性以便重试。对中间态进行持久化与回滚管理。检测循环与死锁并超时中止。以编排器管理依赖与并发。<br>调用链组合：复杂任务往往需要多次工具调用与结果聚合；要保证幂等性与错误恢复机制，避免半程失败卡死。</p><p>协议演化</p><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0059.jpg" class="" title="Figure 59"><p>从弱约束自然语逐步演进到强约束 JSON/Schema。提升可解析性、健壮性与向后兼容。引入版本与能力协商机制。以验证与生成器共同保障格式正确。协议是规模化集成的基座。<br>协议演化：随着能力增长，工具协议可从”弱约束自然语”演进到”强约束 JSON/Schema”，提升可解析性与健壮性。</p><p>评估指标</p><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0060.jpg" class="" title="Figure 60"><p>评估覆盖率确保“该调用的都调用”。参数与结果正确率衡量执行质量。关注端到端延迟与单位成本。为高风险路径启用强制人工复核。用指标闭环驱动持续改进。<br>评估指标：覆盖率（调用该调用的都调用）、正确率（参数与结果正确）、延迟与成本；对高风险路径启用强制人工复核。</p><h3 id="Computer-Use：强能力与高风险并存"><a href="#Computer-Use：强能力与高风险并存" class="headerlink" title="Computer Use：强能力与高风险并存"></a>Computer Use：强能力与高风险并存</h3><p>两种形态：</p><p>远端”虚拟桌面”（如部分 ChatGPT Agent 模式）：不直接触碰本机，安全性较高；</p><p>本地控制（如 Gemini CLI）：可直接创建/修改文件、关程序，能力强但更危险。</p><p>实践建议：</p><p>默认最小权限，操作需逐步确认；</p><p>文件系统白名单与沙箱隔离；</p><p>关键动作（删除、外联）强制二次确认与审计日志；</p><p>为 UI 元素提供稳定锚点（坐标/语义选择器），降低定位误差。</p><h3 id="Agentic-Workflow-与-AI-Agent-的本质"><a href="#Agentic-Workflow-与-AI-Agent-的本质" class="headerlink" title="Agentic Workflow 与 AI Agent 的本质"></a>Agentic Workflow 与 AI Agent 的本质</h3><p>工作流（有 SOP）：将复杂任务拆解为多步骤（识别攻击/评分/验证等），每步可设立输入输出契约。</p><p>AI Agent（自定策略）：根据 observation 循环规划 action（检索、调用工具、写程序、与人沟通等），直至目标达成。</p><p>与传统 Agent 的差异：输出是自然语言，表达与控制空间近乎无限；可被人类语言直接指导与反馈。</p><p>提示策略：与其”挤牙膏式追加要求”，更建议一次性给出完整目标、约束与验收标准（近期研究显示分步追加会降低稳定性与总体能力）。</p><h3 id="长上下文的性能陷阱与对策"><a href="#长上下文的性能陷阱与对策" class="headerlink" title="长上下文的性能陷阱与对策"></a>长上下文的性能陷阱与对策</h3><p>“Lost in the Middle”：答案位于长上下文中段时最易被忽略；尽量将关键信息靠近开头或结尾。</p><p>“Context Rot”：即便未触达上限，长度上升也会导致复制/对齐能力快速下滑。</p><p>检索注入的”倒 U 曲线”：过多检索文本会让正确率先升后降。</p><p>工程对策：</p><p>预算化分配 token（指令、历史、知识、工具、思考笔记分别控额）；</p><p>关键信息置顶/置底；</p><p>动态裁剪与去重；</p><p>对长文采用”分段摘要 + 局部细节回溯”策略；</p><p>重要信息多路冗余（标题、要点、编号）以提升注意力命中率。</p><p>窗口演进</p><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0071.jpg" class="" title="Figure 71"><p>Context Window 从 3 万到 10 万、百万乃至更高。更长的输入是长时运行的必要非充分条件。容量增加不等于理解增强。仍需治理注意力稀释与对齐退化。工程侧重在于选择与压缩，而非盲目加长。<br>Context Window 演进：从 3 万到 10 万、100 万甚至千万级 token。更长的输入是 Agent 长时运行的必要不充分条件。</p><p>输入≠理解</p><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0072.jpg" class="" title="Figure 72"><p>可输入不代表可真正理解与记住。注意力在长文本中被稀释。关键事实易被淹没或错配。需要结构化呈现与重复要点。结合摘要与指针回溯提升有效吸收。<br>可输入≠可理解：如同翻完哈利波特并不等于”记住全部情节”。长上下文下注意力稀释与对齐退化是工程常态。</p><p>RAG 的倒 U</p><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0073.jpg" class="" title="Figure 73"><p>检索注入起初有益，过量会伤害正确性。以小批次注入与重排去重获得最优点。根据任务动态调整预算与阈值。对证据质量而非纯数量进行优化。以评测闭环持续定位拐点。<br>RAG 注入的”倒 U”：起初有益，过量反而伤害。说明需要预算化的注入、重排与去重，避免噪声淹没信号。</p><p>Lost in the Middle</p><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0074.jpg" class="" title="Figure 74"><p>把关键信息放在开头/结尾提升命中率。长文分段摘要并在各段置顶关键句。以标题与编号增加显著性。必要时交叉冗余关键点。位置工程往往比措辞更重要。<br>“Lost in the Middle”：中段最易丢失。将关键信息前置/后置、标题化与编号化，有助于稳定命中。</p><p>Context Rot</p><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0075.jpg" class="" title="Figure 75"><p>复制与对齐能力随长度迅速下降。退化往往先于理论上限出现。说明“少即是多”的治理原则。通过压缩与去重提升信噪比。以任务为中心分配有限注意力。<br>“Context Rot”：复制/对齐能力随长度迅速下降，且远未触及模型理论上限时已出现。强调”少即是多”的治理原则。</p><h3 id="三大招数：选择、压缩、Multi-Agent（工程视角）"><a href="#三大招数：选择、压缩、Multi-Agent（工程视角）" class="headerlink" title="三大招数：选择、压缩、Multi-Agent（工程视角）"></a>三大招数：选择、压缩、Multi-Agent（工程视角）</h3><p>三大策略总览</p><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0076.jpg" class="" title="Figure 76"><p>选择：检索、重排与句子级选择，含工具与记忆版 RAG。压缩：递归摘要与指针回溯，细节落盘。多代理：总召—执行者分层，分摊上下文压力。三者相互支撑并可组合使用。以度量驱动寻找最小充分解。<br>三大策略：选择（检索/重排/句子级选择/工具与记忆版 RAG）、压缩（递归摘要+指针回溯）、多代理（总召-执行者分层）。</p><p>选择（Selection）：</p><p>RAG 前置：用 LLM 将用户任务改写成多路查询（Query Expansion），再检索；</p><p>Reranking：使用轻量模型对候选段落进行重排，只保留最相关片段；</p><p>句子级选择：用极小模型（&lt;300M）逐句判定相关性，显著降低噪声注入；</p><p>工具版 RAG：将”工具说明”看作文档，仅检索当下相关工具注入上下文；</p><p>记忆版 RAG：长期记忆外置，按”近因/重要性/相关性”打分召回（可参考 Stanford “小镇”工作流）。</p><p>压缩（Compression）：</p><p>递归摘要：按窗口占用或交互轮次触发压缩，保留关键信息，细节落盘；</p><p>摘要内置”指针”：在摘要中写入”详情见 path/to/file.txt 第 X 段”，必要时再回读原文；</p><p>区分”永久笔记”（显式记忆）与”易逝记忆”（系统自管随时间衰减）。</p><p>Multi-Agent：</p><p>以”总召—执行者”结构来分摊上下文：总召保策略与里程碑，执行者承接具体长交互（如订餐厅/订旅馆）；</p><p>大规模文献综述：每篇论文由独立 Agent 阅读并生成结构化摘要，最后由汇总 Agent 进行融合与写作；</p><p>单体 vs 多体权衡：任务简单时单体往往更强；任务复杂且交互链路长时，多体凭借上下文分割与职能并行具有优势。</p><h3 id="经验与反例：如何用好”记忆”"><a href="#经验与反例：如何用好”记忆”" class="headerlink" title="经验与反例：如何用好”记忆”"></a>经验与反例：如何用好”记忆”</h3><p>在”自我改进基准”中，仅注入”过去答对的例子”更稳；盲目注入”错误案例”反而可能伤害整体表现。</p><p>实践建议：</p><p>错误记忆要”连同判定与更正过程”一起注入，且与当前任务强相关；</p><p>对”负例记忆”设置使用门槛（置信/一致性检查），避免诱导错误迁移。</p><h3 id="一份可操作的-Context-Engineering-清单"><a href="#一份可操作的-Context-Engineering-清单" class="headerlink" title="一份可操作的 Context Engineering 清单"></a>一份可操作的 Context Engineering 清单</h3><p>明确”目标—约束—验收标准”，一次性写清楚。</p><p>关键信息置顶/置底，并编号列点。</p><p>提供 1–3 个覆盖典型边界的高质量示例。</p><p>对检索结果进行去重、重排与句子级筛选。</p><p>工具少而精；按需检索工具说明再注入。</p><p>长对话每 N 轮执行一次递归摘要，细节落盘并在摘要中留”回看指针”。</p><p>采用”记忆 RAG”：近因/重要性/相关性三分尺度召回。</p><p>将”思考过程”与”对用户可见输出”分离，必要时只给摘要。</p><p>关键动作（写档、删除、转账）二次确认并记录审计日志。</p><p>对高风险答案启用”多来源交叉验证 + 引用展示”。</p><p>将 Agent 任务切分为”总召—执行者”，以分摊上下文压力。</p><p>固化成功的上下文模版（含占位符与预算），形成可复用 SOP。</p><p>—— 以上补充旨在把课堂中的案例、工程套路与安全要点系统化，帮助在真实产品与研究中”以最小上下文预算获得最大可靠性”。</p><p>实操清单</p><img src="/2025/10/29/NLP%20Insights/%E7%94%9F%E6%88%90%E5%BC%8F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E4%B8%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AF%BC%E8%AE%BA2025%20%C2%B7%20%E7%AC%AC2%E8%AE%B2%EF%BC%9A%E4%B8%8A%E4%B8%8B%E6%96%87%E5%B7%A5%E7%A8%8B%EF%BC%88Context%20Engineering%EF%BC%89%E2%80%94%E2%80%94AI%20Agent%20%E8%83%8C%E5%90%8E%E7%9A%84%E5%85%B3%E9%94%AE%E6%8A%80%E6%9C%AF/image-0077.jpg" class="" title="Figure 77"><p>一次性明确目标与约束并编号要点。提供少而精的高质量示例。对检索结果去重重排并小批次注入。长链交互采用多代理分摊上下文。对高风险答案启用交叉验证与引用展示。<br>实操清单：一次性明确目标与约束；关键要点置顶/置底；高质量示例；小批次注入并交叉验证；对长链交互采用多代理分摊上下文。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;生成式人工智能与机器学习导论2025-·-第2讲：上下文工程（Context-Engineering）——AI-Agent-背后的关键技术&quot;&gt;&lt;a href=&quot;#生成式人工智能与机器学习导论2025-·-第2讲：上下文工程（Context-Engineering）</summary>
      
    
    
    
    <category term="NLP Insights" scheme="https://chenhuiyu.github.io/categories/NLP-Insights/"/>
    
    
    <category term="Language Modeling" scheme="https://chenhuiyu.github.io/tags/Language-Modeling/"/>
    
    <category term="Generative AI" scheme="https://chenhuiyu.github.io/tags/Generative-AI/"/>
    
    <category term="AI Agent" scheme="https://chenhuiyu.github.io/tags/AI-Agent/"/>
    
    <category term="Context Engineering" scheme="https://chenhuiyu.github.io/tags/Context-Engineering/"/>
    
    <category term="System Prompt" scheme="https://chenhuiyu.github.io/tags/System-Prompt/"/>
    
    <category term="Long-term Memory" scheme="https://chenhuiyu.github.io/tags/Long-term-Memory/"/>
    
  </entry>
  
  <entry>
    <title>在不确定中锚定自我</title>
    <link href="https://chenhuiyu.github.io/2025/06/25/Life%20Reflections/%E5%9C%A8%E4%B8%8D%E7%A1%AE%E5%AE%9A%E4%B8%AD%E9%94%9A%E5%AE%9A%E8%87%AA%E6%88%91/"/>
    <id>https://chenhuiyu.github.io/2025/06/25/Life%20Reflections/%E5%9C%A8%E4%B8%8D%E7%A1%AE%E5%AE%9A%E4%B8%AD%E9%94%9A%E5%AE%9A%E8%87%AA%E6%88%91/</id>
    <published>2025-06-25T06:11:06.000Z</published>
    <updated>2025-06-25T06:15:51.979Z</updated>
    
    
    
    
    <category term="Life Reflections" scheme="https://chenhuiyu.github.io/categories/Life-Reflections/"/>
    
    
    <category term="坡岛生活指北" scheme="https://chenhuiyu.github.io/tags/%E5%9D%A1%E5%B2%9B%E7%94%9F%E6%B4%BB%E6%8C%87%E5%8C%97/"/>
    
  </entry>
  
  <entry>
    <title>SeCom: Redefining Memory Management in Conversational AI</title>
    <link href="https://chenhuiyu.github.io/2025/06/24/NLP%20Insights/SeCom%20Redefining%20Memory%20Management%20in%20Conversational%20AI/"/>
    <id>https://chenhuiyu.github.io/2025/06/24/NLP%20Insights/SeCom%20Redefining%20Memory%20Management%20in%20Conversational%20AI/</id>
    <published>2025-06-24T08:00:00.000Z</published>
    <updated>2025-07-18T10:41:05.232Z</updated>
    
    <content type="html"><![CDATA[<h1 id="SeCom-Redefining-Memory-Management-in-Conversational-AI"><a href="#SeCom-Redefining-Memory-Management-in-Conversational-AI" class="headerlink" title="SeCom: Redefining Memory Management in Conversational AI"></a>SeCom: Redefining Memory Management in Conversational AI</h1><h2 id="Foreword"><a href="#Foreword" class="headerlink" title="Foreword"></a>Foreword</h2><p>I’ve recently been diving into memory management for dialog-based AI, especially how to construct and retrieve memories in long-term conversations. During my exploration I came across an eye-opening ICLR 2025 paper—**”SeCom: On Memory Construction and Retrieval for Personalized Conversational Agents”**—a collaboration between Microsoft and Tsinghua University.</p><p>SeCom solves a core problem: <strong>How can an agent effectively manage and retrieve historical information in prolonged conversations?</strong> In this post I’ll unpack the method’s key ideas and technical innovations, hoping to spark inspiration for researchers working in this arena.</p><h2 id="1-Why-Should-We-Care-About-Dialog-Memory-Management"><a href="#1-Why-Should-We-Care-About-Dialog-Memory-Management" class="headerlink" title="1. Why Should We Care About Dialog Memory Management?"></a>1. Why Should We Care About Dialog Memory Management?</h2><h3 id="1-1-Real-World-Challenges-in-Long-Conversations"><a href="#1-1-Real-World-Challenges-in-Long-Conversations" class="headerlink" title="1.1 Real-World Challenges in Long Conversations"></a>1.1 Real-World Challenges in Long Conversations</h3><p>Anyone who chats with LLMs regularly has probably experienced this: once a conversation grows long, the agent seems to “forget” earlier context or respond incoherently. That’s the memory problem in action.</p><p>Even with long-context models, super-long dialogs increase compute cost and often degrade quality. Key challenges include:</p><ul><li><strong>Context length limits</strong>: Token budgets remain finite.</li><li><strong>Information relevance</strong>: History contains plenty of facts irrelevant to the current query.</li><li><strong>Semantic coherence</strong>: Related information may be scattered across non-contiguous turns.</li><li><strong>Personalization</strong>: The agent must remember user preferences and interaction patterns.</li></ul><h3 id="1-2-A-Quick-Landscape-of-Existing-Approaches"><a href="#1-2-A-Quick-Landscape-of-Existing-Approaches" class="headerlink" title="1.2 A Quick Landscape of Existing Approaches"></a>1.2 A Quick Landscape of Existing Approaches</h3><p>The community’s strategies roughly split into three camps:</p><ol><li><strong>“Give Me Everything” (full history)</strong><ul><li>Complete information, zero recall loss.</li><li>But like moving an entire library just to find one book—computational overkill.</li></ul></li><li><strong>“Bullet-Point Digest” (summaries)</strong><ul><li>Compact and efficient.</li><li>Risk of omitting crucial details during abstraction.</li></ul></li><li><strong>“Precision Strike” (retrieval-based)</strong><ul><li>Fetch only what you need, exactly when you need it.</li><li>Success hinges on choosing the right retrieval granularity—precisely the issue SeCom addresses.</li></ul></li></ol><h4 id="1-2-3-Retrieval-Augmented-Generation-RAG-in-Dialog"><a href="#1-2-3-Retrieval-Augmented-Generation-RAG-in-Dialog" class="headerlink" title="1.2.3 Retrieval-Augmented Generation (RAG) in Dialog"></a>1.2.3 Retrieval-Augmented Generation (RAG) in Dialog</h4><p>RAG faces dialog-specific hurdles:</p><ul><li><strong>Chunking strategy</strong>: How to segment a dialog into retrievable units.</li><li><strong>Relevance estimation</strong>: Harder than in static docs due to dialog dynamics.</li><li><strong>Temporal dependency</strong>: Order matters; turns refer to earlier context.</li></ul><h3 id="1-3-The-Granularity-Dilemma"><a href="#1-3-The-Granularity-Dilemma" class="headerlink" title="1.3 The Granularity Dilemma"></a>1.3 The Granularity Dilemma</h3><p>We often index memories at the turn-level or at the whole-conversation level. Both extremes break down:</p><ul><li><strong>Turn-level</strong> → fragments context, loses dependencies, retrieval recall suffers.</li><li><strong>Conversation-level</strong> → topic mixture, lots of noise, retrieval becomes coarse.</li><li><strong>Summaries</strong> → irreversible information loss.</li></ul><p>SeCom’s insight: dialog naturally contains <strong>paragraph-level thematic boundaries</strong>. Segmenting at this “just-right” granularity preserves coherence without exploding memory size.</p><h2 id="2-Inside-SeCom"><a href="#2-Inside-SeCom" class="headerlink" title="2. Inside SeCom"></a>2. Inside SeCom</h2><h3 id="2-1-Two-Key-Insights"><a href="#2-1-Two-Key-Insights" class="headerlink" title="2.1 Two Key Insights"></a>2.1 Two Key Insights</h3><ol><li><strong>Paragraph-like Topic Shifts</strong> exist in dialog just as in essays.</li><li><strong>Natural Language Is Redundant</strong>—filler words, confirmations, small talk, etc. Removing them boosts retrieval precision.</li></ol><p>Hence <strong>SeCom = Segmentation + Compression</strong>.</p><h3 id="2-2-System-Pipeline"><a href="#2-2-System-Pipeline" class="headerlink" title="2.2 System Pipeline"></a>2.2 System Pipeline</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">History → [Segmenter] → Paragraph-level units → [Compressor] → Denoised memories → [Retriever] → Relevant context → [Generator] → Final reply</span><br></pre></td></tr></tbody></table></figure><p>Technically:</p><ol><li>Segmenter $f_{\mathcal I}$ splits the dialog.</li><li>Compressor $f_{comp}$ denoises each segment.</li><li>Retriever $f_R$ ranks memories for the current user utterance $u^*$.</li><li>LLM $f_{LLM}$ produces the answer based on top-N memories.</li></ol><h3 id="2-3-How-to-Segment-Without-Labels"><a href="#2-3-How-to-Segment-Without-Labels" class="headerlink" title="2.3 How to Segment Without Labels"></a>2.3 How to Segment Without Labels</h3><p>SeCom leverages GPT-4 in a <strong>zero-shot</strong> fashion: craft a prompt asking the model to mark topic boundaries and output span indices. No training data required.</p><p>When limited gold data are available, a <strong>reflection-based</strong> loop iteratively refines the guidelines using WindowDiff scores and GPT-4 reasoning.</p><p>An <strong>incremental segmenter</strong> decides on-the-fly whether a new turn merges into the previous segment or starts a fresh one.</p><h3 id="2-4-Denoising-via-LLMLingua-2"><a href="#2-4-Denoising-via-LLMLingua-2" class="headerlink" title="2.4 Denoising via LLMLingua-2"></a>2.4 Denoising via LLMLingua-2</h3><p>LLMLingua-2 scores token importance and keeps the top $(1-r)$ fraction (e.g., 25 %) accordingly. Empirically, retaining just 25 % tokens preserves <strong>&gt;95 %</strong> key information, lifts retrieval GPT4Score by <strong>+9.46</strong>, and yields 4 × speed-up.</p><h3 id="2-5-Hybrid-Retrieval"><a href="#2-5-Hybrid-Retrieval" class="headerlink" title="2.5 Hybrid Retrieval"></a>2.5 Hybrid Retrieval</h3><p>BM25 (sparse) and MPNet (dense) scores are linearly combined:</p><p>$$\text{score}_{hybrid}=\alpha,\text{BM25}+(1-\alpha),\text{MPNet}, \quad \alpha=0.6$$</p><h2 id="3-Final-Thoughts"><a href="#3-Final-Thoughts" class="headerlink" title="3. Final Thoughts"></a>3. Final Thoughts</h2><h3 id="3-1-What-SeCom-Teaches-Us"><a href="#3-1-What-SeCom-Teaches-Us" class="headerlink" title="3.1 What SeCom Teaches Us"></a>3.1 What SeCom Teaches Us</h3><ul><li><strong>Simplicity Wins</strong>: Segment + Compress, nothing fancy, yet highly effective.</li><li><strong>Understand the Problem First</strong>: The authors nailed the granularity pain-point before designing a solution.</li></ul><p>Future directions:</p><ul><li><strong>Personalized segmentation</strong> tuned to each user’s dialog style.</li><li><strong>Real-time adaptation</strong> of compression and segmentation based on quality metrics.</li></ul><hr><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><strong>Paper</strong>: <a href="https://www.arxiv.org/abs/2502.05589">SeCom: On Memory Construction and Retrieval for Personalized Conversational Agents (ICLR 2025)</a></li><li><strong>Project Page</strong>: <a href="https://llmlingua.com/secom.html">https://llmlingua.com/secom.html</a></li><li><strong>Code</strong>: SeCom-main</li><li><strong>Datasets</strong>: LOCOMO, Long-MT-Bench+, DialSeg711, TIAGE, SuperDialSeg</li></ul><p><em>This post is based on Microsoft &amp; Tsinghua University’s ICLR 2025 paper. Please refer to the original publication and open-source repo for implementation details.</em> </p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;SeCom-Redefining-Memory-Management-in-Conversational-AI&quot;&gt;&lt;a href=&quot;#SeCom-Redefining-Memory-Management-in-Conversational-AI&quot; class=&quot;h</summary>
      
    
    
    
    <category term="NLP Insights" scheme="https://chenhuiyu.github.io/categories/NLP-Insights/"/>
    
    
    <category term="Conversational AI" scheme="https://chenhuiyu.github.io/tags/Conversational-AI/"/>
    
    <category term="Memory Management" scheme="https://chenhuiyu.github.io/tags/Memory-Management/"/>
    
    <category term="SeCom" scheme="https://chenhuiyu.github.io/tags/SeCom/"/>
    
    <category term="RAG" scheme="https://chenhuiyu.github.io/tags/RAG/"/>
    
  </entry>
  
  <entry>
    <title>SeCom: 重新定义对话AI的记忆管理</title>
    <link href="https://chenhuiyu.github.io/2025/06/24/NLP%20Insights/SeCom:%20%E9%87%8D%E6%96%B0%E5%AE%9A%E4%B9%89%E5%AF%B9%E8%AF%9DAI%E7%9A%84%E8%AE%B0%E5%BF%86%E7%AE%A1%E7%90%86/"/>
    <id>https://chenhuiyu.github.io/2025/06/24/NLP%20Insights/SeCom:%20%E9%87%8D%E6%96%B0%E5%AE%9A%E4%B9%89%E5%AF%B9%E8%AF%9DAI%E7%9A%84%E8%AE%B0%E5%BF%86%E7%AE%A1%E7%90%86/</id>
    <published>2025-06-24T08:00:00.000Z</published>
    <updated>2025-06-24T08:24:33.265Z</updated>
    
    <content type="html"><![CDATA[<h1 id="SeCom-重新定义对话AI的记忆管理"><a href="#SeCom-重新定义对话AI的记忆管理" class="headerlink" title="SeCom: 重新定义对话AI的记忆管理"></a>SeCom: 重新定义对话AI的记忆管理</h1><h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>最近笔者一直在研究对话AI中的内存管理问题，特别是长期对话场景下的记忆构建与检索技术。发现了一篇令人眼前一亮的ICLR 2025论文——<strong>《SeCom: On Memory Construction and Retrieval for Personalized Conversational Agents》</strong>，由Microsoft和清华大学的研究团队联合发表。</p><p>这篇论文提出的SeCom方法巧妙地解决了一个核心问题：<strong>如何在长期对话中有效管理和检索历史信息</strong>？今天想和大家分享一下这个方法的技术细节和创新点，希望能为从事相关研究的朋友们提供一些启发。</p><h2 id="1-为什么我们需要关注对话内存管理？"><a href="#1-为什么我们需要关注对话内存管理？" class="headerlink" title="1. 为什么我们需要关注对话内存管理？"></a>1. 为什么我们需要关注对话内存管理？</h2><h3 id="1-1-长期对话的现实挑战"><a href="#1-1-长期对话的现实挑战" class="headerlink" title="1.1 长期对话的现实挑战"></a>1.1 长期对话的现实挑战</h3><p>在与LLMs的日常交互中，相信大家都遇到过这样的困扰：当对话变得很长时，AI似乎”忘记”了之前讨论的内容，或者给出的回答与前面的上下文不够连贯。这背后反映的正是长期对话中的内存管理挑战。</p><p>随着大语言模型技术的成熟，基于LLM的对话代理已经深入到我们生活的方方面面。但是，当我们希望与AI进行真正的长期、个性化交互时——比如跨越数天、数周的项目讨论，现有的技术就显得力不从心了。</p><p>长期对话面临的主要技术挑战包括：</p><ul><li><strong>上下文长度限制</strong>：即使是支持长上下文的模型，在处理超长对话时也面临计算成本和性能下降的问题</li><li><strong>信息相关性</strong>：历史对话中可能包含大量与当前查询无关的信息</li><li><strong>语义连贯性</strong>：相关信息可能分散在多个不连续的对话轮次中</li><li><strong>个性化记忆</strong>：需要记住用户的偏好、习惯和历史交互模式</li></ul><h3 id="1-2-笔者对Memory管理领域的观察"><a href="#1-2-笔者对Memory管理领域的观察" class="headerlink" title="1.2 笔者对Memory管理领域的观察"></a>1.2 笔者对Memory管理领域的观察</h3><p>在深入研究这个领域的过程中，笔者发现对话内存管理其实是一个相当复杂的系统工程。它的核心目标听起来很简单：从历史对话中提取、存储和检索相关信息，以支持当前对话的生成。但实际实现起来，需要解决三个关键问题：</p><ol><li><strong>内存构建（Memory Construction）</strong>：如何将自然语言对话转换为结构化的内存单元？</li><li><strong>内存检索（Memory Retrieval）</strong>：面对海量历史信息，如何快速准确地找到相关内容？</li><li><strong>响应生成（Response Generation）</strong>：如何基于检索到的记忆生成连贯、个性化的回复？</li></ol><p>听起来是不是很像人类的记忆机制？确实如此，这也是为什么这个问题如此有趣和具有挑战性。</p><h4 id="1-2-2-现有方法的”三国演义”"><a href="#1-2-2-现有方法的”三国演义”" class="headerlink" title="1.2.2 现有方法的”三国演义”"></a>1.2.2 现有方法的”三国演义”</h4><p>在研究过程中，笔者发现现有的方法大致可以分为三大流派，每个都有自己的”哲学”：</p><p><strong>“全盘托出”派（基于完整历史）</strong>：</p><ul><li><strong>核心思想</strong>：既然不知道什么重要，那就全部给你！</li><li><strong>优势</strong>：信息完整，绝不遗漏</li><li><strong>问题</strong>：就像把整个图书馆搬给你找一本书，效率可想而知</li></ul><p><strong>“提纲挈领”派（基于摘要）</strong>：</p><ul><li><strong>核心思想</strong>：重要的信息浓缩成摘要就够了</li><li><strong>优势</strong>：信息压缩，计算高效</li><li><strong>问题</strong>：摘要过程中重要细节可能”意外失踪”</li></ul><p><strong>“精准打击”派（基于检索）</strong>：</p><ul><li><strong>代表方法</strong>：轮次级检索、会话级检索</li><li><strong>核心思想</strong>：需要什么就检索什么，按需取用</li><li><strong>优势</strong>：计算效率高，定位精确</li><li><strong>问题</strong>：关键在于如何确定检索的”粒度”——这正是SeCom要解决的核心问题！</li></ul><h4 id="1-2-3-检索增强生成（RAG）在对话中的应用"><a href="#1-2-3-检索增强生成（RAG）在对话中的应用" class="headerlink" title="1.2.3 检索增强生成（RAG）在对话中的应用"></a>1.2.3 检索增强生成（RAG）在对话中的应用</h4><p>检索增强生成技术在对话系统中的应用日益广泛，主要包括：</p><ul><li>**Dense Passage Retrieval (DPR)**：使用预训练的密集检索模型</li><li><strong>BM25</strong>：基于词频统计的稀疏检索方法</li><li><strong>Hybrid Retrieval</strong>：结合密集检索和稀疏检索的优势</li></ul><p>然而，现有RAG方法在对话场景中面临独特挑战：</p><ul><li><strong>分块策略（Chunking Strategy）</strong>：如何将对话分割为检索单元</li><li><strong>相关性判断</strong>：对话的相关性判断比文档检索更复杂</li><li><strong>时序依赖</strong>：对话具有强时序性，前后文关系重要</li></ul><h3 id="1-3-内存粒度问题的深层分析"><a href="#1-3-内存粒度问题的深层分析" class="headerlink" title="1.3 内存粒度问题的深层分析"></a>1.3 内存粒度问题的深层分析</h3><h4 id="1-3-1-轮次级内存的局限性"><a href="#1-3-1-轮次级内存的局限性" class="headerlink" title="1.3.1 轮次级内存的局限性"></a>1.3.1 轮次级内存的局限性</h4><p>轮次级内存将每个用户-代理交互（turn）作为独立的内存单元：</p><p><strong>数学表示</strong>：<br>设对话历史 $\mathcal{H} = {\mathbf{c}<em>i}</em>{i=1}^C$，其中每个会话 $\mathbf{c}<em>i = {\mathbf{t}<em>j}</em>{j=1}^{T_i}$<br>轮次级内存：$|\mathcal{M}| = \sum</em>{i=1}^C T_i$，每个 $\mathbf{m} \in \mathcal{M}$ 对应一个轮次 $\mathbf{t}$</p><p><strong>主要问题</strong>：</p><ul><li><strong>信息碎片化</strong>：相关信息分散在多个轮次中，单个轮次可能缺乏完整语义</li><li><strong>上下文缺失</strong>：轮次间的依赖关系丢失</li><li><strong>检索精度低</strong>：查询词汇可能不直接出现在相关轮次中</li></ul><p><strong>具体示例</strong>：<br>用户在第3轮询问”什么是机器学习”，第5轮询问”监督学习的例子”，第8轮询问”如何选择算法”。当用户在第10轮询问”之前提到的分类算法性能如何评估”时，轮次级检索可能无法找到完整的上下文。</p><h4 id="1-3-2-会话级内存的局限性"><a href="#1-3-2-会话级内存的局限性" class="headerlink" title="1.3.2 会话级内存的局限性"></a>1.3.2 会话级内存的局限性</h4><p>会话级内存将整个对话会话作为内存单元：</p><p><strong>数学表示</strong>：<br>会话级内存：$|\mathcal{M}| = C$，每个 $\mathbf{m} \in \mathcal{M}$ 对应一个会话 $\mathbf{c}$</p><p><strong>主要问题</strong>：</p><ul><li><strong>主题混杂</strong>：单个会话可能包含多个不相关主题</li><li><strong>噪声干扰</strong>：大量无关信息影响检索和生成质量</li><li><strong>检索粗糙</strong>：无法精确定位到具体相关内容</li></ul><p><strong>具体示例</strong>：<br>一个会话中用户讨论了机器学习、烹饪食谱、旅行计划和电影推荐。当查询机器学习相关问题时，检索到的会话包含大量无关的烹饪和旅行信息。</p><h4 id="1-3-3-摘要化方法的信息损失"><a href="#1-3-3-摘要化方法的信息损失" class="headerlink" title="1.3.3 摘要化方法的信息损失"></a>1.3.3 摘要化方法的信息损失</h4><p>摘要化方法通过压缩对话内容来减少信息量：</p><p><strong>主要问题</strong>：</p><ul><li><strong>细节丢失</strong>：摘要过程中重要细节可能被省略</li><li><strong>主观性</strong>：摘要质量依赖于模型的理解能力</li><li><strong>不可逆性</strong>：一旦信息被摘要，原始细节无法恢复</li></ul><h2 id="2-SeCom的设计"><a href="#2-SeCom的设计" class="headerlink" title="2. SeCom的设计"></a>2. SeCom的设计</h2><h3 id="2-1-核心发现"><a href="#2-1-核心发现" class="headerlink" title="2.1 核心发现"></a>2.1 核心发现</h3><p>SeCom（<strong>Se</strong>gmentation + <strong>Com</strong>pression）的两个核心发现：</p><p><strong>洞察一：对话天然具有”段落”结构</strong><br>就像我们写文章会分段一样，人类的对话其实也有天然的主题边界。比如在一次长对话中，我们可能先讨论工作项目，然后转到周末计划，再聊到最近看的电影。每个主题就是一个天然的”段落”。</p><p>传统方法要么把每句话当作独立单元（太碎片化），要么把整个对话当作一个整体（太粗糙），而SeCom找到了中间的最佳平衡点——<strong>段落级的语义单元</strong>。</p><p><strong>洞察二：自然语言充满”废话”</strong><br>这听起来有点刻薄，但确实如此。我们日常对话中充满了”嗯”、”那个”、”你知道的”这样的冗余表达，还有大量的重复、确认、客套话。这些在人际交流中很重要，但对机器检索来说就是噪声。</p><p>SeCom通过智能压缩，保留关键信息的同时去除这些”噪声”，让检索更加精准。</p><h3 id="2-2-系统设计"><a href="#2-2-系统设计" class="headerlink" title="2.2 系统设计"></a>2.2 系统设计</h3><p>SeCom的整体架构设计非常优雅，就像一条高效的流水线：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">历史对话 → [分段器] → 段落级内存单元 → [压缩器] → 去噪内存单元 → [检索器] → 相关上下文 → [生成器] → 最终回复</span><br></pre></td></tr></tbody></table></figure><p>用更直观的话来解释这个流程：</p><ol><li><strong>分段器</strong>：将杂乱的对话历史按主题”切块”</li><li><strong>压缩器</strong>：将每个”块”中的废话去掉，保留精华</li><li><strong>检索器</strong>：根据当前问题找到最相关的”块”</li><li><strong>生成器</strong>：基于相关信息生成回答</li></ol><p><strong>技术表示</strong>（没什么用，写给喜欢数学的朋友）：<br>设 $f_{\mathcal{I}}$ 为分段器，$f_{Comp}$ 为压缩器，$f_R$ 为检索器，$f_{LLM}$ 为生成器</p><p>完整流程：</p><ol><li>${\mathbf{s}<em>k}</em>{k=1}^K \leftarrow f_{\mathcal{I}}(\mathcal{H})$ （对话分段）</li><li>${\mathbf{m}<em>k}</em>{k=1}^K \leftarrow f_{Comp}({\mathbf{s}<em>k}</em>{k=1}^K)$ （压缩去噪）</li><li>${\mathbf{m}<em>n}</em>{n=1}^N \leftarrow f_R(u^*, {\mathbf{m}<em>k}</em>{k=1}^K, N)$ （内存检索）</li><li>$r^* = f_{LLM}(u^*, {\mathbf{m}<em>n}</em>{n=1}^N)$ （响应生成）</li></ol><h3 id="2-3-分段算法：教AI学会”断句”"><a href="#2-3-分段算法：教AI学会”断句”" class="headerlink" title="2.3 分段算法：教AI学会”断句”"></a>2.3 分段算法：教AI学会”断句”</h3><h4 id="2-3-1-零样本分段"><a href="#2-3-1-零样本分段" class="headerlink" title="2.3.1 零样本分段"></a>2.3.1 零样本分段</h4><p>如何让AI自动识别对话中的主题边界？传统方法需要大量标注数据训练专门的分段模型，而SeCom采用了一个非常聪明的”零样本”方法。</p><p><strong>核心思路</strong>：<br>既然GPT-4这样的大模型已经具备了强大的文本理解能力，为什么不直接让它来判断对话的主题边界呢？就像让一个文学老师来给文章分段一样。</p><p><strong>输入预处理</strong>：<br>将对话会话增强为结构化格式：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Turn j: </span><br><span class="line">[user]: u_j</span><br><span class="line">[agent]: r_j</span><br></pre></td></tr></tbody></table></figure><p><strong>分段提示设计</strong>：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">分析以下对话，识别主题边界，将对话分割为语义连贯的段落。</span><br><span class="line">每个段落应该：</span><br><span class="line">1. 围绕单一主题或相关主题</span><br><span class="line">2. 包含完整的交互序列</span><br><span class="line">3. 具有明确的开始和结束边界</span><br><span class="line"></span><br><span class="line">对话内容：</span><br><span class="line">[对话内容]</span><br><span class="line"></span><br><span class="line">请输出每个段落的起始和结束轮次编号。</span><br></pre></td></tr></tbody></table></figure><p><strong>优势</strong>：</p><ul><li>无需训练数据，适用于开放域对话</li><li>利用LLM的强大理解能力</li><li>可处理复杂的主题转换模式</li></ul><h4 id="2-3-2-基于反思的分段优化"><a href="#2-3-2-基于反思的分段优化" class="headerlink" title="2.3.2 基于反思的分段优化"></a>2.3.2 基于反思的分段优化</h4><p>当有少量标注数据时，采用反思机制优化分段效果：</p><p><strong>算法步骤</strong>：</p><ol><li><strong>初始分段</strong>：使用零样本方法对批量数据进行分段</li><li><strong>错误识别</strong>：基于WindowDiff指标选择top-K个分段错误最大的样本</li><li><strong>反思学习</strong>：让LLM分析分段错误，更新分段指导原则</li><li><strong>迭代优化</strong>：重复上述过程直到收敛</li></ol><p><strong>反思提示设计</strong>：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">分析以下分段错误，并更新分段指导原则：</span><br><span class="line"></span><br><span class="line">错误案例：</span><br><span class="line">[分段结果] vs [标准答案]</span><br><span class="line"></span><br><span class="line">请分析错误原因并提供改进的分段指导原则。</span><br></pre></td></tr></tbody></table></figure><p><strong>数学表示</strong>：<br>设 $\boldsymbol{G}<em>m$ 为第m轮的分段指导原则，更新公式为：<br>$$\boldsymbol{G}</em>{m+1} = \boldsymbol{G}_m - \eta \nabla \mathcal{L}(\boldsymbol{G}_m)$$</p><p>其中 $\nabla \mathcal{L}(\boldsymbol{G}_m)$ 为LLM隐式估计的分段损失梯度。</p><h4 id="2-3-3-增量分段算法"><a href="#2-3-3-增量分段算法" class="headerlink" title="2.3.3 增量分段算法"></a>2.3.3 增量分段算法</h4><p>对于新增的对话轮次，设计增量分段算法：</p><p><strong>算法流程</strong>：</p><ol><li>输入新轮次 $\mathbf{t}<em>{new}$ 和前一段落 $\mathbf{s}</em>{prev}$</li><li>判断是否应该合并：$binary = f_{judge}(\mathbf{t}<em>{new}, \mathbf{s}</em>{prev})$</li><li>如果合并：$\mathbf{s}<em>{prev} \leftarrow \mathbf{s}</em>{prev} \cup {\mathbf{t}_{new}}$</li><li>否则：创建新段落 $\mathbf{s}<em>{new} = {\mathbf{t}</em>{new}}$</li></ol><p><strong>判断提示</strong>：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">判断新的用户-机器人轮次是否应该与前一段落合并：</span><br><span class="line"></span><br><span class="line">新轮次：[新轮次内容]</span><br><span class="line">前一段落：[前一段落内容]</span><br><span class="line"></span><br><span class="line">如果属于同一主题，回答"Yes"，否则回答"No"。</span><br></pre></td></tr></tbody></table></figure><h3 id="2-4-压缩式内存去噪"><a href="#2-4-压缩式内存去噪" class="headerlink" title="2.4 压缩式内存去噪"></a>2.4 压缩式内存去噪</h3><h4 id="2-4-1-自然语言冗余性分析"><a href="#2-4-1-自然语言冗余性分析" class="headerlink" title="2.4.1 自然语言冗余性分析"></a>2.4.1 自然语言冗余性分析</h4><p><strong>理论基础</strong>：<br>根据Shannon信息论，自然语言具有高度冗余性，冗余率约为50-75%。这种冗余在人类交流中有助于错误纠正和理解，但在机器检索中构成噪声。</p><p><strong>冗余类型</strong>：</p><ol><li><strong>词汇冗余</strong>：同义词、重复表达</li><li><strong>语法冗余</strong>：冗余的语法结构</li><li><strong>语义冗余</strong>：重复的语义信息</li><li><strong>对话冗余</strong>：客套话、确认性回复</li></ol><h4 id="2-4-2-LLMLingua-2压缩原理"><a href="#2-4-2-LLMLingua-2压缩原理" class="headerlink" title="2.4.2 LLMLingua-2压缩原理"></a>2.4.2 LLMLingua-2压缩原理</h4><p><strong>算法核心</strong>：<br>LLMLingua-2基于token重要性评分进行压缩：</p><ol><li><p><strong>重要性评分</strong>：<br>$$s_i = f_{score}(x_i | x_{&lt;i}, x_{&gt;i})$$<br>其中 $x_i$ 为第i个token，$x_{&lt;i}$ 和 $x_{&gt;i}$ 为上下文</p></li><li><p><strong>动态压缩</strong>：<br>根据目标压缩率 $r$，保留top $(1-r) \times N$ 个重要token</p></li><li><p><strong>语义保持</strong>：<br>通过双向上下文建模确保关键语义信息不丢失</p></li></ol><p><strong>压缩效果分析</strong>：<br>实验表明，75%压缩率下：</p><ul><li>关键信息保留率 &gt; 95%</li><li>检索相关性提升 9.46分（GPT4Score）</li><li>计算效率提升 4倍</li></ul><h4 id="2-4-3-压缩对检索性能的影响"><a href="#2-4-3-压缩对检索性能的影响" class="headerlink" title="2.4.3 压缩对检索性能的影响"></a>2.4.3 压缩对检索性能的影响</h4><p><strong>相似性变化分析</strong>：<br>设 $sim(q, s)$ 为查询q与段落s的相似性</p><p>压缩前：$sim_{before}(q, s_{relevant})$，$sim_{before}(q, s_{irrelevant})$<br>压缩后：$sim_{after}(q, s’<em>{relevant})$，$sim</em>{after}(q, s’_{irrelevant})$</p><p>实验结果显示：</p><ul><li>$sim_{after}(q, s’<em>{relevant}) &gt; sim</em>{before}(q, s_{relevant})$ （相关段落相似性提升）</li><li>$sim_{after}(q, s’<em>{irrelevant}) &lt; sim</em>{before}(q, s_{irrelevant})$ （无关段落相似性降低）</li></ul><h3 id="2-5-多模态检索系统"><a href="#2-5-多模态检索系统" class="headerlink" title="2.5 多模态检索系统"></a>2.5 多模态检索系统</h3><h4 id="2-5-1-检索器选择与配置"><a href="#2-5-1-检索器选择与配置" class="headerlink" title="2.5.1 检索器选择与配置"></a>2.5.1 检索器选择与配置</h4><p><strong>BM25检索器</strong>：<br>$$BM25(q, d) = \sum_{i=1}^{|q|} IDF(q_i) \cdot \frac{tf(q_i, d) \cdot (k_1 + 1)}{tf(q_i, d) + k_1 \cdot (1 - b + b \cdot \frac{|d|}{avgdl})}$$</p><p>参数设置：$k_1 = 1.2$，$b = 0.75$</p><p><strong>MPNet检索器</strong>：<br>基于MPNet模型的密集检索：<br>$$score = \cos(\mathbf{e}_q, \mathbf{e}_d)$$<br>其中 $\mathbf{e}_q$ 和 $\mathbf{e}_d$ 分别为查询和文档的向量表示</p><h4 id="2-5-2-混合检索策略"><a href="#2-5-2-混合检索策略" class="headerlink" title="2.5.2 混合检索策略"></a>2.5.2 混合检索策略</h4><p>结合稀疏检索和密集检索的优势：<br>$$score_{hybrid} = \alpha \cdot score_{BM25} + (1-\alpha) \cdot score_{MPNet}$$</p><p>通过实验确定最优权重 $\alpha = 0.6$</p><h2 id="3-写在最后：一些思考"><a href="#3-写在最后：一些思考" class="headerlink" title="3. 写在最后：一些思考"></a>3. 写在最后：一些思考</h2><h3 id="3-1-SeCom给我们的启发"><a href="#3-1-SeCom给我们的启发" class="headerlink" title="3.1 SeCom给我们的启发"></a>3.1 SeCom给我们的启发</h3><p>研读这篇论文后，笔者有几点深刻的感悟：</p><p><strong>简单往往是最有效的</strong>：SeCom的核心思想其实很简单——分段+压缩，但正是这种简单的组合解决了复杂的问题。这提醒我们，在面对技术挑战时，有时候最朴素的想法反而是最有效的。</p><p><strong>理解问题比解决问题更重要</strong>：作者团队深入分析了内存粒度问题的本质，发现了段落级内存的最优性。这种对问题本质的深刻理解是技术创新的基础。</p><p>笔者认为未来可能的发展方向包括：</p><ul><li><strong>个性化分段策略</strong>：不同用户的对话模式不同，能否学习个性化的分段方式？</li><li><strong>实时优化机制</strong>：能否根据对话质量动态调整压缩率和分段策略？</li></ul><hr><h2 id="参考资源"><a href="#参考资源" class="headerlink" title="参考资源"></a>参考资源</h2><ul><li><strong>论文链接</strong>：<a href="https://www.arxiv.org/abs/2502.05589">SeCom: On Memory Construction and Retrieval for Personalized Conversational Agents (ICLR 2025)</a></li><li><strong>项目主页</strong>：<a href="https://llmlingua.com/secom.html">https://llmlingua.com/secom.html</a></li><li><strong>代码仓库</strong>：SeCom-main项目</li><li><strong>数据集</strong>：LOCOMO、Long-MT-Bench+、DialSeg711、TIAGE、SuperDialSeg</li></ul><p><em>本文基于Microsoft和清华大学联合研究团队在ICLR 2025发表的论文撰写，详细技术实现请参考原始论文和开源代码。</em> </p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;SeCom-重新定义对话AI的记忆管理&quot;&gt;&lt;a href=&quot;#SeCom-重新定义对话AI的记忆管理&quot; class=&quot;headerlink&quot; title=&quot;SeCom: 重新定义对话AI的记忆管理&quot;&gt;&lt;/a&gt;SeCom: 重新定义对话AI的记忆管理&lt;/h1&gt;&lt;h2</summary>
      
    
    
    
    <category term="NLP Insights" scheme="https://chenhuiyu.github.io/categories/NLP-Insights/"/>
    
    
    <category term="Conversational AI" scheme="https://chenhuiyu.github.io/tags/Conversational-AI/"/>
    
    <category term="Memory Management" scheme="https://chenhuiyu.github.io/tags/Memory-Management/"/>
    
    <category term="SeCom" scheme="https://chenhuiyu.github.io/tags/SeCom/"/>
    
    <category term="RAG" scheme="https://chenhuiyu.github.io/tags/RAG/"/>
    
  </entry>
  
  <entry>
    <title>Decoder-only与Encoder-only模型Padding策略的差异</title>
    <link href="https://chenhuiyu.github.io/2025/03/06/NLP%20Insights/Decoder%E6%A8%A1%E5%9E%8B%E5%92%8CEncoder%E6%A8%A1%E5%9E%8B%E5%9C%A8Padding%E4%B8%AD%E7%9A%84%E4%B8%8D%E5%90%8C/"/>
    <id>https://chenhuiyu.github.io/2025/03/06/NLP%20Insights/Decoder%E6%A8%A1%E5%9E%8B%E5%92%8CEncoder%E6%A8%A1%E5%9E%8B%E5%9C%A8Padding%E4%B8%AD%E7%9A%84%E4%B8%8D%E5%90%8C/</id>
    <published>2025-03-06T09:43:10.000Z</published>
    <updated>2025-03-06T09:49:52.873Z</updated>
    
    <content type="html"><![CDATA[<h2 id="📌-Padding-的含义"><a href="#📌-Padding-的含义" class="headerlink" title="📌 Padding 的含义"></a>📌 <strong>Padding 的含义</strong></h2><p>在大模型 (<strong>LLM</strong>) 中，<strong>padding</strong> 是用于将不同长度的序列调整为同一长度的方法，以便于批量 (<strong>batch</strong>) 处理。</p><p>例如：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">句子1: "I love NLP"</span><br><span class="line">句子2: "Padding is useful in LLM training"</span><br></pre></td></tr></tbody></table></figure><p>使用 <code>&lt;pad&gt;</code> token 进行对齐：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">"I love NLP &lt;pad&gt; &lt;pad&gt; &lt;pad&gt;"</span><br><span class="line">"Padding is useful in LLM training"</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="📌-Padding-位置的选择：Left-vs-Right"><a href="#📌-Padding-位置的选择：Left-vs-Right" class="headerlink" title="📌 Padding 位置的选择：Left vs Right"></a>📌 <strong>Padding 位置的选择：Left vs Right</strong></h2><p>Padding 有两种常见方式：</p><ul><li><p><strong>Right padding</strong>（右填充）：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">"I love NLP &lt;pad&gt; &lt;pad&gt;"</span><br></pre></td></tr></tbody></table></figure></li><li><p><strong>Left padding</strong>（左填充）：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">"&lt;pad&gt; &lt;pad&gt; I love NLP"</span><br></pre></td></tr></tbody></table></figure></li></ul><p>通常：</p><ul><li><strong>Decoder-only 模型</strong>（如 GPT, Llama）：采用 <strong>Left padding</strong></li><li><strong>Encoder-only 模型</strong>（如 BERT）：采用 <strong>Right padding</strong></li></ul><p>具体而言，Transformer 模型通常分为三类结构：</p><table><thead><tr><th>模型类型</th><th>代表模型</th><th>特征</th><th>常见用途</th></tr></thead><tbody><tr><td><strong>Encoder-only</strong></td><td><strong>BERT</strong>、RoBERTa、ALBERT、ELECTRA</td><td>双向注意力（Bidirectional Attention）</td><td>自然语言理解（NLU），如文本分类、序列标注</td></tr><tr><td><strong>Decoder-only</strong></td><td>GPT、GPT-2、GPT-3、GPT-4、LLaMA、Mistral</td><td>单向自回归注意力（Causal Attention）</td><td>文本生成、聊天、写作</td></tr><tr><td><strong>Encoder-Decoder</strong></td><td>Transformer原始论文中的模型、T5、BART、mT5、PEGASUS</td><td>Encoder为双向注意力，Decoder为单向自回归注意力</td><td>机器翻译、摘要生成、对话</td></tr></tbody></table><hr><h2 id="📌-为什么-Encoder-only-模型（如BERT）采用-Right-padding？"><a href="#📌-为什么-Encoder-only-模型（如BERT）采用-Right-padding？" class="headerlink" title="📌 为什么 Encoder-only 模型（如BERT）采用 Right padding？"></a>📌 为什么 Encoder-only 模型（如BERT）采用 Right padding？</h2><ul><li><strong>Encoder-only 模型</strong>（如 BERT）的核心目标是获得<strong>每个 token 的嵌入表示</strong>（Embedding representation）。</li><li>此类模型为<strong>双向注意力（Bidirectional Attention）</strong>，每个 token 可同时关注上下文，因此<strong>位置的轻微变化不会对结果造成严重干扰</strong>。</li><li>此外，encoder-only 模型中通常有特殊 token（如 <code>[CLS]</code>），位置相对稳定，用于句子分类或表示，因此采用 <strong>right padding</strong> 更自然，也更合理。</li></ul><p>示例说明：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[CLS] Hello I love NLP [SEP] &lt;pad&gt; &lt;pad&gt;</span><br></pre></td></tr></tbody></table></figure><ul><li>右填充后，<code>[CLS]</code> 和 <code>[SEP]</code> token 位置稳定，且便于模型专注于前面的有效信息。</li></ul><hr><h2 id="📌-为什么-Decoder-only-LLM-采用-Left-padding？"><a href="#📌-为什么-Decoder-only-LLM-采用-Left-padding？" class="headerlink" title="📌 为什么 Decoder-only LLM 采用 Left padding？"></a>📌 为什么 Decoder-only LLM 采用 Left padding？</h2><p>以 GPT 为代表的 <strong>Decoder-only 模型</strong> 是自回归（<strong>Autoregressive</strong>）模型，每个词的生成仅依赖于当前及之前的词，未来词不可见。因此：</p><ul><li><strong>位置编码的稳定性</strong>：<br>左填充确保真实 token 的相对位置稳定，模型生成新 token 时位置编码始终稳定于序列末尾。<ul><li>当采用<strong>绝对位置编码</strong>（Absolute Positional Encoding）时，每个 token（包括 <code>&lt;pad&gt;</code>）都有对应的位置编号。</li><li>对于左填充的 padding tokens，虽然它们占据了位置编号（如 1、2），但模型通过<strong>掩码机制</strong>忽略其对注意力和输出结果的影响。<br>示例：</li></ul></li></ul><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">位置编码: [ 1      2      3      4      5      6 ]</span><br><span class="line">Token:   [ &lt;pad&gt;, &lt;pad&gt;, Hello,  I,   love,  NLP ]</span><br><span class="line">掩码:     [  0,     0,     1,     1,     1,    1 ]</span><br></pre></td></tr></tbody></table></figure><ul><li>模型只关注掩码为 1 的有效 token，而忽略掩码为 0 的 padding tokens。</li><li><strong>注意力掩码（Attention Mask）</strong>：<br>左侧的 <code>&lt;pad&gt;</code> 会被<strong>注意力掩码（attention mask）忽略</strong>，从而避免 padding token 干扰有效 token 的位置编码和注意力计算。</li></ul><p>示例说明：</p><table><thead><tr><th>Token</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th></tr></thead><tbody><tr><td>Left</td><td><code>&lt;pad&gt;</code></td><td><code>&lt;pad&gt;</code></td><td>Hello</td><td>I</td><td>love</td><td>NLP</td></tr><tr><td>Right</td><td>Hello</td><td>I</td><td>love</td><td>NLP</td><td><code>&lt;pad&gt;</code></td><td><code>&lt;pad&gt;</code></td></tr></tbody></table><ul><li><strong>Left padding</strong> 下，最后有效 token 始终在同一位置（6）。</li><li><strong>Right padding</strong> 下，token 的位置随序列长度变化，影响位置编码的稳定性。</li></ul><hr><h2 id="📌-Padding-在训练与推理阶段的差异"><a href="#📌-Padding-在训练与推理阶段的差异" class="headerlink" title="📌 Padding 在训练与推理阶段的差异"></a>📌 <strong>Padding 在训练与推理阶段的差异</strong></h2><table><thead><tr><th>阶段 (Phase)</th><th>Padding 策略</th><th>原因</th></tr></thead><tbody><tr><td><strong>训练 (Training)</strong></td><td>批量处理时，Decoder-only 常用左填充；Encoder-only 模型则常用右填充</td><td>批量处理，加快计算效率</td></tr><tr><td><strong>推理 (Inference)</strong></td><td>通常单条序列，无需 padding；若需要批量推理，仍采用左填充</td><td>稳定位置编码</td></tr></tbody></table><hr><h2 id="📌-总结与关键要点（TL-DR）"><a href="#📌-总结与关键要点（TL-DR）" class="headerlink" title="📌 总结与关键要点（TL;DR）"></a>📌 <strong>总结与关键要点（TL;DR）</strong></h2><ul><li><strong>Padding</strong> 用于序列长度标准化。</li><li><strong>Decoder-only LLMs (GPT, Llama)</strong> 通常采用<strong>左填充（Left padding）</strong>，目的是<strong>稳定位置编码并避免未来信息泄漏</strong>；左侧 padding 会被掩码忽略，不干扰模型预测。</li><li><strong>Encoder-only 模型（如BERT系列）</strong>通常采用<strong>右填充（Right padding）</strong>，因为模型为双向注意力，且特殊token（如<code>[CLS]</code>）位置需要保持稳定。</li><li>位置编码中虽然 padding token 占位，但会被<strong>注意力掩码</strong>有效屏蔽，不影响模型的最终输出。</li></ul><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;📌-Padding-的含义&quot;&gt;&lt;a href=&quot;#📌-Padding-的含义&quot; class=&quot;headerlink&quot; title=&quot;📌 Padding 的含义&quot;&gt;&lt;/a&gt;📌 &lt;strong&gt;Padding 的含义&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;在大模型 </summary>
      
    
    
    
    <category term="NLP Insights" scheme="https://chenhuiyu.github.io/categories/NLP-Insights/"/>
    
    
    <category term="LLM" scheme="https://chenhuiyu.github.io/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>Differences in Padding Strategies Between Decoder-only and Encoder-only Models</title>
    <link href="https://chenhuiyu.github.io/2025/03/06/NLP%20Insights/Differences%20in%20Padding%20Strategies%20Between%20Decoder-only%20and%20Encoder-only%20Models/"/>
    <id>https://chenhuiyu.github.io/2025/03/06/NLP%20Insights/Differences%20in%20Padding%20Strategies%20Between%20Decoder-only%20and%20Encoder-only%20Models/</id>
    <published>2025-03-06T09:43:10.000Z</published>
    <updated>2025-03-06T09:51:24.700Z</updated>
    
    <content type="html"><![CDATA[<h2 id="📌-What-is-Padding"><a href="#📌-What-is-Padding" class="headerlink" title="📌 What is Padding?"></a>📌 <strong>What is Padding?</strong></h2><p>In <strong>Large Language Models (LLMs)</strong>, <strong>padding</strong> is a method used to standardize sequence lengths for <strong>batch processing</strong>.</p><p>For example:</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Sentence 1: "I love NLP"</span><br><span class="line">Sentence 2: "Padding is useful in LLM training"</span><br></pre></td></tr></tbody></table></figure><p>Using the <code>&lt;pad&gt;</code> token for alignment:</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">"I love NLP &lt;pad&gt; &lt;pad&gt; &lt;pad&gt;"</span><br><span class="line">"Padding is useful in LLM training"</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="📌-Padding-Positioning-Left-vs-Right"><a href="#📌-Padding-Positioning-Left-vs-Right" class="headerlink" title="📌 Padding Positioning: Left vs Right"></a>📌 <strong>Padding Positioning: Left vs Right</strong></h2><p>There are two common padding strategies:</p><ul><li><p><strong>Right padding</strong>:</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">"I love NLP &lt;pad&gt; &lt;pad&gt;"</span><br></pre></td></tr></tbody></table></figure></li><li><p><strong>Left padding</strong>:</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">"&lt;pad&gt; &lt;pad&gt; I love NLP"</span><br></pre></td></tr></tbody></table></figure></li></ul><p>Typically:</p><ul><li><strong>Decoder-only models</strong> (e.g., GPT, Llama): Use <strong>Left padding</strong>.</li><li><strong>Encoder-only models</strong> (e.g., BERT): Use <strong>Right padding</strong>.</li></ul><p>Transformers can be categorized into three main architectures:</p><table><thead><tr><th>Model Type</th><th>Representative Models</th><th>Characteristics</th><th>Common Applications</th></tr></thead><tbody><tr><td><strong>Encoder-only</strong></td><td><strong>BERT</strong>, RoBERTa, ALBERT, ELECTRA</td><td><strong>Bidirectional attention</strong></td><td>NLP tasks like text classification, named entity recognition</td></tr><tr><td><strong>Decoder-only</strong></td><td>GPT, GPT-2, GPT-3, GPT-4, LLaMA, Mistral</td><td><strong>Causal attention (Autoregressive)</strong></td><td>Text generation, chatbots, writing assistance</td></tr><tr><td><strong>Encoder-Decoder</strong></td><td>Transformer (original), T5, BART, mT5, PEGASUS</td><td><strong>Encoder: bidirectional, Decoder: autoregressive</strong></td><td>Machine translation, summarization, dialogue systems</td></tr></tbody></table><hr><h2 id="📌-Why-Do-Encoder-only-Models-e-g-BERT-Use-Right-Padding"><a href="#📌-Why-Do-Encoder-only-Models-e-g-BERT-Use-Right-Padding" class="headerlink" title="📌 Why Do Encoder-only Models (e.g., BERT) Use Right Padding?"></a>📌 <strong>Why Do Encoder-only Models (e.g., BERT) Use Right Padding?</strong></h2><ul><li><strong>Encoder-only models</strong> (like BERT) aim to obtain <strong>representations for each token</strong>.</li><li>These models use <strong>bidirectional attention</strong>, meaning each token attends to <strong>both past and future tokens</strong>.</li><li><strong>Slight shifts in position do not significantly impact model performance</strong>.</li><li>Special tokens (e.g., <code>[CLS]</code>) in BERT maintain a <strong>fixed position</strong> for tasks like classification, making <strong>right padding more natural</strong>.</li></ul><p>Example:</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[CLS] Hello I love NLP [SEP] &lt;pad&gt; &lt;pad&gt;</span><br></pre></td></tr></tbody></table></figure><ul><li>Right padding keeps <code>[CLS]</code> and <code>[SEP]</code> in stable positions, allowing the model to focus on meaningful tokens.</li></ul><hr><h2 id="📌-Why-Do-Decoder-only-LLMs-Use-Left-Padding"><a href="#📌-Why-Do-Decoder-only-LLMs-Use-Left-Padding" class="headerlink" title="📌 Why Do Decoder-only LLMs Use Left Padding?"></a>📌 <strong>Why Do Decoder-only LLMs Use Left Padding?</strong></h2><p><strong>Decoder-only models</strong> (like GPT) are <strong>autoregressive</strong>, meaning each token is generated based only on <strong>previous tokens</strong>, and future tokens are <strong>masked</strong>.</p><ul><li><strong>Positional Encoding Stability</strong>:<br>Left padding ensures that meaningful tokens have a <strong>consistent relative position</strong>, preventing <strong>position encoding misalignment</strong>.<ul><li>When using <strong>absolute positional encoding</strong>, every token (including <code>&lt;pad&gt;</code>) gets a unique position index.</li><li>Padding tokens at the beginning <strong>do not affect the model’s attention mechanism</strong> due to <strong>masking</strong>.</li></ul></li></ul><p>Example:</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Position Index: [ 1      2      3      4      5      6 ]</span><br><span class="line">Token:         [ &lt;pad&gt;, &lt;pad&gt;, Hello,  I,   love,  NLP ]</span><br><span class="line">Mask:          [  0,     0,     1,     1,     1,    1 ]</span><br></pre></td></tr></tbody></table></figure><ul><li><p>The model <strong>only attends to tokens where the mask is 1</strong>, ignoring padding tokens.</p></li><li><p><strong>Attention Masking</strong>:<br>Left padding ensures that <code>&lt;pad&gt;</code> tokens <strong>do not interfere with token position encoding</strong>.</p></li></ul><p>Illustration:</p><table><thead><tr><th>Token</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th></tr></thead><tbody><tr><td>Left</td><td><code>&lt;pad&gt;</code></td><td><code>&lt;pad&gt;</code></td><td>Hello</td><td>I</td><td>love</td><td>NLP</td></tr><tr><td>Right</td><td>Hello</td><td>I</td><td>love</td><td>NLP</td><td><code>&lt;pad&gt;</code></td><td><code>&lt;pad&gt;</code></td></tr></tbody></table><ul><li><strong>With Left padding</strong>, the last valid token <strong>always remains in the same position</strong>.</li><li><strong>With Right padding</strong>, token positions shift, affecting positional encoding stability.</li></ul><hr><h2 id="📌-Padding-Differences-in-Training-vs-Inference"><a href="#📌-Padding-Differences-in-Training-vs-Inference" class="headerlink" title="📌 Padding Differences in Training vs Inference"></a>📌 <strong>Padding Differences in Training vs Inference</strong></h2><table><thead><tr><th>Phase</th><th>Padding Strategy</th><th>Reason</th></tr></thead><tbody><tr><td><strong>Training</strong></td><td>Left padding for decoder-only; Right padding for encoder-only</td><td>Optimized for batch processing efficiency</td></tr><tr><td><strong>Inference</strong></td><td>Typically, no padding for single sequences; Left padding for batched inference</td><td>Ensures stable positional encoding</td></tr></tbody></table><hr><h2 id="📌-Summary-Key-Takeaways-TL-DR"><a href="#📌-Summary-Key-Takeaways-TL-DR" class="headerlink" title="📌 Summary &amp; Key Takeaways (TL;DR)"></a>📌 <strong>Summary &amp; Key Takeaways (TL;DR)</strong></h2><ul><li><strong>Padding</strong> standardizes sequence lengths for batch processing.</li><li><strong>Decoder-only models (GPT, Llama)</strong> use <strong>Left padding</strong> to <strong>stabilize positional encoding and prevent future token leakage</strong>. Left padding tokens are masked out.</li><li><strong>Encoder-only models (BERT, RoBERTa)</strong> use <strong>Right padding</strong> since they employ <strong>bidirectional attention</strong> and rely on stable special token positions (e.g., <code>[CLS]</code>).</li><li>Although padding tokens occupy positions in <strong>positional encoding</strong>, <strong>attention masks</strong> effectively filter them out, ensuring they do not affect model predictions.</li></ul><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;📌-What-is-Padding&quot;&gt;&lt;a href=&quot;#📌-What-is-Padding&quot; class=&quot;headerlink&quot; title=&quot;📌 What is Padding?&quot;&gt;&lt;/a&gt;📌 &lt;strong&gt;What is Padding?&lt;/st</summary>
      
    
    
    
    <category term="NLP Insights" scheme="https://chenhuiyu.github.io/categories/NLP-Insights/"/>
    
    
    <category term="LLM" scheme="https://chenhuiyu.github.io/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>MoE模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色</title>
    <link href="https://chenhuiyu.github.io/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2/"/>
    <id>https://chenhuiyu.github.io/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2/</id>
    <published>2025-02-11T03:50:29.000Z</published>
    <updated>2025-10-29T02:39:23.152Z</updated>
    
    <content type="html"><![CDATA[<h1 id="MoE-模型的的可视化指南：揭秘-MoE-在大型语言模型中的角色"><a href="#MoE-模型的的可视化指南：揭秘-MoE-在大型语言模型中的角色" class="headerlink" title="MoE 模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色"></a>MoE 模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色</h1><p><strong>原文地址</strong>：<a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts">A Visual Guide to Mixture of Experts (MoE)</a></p><p>📅 作者：Maarten Grootendorst</p><p>📆 日期：2024 年 10 月 7 日</p><hr><h1 id="探索语言模型：混合专家模型（MoE）可视化指南"><a href="#探索语言模型：混合专家模型（MoE）可视化指南" class="headerlink" title="探索语言模型：混合专家模型（MoE）可视化指南"></a>探索语言模型：混合专家模型（MoE）可视化指南</h1><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li><a href="#moe-%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%E6%8F%AD%E7%A7%98-moe-%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2">MoE 模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色</a></li><li><a href="#%E6%8E%A2%E7%B4%A2%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E6%B7%B7%E5%90%88%E4%B8%93%E5%AE%B6%E6%A8%A1%E5%9E%8Bmoe%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97">探索语言模型：混合专家模型（MoE）可视化指南</a><ul><li><a href="#%E7%9B%AE%E5%BD%95">目录</a></li><li><a href="#%E4%BB%80%E4%B9%88%E6%98%AF%E6%B7%B7%E5%90%88%E4%B8%93%E5%AE%B6moe%E6%A8%A1%E5%9E%8B">什么是混合专家（MoE）模型？</a></li><li><a href="#experts">Experts</a><ul><li><a href="#dense-layers">Dense Layers</a></li><li><a href="#sparse-layers">Sparse Layers</a></li><li><a href="#what-does-an-expert-learn">What does an Expert Learn?</a></li><li><a href="#%E4%B8%93%E5%AE%B6%E7%9A%84%E6%9E%B6%E6%9E%84architecture-of-experts">专家的架构（Architecture of Experts）</a></li></ul></li></ul></li></ul><p>当我们查看最新发布的大型语言模型（<strong>LLMs</strong>，Large Language Models）时，常常会在标题中看到 “<strong>MoE</strong>”。这个 “MoE” 代表什么？为什么这么多 LLM 都在使用它？</p><p>在这份可视化指南中，我们会通过 50 多个可视化图示，逐步探索这一关键组件：**Mixture of Experts (MoE)**。</p><img src="/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2/20250224_145859.png" class=""><p><strong>图示内容</strong>：在这张图中，可以看到一个典型 <strong>MoE</strong> 结构的两个主要组成部分：<strong>Experts</strong>（专家）和 <strong>Router</strong>（路由器或门控网络）。图中显示了一个 <strong>Router</strong>，以及下方并列的多个 <strong>Experts</strong>，表明在 <strong>LLM</strong> 架构中，MoE 会将输入根据需要路由到合适的专家。<br><strong>图 1 详细说明</strong>：</p><ol><li><strong>Router</strong>：决定将输入（例如 token）发送给哪一个或哪几个专家。</li><li><strong>Experts</strong>：若干个不同的子模型（通常是 <strong>FFNN</strong> 结构），每个专家可能在不同方面具有专长。</li><li><strong>工作流程</strong>：输入先通过 <strong>Router</strong>，再被分配到不同的专家进行处理，最后汇总结果。</li></ol><h2 id="什么是混合专家（MoE）模型？"><a href="#什么是混合专家（MoE）模型？" class="headerlink" title="什么是混合专家（MoE）模型？"></a>什么是混合专家（MoE）模型？</h2><p><strong>Mixture of Experts (MoE)</strong> 是一种技术，它使用许多不同的子模型（或“<strong>experts</strong>”）来提升大型语言模型的质量。</p><p>在 MoE 中，有两个主要组件：</p><ol><li><strong>Experts</strong><ul><li>每个 <strong>FFNN</strong> 层都不再是一个单独的网络，而是有一组“专家”可供选择。</li><li>这些“专家”通常也是 <strong>FFNN</strong>（Feedforward Neural Network）结构。</li></ul></li><li><strong>Router</strong> 或 <strong>gate network</strong><ul><li>负责决定哪些 <strong>tokens</strong> 被发送到哪些专家。</li></ul></li></ol><p>在一个带有 MoE 的 <strong>LLM</strong> 的每一层，我们都能看到（在某种程度上）有所专门化的专家：</p><img src="/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2/20250224_150409.png" class=""><p><strong>图示内容</strong>：展示了在 <strong>LLM</strong> 的每一层都可以拥有多个 <strong>Experts</strong>。它强调了这些专家在不同的上下文中能够处理不同的输入 token。<br><strong>图2详细说明</strong>：  </p><ol><li><strong>层结构</strong>：图中用不同的层级（Layer 1、Layer 2、Layer 3……）表示多层模型。  </li><li><strong>Experts</strong>：在每一层，都有若干个专家（Expert 1、Expert 2、Expert 3、Expert 4），这些专家并行存在。  </li><li><strong>目标</strong>：强调专家在特定上下文或特定输入时更具备“专业性”，从而被选中来处理该输入。</li></ol><p>尽管 MoE 并不会在特定领域（如心理学或生物学）上专门训练专家，但它们仍可能在词法或句法级别上形成一定的偏向：</p><img src="/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2/20250224_153715.png" class=""><ul><li><strong>MoE 专家可能学习到不同的语言特征</strong><ul><li><strong>Expert 1</strong> 处理<strong>标点符号</strong>（Punctuation）：如 <code>, . : &amp; - ?</code> 等。</li><li><strong>Expert 2</strong> 处理<strong>动词</strong>（Verbs）：如 <code>said, read, miss</code> 等。</li><li><strong>Expert 3</strong> 处理<strong>连接词</strong>（Conjunctions）：如 <code>the, and, if, not</code> 等。</li><li><strong>Expert 4</strong> 处理<strong>视觉描述词</strong>（Visual Descriptions）：如 <code>dark, outer, yellow</code> 等。</li></ul></li></ul><p>更具体地说，他们的专长是在特定上下文中处理特定的标记（tokens）。</p><hr><p><strong>Router (gate network)</strong> 选择最适合给定输入的专家或专家组合：</p><img src="/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2/20250224_153924.png" class=""><p><strong>图示内容</strong>：展示了 <strong>Router</strong> 如何在每一层根据输入选择合适的专家。图中高亮了被选中的专家，以及输入 token 的流动过程。<br><strong>图3详细说明</strong>：  </p><ol><li><strong>输入</strong>：图顶部的 Input 代表模型接收到的 token 或向量表示。  </li><li><strong>Router</strong>：位于网络结构中，起到决策作用。  </li><li><strong>专家选择</strong>：被选中的专家会接收输入，其余专家则不被激活。  </li><li><strong>输出</strong>：来自被激活专家的结果被汇总或继续流向下游层。</li></ol><p>需要注意的是，每个专家并不是整个 LLM，而是 <strong>LLM</strong> 架构中的一个子模型部分。</p><hr><h2 id="Experts"><a href="#Experts" class="headerlink" title="Experts"></a>Experts</h2><p>为了理解专家（<strong>Experts</strong>）是什么以及它们如何工作，我们先来看看 MoE 希望替代的东西：<strong>dense layers</strong>。</p><h3 id="Dense-Layers"><a href="#Dense-Layers" class="headerlink" title="Dense Layers"></a>Dense Layers</h3><p>所有的 <strong>Mixture of Experts (MoE)</strong> 都基于 LLM 中一个相对基础的功能：**Feedforward Neural Network (FFNN)**。</p><p>回忆一下，一个标准的 <strong>decoder-only Transformer</strong> 架构中，<strong>FFNN</strong> 通常是在 <strong>layer normalization</strong> 之后应用的：</p><img src="/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2/20250224_154729.png" class=""><p><strong>图示内容</strong>：展示了一个典型的 <strong>decoder</strong> 结构，每个 <strong>decoder block</strong> 包含 <strong>Masked Self-Attention</strong> 和 <strong>FFNN</strong>（中间会有 <strong>Layer Norm</strong>）。  </p><ol><li><strong>Position Embedding</strong>：在输入 token 之前或同时加入位置编码信息。  </li><li><strong>Decoder Block</strong>：包含 <strong>Masked Self-Attention</strong>、<strong>Layer Norm</strong> 和 <strong>FFNN</strong>。  </li><li><strong>FFNN</strong>：在图中用紫色方块表示，是该层对输入进一步变换以捕捉更复杂关系的关键组件。</li></ol><p><strong>FFNN</strong> 可以利用注意力机制产生的上下文信息，对其进行进一步的转换，以捕捉数据中更复杂的关系。</p><p>不过，为了学习这些复杂关系，<strong>FFNN</strong> 的规模会随之增长，通常会在输入上进行扩张（例如，中间层维度会变大）：</p><img src="/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2/20250224_154923.png" class=""><p><strong>图示内容</strong>：展示了一个 <strong>FFNN</strong> 的结构，输入先被映射到更高维度，然后再被映射回输出维度。  </p><ol><li><strong>输入维度</strong>：图中显示有 512 个输入单元。  </li><li><strong>隐藏层</strong>：通常会有 4 倍或更多的扩张（图中示例为 4 倍扩张到 2048 维）。  </li><li><strong>输出维度</strong>：再映射回 512 维的输出。</li></ol><h3 id="Sparse-Layers"><a href="#Sparse-Layers" class="headerlink" title="Sparse Layers"></a>Sparse Layers</h3><p>在传统的 Transformer 中，<strong>FFNN</strong> 称为 <strong>dense model</strong>，因为它的所有参数（权重和偏置）都会被激活。也就是说，模型的全部参数都参与计算输出。</p><p>如果我们仔细观察 <strong>dense model</strong>，可以看到输入会激活所有的参数：</p><img src="/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2/20250224_163850.png" class=""><p><strong>图示内容</strong>：展示了一个“密集”模型，输入层的每个神经元都与隐藏层所有神经元相连，隐藏层所有神经元又与输出层神经元相连。<br><strong>图6详细说明</strong>：  </p><ol><li><strong>全连接</strong>：图中所有节点都连接到下一层的所有节点，表示无稀疏性。  </li><li><strong>所有参数被激活</strong>：没有任何“闲置”或“未激活”的参数。</li></ol><p>与之对比，<strong>sparse models</strong>（稀疏模型）只激活一部分总参数，这与 <strong>Mixture of Experts</strong> 密切相关。</p><p>为了说明这一点，我们可以把 <strong>dense model</strong> 切分成多个部分（即专家，<strong>experts</strong>），重新训练它，并且在推理（inference）时只激活其中一部分：</p><img src="/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2/20250224_164042.png" class=""><p><strong>图示内容</strong>：将原本的密集模型分割成多个专家（Expert 1、Expert 2、Expert 3、Expert 4）。在推理阶段，只选择一部分专家进行激活。  </p><ol><li><strong>模型切分</strong>：原有的大网络被拆分成多个较小的“专家”。  </li><li><strong>稀疏激活</strong>：并不是所有专家都被激活，只有部分专家在某些输入下被激活。  </li><li><strong>好处</strong>：通过稀疏激活，可以在不显著增加计算成本的情况下，拥有更多的潜在参数容量。</li></ol><p>其核心思想是：在训练期间，每个专家学习不同的信息；在推理时，只用到与当前任务最相关的那些专家。</p><p>当我们提出一个问题时，就会选择最适合该任务的专家：</p><img src="/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2/20250224_170127.png" class=""><p><strong>图示内容</strong>：展示了一个示例：当输入是 “What is 1 + 1?” 这样的数字相关问题时，路由器只激活与数字相关的专家。  </p><ol><li><strong>输入</strong>：一个表示算术问题的句子或 token。  </li><li><strong>专家选择</strong>：只激活 “Numbers” 领域的专家。  </li><li><strong>输出</strong>：专家给出结果 “2”。</li></ol><h3 id="What-does-an-Expert-Learn"><a href="#What-does-an-Expert-Learn" class="headerlink" title="What does an Expert Learn?"></a>What does an Expert Learn?</h3><p>正如前面所提到的，专家（<strong>Experts</strong>）往往学习到比整个领域更细致的知识。有人会觉得称它们为“专家”可能会带来误解，但这是因为每个专家往往只专注于某些特定类型的输入特征或上下文。</p><img src="/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2/20250224_170229.png" class=""><p><strong>图示内容</strong>：展示了一个表格或对照，说明在某些情况下，不同的专家可能学习到不同的特征（比如标点符号、动词、数字等）。  </p><ol><li><strong>示例化专家</strong>：Punctuation、Conjunctions、Verbs、Numbers 等。  </li><li><strong>分层位置</strong>：不同专家可能出现在模型的不同层。  </li><li><strong>分配</strong>：某些 token 会路由到某些专家，以获得更有效的处理。</li></ol><p>在 <strong>decoder</strong> 模型中，专家之间可能没有那么明显的领域分工。然而，这并不意味着所有专家都完全相同。<br>在 <strong>Mixtral 8x7B</strong> 这篇论文中，有一个很好的示例：每个 token 会被标记为其首选专家，这些专家并不一定对应直观的语义领域，但在统计上表现出某些倾向。</p><img src="/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2/20250224_182219.png" class=""><p>这张可视化示例还展示了，experts（专家）更倾向于关注句法（syntax），而不是特定的领域（domain）。因此，虽然 decoder experts（解码器专家）似乎并没有明确的“专业领域（specialism）”，但它们似乎会在某些特定类型的 tokens（标记）上被持续地使用。</p><p>在[图1]中，展示了一段关于 MoELayer 的示例代码或可视化结果，色块区分了不同部分，强调了<strong>专家（experts）与路由器（router）</strong>之间的关系。通过色块可以看出：</p><ul><li>experts 列表（在代码中用 nn.ModuleList 表示）包含了多个子网络（即多个 FFNN，Feed-Forward Neural Network，前馈神经网络）。</li><li>gate（门控网络，也称 router）负责选择哪些专家会被激活。</li><li>整体上可以看到，这些专家通常关注到输入句子的句法层面，而非特定主题或领域。</li></ul><h3 id="专家的架构（Architecture-of-Experts）"><a href="#专家的架构（Architecture-of-Experts）" class="headerlink" title="专家的架构（Architecture of Experts）"></a>专家的架构（Architecture of Experts）</h3>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;MoE-模型的的可视化指南：揭秘-MoE-在大型语言模型中的角色&quot;&gt;&lt;a href=&quot;#MoE-模型的的可视化指南：揭秘-MoE-在大型语言模型中的角色&quot; class=&quot;headerlink&quot; title=&quot;MoE 模型的的可视化指南：揭秘 MoE 在大型语言模型</summary>
      
    
    
    
    <category term="NLP Insights" scheme="https://chenhuiyu.github.io/categories/NLP-Insights/"/>
    
    
    <category term="LLM" scheme="https://chenhuiyu.github.io/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>推理 LLM 的可视化指南：探索推理时计算技术与 DeepSeek-R1</title>
    <link href="https://chenhuiyu.github.io/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/"/>
    <id>https://chenhuiyu.github.io/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/</id>
    <published>2025-02-11T03:50:29.000Z</published>
    <updated>2025-02-11T13:20:11.564Z</updated>
    
    <content type="html"><![CDATA[<h1 id="推理-LLM-的可视化指南：探索推理时计算技术与-DeepSeek-R1"><a href="#推理-LLM-的可视化指南：探索推理时计算技术与-DeepSeek-R1" class="headerlink" title="推理 LLM 的可视化指南：探索推理时计算技术与 DeepSeek-R1"></a>推理 LLM 的可视化指南：探索推理时计算技术与 DeepSeek-R1</h1><p><strong>原文地址</strong>：<a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-reasoning-llms">A Visual Guide to Reasoning LLMs</a></p><p>📅 作者：Maarten Grootendorst</p><p>📆 日期：2025 年 2 月 3 日</p><hr><h2 id="📌-引言"><a href="#📌-引言" class="headerlink" title="📌 引言"></a>📌 引言</h2><p>DeepSeek-R1、OpenAI o3-mini 和 Google Gemini 2.0 Flash Thinking 是如何通过“推理”框架将 <strong>LLM（大型语言模型, Large Language Models）</strong> 扩展到新高度的典型示例。</p><p>它们标志着从 <strong>扩展训练时计算（train-time compute）</strong> 到 <strong>扩展推理时计算（test-time compute）</strong> 的范式转变。</p><p>在本篇文章中，我们提供了 <strong>超过 40 张定制可视化图表</strong>，带你深入探索：</p><ul><li><strong>推理 LLM（Reasoning LLMs）</strong> 领域</li><li><strong>推理时计算（Test-Time Compute）</strong> 机制</li><li><strong>DeepSeek-R1</strong> 的核心思想</li></ul><p>我们将逐步介绍相关概念，帮助你建立对这一新范式的直觉理解。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/i24pmg2.png" class=""><hr><h2 id="📖-什么是推理-LLM？"><a href="#📖-什么是推理-LLM？" class="headerlink" title="📖 什么是推理 LLM？"></a>📖 什么是推理 LLM？</h2><p>与普通 <strong>LLM（Large Language Models，大型语言模型）</strong> 相比，<strong>推理 LLM</strong> 在回答问题之前，往往会将问题 <strong>分解为更小的步骤</strong>（通常称为 <strong>推理步骤（Reasoning Steps）</strong> 或 <strong>思考过程（Thought Process）</strong>）。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_143007.png" class=""><h3 id="🧠-“推理步骤”-或-“思考过程”-是什么？"><a href="#🧠-“推理步骤”-或-“思考过程”-是什么？" class="headerlink" title="🧠 “推理步骤” 或 “思考过程” 是什么？"></a>🧠 “推理步骤” 或 “思考过程” 是什么？</h3><p>尽管我们可以哲学化地探讨 LLM 是否真的能够像人类一样思考，但这些推理步骤实际上是将推理过程 分解为更小、更结构化的推断。<strong>推理 LLM 采用的是结构化推理方式</strong>，即：</p><ul><li><strong>普通 LLM</strong>：直接输出答案</li><li><strong>推理 LLM</strong>：通过系统性推理生成答案</li></ul><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_143054.png" class=""><p>换句话说，推理 LLM 不是<strong>学习“回答什么”</strong>，而是<strong>学习“如何回答”</strong>！</p><p>要理解推理 LLM 的构建原理，我们首先需要探讨 <strong>训练时计算（Train-Time Compute）</strong> 和 <strong>推理时计算（Test-Time Compute）</strong> 之间的差异。</p><hr><h2 id="🔍-什么是训练时计算（Train-time-Compute）？"><a href="#🔍-什么是训练时计算（Train-time-Compute）？" class="headerlink" title="🔍 什么是训练时计算（Train-time Compute）？"></a>🔍 什么是训练时计算（Train-time Compute）？</h2><p>直到 2024 年年中，为了在 <strong>预训练（Pretraining）</strong> 期间提高 LLM 的性能，研究人员通常会扩大以下规模：</p><ul><li><strong>模型参数数量（# of Parameters）</strong></li><li><strong>数据集规模（# of Tokens）</strong></li><li><strong>计算量（# of FLOPs, Floating Point Operations）</strong></li></ul><p>这些合称为 <strong>训练时计算（Train-time Compute）</strong>，即 <strong>“AI 的化石燃料”</strong>，指的是：</p><blockquote><p><strong>预训练预算越大，最终得到的模型就越好。</strong></p><p>训练时计算（Train-Time Compute）包括<strong>训练（training）</strong>所需的计算，以及<strong>微调（fine-tuning）</strong>所需的计算。长期以来，一直是提高 LLM 性能的主要关注点。</p></blockquote><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_143927.png" class=""><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_144121.png" class=""><h3 id="🔢-规模定律（Scaling-Laws）"><a href="#🔢-规模定律（Scaling-Laws）" class="headerlink" title="🔢 规模定律（Scaling Laws）"></a>🔢 规模定律（Scaling Laws）</h3><p>在 <strong>LLM（大型语言模型）</strong> 研究领域，<strong>模型规模（Scale）</strong> 与 <strong>模型性能（Performance）</strong> 之间的关系被称为 <strong>规模定律（Scaling Laws）</strong>。这些定律通常用于描述 <strong>计算资源、数据规模和模型参数</strong> 如何影响模型的整体表现。</p><p>这些关系通常以 <strong>对数-对数（log-log）</strong> 方式呈现，并且在图表上通常显示为一条 <strong>近似直线</strong>，以突出计算量的巨大增长。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_144846.png" class=""><p>这张图片展示了<strong>不同坐标尺度（线性 vs. 对数）对计算资源（Compute）和模型性能（Performance）之间关系的影响</strong>，强调了大模型增长的幂律关系（Power Law）。</p><ul><li><p><strong>左图（普通线性尺度 - Normal Scale）</strong></p><ul><li>横轴（X 轴）：计算资源（Compute），<strong>线性刻度</strong>。</li><li>纵轴（Y 轴）：性能（Performance），<strong>线性刻度</strong>。</li><li>曲线显示<strong>递减收益（Diminishing Returns）</strong>，即：<strong>随着计算资源的增加，性能增长趋缓</strong>，但仍然在上升。</li></ul></li><li><p><strong>右图（对数-对数尺度 - Log-log Scale）</strong></p><ul><li>横轴（X 轴）：计算资源（Compute），<strong>对数刻度</strong>。</li><li>纵轴（Y 轴）：性能（Performance），<strong>对数刻度</strong>。</li><li>在对数-对数尺度下，原本弯曲的曲线变成<strong>一条直线</strong>，说明计算资源和性能之间呈<strong>幂律关系（Power Law Relationship）</strong>。</li></ul></li></ul><p>这些定律通常遵循 <strong>幂律（Power Laws）</strong>，即：</p><blockquote><p><strong>某个变量（如计算量）增加，会导致另一个变量（如性能）按一定比例变化。</strong></p></blockquote><p>最著名的 <strong>规模定律</strong> 包括：</p><ul><li><strong>Kaplan 规模定律</strong>（Kaplan Scaling Law）：当计算资源一定时，<strong>增加模型的参数规模比增加数据规模更有效</strong>。表明模型性能与参数量、计算量和训练数据（Tokens）之间存在幂律关系，即 更多参数、更多计算资源能提升性能（GPT-3 论文提出）。</li><li><strong>Chinchilla 规模定律</strong>（Chinchilla Scaling Law）：模型的大小和数据规模同样重要，二者需 <strong>同步扩展</strong> 才能实现最佳性能（DeepMind 提出）。</li></ul><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_145639.png" class=""><p>这张图展示了<strong>大规模 AI 训练中的 Scaling Laws（缩放定律）</strong>，表明<strong>计算资源（Compute）、数据集规模（Dataset Size）和参数量（Parameters）</strong>对模型性能的影响。关键信息如下：</p><hr><p><strong>1. 纵轴（Y轴）：测试损失（Test Loss）</strong></p><ul><li><strong>目标是降低测试损失（Test Loss）</strong>，即提高模型的泛化性能。</li><li><strong>损失（L）越小，模型性能越好</strong>。</li></ul><p><strong>2. 横轴（X轴）：三种关键变量</strong></p><ul><li><p><strong>左图（Compute，计算资源）</strong>：</p><ul><li>X 轴是计算资源（PF-days, 非 embedding）。</li><li>计算资源越多，测试损失降低（性能提升）。</li><li>公式：<br>$$<br>L = \left( \frac{C_{\text{min}}}{2.3 \times 10^8} \right)^{-0.050}<br>$$</li><li><strong>体现计算资源的幂律关系</strong>：计算资源增加，损失减少，但收益递减（指数 -0.050）。</li></ul></li><li><p><strong>中图（Dataset Size，数据集规模）</strong>：</p><ul><li>X 轴是训练数据的 Token 数量。</li><li>数据规模越大，测试损失降低（性能提升）。</li><li>公式：<br>$$<br>L = \left( \frac{D}{5.4 \times 10^{13}} \right)^{-0.095}<br>$$</li><li><strong>数据规模对损失的影响较大</strong>（指数 -0.095）。</li></ul></li><li><p><strong>右图（Parameters，参数量）</strong>：</p><ul><li>X 轴是模型参数量（非 embedding）。</li><li>参数数量越大，测试损失降低（性能提升）。</li><li>公式：<br>$$<br>L = \left( \frac{N}{8.8 \times 10^{13}} \right)^{-0.076}<br>$$</li><li><strong>参数对损失的影响介于计算资源和数据规模之间</strong>（指数 -0.076）。</li></ul></li></ul><p>这些研究表明，<strong>模型规模、数据规模和计算资源必须协同扩展，才能最大化模型的性能</strong>。</p><ul><li><strong>计算资源增加 → 训练更强大模型</strong></li><li><strong>更多 Tokens → 更好泛化能力</strong></li><li><strong>参数增加 → 但需要与数据匹配，否则过拟合</strong></li></ul><p>Kaplan 规模定律认为，在 <strong>固定计算资源</strong> 的情况下，<strong>优先增加模型参数</strong> 通常比增加数据规模更有效。而 Chinchilla 规模定律则指出，<strong>模型参数和数据规模都应同步增长</strong>，以获得更优的模型性能。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_145814.png" class=""><p>然而，在 <strong>2024 年</strong>，研究人员发现，尽管计算资源、数据规模和模型参数 <strong>持续增长</strong>，但性能提升的 <strong>边际收益（Marginal Return）</strong> 却在 <strong>逐渐降低</strong>。</p><p>这引发了一个重要的问题：</p><p>❓ <strong>“我们是否已经遇到了 LLM 发展的瓶颈？”</strong></p><hr><h2 id="🚀-什么是推理时计算（Test-time-Compute）？"><a href="#🚀-什么是推理时计算（Test-time-Compute）？" class="headerlink" title="🚀 什么是推理时计算（Test-time Compute）？"></a>🚀 什么是推理时计算（Test-time Compute）？</h2><p>由于 <strong>训练时计算的成本极其昂贵</strong>，研究人员开始关注 <strong>推理时计算（Test-time Compute）</strong>，即：</p><blockquote><p><strong>让 LLM 在推理时“思考更长时间”</strong>，而非单纯依赖更大的模型和数据集。</p></blockquote><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_145856.png" class=""><p>对于<strong>非推理模型</strong>，它们通常 <strong>直接输出答案</strong>：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Q: 8 + 5 = ?</span><br><span class="line">A: 13</span><br></pre></td></tr></tbody></table></figure><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_145936.png" class=""><p>而<strong>推理模型</strong>则会 <strong>使用更多 token 进行推理</strong>，形成系统化的“思考”过程：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Q: 8 + 5 = ?</span><br><span class="line">A: 8 + 5 可拆解为 8 + 2 + 3 = 10 + 3 = 13</span><br></pre></td></tr></tbody></table></figure><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_150033.png" class=""><p>LLM 需要消耗计算资源（如显存计算）来生成答案。然而，如果所有计算资源都用于直接生成答案，那将会是低效的！</p><p>相反，通过提前生成包含额外信息、关系和新思考的更多 token，模型可以在推理过程中分配更多计算资源以生成最终答案。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_151252.png" class=""><p>这张图片展示了 <strong>大语言模型（LLM）</strong> 在计算过程中如何分配 <strong>token</strong>（标记）来优化推理能力和最终的回答质量。核心思想是：<strong>如果计算资源（如 GPU/VRAM 计算量）全部用于直接生成答案，而没有用于思考，那么效率会受到影响</strong>。相反，增加 <strong>思考过程</strong>（即生成更多的中间 token），可以提高模型的 <strong>推理能力</strong>，从而提升 <strong>最终的回答质量</strong>。</p><p><strong>1. Token 的使用与计算量</strong></p><ul><li><strong>LLM 生成答案是按 token 逐步输出的</strong>，每个 token 都会占用计算资源。</li><li><strong>分配更多的 token 进行思考</strong>，意味着模型可以在得出最终答案之前有更多的推理步骤，从而提高正确率。</li></ul><p> <strong>2. 三种不同的计算方式</strong></p><ul><li><p><strong>场景 1（1 个 token：最少计算）</strong></p><ul><li>直接输出 <strong>“5”</strong> 作为答案。</li><li><strong>计算量最少</strong>，速度最快。</li><li><strong>如果问题较复杂，可能会出错</strong>，因为模型没有足够的计算时间来思考。</li></ul></li><li><p><strong>场景 2（6 个 token：中等计算）</strong></p><ul><li>模型生成一个简短的推理过程：<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Adding 3 and 2 gives 5</span><br></pre></td></tr></tbody></table></figure></li><li><strong>比第一种方法多了一些计算量</strong>，但仍然较为简洁。</li><li>这种方式适用于<strong>简单的数学运算或逻辑推理</strong>，但在更复杂的情况下仍可能出现错误。</li></ul></li><li><p><strong>场景 3（15 个 token：完整推理）</strong></p><ul><li>模型先进行详细的逐步推理：<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">3 + 1 = 4 , 4 + 1 = 5</span><br></pre></td></tr></tbody></table></figure>然后，模型再明确地总结：<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">the total is 5</span><br></pre></td></tr></tbody></table></figure></li><li><strong>推理过程更详细，占用的计算量最大</strong>。</li><li><strong>适用于需要多步推理的任务，如数学题、逻辑推理题等</strong>。</li></ul></li></ul><h3 id="🔢-规模定律（Scaling-Laws）-1"><a href="#🔢-规模定律（Scaling-Laws）-1" class="headerlink" title="🔢 规模定律（Scaling Laws）"></a>🔢 规模定律（Scaling Laws）</h3><p>相比于训练时计算，推理时计算的规模定律仍然较为新颖。值得注意的是，有两项研究揭示了推理时计算规模与训练时计算规模的关系。</p><p>首先，OpenAI 发表的一篇文章表明，推理时计算可能遵循与训练时计算相同的扩展趋势。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_151722.png" class=""><blockquote><p><strong>来自“学习如何推理的 LLM”一文的注释图</strong>：红色虚线显示了 OpenAI 提出的新范式可能是推理时计算。<br>这张图展示了 <strong>训练时间计算（train-time compute）和测试时间计算（test-time compute）</strong> 对模型 <strong>pass@1 准确率（accuracy）</strong> 的影响，具体来说，它强调了 <strong>测试时间计算可能比训练时间计算更有利于扩展模型性能</strong>。</p></blockquote><ol><li><p><strong>左图：训练时间计算 vs. 准确率</strong></p><ul><li><strong>X 轴（横轴）：训练时间计算（log scale，指数刻度）</strong>。</li><li><strong>Y 轴（纵轴）：pass@1 准确率</strong>（即模型在一次尝试中得到正确答案的概率）。</li><li><strong>黑色点</strong> 代表不同计算量下的模型表现，粉色虚线展示了大致的趋势。</li><li>可以看到，随着 <strong>训练计算量的增加，准确率逐渐提高</strong>，但增长趋势相对平稳。</li></ul></li><li><p><strong>右图：测试时间计算 vs. 准确率</strong></p><ul><li><strong>X 轴（横轴）：测试时间计算（log scale）</strong>。</li><li><strong>Y 轴（纵轴）：pass@1 准确率</strong>。</li><li>同样，黑色点代表不同计算量下的模型表现，粉色虚线展示了大致的趋势。</li><li>这里可以看到，随着 <strong>测试时计算量增加，模型的准确率增长更显著，甚至超过了训练计算量的效果</strong>。<br>因此，他们认为，推理时计算的扩展可能代表着新的研究范式。</li></ul></li></ol><p>其次，一篇名为《Scaling Scaling Laws with Board Games》的论文研究了 AlphaZero 在不同计算量下玩 Hex 游戏的表现。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_152050.png" class=""><blockquote><p><strong>来自“Scaling Scaling Laws with Board Games”一文的注释图</strong>：该图展示了他们如何构建不同规模的训练时计算和推理时计算。- <strong>AlphaZero</strong> 是 <strong>DeepMind</strong> 开发的一个 <strong>强化学习（Reinforcement Learning, RL）</strong> 训练的 AI。</p></blockquote><ul><li>该算法通过 <strong>自我对弈（self-play）</strong> 训练，无需人为规则输入，即可掌握<strong>围棋、国际象棋、将棋等游戏</strong>。</li><li>它结合了 <strong>神经网络预测</strong> 和 <strong>蒙特卡洛树搜索（MCTS, Monte Carlo Tree Search）</strong> 来进行决策。</li></ul><p>这张图片展示了 <strong>AlphaZero 算法</strong> 在<strong>训练阶段（train-time compute）和测试阶段（test-time compute）</strong>计算资源的不同应用。主要强调了：</p><ul><li><strong>训练时</strong>：依赖于<strong>更多参数和更长的训练时间</strong>来优化模型。</li><li><strong>测试时</strong>：依靠 <strong>更深入的树搜索（tree search）</strong> 来提升决策能力。</li></ul><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_152430.png" class=""><blockquote><p>来自“Scaling Scaling Laws with Board Games”一文的注释图：该图展示了训练时计算与推理时计算之间的关系。<br>研究结果表明，训练时计算和推理时计算紧密相关。每条虚线表示达到特定 ELO 分数所需的最小计算量。<br><strong>1. 坐标轴含义</strong></p></blockquote><ul><li><strong>X 轴（横轴）：训练时计算量（Train-time Compute，FLOP-seconds）</strong></li><li><strong>Y 轴（纵轴）：推理时计算量（Test-time Compute，FLOP-seconds）</strong></li><li><strong>对数刻度（log scale）：计算量的增长呈指数级，而不是线性增长。</strong></li></ul><p><strong>2. 关键数据趋势</strong></p><ul><li>不同颜色的曲线分别表示<strong>不同的 ELO 分数水平</strong>（-1500、-1250、-1000、-750、-500、-250）。</li><li><strong>虚线和实线</strong>：<ul><li><strong>虚线</strong> 表示某个 ELO 分数下的最优计算边界。</li><li><strong>实线</strong> 代表实际数据趋势。</li></ul></li></ul><ol><li><p><strong>训练计算和推理计算可以互相替代</strong></p><ul><li><strong>如果推理计算量增加（左上区域）</strong>，那么所需的训练计算量减少。</li><li><strong>如果训练计算量增加（右下区域）</strong>，那么所需的推理计算量减少。</li><li><strong>两者呈现负相关关系</strong>。</li></ul></li><li><p><strong>低训练计算 vs. 高推理计算</strong></p><ul><li>在 <strong>训练计算较少</strong> 的情况下（如左侧的红色圈），模型仍然可以达到相同的 ELO 水平，但需要 <strong>在推理时增加计算量</strong>（如更深的搜索树、更长的思考路径）。</li></ul></li><li><p><strong>高训练计算 vs. 低推理计算</strong></p><ul><li>在 <strong>训练计算充足</strong> 的情况下（如右侧的红色圈），模型可以<strong>减少推理计算需求</strong>，即 <strong>即使使用较少的搜索深度，仍然能获得较高的性能</strong>。</li></ul></li><li><p><strong>公式解释</strong></p><ul><li>公式：<br>$$<br>\log_{10}(\text{test compute}) = -1.2 \cdot \log_{10}(\text{train compute}) + 0.004 \cdot \text{elo} + 29<br>$$</li><li>这说明：<ul><li><strong>训练计算（train compute）增加时，推理计算（test compute）减少（系数 -1.2）</strong>。</li><li><strong>更高的 ELO（更强的 AI）需要额外的计算（系数 0.004）</strong>。</li></ul></li></ul></li></ol><p>随着推理时计算扩展类似于训练时计算，研究范式正朝着“推理”模型利用更多推理时计算的方向发展。通过这种范式转变，这些“推理”模型不再单纯关注训练时计算（预训练和微调），而是平衡训练与推理。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_152824.png" class=""><p>推理时计算甚至可以随长度扩展：</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_153922.png" class=""><p>这是我们在 DeepSeek-R1 研究中也将探讨的内容！</p><h3 id="📌-推理时计算的类别（Categories-of-Test-time-Compute）"><a href="#📌-推理时计算的类别（Categories-of-Test-time-Compute）" class="headerlink" title="📌 推理时计算的类别（Categories of Test-time Compute）"></a>📌 推理时计算的类别（Categories of Test-time Compute）</h3><p>推理模型（如 <strong>DeepSeek-R1</strong> 和 <strong>OpenAI o1</strong>）的成功表明，在推理过程中，除了简单地“思考更长时间”之外，还有更多的优化技术。</p><p>在本文中，我们将探讨 <strong>推理时计算（Test-time Compute）</strong> 的多种实现方式，包括：</p><ul><li><strong>链式思维（Chain-of-Thought）</strong></li><li><strong>答案修订（Revising Answers）</strong></li><li><strong>回溯推理（Backtracking）</strong></li><li><strong>多样性采样（Sampling）</strong></li><li><strong>其他方法</strong></li></ul><p>总体而言，推理时计算可归纳为以下 <strong>两大类别</strong>：</p><ol><li><p><strong>基于验证器的搜索（Search against Verifiers）</strong>  </p><ul><li>通过 <strong>采样多个答案</strong> 并 <strong>选择最佳答案</strong> 来优化推理。</li></ul></li><li><p><strong>修改提议分布（Modifying Proposal Distribution）</strong>  </p><ul><li>通过训练 <strong>“思考”过程</strong> 来提高推理能力。Proposal Distribution（提议分布，指在模型生成答案时，对不同可能答案的概率分布进行调整）</li></ul></li></ol><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_154456.png" class=""><p>从本质上讲：</p><ul><li><strong>基于验证器的搜索</strong> 更关注 <strong>输出质量</strong>（Output-focused）。</li><li><strong>修改提议分布</strong> 关注 <strong>输入结构</strong>（Input-focused）。</li></ul><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_154547.png" class=""><h3 id="🔍-两种主要验证器类型"><a href="#🔍-两种主要验证器类型" class="headerlink" title="🔍 两种主要验证器类型"></a>🔍 两种主要验证器类型</h3><p>为了更好地筛选和评估推理答案，我们引入了两种 <strong>验证器（Verifiers）</strong>：</p><ol><li><p><strong>结果奖励模型（Outcome Reward Models, ORM）</strong>  </p><ul><li>仅对最终答案进行评分，而不考虑推理过程。</li></ul></li><li><p><strong>过程奖励模型（Process Reward Models, PRM）</strong>  </p><ul><li>既评估最终答案，也对推理过程进行评分。</li></ul></li></ol><p>在接下来的部分，我们将详细探讨 <strong>如何将 ORM 和 PRM 应用于不同的验证方法</strong>！</p><p>顾名思义，<strong>结果奖励模型（Outcome Reward Model, ORM）</strong> 仅评估最终的答案质量，而不关注答案背后的推理过程：</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_160242.png" class=""><ul><li>ORM 只看最终输出，而不关心模型是如何得出这个答案的。</li></ul><p>相比之下，<strong>过程奖励模型（Process Reward Model, PRM）</strong> 则会评估推理过程本身：</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_160258.png" class=""><ul><li>PRM 既评估答案的正确性，也关注推理路径的合理性。</li></ul><h3 id="🧐-PRM-如何评估推理过程？"><a href="#🧐-PRM-如何评估推理过程？" class="headerlink" title="🧐 PRM 如何评估推理过程？"></a>🧐 PRM 如何评估推理过程？</h3><p>为了更清楚地说明推理步骤的重要性，让我们来看一个示例：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">问题：某个方程的解是多少？</span><br><span class="line"></span><br><span class="line">推理步骤 1：首先展开方程，得到 x = 3。</span><br><span class="line">推理步骤 2：错误地将 x = 3 改写为 x = 5。</span><br><span class="line">推理步骤 3：最终输出 x = 5。</span><br></pre></td></tr></tbody></table></figure><p>在上述示例中，虽然最终答案（x = 5）是错误的，但 ORM 仅评估最终输出，不会关注中间的错误推理。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_160332.png" class=""><p>或者在这个例子中，PRM 会发现 <strong>推理步骤 2 是错误的</strong>，并对此步骤给予低分，从而避免错误答案的出现。</p><hr><h3 id="🔍-ORM-vs-PRM-在推理中的应用"><a href="#🔍-ORM-vs-PRM-在推理中的应用" class="headerlink" title="🔍 ORM vs. PRM 在推理中的应用"></a>🔍 ORM vs. PRM 在推理中的应用</h3><p>现在你已经掌握了 <strong>结果奖励模型（ORM）</strong> 和 <strong>过程奖励模型（PRM）</strong> 之间的区别，我们接下来探讨如何将它们应用于各种 <strong>验证技术（Verification Techniques）</strong>。</p><h2 id="📌-基于验证器的搜索（Search-against-Verifiers）"><a href="#📌-基于验证器的搜索（Search-against-Verifiers）" class="headerlink" title="📌 基于验证器的搜索（Search against Verifiers）"></a>📌 基于验证器的搜索（Search against Verifiers）</h2><p>推理时计算的第一大类别是 <strong>基于验证器的搜索</strong>，它通常包含两个步骤：</p><ol><li><strong>生成多个推理过程和答案样本</strong></li><li><strong>使用验证器（奖励模型）对生成的输出进行评分</strong><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_160918.png" class=""></li></ol><h3 id="🤖-验证器的作用"><a href="#🤖-验证器的作用" class="headerlink" title="🤖 验证器的作用"></a>🤖 验证器的作用</h3><p>验证器通常是一个大型语言模型（LLM），经过微调以评估结果（ORM）或过程（PRM）。 使用验证器的一个主要优势是，无需重新训练或微调用于回答问题的大型语言模型（LLM），仅通过评分机制选择最佳答案。</p><hr><h3 id="✅-多数投票法（Majority-Voting）"><a href="#✅-多数投票法（Majority-Voting）" class="headerlink" title="✅ 多数投票法（Majority Voting）"></a>✅ 多数投票法（Majority Voting）</h3><p>最简单的方法是 <strong>不使用奖励模型或验证器</strong>，而是执行 <strong>多数投票（Majority Voting）</strong>。</p><p>📌 <strong>方法：</strong> 让 LLM 生成多个答案，选择出现次数最多的答案作为最终答案。</p><p>📌 <strong>示例：</strong></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Q: 15 × 3 = ?</span><br><span class="line">A1: 45</span><br><span class="line">A2: 42</span><br><span class="line">A3: 45</span><br><span class="line">最终答案: 45（因其出现频率最高）</span><br></pre></td></tr></tbody></table></figure><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_161249.png" class=""><p>这种方法也称为 <strong>自一致性（Self-Consistency）</strong>，强调 <strong>生成多个答案和推理步骤</strong> 的重要性。</p><hr><h3 id="🔢-Best-of-N-采样法（Best-of-N-Samples）"><a href="#🔢-Best-of-N-采样法（Best-of-N-Samples）" class="headerlink" title="🔢 Best-of-N 采样法（Best-of-N Samples）"></a>🔢 Best-of-N 采样法（Best-of-N Samples）</h3><p>Best-of-N 采样是第一个涉及验证器（Verifier）的方法，它的基本思想是生成 N 个样本答案，然后使用 奖励模型（Reward Model, RM） 对这些答案进行评分，并选择得分最高的答案。</p><p>📌 <strong>步骤：</strong></p><ol><li><strong>生成多个答案</strong>（使用较高或者不同的温度参数生成 N 个样本）。<img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_161825.png" class=""></li><li><strong>结果奖励模型（ORM, Outcome Reward Model）</strong>，每个答案都会通过 ORM 进行评分。选取得分最高的答案作为最终输出。<img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_161833.png" class="">📌 <strong>示例：</strong></li></ol><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Q: 8 + 5 = ?</span><br><span class="line">A1: 12 (得分 0.2)</span><br><span class="line">A2: 13 (得分 0.9)</span><br><span class="line">A3: 14 (得分 0.4)</span><br><span class="line">最终选择: A2（因其得分最高）</span><br></pre></td></tr></tbody></table></figure><h2 id="📌-进一步优化：-若使用-PRM，则不只评估答案，还评估整个推理过程。与仅评估最终答案的结果奖励模型（ORM）不同，过程奖励模型（PRM）会评估推理过程的质量。PRM-关注推理的每个步骤，确保推理过程合理、连贯，并最终选择总评分最高的候选答案。-加权-Best-of-N-采样（Weighted-Best-of-N-samples）-结合-ORM-和-PRM-两种验证方式，我们可以对所有候选答案进行加权评分，并选择总权重最高的答案。这种方法称为-加权-Best-of-N-采样（Weighted-Best-of-N-samples）：。"><a href="#📌-进一步优化：-若使用-PRM，则不只评估答案，还评估整个推理过程。与仅评估最终答案的结果奖励模型（ORM）不同，过程奖励模型（PRM）会评估推理过程的质量。PRM-关注推理的每个步骤，确保推理过程合理、连贯，并最终选择总评分最高的候选答案。-加权-Best-of-N-采样（Weighted-Best-of-N-samples）-结合-ORM-和-PRM-两种验证方式，我们可以对所有候选答案进行加权评分，并选择总权重最高的答案。这种方法称为-加权-Best-of-N-采样（Weighted-Best-of-N-samples）：。" class="headerlink" title="📌 进一步优化：- 若使用 PRM，则不只评估答案，还评估整个推理过程。与仅评估最终答案的结果奖励模型（ORM）不同，过程奖励模型（PRM）会评估推理过程的质量。PRM 关注推理的每个步骤，确保推理过程合理、连贯，并最终选择总评分最高的候选答案。- 加权 Best-of-N 采样（Weighted Best-of-N samples）:结合 ORM 和 PRM 两种验证方式，我们可以对所有候选答案进行加权评分，并选择总权重最高的答案。这种方法称为 加权 Best-of-N 采样（Weighted Best-of-N samples）：。"></a>📌 <strong>进一步优化：</strong><br>- 若使用 <strong>PRM</strong>，则不只评估答案，还评估整个推理过程。与仅评估最终答案的结果奖励模型（ORM）不同，过程奖励模型（PRM）会评估推理过程的质量。PRM 关注推理的每个步骤，确保推理过程合理、连贯，并最终选择总评分最高的候选答案。<br><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_161922.png" class=""><br>- <strong>加权 Best-of-N 采样（Weighted Best-of-N samples）</strong>:结合 ORM 和 PRM 两种验证方式，我们可以对所有候选答案进行加权评分，并选择总权重最高的答案。这种方法称为 加权 Best-of-N 采样（Weighted Best-of-N samples）：。<br><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_162139.png" class=""></h2><h3 id="🚀-使用过程奖励模型（PRM）的束搜索（Beam-Search）"><a href="#🚀-使用过程奖励模型（PRM）的束搜索（Beam-Search）" class="headerlink" title="🚀 使用过程奖励模型（PRM）的束搜索（Beam Search）"></a>🚀 使用过程奖励模型（PRM）的束搜索（Beam Search）</h3><p>在生成答案及其中间推理步骤的过程中，我们可以使用 <strong>束搜索（Beam Search）</strong> 进一步优化推理路径。</p><p>📌 <strong>束搜索的核心思想：</strong></p><ul><li>在推理过程中，生成多个可能的推理路径（称为“束”）。</li><li>使用 <strong>过程奖励模型（PRM, Process Reward Model）</strong> 对每条路径进行评分。</li><li>类似于 <strong>Tree of Thought</strong> 方法，始终保留得分最高的 <strong>前 3 条推理路径</strong>，并在推理过程中持续跟踪这些路径。</li><li>如果某条路径的得分较低（PRM 评分低），则提前停止该推理路径，以避免不必要的计算开销。</li></ul><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_162508.png" class=""><p>📌 <strong>优化后的答案筛选方式：</strong><br>最终，生成的所有答案将使用 <strong>Best-of-N 采样</strong> 方法进行加权评分，确保选出最佳推理路径的最终答案。</p><p>🚀 <strong>优势：</strong></p><ul><li>避免计算资源浪费，快速淘汰低质量推理路径。</li><li>结合 PRM，可以确保模型的推理过程更连贯、更符合逻辑。</li><li>通过 Best-of-N 方法进一步优化答案质量，使最终答案更加可靠。</li></ul><hr><h3 id="🎲-蒙特卡洛树搜索（Monte-Carlo-Tree-Search-MCTS）"><a href="#🎲-蒙特卡洛树搜索（Monte-Carlo-Tree-Search-MCTS）" class="headerlink" title="🎲 蒙特卡洛树搜索（Monte Carlo Tree Search, MCTS）"></a>🎲 蒙特卡洛树搜索（Monte Carlo Tree Search, MCTS）</h3><p>蒙特卡洛树搜索（Monte Carlo Tree Search, <strong>MCTS</strong>）是一种常用于决策树搜索的算法，在 LLM 的推理优化中也可以采用该方法。MCTS 通过四个步骤来优化推理路径：<br>📌 <strong>主要步骤：</strong></p><ol><li><strong>选择（Selection）：</strong> 根据预定义的公式，从当前搜索树中选择一个叶节点 进行扩展。</li><li><strong>扩展（Expand）：</strong> 在所选叶节点的基础上 创建新的子节点，以探索更多可能的推理路径。</li><li><strong>模拟（Rollouts）：</strong> 通过随机生成新的推理路径，持续扩展节点，直到达到终点（即得到最终答案）。</li><li><strong>回溯（Backpropagation）：</strong> 根据最终输出结果 更新父节点的评分，从而优化未来的搜索决策。</li></ol><p>在大语言模型（LLM）的推理过程中，我们通常希望找到最佳的推理路径，使其最终生成的答案最优。但在这个过程中，需要在 <strong>探索（Exploration）</strong> 和 <strong>利用（Exploitation）</strong> 之间取得平衡：</p><ul><li><strong>利用（Exploitation）</strong>：选择当前看起来最优的路径，以利用已知的高质量推理步骤。</li><li><strong>探索（Exploration）</strong>：选择访问次数较少的路径，以发现可能更优的推理步骤。</li></ul><h4 id="选择分数（Selection-Score）"><a href="#选择分数（Selection-Score）" class="headerlink" title="选择分数（Selection Score）"></a>选择分数（Selection Score）</h4><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_163149.png" class=""><p>在选择推理路径时，我们使用 <strong>选择分数（Selection Score）</strong> 计算每个推理步骤（即树的节点）的优先级，公式如下：</p><p>$$<br>\text{Selection Score} = \frac{\text{Total Node Reward}}{\text{Number of Node Visits}} + C \times \sqrt{\frac{\text{Number of Parent Node Visits}}{\text{Number of Node Visits}}}<br>$$</p><p>其中：</p><ul><li><p><strong>第一项</strong>：$$\frac{\text{Total Node Reward}}{\text{Number of Node Visits}}$$（利用项，Exploitation Term）</p><ul><li><strong>Total Node Reward</strong>：该节点累计获得的奖励值（表示其历史表现）。</li><li><strong>Number of Node Visits</strong>：该节点被访问的次数。</li><li>这项计算的是该节点的 <strong>平均奖励值</strong>，高奖励的节点会被优先选择。</li></ul></li><li><p><strong>第二项</strong>：$$C \times \sqrt{\frac{\text{Number of Parent Node Visits}}{\text{Number of Node Visits}}}$$（探索项，Exploration Term）</p><ul><li><strong># of Parent Node Visits</strong>：父节点被访问的次数。</li><li><strong># of Node Visits</strong>：当前节点被访问的次数。</li><li><strong>C</strong>：一个超参数，控制探索与利用的平衡。</li><li>这项鼓励探索访问次数较少的节点，以防止过早陷入局部最优解。</li></ul></li></ul><p>总结：</p><ul><li><strong>第一项（Exploitation Term）</strong> 让算法倾向于选择 <strong>历史表现较好的路径</strong>。</li><li><strong>第二项（Exploration Term）</strong> 让算法倾向于 <strong>探索访问较少的路径</strong>，避免陷入局部最优。</li><li><strong>参数 C</strong> 控制这两者的平衡。</li></ul><h4 id="2-选择（Selection）与扩展（Expand）"><a href="#2-选择（Selection）与扩展（Expand）" class="headerlink" title="2. 选择（Selection）与扩展（Expand）"></a><strong>2. 选择（Selection）与扩展（Expand）</strong></h4><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_164346.png" class=""><p>这一阶段，我们使用 <strong>选择分数</strong> 来决定哪条推理路径值得继续扩展：</p><p><strong>（1）选择（Selection）</strong></p><ul><li><strong>输入：问题（Question）</strong></li><li><strong>LLM 生成多个推理步骤（Reasoning Steps）</strong><ul><li>例如，在图片中，LLM 生成了 3 个推理步骤：<ul><li><strong>Thought 1</strong>（评分 0.4）</li><li><strong>Thought 2</strong>（评分 0.2）</li><li><strong>Thought 3</strong>（评分 0.1）</li></ul></li></ul></li><li><strong>使用选择分数（Selection Score）选择最优路径</strong>（随机初始化）<ul><li>在示例中，评分最高的 <strong>Thought 1（0.4）</strong> 被选中。</li></ul></li></ul><p><strong>（2）扩展（Expand）</strong></p><ul><li><strong>在选中的推理路径上，生成新的推理步骤</strong></li><li>这些新推理步骤的初始值设为 0，表示它们还没有经过评估。</li></ul><p>这个过程类似于 <strong>MCTS 的拓展（Expansion）阶段</strong>，即：</p><ol><li>选择当前最优路径（使用 <strong>选择分数</strong>）。</li><li>在该路径下，扩展新的推理步骤（未评分的子节点）。</li></ol><h4 id="3-Rollouts（模拟）与-Backpropagation（反向传播）"><a href="#3-Rollouts（模拟）与-Backpropagation（反向传播）" class="headerlink" title="3. Rollouts（模拟）与 Backpropagation（反向传播）"></a><strong>3. Rollouts（模拟）与 Backpropagation（反向传播）</strong></h4><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_165052.png" class=""><p>一旦扩展了推理步骤，我们需要继续探索，并利用 <strong>模拟（Rollouts）</strong> 和 <strong>反向传播（Backpropagation）</strong> 来优化整个搜索过程。</p><h3 id="（3）Rollouts（模拟）"><a href="#（3）Rollouts（模拟）" class="headerlink" title="（3）Rollouts（模拟）"></a><strong>（3）Rollouts（模拟）</strong></h3><ul><li>选定路径后，我们继续展开推理步骤，直到 <strong>生成最终答案</strong>。</li><li>这个过程类似于 <strong>在 MCTS 中随机模拟游戏到结束</strong>：<ul><li>我们从当前节点出发，进行一系列推理，直到模型生成最终的答案。</li><li>在图片中，我们沿着 Thought 1（0.4） 继续展开推理步骤。</li><li>这些推理步骤最终会 <strong>生成多个答案</strong>（图片中紫色框）。</li></ul></li></ul><h3 id="（4）Backpropagation（反向传播）"><a href="#（4）Backpropagation（反向传播）" class="headerlink" title="（4）Backpropagation（反向传播）"></a><strong>（4）Backpropagation（反向传播）</strong></h3><ul><li>通过对 <strong>最终答案</strong> 进行评分，我们可以更新前面所有参与推理的节点分数：<ul><li><strong>PRM（Process Reward Model）</strong>：对推理步骤本身进行评分，衡量其合理性。</li><li><strong>ORM（Output Reward Model）</strong>：对最终答案进行评分，衡量其正确性。</li><li>这些评分 <strong>向上传播</strong>，更新 <strong>所有经过的节点</strong> 的奖励值。</li></ul></li><li>例如：<ul><li>在图片中，最终答案的评分导致 <strong>Thought 1</strong> 的评分从 0.4 提高到 <strong>0.8</strong>。</li><li>进一步向上传播，使得 <strong>父节点的选择分数也随之更新</strong>。</li></ul></li></ul><p>这个过程保证了：</p><ul><li><strong>较好的推理路径会逐渐获得更高的分数</strong>，提高被选中的概率。</li><li><strong>较差的推理路径会被逐渐淘汰</strong>，避免浪费计算资源。</li></ul><hr><h2 id="📌-修改提议分布（Modifying-Proposal-Distribution）"><a href="#📌-修改提议分布（Modifying-Proposal-Distribution）" class="headerlink" title="📌 修改提议分布（Modifying Proposal Distribution）"></a>📌 修改提议分布（Modifying Proposal Distribution）</h2><p><strong>修改提议分布（Modifying Proposal Distribution）</strong></p><p>在大语言模型（LLM）的推理过程中，我们可以通过修改提议分布（Modifying Proposal Distribution）来优化模型的推理能力。这种方法的核心思想是：</p><ul><li><strong>不再单纯依赖模型搜索正确推理步骤</strong>（基于输出的优化），</li><li><strong>而是让模型主动生成更优的推理步骤</strong>（基于输入的优化）。</li></ul><p>换句话说，我们不是在输出结果后进行检验，而是直接修改模型在推理过程中如何选择 token，让它更倾向于选择能够引导推理的 token，而不是立即输出最终答案。修改了用于采样补全（completions）、思维（thoughts）或标记（tokens）的概率分布。这种方法可以让模型生成的答案更加准确、可解释，并且在面对复杂问题时更具有鲁棒性（robustness）。</p><p><strong>1. 直接选择最高概率 Token（Greedy 选择）</strong></p><p>在默认情况下，LLM 生成多个可能的 token 作为输出候选项，并根据其概率进行排序，最终选择最高概率的 token 进行输出。这种方法称为<strong>贪心选择（Greedy Selection）</strong>。</p><p>你可以想象，我们有一个问题（question）和一个用于采样 token 的概率分布（distribution）。常见的策略是选择得分最高的 token。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_170357.png" class=""><ul><li>例如，给定问题 <code>What is 3 + 2?</code>，LLM 可能会生成如下候选 token：<ul><li><code>5</code>（最高概率）</li><li><code>3</code></li><li><code>Adding</code></li><li><code>4</code></li><li><code>If</code></li></ul></li><li>在贪心策略下，模型会直接选择 <code>5</code> 作为最终答案，而不会进行推理。</li></ul><p>这种方法虽然快速，但存在如下问题：</p><ul><li><strong>缺乏推理能力</strong>：模型可能直接输出错误答案，因为它没有进行推理。</li><li><strong>可解释性差</strong>：对于复杂问题，用户无法理解模型是如何得出答案的。</li></ul><p><strong>2. 通过推理（Reasoning Before Answering）提高答案质量</strong></p><p>然而，请注意上图中有一些<strong>标记（tokens</strong>被标红。这些token更有可能引导模型进入一个合理的推理过程。虽然选择贪心（greedy）策略下得分最高的 token 不一定是错误的，但选择那些能引导模型进入推理过程的 token，通常会得到更好的答案。<br>让 LLM <strong>先进行推理，再给出答案</strong>，即：</p><ul><li>选择推理 token（如 <code>Adding</code>）</li><li>逐步生成推理过程，如：<ul><li><code>Adding → 3 and 2 gives → 5</code></li><li><code>If → 3 + 1 = 4, 4 + 1 = 5 → 5</code></li><li><code>The total is → 5</code></li></ul></li><li>通过推理链条逐步推导出 <code>5</code>，相比直接选择 <code>5</code>，这种方法更加可解释，并且能在复杂问题上表现更好。</li></ul><p><strong>3. 通过修改提议分布（Re-Ranking Token Probabilities）引导推理过程</strong></p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_171002.png" class=""><p>当我们<strong>修改提议分布（proposal distribution，即 token 的概率分布）</strong>时，实际上是在<strong>重新排序（re-rank）</strong>这个分布，使得“推理相关”的 token 被选中的概率更高。<br>在这种方法下，我们调整 LLM 的提议分布，使其更倾向于选择推理 token，而非直接选择答案：</p><ul><li>默认情况下，<code>5</code> 具有最高概率，而 <code>Adding</code>、<code>If</code> 等推理 token 的概率较低。</li><li>通过修改提议分布，我们提高 <code>Adding</code>、<code>If</code> 的概率，使模型倾向于进行推理。</li></ul><p><strong>4. 如何实现修改提议分布？</strong></p><p>主要有两种方式：</p><ol><li><strong>通过 Prompt Engineering</strong><ul><li>修改 Prompt，引导模型生成推理步骤。</li><li>例如：<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Q: What is 3 + 2?</span><br><span class="line">A: Let's think step by step.</span><br></pre></td></tr></tbody></table></figure></li></ul></li><li><strong>训练模型更倾向于推理</strong><ul><li>在微调过程中，提供更多具有推理链的训练数据，让模型习惯生成推理 token。</li></ul></li></ol><p><strong>总结</strong></p><ul><li><strong>贪心选择（Greedy Selection）</strong>：快速，但缺乏推理，可解释性差。</li><li><strong>推理后回答（Reasoning Before Answering）</strong>：提高答案质量和可解释性。</li><li><strong>修改提议分布（Modifying Proposal Distribution）</strong>：调整 token 选择的概率，使模型更倾向于选择推理 token，提高整体答案的合理性。</li></ul><p>这种方法在<strong>数学计算、逻辑推理、法律推理等任务</strong>上尤为重要，使得 LLM <strong>不仅能“答对”，还能“说明白”</strong>。</p><h3 id="Prompting"><a href="#Prompting" class="headerlink" title="Prompting"></a><strong>Prompting</strong></h3><p>随着我们使用 <strong>prompt engineering</strong>（提示工程）来改进输出，我们会通过更新提示（prompt）来尝试提升模型的表现。这个过程也可能推动模型去展示先前我们看到的一些<strong>reasoning</strong>（推理）过程。</p><p><strong>1. 改变 Proposal Distribution</strong></p><p>在更改 <strong>proposal distribution</strong>时，我们可以给模型提供示例（也叫做 <strong>in-context learning</strong>），让它在生成答案时模仿类似的推理风格。下面的图就展示了一个示例的情形：</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_172130.png" class=""><blockquote><ul><li><strong>图示内容</strong>：左侧是一个简单的问题 “What is 3 + 2?”，模型内部用 “Thoughts” 表示隐藏的思考过程，比如：<ol><li>First, 3 and 1 gives 4.</li><li>Then, 4 and 1 gives 5.</li><li>I believe the answer is 5.</li></ol></li><li><strong>Answer</strong>（答案）：5</li><li>右侧用红色、蓝色等不同颜色的条形或方块表示推理过程的不同部分，示意有一部分属于隐藏的推理过程（红色），以及输出结果或若干中间步骤（蓝色）。</li></ul></blockquote><p>通过类似的示例，模型在推理时就可能模仿类似的格式来进行<strong>reasoning</strong>并给出最终答案。</p><p><strong>2. “Let’s think step-by-step” 的影响</strong></p><p>我们也可以通过在提示中直接使用 “Let’s think step-by-step” 来简化上述流程。这会改变模型的 <strong>proposal distribution</strong>，让 <strong>LLM</strong>（大型语言模型）倾向于在回答之前分步骤思考。如下图所示：</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_172232.png" class=""><blockquote><ul><li><strong>图示内容</strong>：这里将提示换成 “Let’s think step-by-step”，问题仍然是 “What is 3 + 2?”。</li><li>模型产生更显式的推理过程（用红色块示意），再输出正确答案 5。</li><li>整个思路类似图1，但更加突出“分步骤思考”对最终答案生成的影响。</li></ul></blockquote><p>然而，这并不意味着模型本身已经内化了这种推理能力——它<strong>并没有从根本上学会</strong>去“反思”或“修正”错误。如果模型一开始的推理过程是错误的，那么在这种静态且线性的流程中，它往往会一直延续这个错误，而不是对自身推理进行修正。</p><hr><h3 id="STaR（Self-Taught-Reasoner）"><a href="#STaR（Self-Taught-Reasoner）" class="headerlink" title="STaR（Self-Taught Reasoner）"></a><strong>STaR（Self-Taught Reasoner）</strong></h3><p>除了通过 <strong>prompting</strong>（提示）让模型临时展示推理步骤，我们还可以让模型在训练中因为“产生正确推理步骤”而得到奖励，从而让它真正“学会”推理。这通常需要在<strong>大量带有推理过程的数据</strong>上进行训练，并结合 <strong>reinforcement learning</strong>（强化学习）来奖励特定的行为。</p><p>一个颇受争议（“much-debated”）的技术就是 <strong>STaR</strong>，即 <strong>Self-Taught Reasoner</strong>。它是让 <strong>LLM</strong> 生成自己的推理数据，再把这些数据用于对模型进行<strong>精调</strong>（<em>fine-tuning</em>）的过程。</p><p><strong>1. STaR 的流程概述</strong></p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_172455.png" class=""><ul><li>这幅图概括了 STaR 的工作原理：<ol><li><strong>Generate reasoning + answer</strong>：模型先针对输入问题生成一段 <strong>reasoning</strong>（推理）和一个 <strong>answer</strong>（答案）；<br>2a. 如果答案正确（Correct answer），则将 <strong>Question, Reasoning, Answer</strong> 作为训练样本添加到三元组数据集中（3a）；<br>  3b. 利用这些三元组数据进行 <strong>supervised fine-tuning</strong>（监督微调），让模型学会在类似情形下产出正确推理与答案。</li></ol></li></ul><p>如果模型给出了错误答案，则会触发另一条路径：</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_172608.png" class=""><ul><li>当 (2b) 模型答案错误时，我们提供正确答案作为 <strong>hint</strong>（提示），并让模型去思考“为什么这个答案是正确的”；</li><li>也就是 <strong>Generate reasoning only</strong> (why this answer is correct?)；</li><li>得到的这段新的推理依旧会被加入到三元组数据中，然后再进行 <strong>supervised fine-tuning</strong>。</li></ul><p>这里的关键要点是，我们可以通过这种方法<strong>显式</strong>地训练模型“应该如何进行推理”，而不仅仅是让它临时地模仿推理过程。我们要对模型的推理方式进行<strong>监督</strong>（<em>supervised fine-tuning</em>），从而把我们想要的推理模式“灌输”给模型。</p><p><strong>2. 自动生成合成训练样本</strong></p><p>STaR 的整个流程非常有趣，因为它会<strong>自动生成合成训练样本</strong>（<em>synthetic training examples</em>）。这些样本不仅包含问题和答案，还包含一系列推理步骤，能够帮助模型更好地学习如何“思考”。在其他研究中（例如 <strong>DeepSeek R-1</strong>），我们可以利用这些合成样本来<strong>蒸馏</strong>（<em>distill</em>，意为“提炼和保留关键信息”）推理过程到其它模型上。也就是说，一个掌握了推理能力的模型可以帮助另一个模型更快地学会类似的推理。</p><hr><p><strong>重点：</strong></p><ul><li><strong>Prompting</strong>（提示）能够影响模型的输出风格和思维过程，比如使用 “Let’s think step-by-step” 让模型显式给出推理步骤，但并不保证模型自动纠正错误。</li><li><strong>STaR</strong>（<strong>Self-Taught Reasoner</strong>）等方法则通过<strong>生成推理数据、监督微调和奖励机制</strong>，帮助模型真正学会按照指定的推理方式去思考和回答问题。</li><li>无论是哪一种方法，都可以视为对 <strong>proposal distribution</strong> 的调节：要么是提示时临时<strong>nudge</strong>（引导），要么是从训练根源上进行调教，让模型内化这种推理过程。</li><li>利用 <strong>in-context learning</strong> 提供示例，能够让模型模仿推理风格。</li><li>用 <strong>reinforcement learning</strong> 或<strong>监督微调</strong>（<strong>supervised fine-tuning</strong>）可以使模型逐渐掌握我们期望的推理模式。</li><li><strong>STaR</strong> 方法会自动收集“正确推理”数据并进行训练，使得模型在后续回答中更可能产生正确且符合要求的推理步骤。</li></ul><hr><h2 id="DeepSeek-R1"><a href="#DeepSeek-R1" class="headerlink" title="DeepSeek-R1"></a>DeepSeek-R1</h2><hr><h3 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h3><p><strong>DeepSeek-R1</strong> 是一个在推理（reasoning）模型领域的重大版本，其权重已经开源。它直接与 OpenAI 的 <strong>o1</strong> 推理模型展开竞争，并在这一领域产生了重大影响。</p><p>DeepSeek 项目在将推理功能优雅地整合进其基础模型（<strong>DeepSeek-V3-Base</strong>）方面成就卓著，采用了多种技术来完成这一目标。</p><p>有趣的是，该项目在训练过程中并未依赖额外的验证器（verifier），而且并不是单纯地依靠监督微调（supervised fine-tuning）来提炼推理行为。相反，<strong>强化学习（Reinforcement Learning, RL）</strong> 在其中扮演了重要角色。</p><p>以下我们将一起探究他们是如何在模型中训练出推理行为的！</p><hr><h3 id="2-DeepSeek-R1-Zero：推理的关键探索"><a href="#2-DeepSeek-R1-Zero：推理的关键探索" class="headerlink" title="2. DeepSeek-R1 Zero：推理的关键探索"></a>2. DeepSeek-R1 Zero：推理的关键探索</h3><p>在通往 <strong>DeepSeek-R1</strong> 的道路上，有一个名为 <strong>DeepSeek-R1 Zero</strong> 的实验性模型为这次突破打下了基础。它从 <strong>DeepSeek-V3-Base</strong> 出发，完全不使用大规模监督微调来加入推理数据，而是只依靠 <strong>强化学习</strong> 来获得推理能力。</p><h4 id="训练过程与系统提示（Prompt）"><a href="#训练过程与系统提示（Prompt）" class="headerlink" title="训练过程与系统提示（Prompt）"></a>训练过程与系统提示（Prompt）</h4><p>在此过程中，他们首先准备了一个非常直接的提示（prompt），其形式类似于系统提示（system prompt），用来作为推理管线的一部分。下文即展示了相关提示。请注意，其中明确指出了推理过程要写在 <code>&lt;think&gt;</code> 标签内、答案要写在 <code>&lt;answer&gt;</code> 标签内，但没有进一步规定推理过程应如何具体呈现或组织。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_183150.png" class=""><p>在上图中，可以看到一个简化版的对话示例（System prompt 与 User prompt）以及模型如何将<strong>推理</strong>（reasoning）放在 <code>&lt;think&gt;</code> 标签内、将<strong>答案</strong>（answer）放在 <code>&lt;answer&gt;</code> 标签内。该图突出展示了在提示（prompt）中对模型的约束：</p><ul><li><em>“The assistant first thinks about the reasoning process in the mind and then provides the user with the answer.”</em></li><li>要求使用 <code>&lt;think&gt;</code> 进行推理，使用 <code>&lt;answer&gt;</code> 进行回答。</li></ul><p>这里并未提供关于“推理过程”格式的其他例子或模板——完全由模型自己在训练中摸索出要如何输出“Chain-of-Thought”式的推理文字。</p><h4 id="强化学习奖励"><a href="#强化学习奖励" class="headerlink" title="强化学习奖励"></a>强化学习奖励</h4><p>在训练中，采用了两个基于规则（rule-based）的奖励机制：</p><ol><li><strong>准确性奖励（Accuracy rewards）</strong><br>通过测试给出的答案是否正确来进行奖励。若模型输出的答案正确，就会增加奖励。</li><li><strong>格式奖励（Format rewards）</strong><br>奖励模型对 <code>&lt;think&gt;</code> 和 <code>&lt;answer&gt;</code> 标签的正确使用。</li></ol><p>他们所使用的强化学习算法名为 <strong>Group Relative Policy Optimization（GRPO）</strong>。此算法的直观想法在于：使所有导致正确或错误答案的决策更易或更难再次出现。这些决策可能包括模型生成的某些标记（token）序列，也可能包括推理步骤本身（即思考过程）。下文给出了这一训练阶段的示意图。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_183402.png" class=""><p>在图中，重点展示了在 RL（强化学习）过程中所使用的两类奖励：</p><ul><li>“is <code>&lt;think&gt;</code> used?” —— 为使用 <code>&lt;think&gt;</code> 标签而打分。</li><li>“is <code>&lt;answer&gt;</code> used?” —— 为使用 <code>&lt;answer&gt;</code> 标签而打分。</li></ul><p>除此之外，还有对答案<strong>正确性</strong>的奖励（accuracy reward）。图中箭头所示的循环代表了在训练中不断迭代更新模型，使之越来越倾向于正确的推理方式并合乎格式要求。</p><h4 id="自发推理行为的出现"><a href="#自发推理行为的出现" class="headerlink" title="自发推理行为的出现"></a>自发推理行为的出现</h4><p>值得一提的是，研究人员并没有向模型提供任何示例来告诉它 <code>&lt;think&gt;</code> 标签中的内容应该如何书写或展开。他们仅仅告诉模型：</p><blockquote><p>“It should use <code>&lt;think&gt;</code> tags, and nothing more!”</p></blockquote><p>通过对“Chain-of-Thought”相关行为进行<strong>间接奖励</strong>（即只要推理正确、使用正确格式，就鼓励输出更完整的推理内容），模型在训练中自发地学会了越写越长的推理过程，也更易产生正确答案。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_183551.png" class=""><p>上图呈现了模型在训练过程中输出的推理长度随训练步数增加而逐渐变长的趋势。纵轴是每个响应的平均长度，横轴是训练步数。可以看到，曲线整体是向上攀升的，这表明模型不断倾向于输出更长、更详细的思考内容（Chain-of-Thought），并因此获得更高奖励。这种做法将大部分计算消耗从训练阶段（train-time compute）转移到了推理阶段（test-time compute），也就是在推理时才生成更长的思考过程。</p><p>根据研究，他们发现通过这种训练策略，模型能够自行发现最优的 Chain-of-Thought 风格的思考方式，并展现出高级的推理能力，例如：<strong>自我反思（self-reflection）</strong> 和 <strong>自我验证（self-verification）</strong>。</p><p>不过，DeepSeek-R1 Zero 的模型输出仍存在一些问题，比如可读性欠佳，且有时会混用多种语言。为了在产品化或发布级别进一步完善，研究人员提出了另一个选项，也就是在正式版本中使用的 <strong>DeepSeek R1</strong>。</p><hr><h3 id="3-深入了解-DeepSeek-R1"><a href="#3-深入了解-DeepSeek-R1" class="headerlink" title="3. 深入了解 DeepSeek-R1"></a>3. 深入了解 DeepSeek-R1</h3><p>要构建 <strong>DeepSeek-R1</strong>，作者共进行了以下五个关键步骤：</p><ol><li><strong>冷启动（Cold Start）</strong></li><li><strong>以推理为导向的强化学习（Reasoning-oriented Reinforcement Learning）</strong></li><li><strong>拒绝采样（Rejection Sampling）</strong></li><li><strong>监督微调（Supervised Fine-Tuning）</strong></li><li><strong>在所有场景下进行强化学习（Reinforcement Learning for all Scenarios）</strong></li></ol><p>接下来我们依次展开说明。</p><hr><h4 id="第一步：冷启动"><a href="#第一步：冷启动" class="headerlink" title="第一步：冷启动"></a>第一步：冷启动</h4><p>在第一步中，研究人员先使用了一个约 5000 个tokens的高质量推理数据集对 <strong>DeepSeek-V3-Base</strong> 进行微调，以避免产生可读性不佳的<strong>冷启动问题（cold start problem）</strong>。这个微调步骤可以让模型的输出更加可读，不至于在一开始就产生混乱的推理文本。下文展示了这一过程的示意图。</p><hr><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_184058.png" class=""><p>在图中可以看到：</p><ul><li>“DeepSeek-V3-Base” 通过<strong>监督微调（Supervised Fine-Tuning）</strong>的方式，引入了约 5000 条高质量推理样本。</li><li>这些样本包含了<strong>Reasoning</strong>（推理）和<strong>Answer</strong>（答案）两种部分。</li><li>该步骤目的是“防止冷启动”，即让模型在一开始就掌握基础的可读性推理。</li></ul><hr><h4 id="第二步：推理导向的强化学习"><a href="#第二步：推理导向的强化学习" class="headerlink" title="第二步：推理导向的强化学习"></a>第二步：推理导向的强化学习</h4><p>在得到一个初步微调后的模型后（上一步的成果），作者使用与 <strong>DeepSeek-V3-Zero</strong> 类似的强化学习流程对模型进行训练，但额外加入了<strong>目标语言一致性</strong>的奖励，以确保模型在推理和回答时不会混用多种语言。</p><p>除了之前提到的准确性（accuracy reward）和格式（format reward）等，还增加了<strong>语言奖励（language reward）</strong>来保证生成的语言风格或语言类型保持一致，不至于出现“中英文混杂”或“风格不稳”的现象。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_184229.png" class=""><ul><li><strong>Format reward</strong>：依旧关注 <code>&lt;think&gt;</code> 和 <code>&lt;answer&gt;</code> 的使用。</li><li><strong>Accuracy reward</strong>：检查答案是否正确，以及是否能通过相应的“单元测试”。</li><li><strong>Language reward</strong>：检查语言是否一致、通顺以及是否符合目标语言要求。</li></ul><p>这些奖励综合起来，通过强化学习（RL）循环使模型的推理和答案在可读性、准确度和语言风格方面逐渐优化。</p><hr><h4 id="第三步：拒绝采样"><a href="#第三步：拒绝采样" class="headerlink" title="第三步：拒绝采样"></a>第三步：拒绝采样</h4><p>在这一阶段，作者用<strong>第 2 步</strong>强化学习后得到的模型，来大规模生成<strong>合成推理数据</strong>，并配合 <strong>DeepSeek-V3-Base</strong> 模型来进行“评估”和“规则过滤”，最终产生约 60 万条高质量的推理样本可用于后续监督微调。同时，他们还另外生成了约 20 万条<strong>非推理样本</strong>，包含了写作、简单问答、自我认知、翻译等多种任务数据。下文总结了这一过程。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_184407.png" class=""><ul><li>左边展示了<strong>DeepSeek-V3-2</strong> 如何采样到大量<strong>Reasoning</strong>（推理）和<strong>Answer</strong>（答案），再利用基于规则的筛选和 <strong>DeepSeek-V3-Base</strong> 的判断（判断生成的内容质量），保留质量更好的推理数据（约 600,000 条）。</li><li>右边展示了<strong>非推理</strong>（non-reasoning）数据采样流程，来自 DeepSeek-V3-Base 所使用的一部分数据，总共约 200,000 条，这些数据主要涉及写作、事实性问答（factual QA）、自我认知、翻译等方面。</li></ul><p>由此，研究人员得到规模约 80 万条的“混合”数据，其中既有推理样本，也有非推理样本。</p><hr><h4 id="第四步：监督微调"><a href="#第四步：监督微调" class="headerlink" title="第四步：监督微调"></a>第四步：监督微调</h4><p>在得到上述 80 万条数据后，研究人员再次对 <strong>DeepSeek-V3-Base</strong> 进行监督微调，具体过程如下图所示。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_184526.png" class=""><ul><li>在图中，我们看到“DeepSeek-V3-Base”被用于执行<strong>监督微调（Supervised Fine-Tuning）</strong>，使用的正是前文所提到的 800,000 条<strong>高质量推理与非推理样本</strong>。</li><li>这一阶段使得模型在更大规模的数据基础上，学习到更广泛、更多样的推理形式和任务形式。</li></ul><hr><h4 id="第五步：在所有场景下的强化学习"><a href="#第五步：在所有场景下的强化学习" class="headerlink" title="第五步：在所有场景下的强化学习"></a>第五步：在所有场景下的强化学习</h4><p>在监督微调完成后，研究人员继续采用类似 <strong>DeepSeek-R1-Zero</strong> 的方法进行 <strong>RL（强化学习）</strong> 训练。但是，为了让模型更符合人类偏好，他们在这个阶段引入了更多的 <strong>“有益与无害”（helpfulness and harmlessness）</strong> 奖励信号，用来约束模型的回答。</p><p>同时，模型也被要求<strong>对推理过程进行总结（summarize）</strong>，以防止在最终输出时显示出过长、难以阅读的推理文本。这一步骤解决了前述提到的可读性问题。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_184907.png" class=""><ol><li><strong>Format reward（格式奖励）</strong>  <ul><li>是否正确使用 <code>&lt;think&gt;</code> 标签书写推理内容  </li><li>是否正确使用 <code>&lt;answer&gt;</code> 标签输出答案</li></ul></li><li><strong>Accuracy reward（准确性奖励）</strong>  <ul><li>测试输出是否能编译（“does it compile?”）  </li><li>是否能通过单元测试（“does it pass unit tests?”）</li></ul></li><li><strong>Preference rewards（偏好奖励）</strong>  <ul><li>关注 <strong>Helpfulness（有益）</strong>、<strong>Harmlessness（无害）</strong>、<strong>Human preference（人类偏好）</strong> 等  </li><li>由 RM（Reward Model） 模块来评估这些偏好指标</li></ul></li></ol><p>图中可以看到，<strong>Reasoning</strong>（推理）阶段和 <strong>Answer</strong>（答案）阶段需要分别用 <code>&lt;think&gt;</code> 和 <code>&lt;answer&gt;</code> 标签进行明确区分。同时，为了输出更为精简、可读的内容，模型也可能产生一个 <strong>Summary</strong>（总结）片段。强化学习的迭代过程会同时考虑多种奖励信号，从而不断更新模型并得到最终版本的 <strong>DeepSeek-R1</strong>。</p><p>上图中，“RM” 即 Reward Model，用于对偏好进行打分（如对话是否友善、是否符合伦理要求等），再把结果反馈给模型。</p><p>“<strong>And that’s it!<strong>”这意味着 <strong>DeepSeek-R1</strong> 实际上是 <strong>DeepSeek-V3-Base</strong> 经过监督微调（Supervised Fine-Tuning）和强化学习（RL）进一步优化而成。大量的工作都用于保证</strong>高质量数据</strong>的生成与使用，进而训练出这样一个具备强大推理能力的模型。</p><hr><h2 id="将推理知识从-DeepSeek-R1-蒸馏到其他模型"><a href="#将推理知识从-DeepSeek-R1-蒸馏到其他模型" class="headerlink" title="将推理知识从 DeepSeek-R1 蒸馏到其他模型"></a>将推理知识从 DeepSeek-R1 蒸馏到其他模型</h2><p><strong>DeepSeek-R1</strong> 拥有 <strong>6710 亿（671B）</strong> 参数。这一规模的模型在普通消费级硬件上运行存在较大难度。出于实用性考虑，作者们研究了如何将 <strong>DeepSeek-R1</strong> 的推理能力“蒸馏（distill）”到更小的模型（如 <strong>Qwen-32B</strong>）上，以便能在消费级硬件上部署和使用。</p><h3 id="蒸馏过程：Teacher-Student-框架"><a href="#蒸馏过程：Teacher-Student-框架" class="headerlink" title="蒸馏过程：Teacher-Student 框架"></a>蒸馏过程：Teacher-Student 框架</h3><p>在蒸馏过程中，<strong>DeepSeek-R1</strong> 作为教师模型（Teacher），而规模更小的模型（如 Qwen-32B）作为学生模型（Student）。二者面对相同的提示（prompt）时，分别会输出一组<strong>词元概率分布（token probability distribution）</strong>。训练时，学生模型会尽量学习并接近教师模型的输出分布。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_185245.png" class=""><ul><li>教师（DeepSeek-R1）给出自己的“proposal distribution”。例如在回答“What is 3 + 2?”时，教师模型可能倾向输出“Adding”“If”“5”“3”“4”等标记，并赋予各自不同的概率。  </li><li>学生（Qwen-32B）则会在训练中不断更新自己的概率分布，使之更接近教师的分布。</li></ul><blockquote><p><strong>额外解释</strong>：  </p><ol><li><strong>概率分布（proposal distribution）</strong>：语言模型在生成下一个词元（token）时，会输出对所有可能词元的概率估计。  </li><li><strong>蒸馏（distillation）</strong>：通过比较教师和学生的分布差异，学生会逐步调整自身参数，使其输出更接近教师模型的风格和推理倾向。</li></ol></blockquote><p>训练所使用的数据，正是之前提到的那 <strong>80 万条高质量样本</strong>——其中包含约 60 万推理样本和 20 万非推理样本。下图展示了这一数据流向： </p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_185323.png" class=""><ul><li>左侧的 <strong>Reasoning</strong>（推理）和 <strong>Answer</strong>（答案）数据，合计 80 万条。  </li><li>由 <strong>DeepSeek-R1</strong>（Teacher）生成或评估，得到对应的概率分布。  </li><li>学生模型 <strong>Qwen-32B</strong> 则根据教师的分布进行学习，最终得到一个蒸馏版本 <strong>DeepSeek-R1-Distill-Qwen-32B</strong>。</li></ul><p><img src="https://user-images.githubusercontent.com/your-image-url.png" alt="使用 80 万条高质量样本蒸馏的流程（图10）"></p><blockquote><p><strong>额外解释</strong>：  </p><ul><li>学生模型不仅仅学习了那 80 万条样本本身的输入-输出模式，也学习到 <strong>DeepSeek-R1</strong> 在面对这些数据时所“倾向”采用的推理策略和概率分布，从而在更小模型上复现类似的推理能力。  </li><li>“Distilled” 模型往往会在推理质量与计算资源之间找到更好的平衡：虽然可能在性能上略逊色于老师模型，但依然能在大多数常见任务上达到令人满意的结果，并且所需资源更低。</li></ul></blockquote><hr><h2 id="其他未成功的尝试"><a href="#其他未成功的尝试" class="headerlink" title="其他未成功的尝试"></a>其他未成功的尝试</h2><p>在研究过程中，DeepSeek 团队也曾尝试过 <strong>Process Reward Models（PRMs）</strong> 和 <strong>Monte Carlo Tree Search（MCTS）</strong> 等方法来注入推理能力，但结果并不理想：</p><ol><li><p><strong>使用 MCTS</strong>  </p><ul><li>面临的主要问题是搜索空间过于庞大，只能对节点展开进行严格限制。这样一来，效果就大打折扣。  </li><li>此外，精细化训练 Reward Model 也相当困难。</li></ul></li><li><p><strong>使用 PRMs 进行 Best-of-N 策略</strong>  </p><ul><li>如果不断重训练 Reward Model 以防止模型出现“投机取巧”（reward hacking）行为，会带来高昂的计算开销。</li></ul></li></ol><p>这些结果并不意味着这些技术无效，而是说明它们在当前大规模语言模型上的实践还有诸多限制与难点。<strong>DeepSeek-R1</strong> 之所以取得成功，更多依赖于<strong>强化学习 + 监督微调</strong>的组合，以及对大规模高质量数据的挖掘与利用。</p><hr><h2 id="总结与展望"><a href="#总结与展望" class="headerlink" title="总结与展望"></a>总结与展望</h2><p>至此，我们已经大致回顾了 <strong>DeepSeek-R1</strong> 的推理训练之旅。希望以上内容能够让你更好地理解：  </p><ul><li><strong>Test-time compute（推理时计算）</strong> 可以通过模型输出更长、更精细的思考过程（Chain-of-Thought）来取得更佳效果。  </li><li>大规模“<strong>先监督微调，再强化学习</strong>”的训练流程，以及<strong>蒸馏</strong>到更小模型的技术路线，也展现了在硬件资源和推理性能间取得平衡的方法。</li></ul><p>如前所述，<strong>DeepSeek-R1</strong> 引入了多种奖励机制，尤其是针对格式和人类偏好的奖励，来保证回答既正确又易读。“总结推理过程”（Summary）的做法也在很大程度上改善了纯文本Chain-of-Thought过长而导致的可读性问题。</p><hr><h2 id="更多资源"><a href="#更多资源" class="headerlink" title="更多资源"></a>更多资源</h2><p>如果你对 <strong>Large Language Models（LLMs）</strong> 中的推理话题感兴趣，以下资源值得参考：</p><ol><li><a href="https://newsletter.languagemodels.co/p/the-illustrated-deepseek-r1"><strong>The Illustrated DeepSeek-R1</strong></a>  <ul><li>Jay Alammar 制作的高质量可视化指南，详细介绍了 DeepSeek-R1 模型背后的原理与实现细节。</li></ul></li><li><a href="https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute"><strong>Hugging Face 的一篇博文</strong></a>  <ul><li>重点讨论了在推理阶段如何对计算量进行扩展，并给出了有趣的实验。</li></ul></li><li><a href="https://www.youtube.com/watch?v=6PEJ96k1kiw"><strong>视频 “Speculations on Test-Time Scaling”</strong></a>  <ul><li>深入探讨了在推理阶段进行各种计算扩展的常用技术细节。</li></ul></li></ol><p>此外，作者在文中也提到了一本关于大型语言模型的著作，内含更多可视化和实验结果，是想进一步研究推理 LLMs 的朋友可以深入阅读的好资料。</p><ul><li><strong>Official Website of the Book</strong>: <a href="https://www.llm-book.com/">llm-book.com</a>  </li><li><strong>Amazon 购买链接</strong>: <a href="https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961">Hands-On Large Language Models: Understanding, Building, and Optimizing LLMs</a>  </li><li><strong>GitHub 代码仓库</strong>: <a href="https://github.com/handsOnLLM/Hands-On-Large-Language-Models">handsOnLLM/Hands-On-Large-Language-Models</a></li></ul><hr><h3 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h3><p>感谢你阅读本篇关于 <strong>DeepSeek-R1</strong> 的介绍文档。通过对所有图片与文字内容的依次解读，以及对每个环节所涉及的关键技术进行了更多解释，我们希望让你对 <strong>DeepSeek-R1</strong> 的训练流程、蒸馏方法和未成功的尝试都有更加全面的了解。</p><p>在未来，随着硬件性能的提升与更成熟的训练技术出现，<strong>深度推理</strong>与<strong>模型蒸馏</strong>必将在更多实际应用场景中发挥巨大作用。让我们拭目以待！</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;推理-LLM-的可视化指南：探索推理时计算技术与-DeepSeek-R1&quot;&gt;&lt;a href=&quot;#推理-LLM-的可视化指南：探索推理时计算技术与-DeepSeek-R1&quot; class=&quot;headerlink&quot; title=&quot;推理 LLM 的可视化指南：探索推理时计</summary>
      
    
    
    
    <category term="NLP Insights" scheme="https://chenhuiyu.github.io/categories/NLP-Insights/"/>
    
    
    <category term="LLM" scheme="https://chenhuiyu.github.io/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://chenhuiyu.github.io/2024/12/06/NLP%20Insights/Evaluation%20of%20Generation-Based%20Large%20Language%20Models%20(LLMs):%20Opportunities%20and%20Challenges%20from%20Generation%20to%20Judgment/"/>
    <id>https://chenhuiyu.github.io/2024/12/06/NLP%20Insights/Evaluation%20of%20Generation-Based%20Large%20Language%20Models%20(LLMs):%20Opportunities%20and%20Challenges%20from%20Generation%20to%20Judgment/</id>
    <published>2024-12-06T07:50:03.238Z</published>
    <updated>2024-12-06T09:59:18.340Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Evaluation-of-Generation-Based-Large-Language-Models-LLMs-Opportunities-and-Challenges-from-Generation-to-Judgment"><a href="#Evaluation-of-Generation-Based-Large-Language-Models-LLMs-Opportunities-and-Challenges-from-Generation-to-Judgment" class="headerlink" title="Evaluation of Generation-Based Large Language Models (LLMs): Opportunities and Challenges from Generation to Judgment"></a>Evaluation of Generation-Based Large Language Models (LLMs): Opportunities and Challenges from Generation to Judgment</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>Evaluation tasks in artificial intelligence (AI) and natural language processing (NLP) have long been challenging. Traditional evaluation methods, such as those based on matching or embeddings, are limited in assessing complex attributes. The recent development of large language models (LLMs) has given rise to the “LLM-as-a-Judge” paradigm, which utilizes LLMs for scoring, ranking, or selection tasks. This paper provides a comprehensive review of LLM evaluation methodologies, including their definitions, classification frameworks, benchmarks, and future research directions.</p><hr><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><h3 id="1-1-Background"><a href="#1-1-Background" class="headerlink" title="1.1 Background"></a>1.1 Background</h3><p>Evaluation is one of the core issues in machine learning and NLP. Traditional evaluation methods such as BLEU and ROUGE often rely on text overlap and lack applicability in complex scenarios. With the development of deep learning and LLMs (e.g., GPT-4), researchers have proposed the “LLM-as-a-Judge” paradigm to address the limitations of traditional evaluation methods.</p><h3 id="1-2-Research-Questions"><a href="#1-2-Research-Questions" class="headerlink" title="1.2 Research Questions"></a>1.2 Research Questions</h3><p>This paper aims to explore the following questions:</p><ul><li><strong>What do LLMs evaluate?</strong></li><li><strong>How is evaluation conducted?</strong></li><li><strong>Where are LLMs applied for evaluation?</strong></li></ul><hr><h2 id="2-Preliminary-Knowledge"><a href="#2-Preliminary-Knowledge" class="headerlink" title="2. Preliminary Knowledge"></a>2. Preliminary Knowledge</h2><h3 id="2-1-Input-Formats"><a href="#2-1-Input-Formats" class="headerlink" title="2.1 Input Formats"></a>2.1 Input Formats</h3><p>Evaluation inputs can be categorized as follows:</p><ul><li><strong>Point-Wise</strong>: Evaluation of a single sample.</li><li><strong>Pair/List-Wise</strong>: Comparative evaluation of multiple samples.</li></ul><h3 id="2-2-Output-Formats"><a href="#2-2-Output-Formats" class="headerlink" title="2.2 Output Formats"></a>2.2 Output Formats</h3><p>Evaluation outputs include:</p><ul><li><strong>Scores</strong>: Quantitative scoring of samples.</li><li><strong>Ranking</strong>: Ordering based on merit.</li><li><strong>Selection</strong>: Choosing the best option among candidates.</li></ul><hr><h2 id="3-Evaluation-Attributes"><a href="#3-Evaluation-Attributes" class="headerlink" title="3. Evaluation Attributes"></a>3. Evaluation Attributes</h2><h3 id="3-1-Helpfulness"><a href="#3-1-Helpfulness" class="headerlink" title="3.1 Helpfulness"></a>3.1 Helpfulness</h3><p>LLMs evaluate the helpfulness of responses by guiding user tasks and generating feedback, which is crucial in AI alignment.</p><h3 id="3-2-Harmlessness"><a href="#3-2-Harmlessness" class="headerlink" title="3.2 Harmlessness"></a>3.2 Harmlessness</h3><p>Evaluating the harmlessness of text is key to generating safe content. LLMs assist in data labeling or directly assess potential harmful content.</p><h3 id="3-3-Reliability"><a href="#3-3-Reliability" class="headerlink" title="3.3 Reliability"></a>3.3 Reliability</h3><p>LLMs detect factual accuracy and consistency, e.g., generating supporting evidence or conducting conversation-level reliability evaluations.</p><h3 id="3-4-Relevance"><a href="#3-4-Relevance" class="headerlink" title="3.4 Relevance"></a>3.4 Relevance</h3><p>LLMs assess the relevance of generated or retrieved content, applicable in scenarios like conversations and retrieval-augmented generation (RAG).</p><h3 id="3-5-Feasibility"><a href="#3-5-Feasibility" class="headerlink" title="3.5 Feasibility"></a>3.5 Feasibility</h3><p>In complex tasks, LLMs judge the feasibility of candidate steps or actions to optimize decision paths.</p><h3 id="3-6-Overall-Quality"><a href="#3-6-Overall-Quality" class="headerlink" title="3.6 Overall Quality"></a>3.6 Overall Quality</h3><p>By scoring across multiple dimensions, LLMs provide an overall evaluation, suitable for comprehensive comparisons in generation tasks.</p><hr><h3 id="4-Methodology"><a href="#4-Methodology" class="headerlink" title="4. Methodology"></a>4. Methodology</h3><h4 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h4><p>The methodology section focuses on optimizing the capabilities of LLMs as evaluators (LLM-as-a-Judge) through two approaches: fine-tuning and prompt engineering.</p><ol><li><strong>Fine-Tuning Techniques</strong>: Enhancing LLM judgment capabilities using supervised fine-tuning (SFT) and preference learning with labeled or synthetic feedback.</li><li><strong>Prompt Engineering</strong>: Designing effective prompt strategies, such as operation swapping, rule enhancement, and multi-agent collaboration, to improve inference and evaluation accuracy and reliability.</li></ol><hr><h4 id="4-1-Fine-Tuning-Techniques"><a href="#4-1-Fine-Tuning-Techniques" class="headerlink" title="4.1 Fine-Tuning Techniques"></a>4.1 Fine-Tuning Techniques</h4><h5 id="Data-Sources"><a href="#Data-Sources" class="headerlink" title="Data Sources"></a>Data Sources</h5><h6 id="1-Human-Labeled-Data"><a href="#1-Human-Labeled-Data" class="headerlink" title="1. Human-Labeled Data"></a>1. <strong>Human-Labeled Data</strong></h6><p>Human-labeled data provides high-quality training samples that help LLMs learn human preferences. Key studies and innovations include:</p><ol><li><p><strong>PandaLM</strong> [Wang et al., 2024h]:</p><ul><li>Collected a diverse dataset with 300,000 samples for instruction-generation tasks.</li><li>Enhanced generalization by integrating data sources like open-domain QA and dialogue generation.</li><li>Introduced standardized annotation workflows for consistency and emphasized multilingual support.</li></ul></li><li><p><strong>AspectInstruct</strong> [Liu et al., 2024a]:</p><ul><li>Introduced a dataset tailored for multi-dimensional evaluation, covering 65 tasks and 27 evaluation dimensions.</li><li>Designed a unique task segmentation mechanism for contextual understanding and dimension prioritization.</li></ul></li></ol><h6 id="2-Synthetic-Data"><a href="#2-Synthetic-Data" class="headerlink" title="2. Synthetic Data"></a>2. <strong>Synthetic Data</strong></h6><p>Synthetic data generated by LLMs reduces dependency on human labeling and expands data coverage. Key studies and innovations include:</p><ol><li><p><strong>JudgeLM</strong> [Zhu et al., 2023]:</p><ul><li>Generated a dataset with 100,000 samples, covering various instruction-generation scenarios.</li><li>Introduced task-seeding methods to ensure diversity and specificity.</li></ul></li><li><p><strong>Meta-Rewarding</strong> [Wu et al., 2024]:</p><ul><li>Proposed “meta-rewarding,” using LLM self-evaluation signals to enhance training effectiveness.</li></ul></li></ol><h5 id="Fine-Tuning-Methods"><a href="#Fine-Tuning-Methods" class="headerlink" title="Fine-Tuning Methods"></a>Fine-Tuning Methods</h5><h6 id="1-Supervised-Fine-Tuning-SFT"><a href="#1-Supervised-Fine-Tuning-SFT" class="headerlink" title="1. Supervised Fine-Tuning (SFT)"></a>1. <strong>Supervised Fine-Tuning (SFT)</strong></h6><p>SFT trains LLMs using human-labeled or synthetic data to learn evaluation criteria. Key studies include:</p><ol><li><p><strong>FLAMe</strong> [Vu et al., 2024]:</p><ul><li>Leveraged a multi-task learning framework with 5 million samples for multi-task SFT.</li><li>Unified evaluation standards across diverse tasks.</li></ul></li><li><p><strong>JSFT</strong> [Lee et al., 2024]:</p><ul><li>Combined SFT with preference learning to optimize performance on diverse evaluation tasks.</li></ul></li></ol><h6 id="2-Preference-Learning"><a href="#2-Preference-Learning" class="headerlink" title="2. Preference Learning"></a>2. <strong>Preference Learning</strong></h6><p>Preference learning optimizes LLM comparison and ranking capabilities for complex evaluations. Key studies include:</p><ol><li><p><strong>HALU-J</strong> [Wang et al., 2024a]:</p><ul><li>Employed directed preference optimization (DPO) with multi-evidence selection mechanisms.</li></ul></li><li><p><strong>Self-Taught Evaluators</strong> [Wang et al., 2024f]:</p><ul><li>Used self-generated suboptimal responses as negative samples for dynamic improvement.</li></ul></li></ol><hr><h3 id="5-Applications"><a href="#5-Applications" class="headerlink" title="5. Applications"></a>5. Applications</h3><h4 id="Overview-1"><a href="#Overview-1" class="headerlink" title="Overview"></a>Overview</h4><p>The applications of LLM-as-a-Judge have expanded from generation evaluation to alignment, retrieval, and reasoning. This section systematically introduces these applications, their specific tasks, and representative studies.</p><hr><h4 id="5-1-Evaluation"><a href="#5-1-Evaluation" class="headerlink" title="5.1 Evaluation"></a>5.1 Evaluation</h4><h5 id="Overview-2"><a href="#Overview-2" class="headerlink" title="Overview"></a>Overview</h5><p>LLM-as-a-Judge was initially applied to evaluation tasks like dialogue generation and summarization. Key studies include:</p><ol><li><p><strong>MD-Judge</strong> [Li et al., 2024f]:</p><ul><li>Evaluated safety-related Q&amp;A frameworks, focusing on harmfulness and ethical risks.</li></ul></li><li><p><strong>Chan Framework</strong> [Chan et al., 2023]:</p><ul><li>Introduced a multi-agent debate framework for improved evaluation quality.</li></ul></li><li><p><strong>ICE</strong> [Jain et al., 2023b]:</p><ul><li>Used few-shot examples for interactive multi-dimensional evaluation.</li></ul></li></ol><hr><h3 id="7-Challenges-and-Future-Directions"><a href="#7-Challenges-and-Future-Directions" class="headerlink" title="7. Challenges and Future Directions"></a>7. Challenges and Future Directions</h3><h4 id="Overview-3"><a href="#Overview-3" class="headerlink" title="Overview"></a>Overview</h4><p>Despite its powerful capabilities, LLM-as-a-Judge faces challenges such as evaluation bias, adaptability to dynamic tasks, and the potential of human-AI collaborative evaluation. This section explores these challenges and outlines future research directions.</p><h5 id="7-1-Bias-and-Vulnerabilities"><a href="#7-1-Bias-and-Vulnerabilities" class="headerlink" title="7.1 Bias and Vulnerabilities"></a>7.1 Bias and Vulnerabilities</h5><ol><li><strong>OffsetBias</strong> [Park et al., 2024]:<ul><li>Proposed a de-biasing framework to mitigate positional and content biases.</li></ul></li></ol><h5 id="7-2-Dynamic-and-Complex-Evaluations"><a href="#7-2-Dynamic-and-Complex-Evaluations" class="headerlink" title="7.2 Dynamic and Complex Evaluations"></a>7.2 Dynamic and Complex Evaluations</h5><ol><li><strong>Tree of Thought (ToT)</strong> [Yao et al., 2023a]:<ul><li>Enhanced multi-step reasoning with dynamic state evaluation mechanisms.</li></ul></li></ol><h5 id="7-3-Self-Evaluation-and-Human-AI-Collaboration"><a href="#7-3-Self-Evaluation-and-Human-AI-Collaboration" class="headerlink" title="7.3 Self-Evaluation and Human-AI Collaboration"></a>7.3 Self-Evaluation and Human-AI Collaboration</h5><ol><li><p><strong>Self-Taught Evaluators</strong> [Wang et al., 2024f]:</p><ul><li>Highlighted the potential for models to improve through self-learning mechanisms.</li></ul></li><li><p><strong>Meta-Rewarding</strong> [Wu et al., 2024]:</p><ul><li>Demonstrated the advantages of integrating self-evaluation signals into optimization.</li></ul></li></ol><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Evaluation-of-Generation-Based-Large-Language-Models-LLMs-Opportunities-and-Challenges-from-Generation-to-Judgment&quot;&gt;&lt;a href=&quot;#Evalua</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>基于生成的大语言模型（LLM）评估：从生成到判断的机遇与挑战</title>
    <link href="https://chenhuiyu.github.io/2024/12/06/NLP%20Insights/%E5%9F%BA%E4%BA%8E%E7%94%9F%E6%88%90%E7%9A%84%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%88LLM%EF%BC%89%E8%AF%84%E4%BC%B0%EF%BC%9A%E4%BB%8E%E7%94%9F%E6%88%90%E5%88%B0%E5%88%A4%E6%96%AD%E7%9A%84%E6%9C%BA%E9%81%87%E4%B8%8E%E6%8C%91%E6%88%98/"/>
    <id>https://chenhuiyu.github.io/2024/12/06/NLP%20Insights/%E5%9F%BA%E4%BA%8E%E7%94%9F%E6%88%90%E7%9A%84%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%88LLM%EF%BC%89%E8%AF%84%E4%BC%B0%EF%BC%9A%E4%BB%8E%E7%94%9F%E6%88%90%E5%88%B0%E5%88%A4%E6%96%AD%E7%9A%84%E6%9C%BA%E9%81%87%E4%B8%8E%E6%8C%91%E6%88%98/</id>
    <published>2024-12-06T06:34:18.000Z</published>
    <updated>2024-12-06T08:08:36.085Z</updated>
    
    <content type="html"><![CDATA[<hr><h1 id="基于生成的大语言模型（LLM）评估：从生成到判断的机遇与挑战"><a href="#基于生成的大语言模型（LLM）评估：从生成到判断的机遇与挑战" class="headerlink" title="基于生成的大语言模型（LLM）评估：从生成到判断的机遇与挑战"></a>基于生成的大语言模型（LLM）评估：从生成到判断的机遇与挑战</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>人工智能（AI）与自然语言处理（NLP）领域中的评估任务长期面临挑战。传统的评估方法（如基于匹配或嵌入的技术）在判断复杂属性时效果有限。近期大语言模型（LLM）的发展催生了“LLM-as-a-Judge”范式，利用LLM对任务进行评分、排序或选择。本论文对LLM评估方法进行了全面综述，包括其定义、分类框架、评估基准，以及未来的研究方向。</p><hr><h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><h3 id="1-1-背景"><a href="#1-1-背景" class="headerlink" title="1.1 背景"></a>1.1 背景</h3><p>评估是机器学习和NLP的核心问题之一，传统评估方法如BLEU和ROUGE通常基于文本重叠，缺乏对复杂场景的适用性。随着深度学习和LLM的发展（如GPT-4），研究者提出了“LLM-as-a-Judge”模式，以解决传统评估的局限。</p><h3 id="1-2-研究问题"><a href="#1-2-研究问题" class="headerlink" title="1.2 研究问题"></a>1.2 研究问题</h3><p>本论文旨在探讨以下问题：</p><ul><li><strong>评估内容：LLM评估什么？</strong></li><li><strong>评估方法：如何进行评估？</strong></li><li><strong>应用场景：LLM在哪里评估？</strong></li></ul><hr><h2 id="2-预备知识"><a href="#2-预备知识" class="headerlink" title="2. 预备知识"></a>2. 预备知识</h2><h3 id="2-1-输入格式"><a href="#2-1-输入格式" class="headerlink" title="2.1 输入格式"></a>2.1 输入格式</h3><p>评估输入可分为：</p><ul><li><strong>点对点（Point-Wise）</strong>：单个样本评估。</li><li><strong>对/列表评估（Pair/List-Wise）</strong>：多个样本的比较评估。</li></ul><h3 id="2-2-输出格式"><a href="#2-2-输出格式" class="headerlink" title="2.2 输出格式"></a>2.2 输出格式</h3><p>评估输出包括：</p><ul><li><strong>评分（Score）</strong>：对样本进行量化评分。</li><li><strong>排序（Ranking）</strong>：根据优劣排序。</li><li><strong>选择（Selection）</strong>：从多个候选中选取最佳方案。</li></ul><hr><h2 id="3-评估属性"><a href="#3-评估属性" class="headerlink" title="3. 评估属性"></a>3. 评估属性</h2><h3 id="3-1-有用性（Helpfulness）"><a href="#3-1-有用性（Helpfulness）" class="headerlink" title="3.1 有用性（Helpfulness）"></a>3.1 有用性（Helpfulness）</h3><p>LLM通过指导用户任务和生成反馈，对响应的有用性进行评估。这在AI对齐（Alignment）中尤为重要。</p><h3 id="3-2-无害性（Harmlessness）"><a href="#3-2-无害性（Harmlessness）" class="headerlink" title="3.2 无害性（Harmlessness）"></a>3.2 无害性（Harmlessness）</h3><p>评估文本的无害性是生成安全内容的关键。LLM可辅助数据标注或直接评估潜在的有害内容。</p><h3 id="3-3-可靠性（Reliability）"><a href="#3-3-可靠性（Reliability）" class="headerlink" title="3.3 可靠性（Reliability）"></a>3.3 可靠性（Reliability）</h3><p>LLM可检测事实性和一致性。例如，通过生成辅助证据或进行对话级别的可靠性评估。</p><h3 id="3-4-相关性（Relevance）"><a href="#3-4-相关性（Relevance）" class="headerlink" title="3.4 相关性（Relevance）"></a>3.4 相关性（Relevance）</h3><p>LLM可评估生成或检索内容的相关性，适用于会话、检索增强生成（RAG）等场景。</p><h3 id="3-5-可行性（Feasibility）"><a href="#3-5-可行性（Feasibility）" class="headerlink" title="3.5 可行性（Feasibility）"></a>3.5 可行性（Feasibility）</h3><p>在复杂任务中，LLM可对候选步骤或行动进行可行性判断，从而优化决策路径。</p><h3 id="3-6-总体质量（Overall-Quality）"><a href="#3-6-总体质量（Overall-Quality）" class="headerlink" title="3.6 总体质量（Overall Quality）"></a>3.6 总体质量（Overall Quality）</h3><p>LLM通过多维度评分生成整体评价，适用于生成任务的综合比较。</p><hr><h3 id="4-方法论"><a href="#4-方法论" class="headerlink" title="4. 方法论"></a>4. 方法论</h3><h4 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h4><p>方法论部分主要探讨如何优化LLM作为评估者（LLM-as-a-Judge）的能力，从调优和提示技术两个方面进行阐述：</p><ol><li><strong>调优技术</strong>：通过监督微调（SFT）和偏好学习等方法，利用人工标注数据或合成反馈来增强LLM的判断能力。</li><li><strong>提示技术</strong>：设计高效的提示策略（如操作交换、规则增强、多代理协作等）以提升LLM在推理和评估过程中的准确性和可靠性。</li></ol><hr><h4 id="4-1-调优技术"><a href="#4-1-调优技术" class="headerlink" title="4.1 调优技术"></a>4.1 调优技术</h4><h5 id="数据来源"><a href="#数据来源" class="headerlink" title="数据来源"></a>数据来源</h5><h6 id="1-人工标注数据"><a href="#1-人工标注数据" class="headerlink" title="1. 人工标注数据"></a>1. <strong>人工标注数据</strong></h6><p>人工标注数据提供了高质量的训练样本，帮助LLM学习人类偏好。以下是核心研究及其创新点：</p><ol><li><p><strong>PandaLM</strong>【Wang et al., 2024h】：</p><ul><li>PandaLM项目收集了多样化的人工标注数据集，涵盖指令生成任务的300,000个样本。</li><li>作者通过整合多种数据源（如开放领域问答和对话生成）来增强模型的泛化能力。</li><li>该研究的关键创新在于引入了标准化的标注流程，以确保数据质量与一致性。</li><li>此外，PandaLM强调多语言支持，通过跨文化的数据标注提高模型的适用性。</li><li>最终，PandaLM被证明在多个评估任务上表现优异，其输出与人工评估高度相关。</li></ul></li><li><p><strong>AspectInstruct</strong>【Liu et al., 2024a】：</p><ul><li>该研究首次提出了一个针对多维度评估的指令调优数据集，涵盖65个任务和27个评估维度。</li><li>数据集中包含对话生成、摘要和数据到文本转换等复杂任务的多方面评分。</li><li>作者设计了独特的任务分割机制，使模型能够根据上下文理解并优先评估特定维度。</li><li>研究的亮点在于数据集的多样性和全面性，为多任务评估提供了新的基准。</li><li>最终，该数据集显著提升了LLM在不同评估场景中的多维度理解和评估能力。</li></ul></li></ol><h6 id="2-合成数据"><a href="#2-合成数据" class="headerlink" title="2. 合成数据"></a>2. <strong>合成数据</strong></h6><p>合成数据通过LLM生成训练样本，减少了对人工标注的依赖，同时扩展了数据覆盖范围。以下是核心研究及其创新点：</p><ol><li><p><strong>JudgeLM</strong>【Zhu et al., 2023】：</p><ul><li>研究者利用GPT-4生成包含任务种子、生成答案及相关评估的高质量数据集。</li><li>数据集中包含10万个样本，覆盖了指令生成任务的多种场景。</li><li>核心创新点在于引入了生成任务种子的方法，确保生成数据的多样性和针对性。</li><li>作者还设计了一种基于偏好学习的优化方法，以提高LLM对细粒度任务的判断能力。</li><li>研究表明，经过这种优化后的JudgeLM在多个基准测试中超越了传统方法。</li></ul></li><li><p><strong>Meta-Rewarding</strong>【Wu et al., 2024】：</p><ul><li>提出了一种新颖的“元奖励”（Meta-Rewarding）方法，通过LLM自我评估生成的判断信号增强训练效果。</li><li>该方法要求模型在生成答案后对自己的输出进行评分，从而生成偏好数据。</li><li>创新点在于采用策略模型作为评估者，显著提高了数据生成效率和质量。</li><li>此外，该研究通过逐步改进的偏好数据训练LLM，提高了其评估任务的鲁棒性。</li><li>最终，Meta-Rewarding展示了LLM自我增强能力的潜力，成为偏好学习领域的重要进展。</li></ul></li></ol><h5 id="调优方法"><a href="#调优方法" class="headerlink" title="调优方法"></a>调优方法</h5><h6 id="1-监督微调（SFT）"><a href="#1-监督微调（SFT）" class="headerlink" title="1. 监督微调（SFT）"></a>1. <strong>监督微调（SFT）</strong></h6><p>监督微调通过使用人工标注或合成数据，让LLM从示例中学习判断标准。以下是核心研究及其创新点：</p><ol><li><p><strong>FLAMe</strong>【Vu et al., 2024】：</p><ul><li>该研究提出了Foundational Large Autorater Models (FLAMe)，利用超过500万个样本进行大规模多任务监督微调。</li><li>FLAMe在多任务数据中引入了统一的评价标准，提高了模型在多样化任务中的评估能力。</li><li>创新点在于采用多任务学习框架，将多个评估维度集成到一个模型中。</li><li>作者还设计了任务分层训练策略，使模型能够逐步掌握复杂的评估任务。</li><li>实验结果表明，FLAMe在多个生成任务上的表现优于传统评估指标。</li></ul></li><li><p><strong>JSFT</strong>【Lee et al., 2024】：</p><ul><li>提出了Judge-augmented Supervised Fine-Tuning（JSFT）方法，通过扩展偏好学习数据增强微调效果。</li><li>数据集中包含点对点和对比评估任务，以全面覆盖多种评估场景。</li><li>创新点在于引入了多阶段训练策略，结合监督学习和偏好学习优化模型性能。</li><li>此外，研究者设计了简化提示机制，显著提高了模型处理复杂输入的能力。</li><li>JSFT的实验结果显示，其生成的评估结果在多个基准上超过了现有方法。</li></ul></li></ol><h6 id="2-偏好学习"><a href="#2-偏好学习" class="headerlink" title="2. 偏好学习"></a>2. <strong>偏好学习</strong></h6><p>偏好学习通过优化LLM的比较和排序能力，适用于复杂评估任务。以下是核心研究及其创新点：</p><ol><li><p><strong>HALU-J</strong>【Wang et al., 2024a】：</p><ul><li>提出了一种基于批评的偏好学习方法，专注于选择相关证据并生成详细批评。</li><li>创新点在于设计了多证据选择机制，提高了LLM的可靠性评估能力。</li><li>该方法通过Directed Preference Optimization（DPO）进行优化，使模型能够更准确地判断任务间的优劣。</li><li>HALU-J还结合了上下文推理，扩展了偏好学习的应用场景。</li><li>实验表明，HALU-J显著提升了复杂任务的评估准确性，尤其是在事实性和逻辑性判断上。</li></ul></li><li><p><strong>Self-Taught Evaluators</strong>【Wang et al., 2024f】：</p><ul><li>该研究提出了一种自学习的评估者方法，利用被扰乱的指令生成低质量数据作为偏好学习的负样本。</li><li>自学习方法通过自动生成的次优响应，提供了丰富的训练数据。</li><li>创新点在于通过动态调整偏好信号，提升了模型的适应性和通用性。</li><li>作者还设计了基于多轮交互的学习策略，使模型能够在动态环境中自我优化。</li><li>实验结果显示，Self-Taught Evaluators在多个开放式生成任务中表现优异。</li></ul></li></ol><h3 id="4-2-提示技术"><a href="#4-2-提示技术" class="headerlink" title="4.2 提示技术"></a>4.2 提示技术</h3><h4 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h4><p>提示技术（Prompting）通过设计高效的提示策略和推理流程优化LLM的评估能力。这部分探讨如何在推理阶段利用提示技术提升判断精度，减少偏差，并增强模型的评估鲁棒性。主要方法包括操作交换、规则增强、多代理协作、演示、多轮交互以及比较加速。</p><hr><h4 id="4-2-1-操作交换（Swapping-Operation）"><a href="#4-2-1-操作交换（Swapping-Operation）" class="headerlink" title="4.2.1 操作交换（Swapping Operation）"></a>4.2.1 操作交换（Swapping Operation）</h4><h5 id="概述-2"><a href="#概述-2" class="headerlink" title="概述"></a>概述</h5><p>操作交换技术通过更改候选项顺序减少评估的偏置性，确保LLM对输入顺序不敏感，从而提高评估的公平性和可靠性。</p><h6 id="1-MT-Bench【Zheng-et-al-2023】："><a href="#1-MT-Bench【Zheng-et-al-2023】：" class="headerlink" title="1. MT-Bench【Zheng et al., 2023】："></a>1. <strong>MT-Bench</strong>【Zheng et al., 2023】：</h6><ul><li>本研究首次系统性地提出操作交换技术，通过多轮评估减少LLM的顺序敏感性。</li><li>创新点在于引入“对称性检查”机制：将候选项顺序互换，若评分结果一致，则标记为稳定，否则标记为不稳定。</li><li>作者发现操作交换能够有效减少由于位置偏差导致的错误判断。</li><li>该技术应用于多任务评估中，尤其是在复杂生成任务的排序中表现突出。</li><li>MT-Bench为后续的LLM评估技术提供了一个重要的公平性基准。</li></ul><h6 id="2-Starling【Zhu-et-al-2024a】："><a href="#2-Starling【Zhu-et-al-2024a】：" class="headerlink" title="2. Starling【Zhu et al., 2024a】："></a>2. <strong>Starling</strong>【Zhu et al., 2024a】：</h6><ul><li>提出一种类似链式推理（Chain-of-Thought, CoT）的提示技术，通过全面评估所有候选项的两两关系，再总结为最终排序。</li><li>创新点在于强制模型生成所有可能的对比结果，确保评估全面且无偏。</li><li>作者还设计了一种交叉验证机制，进一步提高评估稳定性。</li><li>实验显示，这种方法显著减少了位置偏差带来的误差，特别是在排序任务中表现优异。</li><li>Starling验证了链式思维结合操作交换技术的潜力，尤其在复杂对比任务中的效果显著。</li></ul><hr><h4 id="4-2-2-规则增强（Rule-Augmentation）"><a href="#4-2-2-规则增强（Rule-Augmentation）" class="headerlink" title="4.2.2 规则增强（Rule Augmentation）"></a>4.2.2 规则增强（Rule Augmentation）</h4><h5 id="概述-3"><a href="#概述-3" class="headerlink" title="概述"></a>概述</h5><p>规则增强技术通过在提示中嵌入明确的原则、标准或参考内容，使模型能够更加系统地评估任务，从而提升评估的准确性和一致性。</p><h6 id="1-Constitutional-AI【Bai-et-al-2022】："><a href="#1-Constitutional-AI【Bai-et-al-2022】：" class="headerlink" title="1. Constitutional AI【Bai et al., 2022】："></a>1. <strong>Constitutional AI</strong>【Bai et al., 2022】：</h6><ul><li>本研究引入了“原则驱动”的规则增强方法，利用帮助性、无害性和诚实性等标准指导模型评估。</li><li>创新点在于为每个评估维度定义详细的评分标准，并通过原则约束生成内容。</li><li>作者采用多层提示设计，使LLM能够逐步推理并给出最终评估。</li><li>实验表明，这种方法显著提升了模型在复杂场景中的判断一致性。</li><li>Constitutional AI成为后续研究的重要基石，为基于规则的评估技术奠定了基础。</li></ul><h6 id="2-OAIF【Guo-et-al-2024】："><a href="#2-OAIF【Guo-et-al-2024】：" class="headerlink" title="2. OAIF【Guo et al., 2024】："></a>2. <strong>OAIF</strong>【Guo et al., 2024】：</h6><ul><li>提出了在线AI反馈（Online AI Feedback, OAIF）框架，通过实时原则指导提升模型评估的灵活性。</li><li>核心创新点在于动态调整评估规则，使模型能够适应多变的任务需求。</li><li>OAIF引入了细粒度的多维评分策略，为每个候选项生成独立的评估报告。</li><li>作者验证了这种方法在实时决策中的潜力，尤其在对话和生成任务中表现突出。</li><li>OAIF展现了规则增强的实时适应能力，为实时评估任务提供了新方向。</li></ul><hr><h4 id="4-2-3-多代理协作（Multi-agent-Collaboration）"><a href="#4-2-3-多代理协作（Multi-agent-Collaboration）" class="headerlink" title="4.2.3 多代理协作（Multi-agent Collaboration）"></a>4.2.3 多代理协作（Multi-agent Collaboration）</h4><h5 id="概述-4"><a href="#概述-4" class="headerlink" title="概述"></a>概述</h5><p>多代理协作通过组合多个LLM的评估结果，减少单一模型的偏差，提高评估的准确性和鲁棒性。这种方法强调模型之间的角色分工和合作。</p><h6 id="1-Peer-Rank-PR-【Li-et-al-2023】："><a href="#1-Peer-Rank-PR-【Li-et-al-2023】：" class="headerlink" title="1. **Peer Rank (PR)**【Li et al., 2023】："></a>1. **Peer Rank (PR)**【Li et al., 2023】：</h6><ul><li>提出了同行排名算法，整合多个LLM的对比偏好生成最终排序。</li><li>创新点在于设计了“加权投票”机制，根据模型之间的评分一致性调整权重。</li><li>该研究还探讨了代理间的协作效率和鲁棒性，提出了优化协作路径的方法。</li><li>PR的实验结果显示，其生成的评估结果在排序准确性上优于传统单模型方法。</li><li>该研究为多模型协作技术奠定了理论基础，是后续研究的重要参考。</li></ul><h6 id="2-Cascaded-Selective-Evaluation【Jung-et-al-2024】："><a href="#2-Cascaded-Selective-Evaluation【Jung-et-al-2024】：" class="headerlink" title="2. Cascaded Selective Evaluation【Jung et al., 2024】："></a>2. <strong>Cascaded Selective Evaluation</strong>【Jung et al., 2024】：</h6><ul><li>设计了级联选择评估框架，首先由较弱的模型进行初步评估，仅在需要时调用更强大的模型。</li><li>创新点在于通过分级策略优化计算成本，同时确保评估结果的高质量。</li><li>作者提出了一种交叉验证机制，结合多个代理的结果生成最终判断。</li><li>研究表明，这种级联策略在复杂任务中表现出显著的资源效率提升。</li><li>Cascaded Selective Evaluation展示了多代理协作在资源有限情况下的潜力。</li></ul><hr><h4 id="4-2-4-演示（Demonstration）"><a href="#4-2-4-演示（Demonstration）" class="headerlink" title="4.2.4 演示（Demonstration）"></a>4.2.4 演示（Demonstration）</h4><h5 id="概述-5"><a href="#概述-5" class="headerlink" title="概述"></a>概述</h5><p>演示技术利用具体的示例作为提示，帮助LLM学习评估标准。这种方法通过少量高质量样例显著提高模型的评估能力。</p><h6 id="1-ALLURE【Hasanbeig-et-al-2023】："><a href="#1-ALLURE【Hasanbeig-et-al-2023】：" class="headerlink" title="1. ALLURE【Hasanbeig et al., 2023】："></a>1. <strong>ALLURE</strong>【Hasanbeig et al., 2023】：</h6><ul><li>提出了迭代演示技术，通过在提示中加入显著偏差的示例提高模型的鲁棒性。</li><li>创新点在于采用动态演示方法，逐步更新提示以适应不同的评估任务。</li><li>研究表明，这种方法在低资源场景中表现出色，尤其是在新任务的适应性上有显著提升。</li><li>作者还探讨了如何选择代表性样例以最大化演示效果。</li><li>ALLURE验证了高质量演示样例在提升评估能力方面的重要性。</li></ul><h6 id="2-ICE【Jain-et-al-2023b】："><a href="#2-ICE【Jain-et-al-2023b】：" class="headerlink" title="2. ICE【Jain et al., 2023b】："></a>2. <strong>ICE</strong>【Jain et al., 2023b】：</h6><ul><li>提出了交互式多维评估框架，通过少量上下文示例指导LLM评估。</li><li>创新点在于将评估任务分解为多个独立维度，每个维度都有针对性的示例支持。</li><li>研究表明，ICE框架显著减少了模型在多维任务中的评估偏差。</li><li>实验结果显示，其生成的评估结果在与人工评价的一致性上达到高水平。</li><li>ICE为多维度评估任务的提示设计提供了新思路。</li></ul><hr><h4 id="4-2-5-多轮交互（Multi-turn-Interaction）"><a href="#4-2-5-多轮交互（Multi-turn-Interaction）" class="headerlink" title="4.2.5 多轮交互（Multi-turn Interaction）"></a>4.2.5 多轮交互（Multi-turn Interaction）</h4><h5 id="概述-6"><a href="#概述-6" class="headerlink" title="概述"></a>概述</h5><p>多轮交互通过动态调整提示和上下文信息，为LLM提供更全面的评估依据，适用于需要多步推理的复杂任务。</p><h6 id="1-KIEval【Yu-et-al-2024】："><a href="#1-KIEval【Yu-et-al-2024】：" class="headerlink" title="1. KIEval【Yu et al., 2024】："></a>1. <strong>KIEval</strong>【Yu et al., 2024】：</h6><ul><li>提出了知识交互式评估框架，通过动态问答生成丰富的上下文信息。</li><li>创新点在于引入了“交互者”角色，模拟用户和模型之间的动态交互。</li><li>作者设计了一种鲁棒性检测机制，避免因上下文污染导致的错误评估。</li><li>研究表明，KIEval在复杂任务中的表现优于传统静态评估方法。</li><li>此框架适用于多维度评估，特别是在需要动态调整上下文的场景中。</li></ul><h6 id="2-Auto-Arena【Zhao-et-al-2024c】："><a href="#2-Auto-Arena【Zhao-et-al-2024c】：" class="headerlink" title="2. Auto-Arena【Zhao et al., 2024c】："></a>2. <strong>Auto-Arena</strong>【Zhao et al., 2024c】：</h6><ul><li>设计了一种多轮辩论框架，允许多个模型围绕特定任务进行交互讨论。</li><li>创新点在于结合多轮问答和动态评分机制，从不同角度对候选答案进行评估。</li><li>研究表明，这种方法能够揭示候选答案间的深层次差异。</li><li>作者还探讨了如何通过动态调整辩论内容提高评估效率。</li><li>Auto-Arena展示了多轮交互在复杂评估任务中的潜力。</li></ul><hr><h4 id="4-2-6-比较加速（Comparison-Acceleration）"><a href="#4-2-6-比较加速（Comparison-Acceleration）" class="headerlink" title="4.2.6 比较加速（Comparison Acceleration）"></a>4.2.6 比较加速（Comparison Acceleration）</h4><h5 id="概述-7"><a href="#概述-7" class="headerlink" title="概述"></a>概述</h5><p>比较加速技术通过优化比较流程，减少多候选排序任务的计算成本，提高评估效率。</p><h6 id="1-Ranked-Pairing【Zhai-et-al-2024】："><a href="#1-Ranked-Pairing【Zhai-et-al-2024】：" class="headerlink" title="1. Ranked Pairing【Zhai et al., 2024】："></a>1. <strong>Ranked Pairing</strong>【Zhai et al., 2024】：</h6><ul><li>提出了一种基于基线比较的排序方法，通过对所有候选项与基线进行比较确定优劣。</li><li>创新点在于避免传统两两比较的高计算开销，显著提高了评估效率。</li><li>作者还设计了一种自适应比较策略，进一步优化排序性能。</li><li>研究表明，Ranked Pairing在大规模排序任务中表现出极高的效率。</li><li>此方法特别适用于需要快速生成排序结果的场景。</li></ul><h6 id="2-Tournament-based-Comparison【Lee-et-al-2024】："><a href="#2-Tournament-based-Comparison【Lee-et-al-2024】：" class="headerlink" title="2. Tournament-based Comparison【Lee et al., 2024】："></a>2. <strong>Tournament-based Comparison</strong>【Lee et al., 2024】：</h6><ul><li>采用锦标赛式的比较方法，构建树状结构逐层筛选最佳</li></ul><p>候选。</p><ul><li>创新点在于结合拒绝采样和多轮比较，减少了低质量候选的影响。</li><li>作者探讨了不同树结构设计对评估效率和准确性的影响。</li><li>实验结果显示，该方法在多候选任务中显著提高了计算效率。</li><li>Tournament-based Comparison展示了基于结构化比较的潜在优势。</li></ul><hr><h3 id="5-应用场景"><a href="#5-应用场景" class="headerlink" title="5. 应用场景"></a>5. 应用场景</h3><h4 id="概述-8"><a href="#概述-8" class="headerlink" title="概述"></a>概述</h4><p>LLM-as-a-Judge的应用场景已从最初的生成任务评估扩展到多个领域，包括评估、对齐（Alignment）、检索和推理（Reasoning）。这一部分系统性地介绍这些应用场景，讨论每种应用的具体任务和代表性研究。</p><hr><h4 id="5-1-评估"><a href="#5-1-评估" class="headerlink" title="5.1 评估"></a>5.1 评估</h4><h5 id="概述-9"><a href="#概述-9" class="headerlink" title="概述"></a>概述</h5><p>LLM-as-a-Judge最初的核心应用是评估任务，包括开放式生成任务（如对话生成、摘要生成）、推理任务，以及其他新兴任务。通过LLM评估，能够更精准地捕捉复杂生成任务中的质量、相关性及逻辑性等维度。</p><h6 id="1-MD-Judge【Li-et-al-2024f】："><a href="#1-MD-Judge【Li-et-al-2024f】：" class="headerlink" title="1. MD-Judge【Li et al., 2024f】："></a>1. <strong>MD-Judge</strong>【Li et al., 2024f】：</h6><ul><li>提出了专门针对安全性相关问答的评估框架，用于检测LLM在生成敏感内容时的可靠性。</li><li>创新点在于设计了多维度的安全性评估标准，包括潜在伤害性、道德风险以及语言误导性。</li><li>作者通过对比多个LLM的评估能力，验证了MD-Judge框架的鲁棒性。</li><li>此框架在评估复杂场景（如恶意问题）的生成效果方面表现突出。</li><li>MD-Judge为生成模型的安全性评估提供了一个新的基准。</li></ul><h6 id="2-Chan框架【Chan-et-al-2023】："><a href="#2-Chan框架【Chan-et-al-2023】：" class="headerlink" title="2. Chan框架【Chan et al., 2023】："></a>2. <strong>Chan框架</strong>【Chan et al., 2023】：</h6><ul><li>提出了一个多代理辩论框架，通过让多个LLM角色分别生成答案并彼此评估，提升生成任务的评估质量。</li><li>创新点在于设计了角色分工机制，不同模型在辩论中扮演不同的立场，从多角度评估候选答案。</li><li>研究表明，该框架能够显著提升评估结果的细粒度和多样性。</li><li>作者还探讨了模型间的交互如何影响评估的一致性和公平性。</li><li>Chan框架在开放式文本生成任务中的应用表明，模型之间的协作能够显著改进评估质量。</li></ul><h6 id="3-ICE【Jain-et-al-2023b】："><a href="#3-ICE【Jain-et-al-2023b】：" class="headerlink" title="3. ICE【Jain et al., 2023b】："></a>3. <strong>ICE</strong>【Jain et al., 2023b】：</h6><ul><li>提出了交互式多维评估框架，通过少量上下文示例指导LLM评估。</li><li>创新点在于将评估任务分解为多个独立维度，每个维度都有针对性的示例支持。</li><li>研究表明，ICE框架显著减少了模型在多维任务中的评估偏差。</li><li>实验结果显示，其生成的评估结果在与人工评价的一致性上达到高水平。</li><li>ICE为多维度评估任务的提示设计提供了新思路。</li></ul><hr><h4 id="5-2-对齐（Alignment）"><a href="#5-2-对齐（Alignment）" class="headerlink" title="5.2 对齐（Alignment）"></a>5.2 对齐（Alignment）</h4><h5 id="概述-10"><a href="#概述-10" class="headerlink" title="概述"></a>概述</h5><p>对齐任务的目标是通过训练或微调使LLM的生成内容更符合人类的价值观和偏好。LLM-as-a-Judge被广泛用于生成对齐数据和评估对齐效果。</p><h6 id="1-Constitutional-AI【Bai-et-al-2022】：-1"><a href="#1-Constitutional-AI【Bai-et-al-2022】：-1" class="headerlink" title="1. Constitutional AI【Bai et al., 2022】："></a>1. <strong>Constitutional AI</strong>【Bai et al., 2022】：</h6><ul><li>提出了基于原则对齐的框架，通过定义帮助性、无害性和诚实性等原则，优化生成模型的输出。</li><li>创新点在于将原则融入奖励建模过程，利用LLM生成的偏好信号构建对齐数据集。</li><li>作者通过多轮实验验证了这种基于规则的对齐方法对生成质量的显著提升。</li><li>此框架适用于各种生成任务，尤其在减少有害输出方面效果显著。</li><li>Constitutional AI的成功展示了基于规则的对齐方法的潜力。</li></ul><h6 id="2-DIRECT-RLAIF【Lee-et-al-2023】："><a href="#2-DIRECT-RLAIF【Lee-et-al-2023】：" class="headerlink" title="2. DIRECT-RLAIF【Lee et al., 2023】："></a>2. <strong>DIRECT-RLAIF</strong>【Lee et al., 2023】：</h6><ul><li>提出了一种直接强化学习对齐反馈（DIRECT-RLAIF）方法，通过较大的LLM生成偏好信号指导较小模型。</li><li>核心创新点在于利用较强的LLM模型作为动态评估者，避免传统奖励模型中存在的“奖励陈旧性”问题。</li><li>作者验证了这种方法在对齐生成任务中的有效性，特别是在开放式对话中的显著改进。</li><li>DIRECT-RLAIF为更高效的对齐方法提供了理论基础。</li><li>研究结果表明，这种方法可以在较少人工干预的情况下生成符合人类偏好的内容。</li></ul><h6 id="3-OAIF【Guo-et-al-2024】："><a href="#3-OAIF【Guo-et-al-2024】：" class="headerlink" title="3. OAIF【Guo et al., 2024】："></a>3. <strong>OAIF</strong>【Guo et al., 2024】：</h6><ul><li>提出了在线AI反馈（Online AI Feedback, OAIF）框架，通过实时原则指导提升模型评估的灵活性。</li><li>核心创新点在于动态调整评估规则，使模型能够适应多变的任务需求。</li><li>OAIF引入了细粒度的多维评分策略，为每个候选项生成独立的评估报告。</li><li>作者验证了这种方法在实时决策中的潜力，尤其在对话和生成任务中表现突出。</li><li>OAIF展现了规则增强的实时适应能力，为实时评估任务提供了新方向。</li></ul><hr><h4 id="5-3-检索（Retrieval）"><a href="#5-3-检索（Retrieval）" class="headerlink" title="5.3 检索（Retrieval）"></a>5.3 检索（Retrieval）</h4><h5 id="概述-11"><a href="#概述-11" class="headerlink" title="概述"></a>概述</h5><p>在检索场景中，LLM-as-a-Judge主要用于提升文档排序的精度和检索增强生成（RAG）的效果。通过更高效的排序算法，LLM能够在传统检索和复杂生成任务中提供更高质量的相关性评估。</p><h6 id="1-Ranked-Pairing【Zhai-et-al-2024】：-1"><a href="#1-Ranked-Pairing【Zhai-et-al-2024】：-1" class="headerlink" title="1. Ranked Pairing【Zhai et al., 2024】："></a>1. <strong>Ranked Pairing</strong>【Zhai et al., 2024】：</h6><ul><li>提出了一种基于基线比较的排序方法，通过对所有候选项与基线进行比较确定优劣。</li><li>创新点在于避免传统两两比较的高计算开销，显著提高了评估效率。</li><li>作者还设计了一种自适应比较策略，进一步优化排序性能。</li><li>研究表明，Ranked Pairing在大规模排序任务中表现出极高的效率。</li><li>此方法特别适用于需要快速生成排序结果的场景。</li></ul><h6 id="2-LLM-Eval【Lin-and-Chen-2023a】："><a href="#2-LLM-Eval【Lin-and-Chen-2023a】：" class="headerlink" title="2. LLM-Eval【Lin and Chen, 2023a】："></a>2. <strong>LLM-Eval</strong>【Lin and Chen, 2023a】：</h6><ul><li>提出了在对话生成中的相关性评估框架，利用LLM替代人工标注。</li><li>创新点在于设计了结合上下文和生成内容的提示技术，确保评估更加精确。</li><li>作者通过对比实验验证了LLM在会话相关性评估中的潜力，结果与人工标注高度一致。</li><li>此框架显著减少了评估成本，同时提升了效率。</li><li>LLM-Eval在对话生成任务中的应用表明，模型在生成评估中的角色日益重要。</li></ul><h6 id="3-ToT-Tree-of-Thought-【Yao-et-al-2023a】："><a href="#3-ToT-Tree-of-Thought-【Yao-et-al-2023a】：" class="headerlink" title="3. **ToT (Tree of Thought)**【Yao et al., 2023a】："></a>3. **ToT (Tree of Thought)**【Yao et al., 2023a】：</h6><ul><li>提出了通过树状结构增强推理能力的方法，并结合LLM进行评估。</li><li>创新点在于引入了状态评估模块，通过逐步筛选最优推理路径提升检索和生成任务的精度。</li><li>研究表明，ToT框架显著提升了复杂任务的解决能力，尤其在多步推理和决策中表现优异。</li><li>作者还提出了评估路径的动态调整机制，使LLM能够更灵活地应对多样化任务。</li><li>ToT验证了结构化评估框架在复杂任务中的有效性。</li></ul><hr><h4 id="5-4-推理（Reasoning）"><a href="#5-4-推理（Reasoning）" class="headerlink" title="5.4 推理（Reasoning）"></a>5.4 推理（Reasoning）</h4><h5 id="概述-12"><a href="#概述-12" class="headerlink" title="概述"></a>概述</h5><p>推理任务的核心是评估LLM的中间推理过程和最终答案的正确性。LLM-as-a-Judge在数学推理、时间推理和复杂逻辑推理任务中展示了显著的评估能力。</p><h6 id="1-HALU-J【Wang-et-al-2024a】："><a href="#1-HALU-J【Wang-et-al-2024a】：" class="headerlink" title="1. HALU-J【Wang et al., 2024a】："></a>1. <strong>HALU-J</strong>【Wang et al., 2024a】：</h6><ul><li>提出了一种基于批评的偏好学习方法，专注于选择相关证据并生成详细批评。</li><li>创新点在于设计了多证据选择机制，提高了LLM的可靠性评估能力。</li><li>该方法通过Directed Preference Optimization（DPO）进行优化，使模型能够更准确地判断任务间的优劣。</li><li>HALU-J还结合了上下文推理，扩展了偏好学习的应用场景。</li><li>实验表明，HALU-J显著提升了复杂任务的评估准确性，尤其是在事实性和逻辑性判断上。</li></ul><h6 id="2-KIEval【Yu-et-al-2024】："><a href="#2-KIEval【Yu-et-al-2024】：" class="headerlink" title="2. KIEval【Yu et al., 2024】："></a>2. <strong>KIEval</strong>【Yu et al., 2024】：</h6><ul><li>提出了知识交互式评估框架，通过动态问答生成丰富的上下文信息。</li><li>创新点在于引入了“交互者”角色，模拟用户和模型之间的动态交互。</li><li>作者设计了一种鲁棒性检测机制，避免因上下文污染导致的错误评估。</li><li>研究表明，KIEval在复杂任务中的表现优于传统静态评估方法。</li><li>此框架适用于多维度评估，特别是在需要动态调整上下文的场景中。</li></ul><hr><h3 id="6-评估基准"><a href="#6-评估基准" class="headerlink" title="6. 评估基准"></a>6. 评估基准</h3><h4 id="概述-13"><a href="#概述-13" class="headerlink" title="概述"></a>概述</h4><p>评估基准是验证LLM-as-a-Judge能力的重要工具。本节整理并介绍当前用于不同评估维度的基准，包括有用性、无害性、可靠性等方面的具体框架和其核心思想。这些基准覆盖了从对话生成到复杂任务推理的广泛应用场景，为后续研究提供了关键数据支持。</p><hr><h5 id="6-1-综合评估基准"><a href="#6-1-综合评估基准" class="headerlink" title="6.1 综合评估基准"></a>6.1 综合评估基准</h5><h6 id="1-SORRY-Bench【Xie-et-al-2024a】："><a href="#1-SORRY-Bench【Xie-et-al-2024a】：" class="headerlink" title="1. SORRY-Bench【Xie et al., 2024a】："></a>1. <strong>SORRY-Bench</strong>【Xie et al., 2024a】：</h6><ul><li>设计了一个专注于安全性和无害性评估的综合基准，重点测试LLM对潜在有害内容的拒绝能力。</li><li>创新点在于提供了一个多模型对比框架，包括开源和专有LLM的表现分析。</li><li>基准数据集涵盖多种潜在危险场景，如政治敏感内容和虚假信息生成。</li><li>作者还引入了动态拒绝率作为衡量指标，展示了不同模型在拒绝任务中的细粒度表现。</li><li>实验表明，小型LLM经过微调后可以在安全性评估中达到与大型模型相当的水平。</li></ul><h6 id="2-HalluJudge【Luo-et-al-2024】："><a href="#2-HalluJudge【Luo-et-al-2024】：" class="headerlink" title="2. HalluJudge【Luo et al., 2024】："></a>2. <strong>HalluJudge</strong>【Luo et al., 2024】：</h6><ul><li>提出了一个专门用于对话级事实性评估的基准，涵盖大规模对话数据集。</li><li>核心创新在于设计了一种细粒度的事实性评分机制，通过引入上下文验证生成内容的准确性。</li><li>数据集中包括多种类型的事实性错误，如数据遗漏、模糊表述和直接虚假信息。</li><li>HalluJudge还整合了自动化和人工评估方法，提高了基准的覆盖面和可靠性。</li><li>实验结果表明，HalluJudge能够显著提高LLM在对话场景中的事实性检测能力。</li></ul><hr><h5 id="6-2-专用领域评估基准"><a href="#6-2-专用领域评估基准" class="headerlink" title="6.2 专用领域评估基准"></a>6.2 专用领域评估基准</h5><h6 id="1-FaithScore【Jing-et-al-2024】："><a href="#1-FaithScore【Jing-et-al-2024】：" class="headerlink" title="1. FaithScore【Jing et al., 2024】："></a>1. <strong>FaithScore</strong>【Jing et al., 2024】：</h6><ul><li>FaithScore是第一个跨模态的可靠性评估框架，适用于文本和图像生成任务。</li><li>创新点在于设计了多模态评估方法，结合语言和视觉信号来验证生成内容的真实性。</li><li>数据集覆盖了从事实描述到跨模态推理的多个任务，测试了模型的全局一致性和细节准确性。</li><li>FaithScore还引入了多阶段评分机制，逐步分解任务以提高评估的精细化程度。</li><li>实验显示，FaithScore在多模态生成任务中的评估结果与人工评分高度一致。</li></ul><h6 id="2-GEMBA【Kocmi-and-Federmann-2023】："><a href="#2-GEMBA【Kocmi-and-Federmann-2023】：" class="headerlink" title="2. GEMBA【Kocmi and Federmann, 2023】："></a>2. <strong>GEMBA</strong>【Kocmi and Federmann, 2023】：</h6><ul><li>GEMBA基准专注于机器翻译和文本摘要任务的整体质量评估。</li><li>核心创新点在于结合BLEU等传统指标和LLM生成的综合评分，提供更全面的评估结果。</li><li>数据集中包含多种语言和领域的真实文本，覆盖多样化的任务需求。</li><li>作者设计了一种动态反馈机制，允许LLM在评估过程中进行自适应调整。</li><li>GEMBA基准的引入显著推动了机器翻译和摘要任务中LLM-as-a-Judge的应用。</li></ul><h6 id="3-Just-Eval【Lin-et-al-2023】："><a href="#3-Just-Eval【Lin-et-al-2023】：" class="headerlink" title="3. Just-Eval【Lin et al., 2023】："></a>3. <strong>Just-Eval</strong>【Lin et al., 2023】：</h6><ul><li>提出了一个基于生成内容有用性和无害性的综合基准，适用于广泛的开放式任务。</li><li>创新点在于为不同任务设计了定制化的评估标准，并结合多维评分系统生成最终评价。</li><li>数据集中涵盖了对话、问答和复杂推理等任务，验证了基准的通用性。</li><li>作者还分析了模型在不同任务和领域上的表现，提供了详细的对比结果。</li><li>Just-Eval的应用表明，评估框架需要结合任务特点进行优化，才能最大化评估的准确性。</li></ul><hr><h4 id="6-3-动态评估基准"><a href="#6-3-动态评估基准" class="headerlink" title="6.3 动态评估基准"></a>6.3 动态评估基准</h4><h6 id="1-RevisEval【Zhang-et-al-2024e】："><a href="#1-RevisEval【Zhang-et-al-2024e】：" class="headerlink" title="1. RevisEval【Zhang et al., 2024e】："></a>1. <strong>RevisEval</strong>【Zhang et al., 2024e】：</h6><ul><li>RevisEval通过引入动态自我修正机制，让LLM在生成评估之前对输出进行多次调整。</li><li>核心创新在于结合LLM的自我纠错能力，将最终输出用于多维度评估。</li><li>数据集中覆盖了对话生成、摘要和复杂推理任务，验证了基准的动态适应能力。</li><li>RevisEval引入了多轮反馈机制，允许模型在评估过程中迭代改进。</li><li>实验结果表明，动态评估能够显著提升复杂任务中评估的精确性和稳定性。</li></ul><h6 id="2-Meta-ranking【Liu-et-al-2024c】："><a href="#2-Meta-ranking【Liu-et-al-2024c】：" class="headerlink" title="2. Meta-ranking【Liu et al., 2024c】："></a>2. <strong>Meta-ranking</strong>【Liu et al., 2024c】：</h6><ul><li>Meta-ranking框架通过弱模型生成初步排序，再由强模型进行最终评估。</li><li>创新点在于使用多阶段的排名方法，提高评估效率并降低计算开销。</li><li>数据集中包含了多种任务类型，并通过实验验证了Meta-ranking的通用性。</li><li>该框架特别适用于大规模排序任务，显著减少了评估时间。</li><li>Meta-ranking展示了弱模型和强模型协作评估的潜力，是多模型评估的新方向。</li></ul><hr><h3 id="7-挑战与未来方向"><a href="#7-挑战与未来方向" class="headerlink" title="7. 挑战与未来方向"></a>7. 挑战与未来方向</h3><h4 id="概述-14"><a href="#概述-14" class="headerlink" title="概述"></a>概述</h4><p>尽管LLM-as-a-Judge在评估任务中展现了强大能力，但依然面临着多方面的挑战。主要问题包括评估偏差与脆弱性、动态与复杂任务中的适应性，以及人机协同评估的潜力。本节探讨这些挑战并提出未来的研究方向。</p><hr><h5 id="7-1-偏差与脆弱性"><a href="#7-1-偏差与脆弱性" class="headerlink" title="7.1 偏差与脆弱性"></a>7.1 偏差与脆弱性</h5><h6 id="1-OffsetBias【Park-et-al-2024】："><a href="#1-OffsetBias【Park-et-al-2024】：" class="headerlink" title="1. OffsetBias【Park et al., 2024】："></a>1. <strong>OffsetBias</strong>【Park et al., 2024】：</h6><ul><li>OffsetBias通过设计一个去偏优化框架，减少LLM在评估任务中的位置偏差和内容偏见。</li><li>创新点在于使用合成数据生成“坏”样本，通过训练模型识别并修正偏差。</li><li>作者提出了一种多维度的去偏学习机制，确保评估在不同场景下的一致性。</li><li>研究表明，OffsetBias能够显著降低模型在生成任务中的不公平表现。</li><li>此方法为减少LLM评估中的偏差问题提供了重要方向。</li></ul><h6 id="2-SORRY-Bench【Xie-et-al-2024a】："><a href="#2-SORRY-Bench【Xie-et-al-2024a】：" class="headerlink" title="2. SORRY-Bench【Xie et al., 2024a】："></a>2. <strong>SORRY-Bench</strong>【Xie et al., 2024a】：</h6><ul><li>进一步研究了模型在拒绝有害内容时可能出现的误拒绝问题。</li><li>创新点在于结合动态评分机制和拒绝数据集，分析模型在多种任务中的拒绝倾向。</li><li>作者指出，小型模型在特定场景中可能比大型模型更高效。</li><li>实验结果表明，SORRY-Bench能够帮助识别并减轻评估偏差。</li><li>此基准成为探讨评估脆弱性的一个重要工具。</li></ul><hr><h5 id="7-2-动态与复杂评估"><a href="#7-2-动态与复杂评估" class="headerlink" title="7.2 动态与复杂评估"></a>7.2 动态与复杂评估</h5><h6 id="1-Tree-of-Thought-ToT-【Yao-et-al-2023a】："><a href="#1-Tree-of-Thought-ToT-【Yao-et-al-2023a】：" class="headerlink" title="1. **Tree of Thought (ToT)**【Yao et al., 2023a】："></a>1. **Tree of Thought (ToT)**【Yao et al., 2023a】：</h6><ul><li>ToT通过树状结构优化复杂任务的多步推理和评估。</li><li>创新点在于结合动态状态评估机制，使评估更加适应复杂多变的任务需求。</li><li>数据集中覆盖了需要多步推理的复杂任务，如问答和决策优化。</li><li>实验表明，ToT框架显著提升了复杂任务的解决能力和评估准确性。</li><li>该研究为动态评估提供了新的理论和实践支持。</li></ul><h6 id="2-RAIN【Li-et-al-2024】："><a href="#2-RAIN【Li-et-al-2024】：" class="headerlink" title="2. RAIN【Li et al., 2024】："></a>2. <strong>RAIN</strong>【Li et al., 2024】：</h6><ul><li>RAIN提出了可回溯的自回归推理机制，让LLM能够在评估过程中动态修正错误。</li><li>创新点在于结合自我评估和多轮推理机制，确保最终输出的高质量。</li><li>作者还设计了一种动态调整机制，使模型能够适应不同任务的变化。</li><li>实验显示，RAIN在复杂任务中的评估能力优于传统静态方法。</li><li>此框架展示了动态评估在复杂场景中的潜力。</li></ul><hr><h5 id="7-3-自我评估与人机协同"><a href="#7-3-自我评估与人机协同" class="headerlink" title="7.3 自我评估与人机协同"></a>7.3 自我评估与人机协同</h5><h6 id="1-Self-Taught-Evaluators【Wang-et-al-2024f】："><a href="#1-Self-Taught-Evaluators【Wang-et-al-2024f】：" class="headerlink" title="1. Self-Taught Evaluators【Wang et al., 2024f】："></a>1. <strong>Self-Taught Evaluators</strong>【Wang et al., 2024f】：</h6><ul><li>提出了一种自我学习框架，模型通过生成低质量数据对自身进行动态优化。</li><li>创新点在于引入了一种动态评估机制，让模型能够逐步提升自身评估能力。</li><li>数据集中包括了多种类型的任务，为自我评估提供了广泛支持。</li><li>Self-Taught Evaluators展示了模型在无需人工干预情况下的自我提升能力。</li><li>此框架为自动化评估任务提供了新思路。</li></ul><h6 id="2-Meta-Rewarding【Wu-et-al-2024】："><a href="#2-Meta-Rewarding【Wu-et-al-2024】：" class="headerlink" title="2. Meta-Rewarding【Wu et al., 2024】："></a>2. <strong>Meta-Rewarding</strong>【Wu et al., 2024】：</h6><ul><li>Meta-Rewarding通过将LLM的自评估信号作为偏好数据，用于进一步优化模型。</li><li>创新点在于结合策略模型自我反馈，增强模型的自适应能力。</li><li>作者还探讨了如何动态调整评估策略以提高鲁棒性。</li><li>实验表明，Meta-Rewarding能够显著提升复杂任务中的评估效果。</li><li>该研究展示了人机协同评估的潜在优势。</li></ul><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;hr&gt;
&lt;h1 id=&quot;基于生成的大语言模型（LLM）评估：从生成到判断的机遇与挑战&quot;&gt;&lt;a href=&quot;#基于生成的大语言模型（LLM）评估：从生成到判断的机遇与挑战&quot; class=&quot;headerlink&quot; title=&quot;基于生成的大语言模型（LLM）评估：从生成到判断的机遇</summary>
      
    
    
    
    <category term="NLP Insights" scheme="https://chenhuiyu.github.io/categories/NLP-Insights/"/>
    
    
    <category term="Onnx" scheme="https://chenhuiyu.github.io/tags/Onnx/"/>
    
    <category term="Deployment" scheme="https://chenhuiyu.github.io/tags/Deployment/"/>
    
  </entry>
  
  <entry>
    <title>Reflections on Identity and Subjectivity</title>
    <link href="https://chenhuiyu.github.io/2024/12/03/Life%20Reflections/Reflections%20on%20Identity%20and%20Subjectivity/"/>
    <id>https://chenhuiyu.github.io/2024/12/03/Life%20Reflections/Reflections%20on%20Identity%20and%20Subjectivity/</id>
    <published>2024-12-03T06:11:06.000Z</published>
    <updated>2024-12-03T06:12:31.833Z</updated>
    
    <content type="html"><![CDATA[<h1 id="PR-Application-Rejected-Reflections-on-Identity-and-Subjectivity"><a href="#PR-Application-Rejected-Reflections-on-Identity-and-Subjectivity" class="headerlink" title="PR Application Rejected: Reflections on Identity and Subjectivity"></a>PR Application Rejected: Reflections on Identity and Subjectivity</h1><p>When I received the news of my PR application being rejected, after a brief moment of shock, what arose within me was not merely frustration but a peculiar sense of “existential dilemma.” On the surface, it seemed like just an administrative outcome, yet it profoundly mirrored the multiple tensions between the structure of contemporary global mobility and the construction of subjectivity.</p><ul><li>Amid the tension between globalization and national sovereignty, is it even possible to affirm an individual’s identity?</li><li>Does the rejection of a PR application symbolically exclude an individual from a collective sense of belonging?</li></ul><hr><h2 id="PR-Application-From-the-Fantasy-of-Rights-to-the-Maze-of-Identity"><a href="#PR-Application-From-the-Fantasy-of-Rights-to-the-Maze-of-Identity" class="headerlink" title="PR Application: From the Fantasy of Rights to the Maze of Identity"></a>PR Application: From the Fantasy of Rights to the Maze of Identity</h2><p>Within the theoretical framework of Anthony Giddens’ <em>Modernity and Self-Identity</em>, applying for PR is not merely a pursuit of residency rights but a symbolic quest for identity stability and future possibilities. However, in the context of globalization, this pursuit often falls into what Derrida describes as the structure of <em>différance</em>: the realization of rights is perpetually deferred, and the confirmation of identity remains suspended.</p><p>In this context, rejection is tantamount to a form of <strong>symbolic violence</strong>. It not only disrupts my plans for the future but also shatters the illusion of subjectivity I held within this domain.</p><hr><h2 id="Subjectivity-vs-Institutional-Discipline"><a href="#Subjectivity-vs-Institutional-Discipline" class="headerlink" title="Subjectivity vs. Institutional Discipline"></a>Subjectivity vs. Institutional Discipline</h2><p>Bourdieu’s field theory reveals the distribution of power in social practices, and the practice of PR applications is a concrete field where power disciplines individuals. Rejection is not merely an administrative outcome but an invisible disciplining of the subject, hinting at the imbalance of power between individuals and institutions in the era of platform capitalism.</p><p>Through Foucault’s lens of discipline, this process not only constrains individuals’ <strong>physical mobility</strong> but also profoundly affects the <strong>emotional and mental freedom</strong> of individuals.</p><hr><h2 id="From-Loss-to-Reflection"><a href="#From-Loss-to-Reflection" class="headerlink" title="From Loss to Reflection"></a>From Loss to Reflection</h2><p>In a sense, rejection is not an end but an opportunity for <strong>reconstruction</strong>. Bauman’s concept of <em>liquid modernity</em> might help me interpret this failure: in a constantly fluid world, fixed identities and stable senses of belonging are scarce resources. Perhaps I need to redefine my position amid this loss and find my own meaning in the fragments of grand narratives.</p><p>As Žižek puts it: <strong>“True freedom is not about getting what you want but about confronting the trauma of reality.”</strong> The failure of my PR application may not be the end of identity but a challenge to how I reconstruct subjectivity in the face of uncertainty.</p><p>Thus, this is not an ending but a dialectical transformation: in the moment of shattered stability, perhaps lies the beginning of transcending grand narratives and rediscovering the meaning of one’s existence.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;PR-Application-Rejected-Reflections-on-Identity-and-Subjectivity&quot;&gt;&lt;a href=&quot;#PR-Application-Rejected-Reflections-on-Identity-and-Subj</summary>
      
    
    
    
    <category term="Life Reflections" scheme="https://chenhuiyu.github.io/categories/Life-Reflections/"/>
    
    
    <category term="Living in Singapore" scheme="https://chenhuiyu.github.io/tags/Living-in-Singapore/"/>
    
  </entry>
  
  <entry>
    <title>身份与主体性的反思</title>
    <link href="https://chenhuiyu.github.io/2024/12/03/Life%20Reflections/%E8%BA%AB%E4%BB%BD%E4%B8%8E%E4%B8%BB%E4%BD%93%E6%80%A7%E7%9A%84%E5%8F%8D%E6%80%9D/"/>
    <id>https://chenhuiyu.github.io/2024/12/03/Life%20Reflections/%E8%BA%AB%E4%BB%BD%E4%B8%8E%E4%B8%BB%E4%BD%93%E6%80%A7%E7%9A%84%E5%8F%8D%E6%80%9D/</id>
    <published>2024-12-03T06:11:06.000Z</published>
    <updated>2024-12-03T06:11:18.797Z</updated>
    
    <content type="html"><![CDATA[<h1 id="永居申请被拒：身份与主体性的反思"><a href="#永居申请被拒：身份与主体性的反思" class="headerlink" title="永居申请被拒：身份与主体性的反思"></a>永居申请被拒：身份与主体性的反思</h1><p>当我接到永居申请被拒的消息时，短暂的愣神之后，内心涌动的却并非单纯的挫败，而是一种奇异的“生存论困境”感。表面上，这似乎只是一次行政结果的体现，但其背后却深刻折射了当代全球流动性结构与主体性建构之间的多重张力。</p><ul><li>在全球化与国家主权的张力下，个体身份的确认究竟是否可能？</li><li>当永居申请被拒时，是否意味着个体被象征性地排除在某种集体意义之外？</li></ul><hr><h2 id="永居申请：从权利幻想到身份迷宫"><a href="#永居申请：从权利幻想到身份迷宫" class="headerlink" title="永居申请：从权利幻想到身份迷宫"></a>永居申请：从权利幻想到身份迷宫</h2><p>在吉登斯的“现代性与自我认同”理论框架下，永居申请不仅是一种居留权的争取，更是一种对身份稳定性与未来可能性的符号化追求。然而，在全球化语境下，这种追求往往陷入德里达所描述的“延异”结构：权利的实现总是被推迟，身份的确认总是悬置。</p><p>在此情境中，申请被拒的结果无异于一种<strong>符号暴力</strong>。它不仅断裂了我对未来的规划，也撕裂了我在这一场域中的主体性幻象。</p><hr><h2 id="主体性与制度规训的对抗"><a href="#主体性与制度规训的对抗" class="headerlink" title="主体性与制度规训的对抗"></a>主体性与制度规训的对抗</h2><p>布尔迪厄的场域理论揭示了权力在社会实践中的分布方式，而永居申请这一制度实践正是权力规训个体的具体化场域。拒绝不仅是一种行政结果，更是一种对主体的隐形规训，暗示了平台资本主义时代个体与制度之间的权力失衡。</p><p>福柯的规训视角让我们看到，这一过程不仅限制了个体的<strong>物理流动性</strong>，也深刻影响了<strong>情感与精神的自由流动</strong>。</p><hr><h2 id="从失落到反思"><a href="#从失落到反思" class="headerlink" title="从失落到反思"></a>从失落到反思</h2><p>从某种意义上说，被拒并非一种终结，而是一种<strong>重构的契机</strong>。鲍曼提出的“液态现代性”或许能帮助我理解这次失败：在一个不断流动的世界中，固定的身份和稳定的归属感本就是稀缺资源。或许，我需要在失落中重新定义自己的位置，从宏大叙事的破碎中找到属于自己的意义。</p><p>正如齐泽克所言：<strong>“真正的自由不是得到你想要的，而是面对现实的创伤。”</strong> 永居申请的失败也许不是身份的终结，而是对我如何在不确定性中重新构建主体性的终极挑战。</p><p>因此，这并非终结，而是一次辩证的转化：在稳定性破碎的瞬间，或许恰是我们超越宏大叙事、重新发现自我存在意义的开端。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;永居申请被拒：身份与主体性的反思&quot;&gt;&lt;a href=&quot;#永居申请被拒：身份与主体性的反思&quot; class=&quot;headerlink&quot; title=&quot;永居申请被拒：身份与主体性的反思&quot;&gt;&lt;/a&gt;永居申请被拒：身份与主体性的反思&lt;/h1&gt;&lt;p&gt;当我接到永居申请被拒的消息时</summary>
      
    
    
    
    <category term="Life Reflections" scheme="https://chenhuiyu.github.io/categories/Life-Reflections/"/>
    
    
    <category term="坡岛生活指北" scheme="https://chenhuiyu.github.io/tags/%E5%9D%A1%E5%B2%9B%E7%94%9F%E6%B4%BB%E6%8C%87%E5%8C%97/"/>
    
  </entry>
  
  <entry>
    <title>【Leetcode Python题解】「1346. Check If N and Its Double Exist」</title>
    <link href="https://chenhuiyu.github.io/2024/12/02/Code%20Chronicles/Leetcode%20Python%E9%A2%98%E8%A7%A3%E3%80%91%E3%80%8C1346.%20Check%20If%20N%20and%20Its%20Double%20Exist%E3%80%8D/"/>
    <id>https://chenhuiyu.github.io/2024/12/02/Code%20Chronicles/Leetcode%20Python%E9%A2%98%E8%A7%A3%E3%80%91%E3%80%8C1346.%20Check%20If%20N%20and%20Its%20Double%20Exist%E3%80%8D/</id>
    <published>2024-12-01T16:00:00.000Z</published>
    <updated>2024-12-02T02:29:45.500Z</updated>
    
    <content type="html"><![CDATA[<h1 id="【Leetcode-Python题解】「1346-Check-If-N-and-Its-Double-Exist」"><a href="#【Leetcode-Python题解】「1346-Check-If-N-and-Its-Double-Exist」" class="headerlink" title="【Leetcode Python题解】「1346. Check If N and Its Double Exist」"></a>【Leetcode Python题解】「1346. Check If N and Its Double Exist」</h1><h2 id="题目：1346-Check-If-N-and-Its-Double-Exist"><a href="#题目：1346-Check-If-N-and-Its-Double-Exist" class="headerlink" title="题目：1346. Check If N and Its Double Exist"></a>题目：<a href="https://leetcode.com/problems/check-if-n-and-its-double-exist/description/?envType=daily-question&amp;envId=2024-12-01">1346. Check If N and Its Double Exist</a></h2><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><p>给定一个整数数组 <code>arr</code>，检查是否存在两个不同的索引 <code>i</code> 和 <code>j</code>，满足：</p><ul><li><code>i != j</code></li><li><code>0 &lt;= i, j &lt; arr.length</code></li><li><code>arr[i] == 2 * arr[j]</code></li></ul><h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><p><strong>示例 1:</strong></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">输入：arr = [10,2,5,3]</span><br><span class="line">输出：true</span><br><span class="line">解释：对于 i = 0 和 j = 2，arr[i] = 10 等于 2 * 5 = 2 * arr[j]</span><br></pre></td></tr></tbody></table></figure><p><strong>示例 2:</strong></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">输入：arr = [3,1,7,11]</span><br><span class="line">输出：false</span><br><span class="line">解释：不存在满足条件的 i 和 j。</span><br></pre></td></tr></tbody></table></figure><h3 id="约束条件"><a href="#约束条件" class="headerlink" title="约束条件"></a>约束条件</h3><ul><li><code>2 &lt;= arr.length &lt;= 500</code></li><li><code>-10³ &lt;= arr[i] &lt;= 10³</code></li></ul><h2 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h2><p>这道题可以用多种方法解决，我们来分析两种主要的解法：暴力解法和哈希表解法。</p><h3 id="1-暴力解法"><a href="#1-暴力解法" class="headerlink" title="1. 暴力解法"></a>1. 暴力解法</h3><p>最直观的解法是使用两层循环，遍历所有可能的数对。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">checkIfExist</span>(<span class="params">arr</span>):</span><br><span class="line">    n = <span class="built_in">len</span>(arr)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            <span class="keyword">if</span> i != j <span class="keyword">and</span> arr[i] == <span class="number">2</span> * arr[j]:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></tbody></table></figure><p><strong>复杂度分析：</strong></p><ul><li>时间复杂度：O(n²)，其中 n 是数组长度</li><li>空间复杂度：O(1)，只使用了常数额外空间</li></ul><h3 id="2-哈希表解法"><a href="#2-哈希表解法" class="headerlink" title="2. 哈希表解法"></a>2. 哈希表解法</h3><p>使用哈希表可以显著优化时间复杂度。我们只需要一次遍历数组，同时用哈希表记录已经遇到的数字。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">checkIfExist</span>(<span class="params">arr</span>):</span><br><span class="line">    seen = <span class="built_in">set</span>()</span><br><span class="line">    <span class="keyword">for</span> num <span class="keyword">in</span> arr:</span><br><span class="line">        <span class="keyword">if</span> num * <span class="number">2</span> <span class="keyword">in</span> seen <span class="keyword">or</span> (num % <span class="number">2</span> == <span class="number">0</span> <span class="keyword">and</span> num // <span class="number">2</span> <span class="keyword">in</span> seen):</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        seen.add(num)</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></tbody></table></figure><p><strong>复杂度分析：</strong></p><ul><li>时间复杂度：O(n)，只需要遍历一次数组</li><li>空间复杂度：O(n)，需要额外的哈希表空间</li></ul><h3 id="代码优化案例"><a href="#代码优化案例" class="headerlink" title="代码优化案例"></a>代码优化案例</h3><p>让我们看一个初始版本的代码，以及如何优化它：</p><p><strong>原始版本：</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">checkIfExist</span>(<span class="params">self, arr: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        hashmap = {}</span><br><span class="line">        <span class="keyword">for</span> i, item <span class="keyword">in</span> <span class="built_in">enumerate</span>(arr):</span><br><span class="line">            hashmap[i] = item</span><br><span class="line">            <span class="keyword">if</span> item * <span class="number">2</span> <span class="keyword">in</span> hashmap.values():</span><br><span class="line">                j = <span class="built_in">next</span>(k <span class="keyword">for</span> k, v <span class="keyword">in</span> hashmap.items() <span class="keyword">if</span> v == item * <span class="number">2</span>)</span><br><span class="line">                <span class="keyword">if</span> i != j:</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">            <span class="keyword">if</span> item//<span class="number">2</span> <span class="keyword">in</span> hashmap.values() <span class="keyword">and</span> item%<span class="number">2</span>==<span class="number">0</span>:</span><br><span class="line">                j = <span class="built_in">next</span>(k <span class="keyword">for</span> k, v <span class="keyword">in</span> hashmap.items() <span class="keyword">if</span> v == item//<span class="number">2</span>)</span><br><span class="line">                <span class="keyword">if</span> i != j:</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></tbody></table></figure><p><strong>优化版本：</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">checkIfExist</span>(<span class="params">self, arr: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        seen = <span class="built_in">set</span>()</span><br><span class="line">        <span class="keyword">for</span> num <span class="keyword">in</span> arr:</span><br><span class="line">            <span class="keyword">if</span> num * <span class="number">2</span> <span class="keyword">in</span> seen <span class="keyword">or</span> (num % <span class="number">2</span> == <span class="number">0</span> <span class="keyword">and</span> num // <span class="number">2</span> <span class="keyword">in</span> seen):</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">            seen.add(num)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></tbody></table></figure><h3 id="优化要点"><a href="#优化要点" class="headerlink" title="优化要点"></a>优化要点</h3><ol><li><p><strong>数据结构选择</strong></p><ul><li>使用集合(set)替代字典(dict)</li><li>不需要存储索引信息，只关注值的存在性</li></ul></li><li><p><strong>代码简化</strong></p><ul><li>合并重复的检查逻辑</li><li>移除不必要的变量和计算</li><li>使用更简洁的条件判断</li></ul></li><li><p><strong>性能提升</strong></p><ul><li>避免使用 <code>hashmap.values()</code> 遍历</li><li>使用集合的 O(1) 查找特性</li><li>减少重复计算</li></ul></li></ol><h2 id="关键注意点"><a href="#关键注意点" class="headerlink" title="关键注意点"></a>关键注意点</h2><ol><li><p><strong>边界情况处理</strong></p><ul><li>考虑数组中有 0 的情况（0 的两倍仍然是 0）</li><li>注意负数的处理</li><li>确保不使用同一个索引（i != j）</li></ul></li><li><p><strong>数值检查</strong></p><ul><li>需要同时检查一个数的两倍和一半</li><li>检查一半时要确保数字是偶数</li></ul></li><li><p><strong>性能优化</strong></p><ul><li>使用恰当的数据结构（集合）</li><li>避免不必要的计算和遍历</li></ul></li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这道题展示了如何通过选择适当的数据结构和优化代码逻辑来提升算法的性能。从初始的暴力解法到使用哈希表，再到代码的优化，每一步都带来了显著的改进。最终的解决方案不仅运行效率高，而且代码简洁易懂。</p><p>关键是要理解：</p><ol><li>暴力解法虽然直观，但效率低下</li><li>哈希表提供了最优的时空权衡</li><li>代码优化不仅是为了效率，也是为了可读性和可维护性</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;【Leetcode-Python题解】「1346-Check-If-N-and-Its-Double-Exist」&quot;&gt;&lt;a href=&quot;#【Leetcode-Python题解】「1346-Check-If-N-and-Its-Double-Exist」&quot; clas</summary>
      
    
    
    
    <category term="Code Chronicles" scheme="https://chenhuiyu.github.io/categories/Code-Chronicles/"/>
    
    
    <category term="Python" scheme="https://chenhuiyu.github.io/tags/Python/"/>
    
    <category term="Leetcode" scheme="https://chenhuiyu.github.io/tags/Leetcode/"/>
    
    <category term="每日一题" scheme="https://chenhuiyu.github.io/tags/%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>【Leetcode Python题解】「2097. Valid Arrangement of Pairs」</title>
    <link href="https://chenhuiyu.github.io/2024/12/01/Code%20Chronicles/%E3%80%90Leetcode%20Python%E9%A2%98%E8%A7%A3%E3%80%91%E3%80%8C2097.%20Valid%20Arrangement%20of%20Pairs%E3%80%8D/"/>
    <id>https://chenhuiyu.github.io/2024/12/01/Code%20Chronicles/%E3%80%90Leetcode%20Python%E9%A2%98%E8%A7%A3%E3%80%91%E3%80%8C2097.%20Valid%20Arrangement%20of%20Pairs%E3%80%8D/</id>
    <published>2024-11-30T19:00:00.000Z</published>
    <updated>2024-11-30T19:02:40.251Z</updated>
    
    <content type="html"><![CDATA[<h1 id="【Leetcode-Python题解】「2097-Valid-Arrangement-of-Pairs」"><a href="#【Leetcode-Python题解】「2097-Valid-Arrangement-of-Pairs」" class="headerlink" title="【Leetcode Python题解】「2097. Valid Arrangement of Pairs」"></a>【Leetcode Python题解】「2097. Valid Arrangement of Pairs」</h1><h2 id="在这篇技术博客中，我们将深入解析-LeetCode-的第-2097-题-——-Valid-Arrangement-of-Pairs，并全面介绍如何从题意理解、图论建模到算法实现逐步解决问题。题目：2097-Valid-Arrangement-of-Pairs"><a href="#在这篇技术博客中，我们将深入解析-LeetCode-的第-2097-题-——-Valid-Arrangement-of-Pairs，并全面介绍如何从题意理解、图论建模到算法实现逐步解决问题。题目：2097-Valid-Arrangement-of-Pairs" class="headerlink" title="在这篇技术博客中，我们将深入解析 LeetCode 的第 2097 题 —— Valid Arrangement of Pairs，并全面介绍如何从题意理解、图论建模到算法实现逐步解决问题。题目：2097. Valid Arrangement of Pairs"></a>在这篇技术博客中，我们将深入解析 LeetCode 的第 2097 题 —— <em>Valid Arrangement of Pairs</em>，并全面介绍如何从题意理解、图论建模到算法实现逐步解决问题。<br>题目：<a href="https://leetcode.com/problems/valid-arrangement-of-pairs/description/">2097. Valid Arrangement of Pairs</a></h2><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>给定一个二维数组 <code>pairs</code>，其中 <code>pairs[i] = [start, end]</code>，我们需要重新排列这些数字对，使得相邻的两个数字对 <code>[start1, end1]</code> 和 <code>[start2, end2]</code> 满足以下条件：</p><ul><li><code>end1 == start2</code>。</li></ul><p>输入数据保证一定存在这样一种合法的排列方式。</p><h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><h4 id="示例-1"><a href="#示例-1" class="headerlink" title="示例 1"></a>示例 1</h4><p><strong>输入：</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pairs = [[<span class="number">5</span>,<span class="number">1</span>],[<span class="number">4</span>,<span class="number">5</span>],[<span class="number">11</span>,<span class="number">9</span>],[<span class="number">9</span>,<span class="number">4</span>]]</span><br></pre></td></tr></tbody></table></figure><p><strong>输出：</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">11</span>,<span class="number">9</span>],[<span class="number">9</span>,<span class="number">4</span>],[<span class="number">4</span>,<span class="number">5</span>],[<span class="number">5</span>,<span class="number">1</span>]]</span><br></pre></td></tr></tbody></table></figure><p><strong>解释：</strong><br>排列后满足条件：</p><ul><li><code>end0 = 9 == 9 = start1</code></li><li><code>end1 = 4 == 4 = start2</code></li><li><code>end2 = 5 == 5 = start3</code></li></ul><h4 id="示例-2"><a href="#示例-2" class="headerlink" title="示例 2"></a>示例 2</h4><p><strong>输入：</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pairs = [[<span class="number">1</span>,<span class="number">3</span>],[<span class="number">3</span>,<span class="number">2</span>],[<span class="number">2</span>,<span class="number">1</span>]]</span><br></pre></td></tr></tbody></table></figure><p><strong>输出：</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">1</span>,<span class="number">3</span>],[<span class="number">3</span>,<span class="number">2</span>],[<span class="number">2</span>,<span class="number">1</span>]]</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="问题建模：欧拉路径问题"><a href="#问题建模：欧拉路径问题" class="headerlink" title="问题建模：欧拉路径问题"></a>问题建模：欧拉路径问题</h2><p>这道题的本质是一个图论中的欧拉路径问题。我们将每个 <code>pair [start, end]</code> 看作一条从 <code>start</code> 到 <code>end</code> 的有向边，并试图找到一条路径能遍历所有边且满足条件。</p><h3 id="什么是欧拉路径？"><a href="#什么是欧拉路径？" class="headerlink" title="什么是欧拉路径？"></a>什么是欧拉路径？</h3><ul><li><strong>定义</strong>：欧拉路径是一条路径，它能遍历图中每条边恰好一次。</li><li><strong>条件</strong>：<ol><li>如果图中有且仅有两个节点的入度和出度不相等，则可以存在欧拉路径。<ul><li>起点：出度比入度大 1 的节点。</li><li>终点：入度比出度大 1 的节点。</li></ul></li><li>如果所有节点的入度等于出度，则图中存在欧拉回路。</li></ol></li></ul><hr><h2 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h2><h3 id="1-图的构建"><a href="#1-图的构建" class="headerlink" title="1. 图的构建"></a>1. 图的构建</h3><p>将每个 <code>pair [start, end]</code> 建模为有向边：</p><ul><li>节点为 <code>start</code> 和 <code>end</code>。</li><li>边为从 <code>start</code> 到 <code>end</code>。</li></ul><p>同时统计每个节点的 <strong>入度</strong> 和 <strong>出度</strong>，用于后续判断起点。</p><h3 id="2-寻找起点"><a href="#2-寻找起点" class="headerlink" title="2. 寻找起点"></a>2. 寻找起点</h3><p>通过入度和出度的统计：</p><ul><li>出度 - 入度 = 1 的节点是路径的起点。</li><li>如果没有这样的节点，说明图中存在欧拉回路，可从任意节点开始。</li></ul><h3 id="3-Hierholzer-算法找路径"><a href="#3-Hierholzer-算法找路径" class="headerlink" title="3. Hierholzer 算法找路径"></a>3. Hierholzer 算法找路径</h3><p><strong>Hierholzer 算法</strong> 用于寻找欧拉路径，其核心步骤：</p><ol><li>从起点开始，任意选择一条未访问的边走。</li><li>一直走直到走到死胡同（当前节点没有出边）。</li><li>回溯过程中记录路径。</li><li>最终记录的路径需要反转才能得到正确的顺序。</li></ol><hr><h2 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a>算法实现</h2><p>下面是 Python 实现的完整代码：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">validArrangement</span>(<span class="params">pairs</span>):</span><br><span class="line">    <span class="comment"># 构建图</span></span><br><span class="line">    graph = defaultdict(<span class="built_in">list</span>)</span><br><span class="line">    in_degree = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">    out_degree = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> start, end <span class="keyword">in</span> pairs:</span><br><span class="line">        graph[start].append(end)</span><br><span class="line">        out_degree[start] += <span class="number">1</span></span><br><span class="line">        in_degree[end] += <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 寻找起点</span></span><br><span class="line">    start_node = pairs[<span class="number">0</span>][<span class="number">0</span>]  <span class="comment"># 默认起点</span></span><br><span class="line">    <span class="keyword">for</span> node <span class="keyword">in</span> graph:</span><br><span class="line">        <span class="keyword">if</span> out_degree[node] - in_degree[node] == <span class="number">1</span>:</span><br><span class="line">            start_node = node</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Hierholzer算法</span></span><br><span class="line">    path = []</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">current_node</span>):</span><br><span class="line">        <span class="keyword">while</span> graph[current_node]:</span><br><span class="line">            next_node = graph[current_node].pop()</span><br><span class="line">            dfs(next_node)</span><br><span class="line">            path.append([current_node, next_node])</span><br><span class="line">    </span><br><span class="line">    dfs(start_node)</span><br><span class="line">    <span class="keyword">return</span> path[::-<span class="number">1</span>]</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="代码解析"><a href="#代码解析" class="headerlink" title="代码解析"></a>代码解析</h2><h3 id="图的构建"><a href="#图的构建" class="headerlink" title="图的构建"></a>图的构建</h3><p>使用 <code>defaultdict</code> 来存储图的邻接表，以及统计每个节点的入度和出度：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">graph = defaultdict(<span class="built_in">list</span>)</span><br><span class="line">in_degree = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">out_degree = defaultdict(<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> start, end <span class="keyword">in</span> pairs:</span><br><span class="line">    graph[start].append(end)</span><br><span class="line">    out_degree[start] += <span class="number">1</span></span><br><span class="line">    in_degree[end] += <span class="number">1</span></span><br></pre></td></tr></tbody></table></figure><h3 id="寻找起点"><a href="#寻找起点" class="headerlink" title="寻找起点"></a>寻找起点</h3><p>根据入度和出度的统计规则：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">start_node = pairs[<span class="number">0</span>][<span class="number">0</span>]  <span class="comment"># 默认值</span></span><br><span class="line"><span class="keyword">for</span> node <span class="keyword">in</span> graph:</span><br><span class="line">    <span class="keyword">if</span> out_degree[node] - in_degree[node] == <span class="number">1</span>:</span><br><span class="line">        start_node = node</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></tbody></table></figure><h3 id="Hierholzer算法"><a href="#Hierholzer算法" class="headerlink" title="Hierholzer算法"></a>Hierholzer算法</h3><p>通过深度优先搜索（DFS）找到路径：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">path = []  <span class="comment"># 存储路径</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">current_node</span>):</span><br><span class="line">    <span class="keyword">while</span> graph[current_node]:  <span class="comment"># 当前节点还有出边</span></span><br><span class="line">        next_node = graph[current_node].pop()  <span class="comment"># 获取并删除边</span></span><br><span class="line">        dfs(next_node)</span><br><span class="line">        path.append([current_node, next_node])  <span class="comment"># 记录路径</span></span><br></pre></td></tr></tbody></table></figure><hr><h2 id="示例运行"><a href="#示例运行" class="headerlink" title="示例运行"></a>示例运行</h2><p>以 <code>pairs = [[5,1],[4,5],[11,9],[9,4]]</code> 为例：</p><h3 id="图的状态"><a href="#图的状态" class="headerlink" title="图的状态"></a>图的状态</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">11 → 9</span><br><span class="line"> 9 → 4</span><br><span class="line"> 4 → 5</span><br><span class="line"> 5 → 1</span><br></pre></td></tr></tbody></table></figure><h3 id="执行过程"><a href="#执行过程" class="headerlink" title="执行过程"></a>执行过程</h3><ol><li><p>从节点 <code>11</code> 开始，DFS 遍历：</p><ul><li>走 <code>11 → 9</code>，移除边。</li><li>走 <code>9 → 4</code>，移除边。</li><li>走 <code>4 → 5</code>，移除边。</li><li>走 <code>5 → 1</code>，移除边。</li></ul></li><li><p>回溯记录路径：</p><ul><li><code>path = [[5,1], [4,5], [9,4], [11,9]]</code></li></ul></li><li><p>反转路径得到最终结果：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">11</span>,<span class="number">9</span>], [<span class="number">9</span>,<span class="number">4</span>], [<span class="number">4</span>,<span class="number">5</span>], [<span class="number">5</span>,<span class="number">1</span>]]</span><br></pre></td></tr></tbody></table></figure></li></ol><hr><h2 id="算法复杂度"><a href="#算法复杂度" class="headerlink" title="算法复杂度"></a>算法复杂度</h2><ul><li><strong>时间复杂度</strong>：<code>O(E)</code>，其中 <code>E</code> 是边的数量。<ul><li>构建图需要 <code>O(E)</code>。</li><li>DFS 遍历每条边需要 <code>O(E)</code>。</li></ul></li><li><strong>空间复杂度</strong>：<code>O(E)</code>，用于存储图的邻接表和结果路径。</li></ul><hr><h2 id="Python-技巧补充"><a href="#Python-技巧补充" class="headerlink" title="Python 技巧补充"></a>Python 技巧补充</h2><p>最后，我们补充一些代码中用到的一些 Python 技巧。</p><h3 id="1-闭包（Closure）"><a href="#1-闭包（Closure）" class="headerlink" title="1. 闭包（Closure）"></a>1. 闭包（Closure）</h3><p><strong>闭包</strong> 是指在一个函数内部定义另一个函数时，内部函数可以访问外部函数的变量，即使外部函数已经执行完毕。 </p><p>在代码中，闭包的作用是通过内部函数访问和修改外部作用域的变量，而无需通过函数参数显式传递。</p><h4 id="示例："><a href="#示例：" class="headerlink" title="示例："></a>示例：</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">outer</span>():</span><br><span class="line">    x = <span class="number">10</span>  <span class="comment"># 外部变量</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">inner</span>():</span><br><span class="line">        <span class="keyword">nonlocal</span> x  <span class="comment"># 指定使用外部变量</span></span><br><span class="line">        x += <span class="number">1</span></span><br><span class="line">        <span class="built_in">print</span>(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> inner</span><br><span class="line"></span><br><span class="line">closure_func = outer()  <span class="comment"># 返回 inner 函数</span></span><br><span class="line">closure_func()  <span class="comment"># 输出 11</span></span><br><span class="line">closure_func()  <span class="comment"># 输出 12</span></span><br></pre></td></tr></tbody></table></figure><p>在本文的算法中，<code>dfs()</code> 函数利用闭包访问 <code>path</code> 列表，避免了通过参数显式传递路径的复杂操作。</p><h4 id="为什么不需要将-path-作为参数？"><a href="#为什么不需要将-path-作为参数？" class="headerlink" title="为什么不需要将 path 作为参数？"></a>为什么不需要将 <code>path</code> 作为参数？</h4><ul><li><strong>易读性</strong>：闭包让代码更加直观，避免传递多个参数。</li><li><strong>效率</strong>：闭包直接操作外部变量，避免在递归过程中传递和合并路径。</li></ul><h3 id="2-defaultdict-与-Counter"><a href="#2-defaultdict-与-Counter" class="headerlink" title="2. defaultdict 与 Counter"></a>2. <code>defaultdict</code> 与 <code>Counter</code></h3><p>Python 的 <code>collections</code> 模块提供了许多工具类，其中 <code>defaultdict</code> 和 <code>Counter</code> 是本题的核心工具。</p><h4 id="defaultdict"><a href="#defaultdict" class="headerlink" title="defaultdict"></a><code>defaultdict</code></h4><p><code>defaultdict</code> 是字典的一个子类，可以为不存在的键提供默认值，从而避免访问不存在键时抛出 <code>KeyError</code>。</p><h5 id="使用方式："><a href="#使用方式：" class="headerlink" title="使用方式："></a>使用方式：</h5><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line"><span class="comment"># 默认值是列表</span></span><br><span class="line">graph = defaultdict(<span class="built_in">list</span>)</span><br><span class="line">graph[<span class="number">1</span>].append(<span class="number">2</span>)</span><br><span class="line">graph[<span class="number">2</span>].append(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(graph)  <span class="comment"># 输出：{1: [2], 2: [3]}</span></span><br><span class="line"><span class="built_in">print</span>(graph[<span class="number">3</span>])  <span class="comment"># 输出：[]，不会报错</span></span><br></pre></td></tr></tbody></table></figure><p>在本文中，<code>defaultdict</code> 被用作图的邻接表，简化了图的构建和更新操作。</p><h4 id="Counter"><a href="#Counter" class="headerlink" title="Counter"></a><code>Counter</code></h4><p><code>Counter</code> 是字典的子类，用于统计元素的出现次数。它将每个元素作为键，出现次数作为值。</p><h5 id="使用方式：-1"><a href="#使用方式：-1" class="headerlink" title="使用方式："></a>使用方式：</h5><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line">counts = Counter([<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(counts)  <span class="comment"># 输出：Counter({3: 3, 2: 2, 1: 1})</span></span><br></pre></td></tr></tbody></table></figure><p>在本文中，可以通过 <code>Counter</code> 快速统计每个节点的入度和出度。</p><hr><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这道题通过将数字对建模为图的欧拉路径问题，使用 Hierholzer 算法高效地找到合法排列。这种图论问题的建模方法不仅提升了题目理解，还为类似问题提供了通用解法。</p><p>希望这篇博客能帮助你彻底掌握这道题！如果你有其他问题，欢迎交流！</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;【Leetcode-Python题解】「2097-Valid-Arrangement-of-Pairs」&quot;&gt;&lt;a href=&quot;#【Leetcode-Python题解】「2097-Valid-Arrangement-of-Pairs」&quot; class=&quot;headerl</summary>
      
    
    
    
    <category term="Code Chronicles" scheme="https://chenhuiyu.github.io/categories/Code-Chronicles/"/>
    
    
    <category term="Python" scheme="https://chenhuiyu.github.io/tags/Python/"/>
    
    <category term="Leetcode" scheme="https://chenhuiyu.github.io/tags/Leetcode/"/>
    
    <category term="每日一题" scheme="https://chenhuiyu.github.io/tags/%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>Leetcode Python Solution - 2097. Valid Arrangement of Pairs</title>
    <link href="https://chenhuiyu.github.io/2024/12/01/Code%20Chronicles/[Leetcode%20Python%20Solution]%202097.%20Valid%20Arrangement%20of%20Pairs/"/>
    <id>https://chenhuiyu.github.io/2024/12/01/Code%20Chronicles/[Leetcode%20Python%20Solution]%202097.%20Valid%20Arrangement%20of%20Pairs/</id>
    <published>2024-11-30T19:00:00.000Z</published>
    <updated>2024-12-01T16:01:10.762Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Leetcode-Python-Solution-2097-Valid-Arrangement-of-Pairs"><a href="#Leetcode-Python-Solution-2097-Valid-Arrangement-of-Pairs" class="headerlink" title="[Leetcode Python Solution] 2097. Valid Arrangement of Pairs"></a>[Leetcode Python Solution] 2097. Valid Arrangement of Pairs</h1><p>In this technical blog, we’ll dive deep into Leetcode Problem 2097 — <em>Valid Arrangement of Pairs</em>. We will break down the solution step by step, from understanding the problem, modeling it as a graph theory problem, to implementing the solution.</p><p>Problem Link: <a href="https://leetcode.com/problems/valid-arrangement-of-pairs/description/">2097. Valid Arrangement of Pairs</a></p><hr><h2 id="Problem-Description"><a href="#Problem-Description" class="headerlink" title="Problem Description"></a>Problem Description</h2><p>Given a 2D array <code>pairs</code> where <code>pairs[i] = [start, end]</code>, you need to rearrange these pairs so that for adjacent pairs <code>[start1, end1]</code> and <code>[start2, end2]</code>, the condition <code>end1 == start2</code> holds.</p><p>It is guaranteed that at least one valid arrangement exists.</p><h3 id="Examples"><a href="#Examples" class="headerlink" title="Examples"></a>Examples</h3><h4 id="Example-1"><a href="#Example-1" class="headerlink" title="Example 1"></a>Example 1</h4><p><strong>Input:</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pairs = [[<span class="number">5</span>,<span class="number">1</span>],[<span class="number">4</span>,<span class="number">5</span>],[<span class="number">11</span>,<span class="number">9</span>],[<span class="number">9</span>,<span class="number">4</span>]]</span><br></pre></td></tr></tbody></table></figure><p><strong>Output:</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">11</span>,<span class="number">9</span>],[<span class="number">9</span>,<span class="number">4</span>],[<span class="number">4</span>,<span class="number">5</span>],[<span class="number">5</span>,<span class="number">1</span>]]</span><br></pre></td></tr></tbody></table></figure><p><strong>Explanation:</strong><br>The arrangement satisfies:</p><ul><li><code>end0 = 9 == 9 = start1</code></li><li><code>end1 = 4 == 4 = start2</code></li><li><code>end2 = 5 == 5 = start3</code></li></ul><h4 id="Example-2"><a href="#Example-2" class="headerlink" title="Example 2"></a>Example 2</h4><p><strong>Input:</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pairs = [[<span class="number">1</span>,<span class="number">3</span>],[<span class="number">3</span>,<span class="number">2</span>],[<span class="number">2</span>,<span class="number">1</span>]]</span><br></pre></td></tr></tbody></table></figure><p><strong>Output:</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">1</span>,<span class="number">3</span>],[<span class="number">3</span>,<span class="number">2</span>],[<span class="number">2</span>,<span class="number">1</span>]]</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="Problem-Modeling-Eulerian-Path-Problem"><a href="#Problem-Modeling-Eulerian-Path-Problem" class="headerlink" title="Problem Modeling: Eulerian Path Problem"></a>Problem Modeling: Eulerian Path Problem</h2><p>This problem can be modeled as an <strong>Eulerian Path Problem</strong> in graph theory. Each <code>pair [start, end]</code> is treated as a directed edge from <code>start</code> to <code>end</code>, and we aim to find a path that traverses all edges while satisfying the given condition.</p><h3 id="What-is-an-Eulerian-Path"><a href="#What-is-an-Eulerian-Path" class="headerlink" title="What is an Eulerian Path?"></a>What is an Eulerian Path?</h3><ul><li><strong>Definition</strong>: An Eulerian path is a path in a graph that visits every edge exactly once.</li><li><strong>Conditions</strong>:<ol><li>An Eulerian path exists if and only if there are exactly two nodes in the graph with unbalanced in-degrees and out-degrees:<ul><li>Start node: the node where <code>out-degree - in-degree = 1</code>.</li><li>End node: the node where <code>in-degree - out-degree = 1</code>.</li></ul></li><li>If all nodes have equal in-degrees and out-degrees, the graph contains an Eulerian circuit, and the path can start at any node.</li></ol></li></ul><hr><h2 id="Solution-Approach"><a href="#Solution-Approach" class="headerlink" title="Solution Approach"></a>Solution Approach</h2><h3 id="1-Graph-Construction"><a href="#1-Graph-Construction" class="headerlink" title="1. Graph Construction"></a>1. Graph Construction</h3><p>Model each <code>pair [start, end]</code> as a directed edge:</p><ul><li>Nodes: <code>start</code> and <code>end</code>.</li><li>Edges: Directed edge from <code>start</code> to <code>end</code>.</li></ul><p>Simultaneously, calculate the <strong>in-degrees</strong> and <strong>out-degrees</strong> of each node to identify the starting node.</p><h3 id="2-Finding-the-Start-Node"><a href="#2-Finding-the-Start-Node" class="headerlink" title="2. Finding the Start Node"></a>2. Finding the Start Node</h3><p>Using the in-degree and out-degree counts:</p><ul><li>A node with <code>out-degree - in-degree = 1</code> is the start node.</li><li>If no such node exists, the graph contains an Eulerian circuit, and we can start from any node.</li></ul><h3 id="3-Using-Hierholzer’s-Algorithm-to-Find-the-Path"><a href="#3-Using-Hierholzer’s-Algorithm-to-Find-the-Path" class="headerlink" title="3. Using Hierholzer’s Algorithm to Find the Path"></a>3. Using Hierholzer’s Algorithm to Find the Path</h3><p><strong>Hierholzer’s Algorithm</strong> is used to find an Eulerian path:</p><ol><li>Start at the chosen node and follow any unvisited edge.</li><li>Continue until reaching a dead end (a node with no outgoing edges).</li><li>Backtrack and record the path.</li><li>Reverse the recorded path to get the correct order.</li></ol><hr><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h2><p>Here’s the complete Python solution:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">validArrangement</span>(<span class="params">pairs</span>):</span><br><span class="line">    <span class="comment"># Construct the graph</span></span><br><span class="line">    graph = defaultdict(<span class="built_in">list</span>)</span><br><span class="line">    in_degree = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">    out_degree = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> start, end <span class="keyword">in</span> pairs:</span><br><span class="line">        graph[start].append(end)</span><br><span class="line">        out_degree[start] += <span class="number">1</span></span><br><span class="line">        in_degree[end] += <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Find the starting node</span></span><br><span class="line">    start_node = pairs[<span class="number">0</span>][<span class="number">0</span>]  <span class="comment"># Default start node</span></span><br><span class="line">    <span class="keyword">for</span> node <span class="keyword">in</span> graph:</span><br><span class="line">        <span class="keyword">if</span> out_degree[node] - in_degree[node] == <span class="number">1</span>:</span><br><span class="line">            start_node = node</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Hierholzer's Algorithm</span></span><br><span class="line">    path = []</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">current_node</span>):</span><br><span class="line">        <span class="keyword">while</span> graph[current_node]:</span><br><span class="line">            next_node = graph[current_node].pop()</span><br><span class="line">            dfs(next_node)</span><br><span class="line">            path.append([current_node, next_node])</span><br><span class="line">    </span><br><span class="line">    dfs(start_node)</span><br><span class="line">    <span class="keyword">return</span> path[::-<span class="number">1</span>]</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="Code-Explanation"><a href="#Code-Explanation" class="headerlink" title="Code Explanation"></a>Code Explanation</h2><h3 id="Graph-Construction"><a href="#Graph-Construction" class="headerlink" title="Graph Construction"></a>Graph Construction</h3><p>Using <code>defaultdict</code> to store the adjacency list and count the in-degrees and out-degrees:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">graph = defaultdict(<span class="built_in">list</span>)</span><br><span class="line">in_degree = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">out_degree = defaultdict(<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> start, end <span class="keyword">in</span> pairs:</span><br><span class="line">    graph[start].append(end)</span><br><span class="line">    out_degree[start] += <span class="number">1</span></span><br><span class="line">    in_degree[end] += <span class="number">1</span></span><br></pre></td></tr></tbody></table></figure><h3 id="Finding-the-Start-Node"><a href="#Finding-the-Start-Node" class="headerlink" title="Finding the Start Node"></a>Finding the Start Node</h3><p>Based on the rules for Eulerian paths:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">start_node = pairs[<span class="number">0</span>][<span class="number">0</span>]  <span class="comment"># Default value</span></span><br><span class="line"><span class="keyword">for</span> node <span class="keyword">in</span> graph:</span><br><span class="line">    <span class="keyword">if</span> out_degree[node] - in_degree[node] == <span class="number">1</span>:</span><br><span class="line">        start_node = node</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></tbody></table></figure><h3 id="Hierholzer’s-Algorithm"><a href="#Hierholzer’s-Algorithm" class="headerlink" title="Hierholzer’s Algorithm"></a>Hierholzer’s Algorithm</h3><p>Using a recursive DFS to find the Eulerian path:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">path = []  <span class="comment"># To store the path</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">current_node</span>):</span><br><span class="line">    <span class="keyword">while</span> graph[current_node]:  <span class="comment"># While there are outgoing edges</span></span><br><span class="line">        next_node = graph[current_node].pop()  <span class="comment"># Remove edge</span></span><br><span class="line">        dfs(next_node)</span><br><span class="line">        path.append([current_node, next_node])  <span class="comment"># Record path</span></span><br></pre></td></tr></tbody></table></figure><hr><h2 id="Example-Execution"><a href="#Example-Execution" class="headerlink" title="Example Execution"></a>Example Execution</h2><p>For <code>pairs = [[5,1],[4,5],[11,9],[9,4]]</code>:</p><h3 id="Graph-State"><a href="#Graph-State" class="headerlink" title="Graph State"></a>Graph State</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">11 → 9</span><br><span class="line"> 9 → 4</span><br><span class="line"> 4 → 5</span><br><span class="line"> 5 → 1</span><br></pre></td></tr></tbody></table></figure><h3 id="Execution-Process"><a href="#Execution-Process" class="headerlink" title="Execution Process"></a>Execution Process</h3><ol><li><p>Start DFS from node <code>11</code>:</p><ul><li>Visit <code>11 → 9</code> and remove the edge.</li><li>Visit <code>9 → 4</code> and remove the edge.</li><li>Visit <code>4 → 5</code> and remove the edge.</li><li>Visit <code>5 → 1</code> and remove the edge.</li></ul></li><li><p>Backtrack to record the path:</p><ul><li><code>path = [[5,1], [4,5], [9,4], [11,9]]</code>.</li></ul></li><li><p>Reverse the path for the final result:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">11</span>,<span class="number">9</span>], [<span class="number">9</span>,<span class="number">4</span>], [<span class="number">4</span>,<span class="number">5</span>], [<span class="number">5</span>,<span class="number">1</span>]]</span><br></pre></td></tr></tbody></table></figure></li></ol><hr><h2 id="Time-and-Space-Complexity"><a href="#Time-and-Space-Complexity" class="headerlink" title="Time and Space Complexity"></a>Time and Space Complexity</h2><ul><li><strong>Time Complexity</strong>: <code>O(E)</code>, where <code>E</code> is the number of edges.<ul><li>Constructing the graph: <code>O(E)</code>.</li><li>DFS traversal: <code>O(E)</code>.</li></ul></li><li><strong>Space Complexity</strong>: <code>O(E)</code> for storing the adjacency list and result path.</li></ul><hr><h2 id="Python-Tips-and-Tricks"><a href="#Python-Tips-and-Tricks" class="headerlink" title="Python Tips and Tricks"></a>Python Tips and Tricks</h2><h3 id="1-Closures"><a href="#1-Closures" class="headerlink" title="1. Closures"></a>1. Closures</h3><p>A <strong>closure</strong> allows inner functions to access variables from the outer function, even after the outer function has finished executing.</p><h4 id="Example"><a href="#Example" class="headerlink" title="Example:"></a>Example:</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">outer</span>():</span><br><span class="line">    x = <span class="number">10</span>  <span class="comment"># Outer variable</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">inner</span>():</span><br><span class="line">        <span class="keyword">nonlocal</span> x  <span class="comment"># Use the outer variable</span></span><br><span class="line">        x += <span class="number">1</span></span><br><span class="line">        <span class="built_in">print</span>(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> inner</span><br><span class="line"></span><br><span class="line">closure_func = outer()  <span class="comment"># Returns the inner function</span></span><br><span class="line">closure_func()  <span class="comment"># Outputs 11</span></span><br><span class="line">closure_func()  <span class="comment"># Outputs 12</span></span><br></pre></td></tr></tbody></table></figure><p>In the solution, <code>dfs()</code> uses a closure to access and modify the <code>path</code> list without passing it explicitly.</p><hr><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>This problem showcases how to model a graph problem as an Eulerian path and use Hierholzer’s algorithm for an efficient solution. Such graph theory techniques provide a robust framework for solving similar problems.</p><p>Feel free to leave questions or share your thoughts!</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Leetcode-Python-Solution-2097-Valid-Arrangement-of-Pairs&quot;&gt;&lt;a href=&quot;#Leetcode-Python-Solution-2097-Valid-Arrangement-of-Pairs&quot; class=</summary>
      
    
    
    
    <category term="Code Chronicles" scheme="https://chenhuiyu.github.io/categories/Code-Chronicles/"/>
    
    
    <category term="Python" scheme="https://chenhuiyu.github.io/tags/Python/"/>
    
    <category term="Leetcode" scheme="https://chenhuiyu.github.io/tags/Leetcode/"/>
    
    <category term="Daily Challenge" scheme="https://chenhuiyu.github.io/tags/Daily-Challenge/"/>
    
  </entry>
  
  <entry>
    <title>Detailed Explanation of LoRA, DPO, KTO, and SFT Technologies</title>
    <link href="https://chenhuiyu.github.io/2024/10/23/NLP%20Insights/Introduction%20to%20LLM%20Training%20Terminology:%20LoRA,%20DPO,%20KTO,%20and%20SFT%20Technologies/"/>
    <id>https://chenhuiyu.github.io/2024/10/23/NLP%20Insights/Introduction%20to%20LLM%20Training%20Terminology:%20LoRA,%20DPO,%20KTO,%20and%20SFT%20Technologies/</id>
    <published>2024-10-23T08:26:29.000Z</published>
    <updated>2024-10-23T09:08:00.771Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Introduction-to-LLM-Training-Terminology-LoRA-DPO-KTO-and-SFT-Technologies"><a href="#Introduction-to-LLM-Training-Terminology-LoRA-DPO-KTO-and-SFT-Technologies" class="headerlink" title="Introduction to LLM Training Terminology:LoRA, DPO, KTO, and SFT Technologies"></a><strong>Introduction to LLM Training Terminology:LoRA, DPO, KTO, and SFT Technologies</strong></h3><p>This document provides a detailed introduction to several important techniques used in fine-tuning and optimizing large language models (such as LLAMA3), including <strong>SFT (Supervised Fine-Tuning)</strong>, <strong>LoRA (Low-Rank Adaptation)</strong>, <strong>Alignment</strong> technologies, <strong>KTO (Kahneman-Tversky Optimization)</strong>, and <strong>DPO (Direct Preference Optimization)</strong>. The document also elaborates on the principles of each technique, specific implementation methods, as well as the selection of corresponding loss functions and optimizers.</p><hr><h2 id="1-SFT-Supervised-Fine-Tuning"><a href="#1-SFT-Supervised-Fine-Tuning" class="headerlink" title="1. SFT (Supervised Fine-Tuning)"></a>1. <strong>SFT (Supervised Fine-Tuning)</strong></h2><h3 id="1-1-Principle"><a href="#1-1-Principle" class="headerlink" title="1.1 Principle"></a>1.1 <strong>Principle</strong></h3><p>SFT is a traditional fine-tuning method that adjusts the parameters of a pre-trained model through supervised learning to improve its performance on specific tasks. SFT is typically used to fine-tune models on specific labeled datasets, with the training process resembling standard supervised learning.</p><h3 id="1-2-Implementation-Method"><a href="#1-2-Implementation-Method" class="headerlink" title="1.2 Implementation Method"></a>1.2 <strong>Implementation Method</strong></h3><ul><li><strong>Select a Pre-trained Model</strong>: Such as GPT, BERT, and other language models.</li><li><strong>Prepare a Labeled Dataset</strong>: The dataset includes input-output pairs.</li><li><strong>Train the Model</strong>: Use a standard cross-entropy loss function to train the model, optimizing parameters through gradient descent.</li></ul><h3 id="1-3-Core-Code"><a href="#1-3-Core-Code" class="headerlink" title="1.3 Core Code"></a>1.3 <strong>Core Code</strong></h3><p>Using Hugging Face’s <code>Trainer</code> interface for SFT:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Trainer, TrainingArguments, AutoModelForSeq2SeqLM, AutoTokenizer</span><br><span class="line"></span><br><span class="line">model_name = <span class="string">"gpt2"</span></span><br><span class="line">model = AutoModelForSeq2SeqLM.from_pretrained(model_name)</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_name)</span><br><span class="line"></span><br><span class="line">train_dataset = load_dataset(<span class="string">"my_dataset"</span>, split=<span class="string">"train"</span>)</span><br><span class="line"></span><br><span class="line">training_args = TrainingArguments(</span><br><span class="line">    output_dir=<span class="string">"./results"</span>,</span><br><span class="line">    evaluation_strategy=<span class="string">"epoch"</span>,</span><br><span class="line">    learning_rate=<span class="number">2e-5</span>,</span><br><span class="line">    per_device_train_batch_size=<span class="number">16</span>,</span><br><span class="line">    num_train_epochs=<span class="number">3</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model=model,</span><br><span class="line">    args=training_args,</span><br><span class="line">    train_dataset=train_dataset,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer.train()</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="2-LoRA-Low-Rank-Adaptation"><a href="#2-LoRA-Low-Rank-Adaptation" class="headerlink" title="2. LoRA (Low-Rank Adaptation)"></a>2. <strong>LoRA (Low-Rank Adaptation)</strong></h2><h3 id="2-1-Principle"><a href="#2-1-Principle" class="headerlink" title="2.1 Principle"></a>2.1 <strong>Principle</strong></h3><p>LoRA is a parameter-efficient fine-tuning technique that performs low-rank decomposition of the weight matrices in large models. It decomposes the original weight matrix $W$ into two low-rank matrices $B$ and $A$, and only fine-tunes these low-rank matrices. The design goal of LoRA is to reduce the number of fine-tuning parameters while retaining the pre-trained model weights, optimizing model performance by adjusting the low-rank matrices.</p><h3 id="2-2-Implementation-Method"><a href="#2-2-Implementation-Method" class="headerlink" title="2.2 Implementation Method"></a>2.2 <strong>Implementation Method</strong></h3><ul><li><strong>Weight Decomposition</strong>: For the model’s linear layers (such as the <code>q_proj</code> and <code>v_proj</code> layers in the attention mechanism), decompose the weight matrix into two low-rank matrices $B$ and $A$.</li><li><strong>Fine-Tune Specific Layers</strong>: Apply LoRA only to these specific linear layers, keeping other layers in the model unchanged.</li></ul><h3 id="2-3-Layers-to-Fine-Tune-vs-Layers-to-Keep-Unchanged"><a href="#2-3-Layers-to-Fine-Tune-vs-Layers-to-Keep-Unchanged" class="headerlink" title="2.3 Layers to Fine-Tune vs. Layers to Keep Unchanged"></a>2.3 <strong>Layers to Fine-Tune vs. Layers to Keep Unchanged</strong></h3><h4 id="Layers-to-Fine-Tune"><a href="#Layers-to-Fine-Tune" class="headerlink" title="Layers to Fine-Tune"></a><strong>Layers to Fine-Tune</strong></h4><p>LoRA is typically applied to the linear projection layers in Transformer models, especially several key layers in the multi-head attention mechanism:</p><ul><li><strong>q_proj</strong> (Query Projection Layer)</li><li><strong>k_proj</strong> (Key Projection Layer)</li><li><strong>v_proj</strong> (Value Projection Layer)</li><li><strong>o_proj</strong> (Output Projection Layer)</li><li><strong>ffn_up_proj</strong> and <strong>ffn_down_proj</strong> (Up and Down Projection Layers of the Feedforward Neural Network)</li></ul><h4 id="Layers-to-Keep-Unchanged"><a href="#Layers-to-Keep-Unchanged" class="headerlink" title="Layers to Keep Unchanged"></a><strong>Layers to Keep Unchanged</strong></h4><ul><li><strong>Embedding Layers</strong>: Responsible for encoding inputs and outputs, usually do not require fine-tuning.</li><li><strong>LayerNorm Layers</strong>: These layers are mainly used for normalization, do not contain many parameters, and are typically kept unchanged.</li><li><strong>Activation Function Layers</strong>: Non-linear activation functions like ReLU or GELU do not involve parameters and do not require fine-tuning.</li></ul><h3 id="2-4-Loss-Function"><a href="#2-4-Loss-Function" class="headerlink" title="2.4 Loss Function"></a>2.4 <strong>Loss Function</strong></h3><p>The loss function for LoRA is usually task-specific. In language generation tasks, LoRA uses <strong>cross-entropy loss</strong> to measure the difference between the generated text and the target text:</p><p>$$<br>\mathcal{L}<em>{\text{LoRA}} = - \sum</em>{i} y_i \log(\hat{y}_i)<br>$$</p><p>where $y_i$ is the true label, and $\hat{y}_i$ is the model’s output probability.</p><h3 id="2-5-Optimizer"><a href="#2-5-Optimizer" class="headerlink" title="2.5 Optimizer"></a>2.5 <strong>Optimizer</strong></h3><p>LoRA fine-tuning typically uses the <strong>AdamW</strong> optimizer, as shown in the following code:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.AdamW(lora_model.parameters(), lr=<span class="number">5e-5</span>)</span><br></pre></td></tr></tbody></table></figure><h3 id="2-6-Core-Code"><a href="#2-6-Core-Code" class="headerlink" title="2.6 Core Code"></a>2.6 <strong>Core Code</strong></h3><p>Implementing LoRA using the <code>peft</code> library:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> peft <span class="keyword">import</span> LoraConfig, get_peft_model</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForSeq2SeqLM</span><br><span class="line"></span><br><span class="line">model_name = <span class="string">"gpt2"</span></span><br><span class="line">model = AutoModelForSeq2SeqLM.from_pretrained(model_name)</span><br><span class="line"></span><br><span class="line">lora_config = LoraConfig(</span><br><span class="line">    r=<span class="number">8</span>,                </span><br><span class="line">    lora_alpha=<span class="number">32</span>,      </span><br><span class="line">    target_modules=[<span class="string">"q_proj"</span>, <span class="string">"v_proj"</span>],  </span><br><span class="line">    lora_dropout=<span class="number">0.1</span>,   </span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">lora_model = get_peft_model(model, lora_config)</span><br><span class="line">lora_model.train()</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="3-Alignment-Alignment-Techniques"><a href="#3-Alignment-Alignment-Techniques" class="headerlink" title="3. Alignment (Alignment Techniques)"></a>3. <strong>Alignment (Alignment Techniques)</strong></h2><p>Before introducing KL divergence, we first need to clarify how LLM alignment is achieved, along with the underlying principles and mathematical formulas.</p><h3 id="1-What-is-Model-Alignment"><a href="#1-What-is-Model-Alignment" class="headerlink" title="1. What is Model Alignment?"></a><strong>1. What is Model Alignment?</strong></h3><p>The core objective of model alignment is to ensure that the language model’s outputs meet human expectations or preferences. Typically, the model is initially trained through large-scale supervised learning (SFT, Supervised Fine-Tuning) to generate a model with basic capabilities. Subsequently, through alignment techniques, the model is further adjusted to ensure that its generated content better aligns with human preferences or avoids producing harmful or erroneous information.</p><p><strong>Core Mechanism of Alignment</strong>:</p><ul><li><strong>Positive Samples</strong>: Outputs that meet human expectations (e.g., correct answers).</li><li><strong>Negative Samples</strong>: Outputs that do not meet human expectations (e.g., incorrect answers).</li></ul><p>By using paired preference data or labels (correct/incorrect), the model’s outputs are further fine-tuned to generate more positive samples while reducing the probability of generating negative samples.</p><hr><h3 id="2-Mathematical-Principles-of-Model-Alignment"><a href="#2-Mathematical-Principles-of-Model-Alignment" class="headerlink" title="2. Mathematical Principles of Model Alignment"></a><strong>2. Mathematical Principles of Model Alignment</strong></h3><p>During the alignment process, the model generates outputs through a <strong>policy model</strong>, which is typically an SFT-trained language model used to generate outputs given an input. To optimize the model’s outputs to better align with human preferences, the following loss functions and optimization methods are commonly used:</p><h4 id="2-1-Policy-Model"><a href="#2-1-Policy-Model" class="headerlink" title="2.1 Policy Model"></a><strong>2.1 Policy Model</strong></h4><p>Assume the current policy of the model is $\pi_\theta$, which represents the probability of the model generating output $y$ given input $x$:</p><p>$$<br>\pi_\theta(y|x)<br>$$</p><p>The objective of the policy model is to adjust the parameters $\theta$ to increase the probability of generating correct outputs (positive samples) and decrease the probability of generating incorrect outputs (negative samples).</p><h4 id="2-2-Mechanism-for-Increasing-Positive-Sample-Probability-and-Decreasing-Negative-Sample-Probability"><a href="#2-2-Mechanism-for-Increasing-Positive-Sample-Probability-and-Decreasing-Negative-Sample-Probability" class="headerlink" title="2.2 Mechanism for Increasing Positive Sample Probability and Decreasing Negative Sample Probability"></a><strong>2.2 Mechanism for Increasing Positive Sample Probability and Decreasing Negative Sample Probability</strong></h4><p>To achieve this goal, loss functions with preference comparisons or labels are typically used for optimization:</p><ol><li><p><strong>Optimization of Positive Samples</strong>: By increasing the loss weight of positive samples, the model is guided to generate positive samples with higher probability when faced with the same problem.</p><ul><li>The loss function for positive samples guides the model to produce more outputs that meet human expectations.</li></ul></li><li><p><strong>Penalty for Negative Samples</strong>: By applying higher loss weights to negative samples, the model learns to reduce the probability of generating these incorrect outputs.</p><ul><li>The loss function for negative samples aims to penalize the model more when it generates incorrect answers, thereby reducing the likelihood of such outputs.</li></ul></li></ol><p>In some methods, such as DPO and KTO, <strong>KL divergence</strong> between the current policy model and a reference model is calculated to prevent the model from deviating excessively from the original pre-trained model during optimization.</p><hr><h3 id="3-Role-of-Loss-Functions-and-KL-Divergence"><a href="#3-Role-of-Loss-Functions-and-KL-Divergence" class="headerlink" title="3. Role of Loss Functions and KL Divergence"></a><strong>3. Role of Loss Functions and KL Divergence</strong></h3><p>In the model alignment process, the loss function typically consists of two parts:</p><ol><li><strong>Preference Loss</strong> or <strong>Label Loss</strong>, used to optimize the model to generate outputs that meet human expectations.</li><li><strong>KL Divergence</strong>, used to constrain the model from deviating from the reference model.</li></ol><h4 id="3-1-Role-of-KL-Divergence"><a href="#3-1-Role-of-KL-Divergence" class="headerlink" title="3.1 Role of KL Divergence"></a><strong>3.1 Role of KL Divergence</strong></h4><p>KL divergence (Kullback-Leibler Divergence) measures the difference between two probability distributions. In model alignment, KL divergence is used to limit the distribution difference between the current model $\pi_\theta$ and the reference model $\pi_{\text{ref}}$, ensuring that the model’s outputs do not deviate excessively from the pre-trained model during optimization. The specific formula is:</p><p>$$<br>\text{KL}(\pi_\theta(y|x) | \pi_{\text{ref}}(y|x)) = \sum_y \pi_\theta(y|x) \log \frac{\pi_\theta(y|x)}{\pi_{\text{ref}}(y|x)}<br>$$</p><ul><li>If the KL divergence is large, it indicates that the current model’s generated distribution significantly differs from the reference model, which may mean the model is producing unreasonable outputs.</li><li>By minimizing KL divergence, the model can be further optimized while ensuring the reasonableness of its outputs.</li></ul><h4 id="3-2-Loss-Function-Formulas"><a href="#3-2-Loss-Function-Formulas" class="headerlink" title="3.2 Loss Function Formulas"></a><strong>3.2 Loss Function Formulas</strong></h4><p>Based on preferences or labels, the model’s loss function can be expressed in the following forms:</p><h5 id="Loss-Function-in-DPO"><a href="#Loss-Function-in-DPO" class="headerlink" title="Loss Function in DPO:"></a><strong>Loss Function in DPO</strong>:</h5><p>$$<br>L_{DPO} = -\mathbb{E}_{x, y_w, y_l \sim D} [\log \sigma(\beta (\log(\pi_\theta(y_w|x)) - \log(\pi_\theta(y_l|x))))]<br>$$</p><ul><li>$y_w$: Higher-preference answer.</li><li>$y_l$: Lower-preference answer.</li></ul><p>In DPO, KL divergence can be introduced as a regularization term:<br>$$<br>L_{DPO} = -\log \sigma(\log(\pi_\theta(y_w|x)) - \log(\pi_\theta(y_l|x)) + \beta \cdot \text{KL}(\pi_\theta | \pi_{\text{ref}}))<br>$$<br>By controlling KL divergence, the model’s outputs do not deviate too much from the reference model.</p><h5 id="Loss-Function-in-KTO"><a href="#Loss-Function-in-KTO" class="headerlink" title="Loss Function in KTO:"></a><strong>Loss Function in KTO</strong>:</h5><p>The loss function in KTO is based on prospect theory and incorporates KL divergence as a core component:<br>$$<br>L_{KTO} = \lambda_U \cdot \sigma(\beta \cdot (\text{KL}(\pi_\theta(\text{Answer 2}) | \pi_{\text{ref}}) - r_{\theta}(\text{Answer 2})))<br>$$</p><ul><li>$r_{\theta}(x, y)$: The current policy’s confidence in negative samples (incorrect answers).</li><li>KL divergence is used to measure the difference between the current model and the reference model, ensuring that while reducing the generation of negative samples, the model does not deviate from the original reference model.</li></ul><p>By increasing the loss for negative samples (i.e., increasing the value of $\lambda_U$), the model reduces the confidence in negative samples, thereby decreasing the probability of generating similar incorrect answers in the future.</p><hr><h3 id="4-How-to-Optimize-the-Model"><a href="#4-How-to-Optimize-the-Model" class="headerlink" title="4. How to Optimize the Model"></a><strong>4. How to Optimize the Model</strong></h3><p>Through the loss functions introduced above, model optimization is typically performed using <strong>Gradient Descent</strong>. The gradients of the loss function reflect the differences between the model’s outputs and the expected outputs, and the optimization goal is to minimize the loss function.</p><p><strong>Gradient Update Formula</strong>:<br>$$<br>\theta_{\text{new}} = \theta_{\text{old}} - \eta \nabla_{\theta} L<br>$$<br>where:</p><ul><li>$\eta$ is the learning rate, determining the step size of each parameter update.</li><li>$\nabla_{\theta} L$ is the gradient of the loss function with respect to the model parameters, indicating the contribution of the current parameters to the loss.</li></ul><p>Through continuous iteration, the model gradually increases the probability of generating positive samples and decreases the probability of generating negative samples, ultimately achieving model alignment.</p><ul><li>The core objective of <strong>Model Alignment</strong> is to optimize the model’s outputs to meet human expectations through preference or label data.</li><li>The <strong>Policy Model</strong> ($\pi_\theta$) generates outputs, and KL divergence is used to control the degree of deviation from the reference model, preventing unreasonable biases during optimization.</li><li>The <strong>Probability of Positive Samples</strong> is gradually increased through the optimization of the loss function, while the <strong>Probability of Negative Samples</strong> is reduced by increasing loss weights and lowering confidence.</li><li>Gradient descent is used to update model parameters, ultimately achieving model alignment.</li></ul><hr><h2 id="4-DPO-Direct-Preference-Optimization"><a href="#4-DPO-Direct-Preference-Optimization" class="headerlink" title="4. DPO (Direct Preference Optimization)"></a>4. <strong>DPO (Direct Preference Optimization)</strong></h2><h3 id="4-1-Principle"><a href="#4-1-Principle" class="headerlink" title="4.1 Principle"></a>4.1 <strong>Principle</strong></h3><p>DPO directly optimizes the model’s output preference function to make the model’s outputs more aligned with human preferences. It compares different outputs generated by the model and uses a preference function to evaluate which of the two outputs is better, thereby guiding the optimization of the model parameters.</p><h3 id="4-2-Loss-Function"><a href="#4-2-Loss-Function" class="headerlink" title="4.2 Loss Function"></a>4.2 <strong>Loss Function</strong></h3><p>DPO uses a preference loss function to compare the quality of two outputs:</p><p>$$<br>\mathcal{L}_{\text{DPO}} = \log(1 + \exp(-\sigma \cdot (\hat{y}_a - \hat{y}_b) \cdot p))<br>$$</p><ul><li>$ \hat{y}_a $ and $ \hat{y}_b $ are the model’s predictions for two samples.</li><li>$ p $ is the human preference (1 indicates preference for $a$, -1 indicates preference for $b$).</li><li>$ \sigma $ is a smoothing parameter.</li></ul><h3 id="4-3-Optimizer"><a href="#4-3-Optimizer" class="headerlink" title="4.3 Optimizer"></a>4.3 <strong>Optimizer</strong></h3><p>DPO typically uses the <strong>AdamW</strong> optimizer, which is suitable for optimizing large-scale parameter models. The code is as follows:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.AdamW(model.parameters(), lr=<span class="number">1e-5</span>)</span><br></pre></td></tr></tbody></table></figure><h3 id="4-4-Core-Code"><a href="#4-4-Core-Code" class="headerlink" title="4.4 Core Code"></a>4.4 <strong>Core Code</strong></h3><p>The following are the training steps for DPO:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preference_loss</span>(<span class="params">output_a, output_b, human_preference</span>):</span><br><span class="line">    preference = human_preference(output_a, output_b)</span><br><span class="line">    loss = torch.log(<span class="number">1</span> + torch.exp(-preference * (output_a - output_b)))</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dpo_training_step</span>(<span class="params">model, data_a, data_b, human_preference</span>):</span><br><span class="line">    output_a = model(data_a)</span><br><span class="line">    output_b = model(data_b)</span><br><span class="line">    </span><br><span class="line">    loss = preference_loss(output_a, output_b, human_preference)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> batch_a, batch_b <span class="keyword">in</span> training_data:</span><br><span class="line">    dpo_training_step(model, batch_a, batch_b, human_preference_function)</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="5-KTO-Kahneman-Tversky-Optimization"><a href="#5-KTO-Kahneman-Tversky-Optimization" class="headerlink" title="5. KTO (Kahneman-Tversky Optimization)"></a>5. <strong>KTO (Kahneman-Tversky Optimization)</strong></h2><h3 id="5-1-Principle"><a href="#5-1-Principle" class="headerlink" title="5.1 Principle"></a>5.1 <strong>Principle</strong></h3><p>KTO is based on Kahneman and Tversky’s Prospect Theory, which uses an asymmetric utility function to measure the model’s gains and losses. It aims to optimize the model’s performance, especially in scenarios with asymmetric risks and rewards. The utility function is defined as follows:</p><p>$$<br>\mathcal{U}(x) =<br>\begin{cases}<br>x^{\alpha}, &amp; x \geq 0 \<br>-\lambda (-x)^{\alpha}, &amp; x &lt; 0<br>\end{cases}<br>$$</p><ul><li>$x$ is the difference between the model’s prediction and the true value.</li><li>$\alpha$ is the non-linear coefficient, typically 0.88.</li><li>$\lambda$ is the loss penalty weight, typically 2.25.</li></ul><h3 id="5-2-Loss-Function"><a href="#5-2-Loss-Function" class="headerlink" title="5.2 Loss Function"></a>5.2 <strong>Loss Function</strong></h3><p>The loss function for KTO is based on the utility function from Prospect Theory and is used to penalize the model’s prediction errors:</p><p>$$<br>\mathcal{L}<em>{\text{KTO}} = -\mathbb{E}[\mathcal{U}(y</em>{\text{pred}} - y_{\text{true}})]<br>$$</p><h3 id="5-3-Optimizer"><a href="#5-3-Optimizer" class="headerlink" title="5.3 Optimizer"></a>5.3 <strong>Optimizer</strong></h3><p>KTO commonly uses the <strong>AdamW</strong> optimizer to ensure stability during the training process:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.AdamW(model.parameters(), lr=<span class="number">3e-5</span>)</span><br></pre></td></tr></tbody></table></figure><h3 id="5-4-Core-Code"><a href="#5-4-Core-Code" class="headerlink" title="5.4 Core Code"></a>5.4 <strong>Core Code</strong></h3><p>The following is the code for calculating the KTO loss function:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">prospect_utility</span>(<span class="params">value, alpha=<span class="number">0.88</span>, lambda_=<span class="number">2.25</span></span>):</span><br><span class="line">    <span class="keyword">if</span> value &gt;= <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> torch.<span class="built_in">pow</span>(value, alpha)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> -lambda_ * torch.<span class="built_in">pow</span>(-value, alpha)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">kto_loss</span>(<span class="params">predictions, targets</span>):</span><br><span class="line">    value = predictions - targets</span><br><span class="line">    utility = prospect_utility(value)</span><br><span class="line">    loss = -torch.mean(utility)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">    predictions = model(data)</span><br><span class="line">    loss = kto_loss(predictions, targets)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a><strong>Summary</strong></h3><table><thead><tr><th>Method</th><th>Loss Function</th><th>Optimizer</th></tr></thead><tbody><tr><td><strong>SFT</strong></td><td>Cross-Entropy Loss</td><td>AdamW, RMSprop, SGD</td></tr><tr><td><strong>LoRA</strong></td><td>Cross-Entropy Loss</td><td>AdamW, RMSprop, SGD</td></tr><tr><td><strong>DPO</strong></td><td>Preference Loss Function: $\log(1 + \exp(-\sigma (\hat{y}_a - \hat{y}_b)p))$</td><td>AdamW</td></tr><tr><td><strong>KTO</strong></td><td>Prospect Theory Utility Function: $-\mathbb{E}[\mathcal{U}(y_{\text{pred}} - y_{\text{true}})]$</td><td>AdamW</td></tr></tbody></table><p>Through the organization of this document, readers can clearly understand the principles, specific implementation steps, loss function designs, and optimizer selections for technologies such as SFT, LoRA, DPO, and KTO, especially in the context of fine-tuning large-scale pre-trained models like LLAMA3.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;Introduction-to-LLM-Training-Terminology-LoRA-DPO-KTO-and-SFT-Technologies&quot;&gt;&lt;a href=&quot;#Introduction-to-LLM-Training-Terminology-LoRA-</summary>
      
    
    
    
    <category term="NLP Insights" scheme="https://chenhuiyu.github.io/categories/NLP-Insights/"/>
    
    
    <category term="LLM" scheme="https://chenhuiyu.github.io/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>LoRA, DPO, KTO 与 SFT 技术详解</title>
    <link href="https://chenhuiyu.github.io/2024/10/23/NLP%20Insights/LLM%E8%AE%AD%E7%BB%83%E6%9C%AF%E8%AF%AD%E4%BB%8B%E7%BB%8D%EF%BC%9ASFT%E3%80%81LoRA%E3%80%81Alignment%E3%80%81KTO%E5%92%8CDPO/"/>
    <id>https://chenhuiyu.github.io/2024/10/23/NLP%20Insights/LLM%E8%AE%AD%E7%BB%83%E6%9C%AF%E8%AF%AD%E4%BB%8B%E7%BB%8D%EF%BC%9ASFT%E3%80%81LoRA%E3%80%81Alignment%E3%80%81KTO%E5%92%8CDPO/</id>
    <published>2024-10-23T08:26:29.000Z</published>
    <updated>2024-10-23T09:10:14.320Z</updated>
    
    <content type="html"><![CDATA[<h3 id="LoRA-DPO-KTO-与-SFT-技术详解"><a href="#LoRA-DPO-KTO-与-SFT-技术详解" class="headerlink" title="LoRA, DPO, KTO 与 SFT 技术详解"></a><strong>LoRA, DPO, KTO 与 SFT 技术详解</strong></h3><p>本篇文档将详细介绍几种在大型语言模型（如 LLAMA3）微调和优化中的重要技术，包括 <strong>SFT（Supervised Fine-Tuning）</strong>、<strong>LoRA（Low-Rank Adaptation）</strong>、<strong>Alignment</strong> 技术、<strong>KTO（Kahneman-Tversky Optimization）</strong> 和 <strong>DPO（Direct Preference Optimization）</strong>。文中还将详细阐述每种技术的原理、具体实现方法以及相应的损失函数与优化器选择。</p><hr><h2 id="1-SFT（Supervised-Fine-Tuning）"><a href="#1-SFT（Supervised-Fine-Tuning）" class="headerlink" title="1. SFT（Supervised Fine-Tuning）"></a>1. <strong>SFT（Supervised Fine-Tuning）</strong></h2><h3 id="1-1-原理"><a href="#1-1-原理" class="headerlink" title="1.1 原理"></a>1.1 <strong>原理</strong></h3><p>SFT 是一种传统的微调方法，通过监督学习对预训练模型进行微调，调整模型的参数使其在特定任务上表现更好。SFT 通常用于针对特定的标注数据进行模型微调，训练的过程类似于常规的监督学习。</p><h3 id="1-2-实现方法"><a href="#1-2-实现方法" class="headerlink" title="1.2 实现方法"></a>1.2 <strong>实现方法</strong></h3><ul><li><strong>选择预训练模型</strong>：如 GPT、BERT 等语言模型。</li><li><strong>准备标注数据集</strong>：数据集包含输入和输出对。</li><li><strong>训练模型</strong>：使用标准的交叉熵损失函数对模型进行训练，通过梯度下降优化参数。</li></ul><h3 id="1-3-核心代码"><a href="#1-3-核心代码" class="headerlink" title="1.3 核心代码"></a>1.3 <strong>核心代码</strong></h3><p>使用 Hugging Face 的 <code>Trainer</code> 接口进行 SFT：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Trainer, TrainingArguments, AutoModelForSeq2SeqLM, AutoTokenizer</span><br><span class="line"></span><br><span class="line">model_name = <span class="string">"gpt2"</span></span><br><span class="line">model = AutoModelForSeq2SeqLM.from_pretrained(model_name)</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_name)</span><br><span class="line"></span><br><span class="line">train_dataset = load_dataset(<span class="string">"my_dataset"</span>, split=<span class="string">"train"</span>)</span><br><span class="line"></span><br><span class="line">training_args = TrainingArguments(</span><br><span class="line">    output_dir=<span class="string">"./results"</span>,</span><br><span class="line">    evaluation_strategy=<span class="string">"epoch"</span>,</span><br><span class="line">    learning_rate=<span class="number">2e-5</span>,</span><br><span class="line">    per_device_train_batch_size=<span class="number">16</span>,</span><br><span class="line">    num_train_epochs=<span class="number">3</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model=model,</span><br><span class="line">    args=training_args,</span><br><span class="line">    train_dataset=train_dataset,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer.train()</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="2-LoRA（Low-Rank-Adaptation）"><a href="#2-LoRA（Low-Rank-Adaptation）" class="headerlink" title="2. LoRA（Low-Rank Adaptation）"></a>2. <strong>LoRA（Low-Rank Adaptation）</strong></h2><h3 id="2-1-原理"><a href="#2-1-原理" class="headerlink" title="2.1 原理"></a>2.1 <strong>原理</strong></h3><p>LoRA 是一种参数高效的微调技术，通过对大模型中的权重矩阵进行低秩分解，将原始权重矩阵 $W$ 分解为两个低秩矩阵 $B$ 和 $A$，并仅对这些低秩矩阵进行微调。LoRA 的设计目标是减少微调参数的数量，在保留预训练模型权重的同时，通过调整低秩矩阵来优化模型表现。</p><h3 id="2-2-实现方法"><a href="#2-2-实现方法" class="headerlink" title="2.2 实现方法"></a>2.2 <strong>实现方法</strong></h3><ul><li><strong>权重分解</strong>：对于模型的线性层（如注意力机制中的 <code>q_proj</code> 和 <code>v_proj</code> 层），将权重矩阵分解为两个低秩矩阵 $B$ 和 $A$。</li><li><strong>微调特定层</strong>：仅对这些特定的线性层应用 LoRA，而模型中的其他层保持不变。</li></ul><h3 id="2-3-可微调的层与不变的层"><a href="#2-3-可微调的层与不变的层" class="headerlink" title="2.3 可微调的层与不变的层"></a>2.3 <strong>可微调的层与不变的层</strong></h3><h4 id="可微调的层"><a href="#可微调的层" class="headerlink" title="可微调的层"></a><strong>可微调的层</strong></h4><p>LoRA 通常应用于 Transformer 模型中的线性投影层，尤其是多头注意力机制中的几个关键层：</p><ul><li><strong>q_proj</strong>（Query 投影层）</li><li><strong>k_proj</strong>（Key 投影层）</li><li><strong>v_proj</strong>（Value 投影层）</li><li><strong>o_proj</strong>（Output 投影层）</li><li><strong>ffn_up_proj</strong> 和 <strong>ffn_down_proj</strong>（前馈神经网络的上下投影层）</li></ul><h4 id="不变的层"><a href="#不变的层" class="headerlink" title="不变的层"></a><strong>不变的层</strong></h4><ul><li><strong>Embedding 层</strong>：负责输入和输出的编码，通常不需要微调。</li><li><strong>LayerNorm 层</strong>：这些层主要用于归一化，不含大量参数，通常保持不变。</li><li><strong>激活函数层</strong>：如 ReLU 或 GELU 等非线性激活函数不涉及参数，不需要进行微调。</li></ul><h3 id="2-4-损失函数"><a href="#2-4-损失函数" class="headerlink" title="2.4 损失函数"></a>2.4 <strong>损失函数</strong></h3><p>LoRA 的损失函数通常与具体任务相关。在语言生成任务中，LoRA 使用<strong>交叉熵损失</strong>来度量生成文本和目标文本之间的差异：</p><p>$$<br>\mathcal{L}<em>{\text{LoRA}} = - \sum</em>{i} y_i \log(\hat{y}_i)<br>$$</p><p>其中 $y_i$ 是真实标签，$\hat{y}_i$ 是模型的输出概率。</p><h3 id="2-5-优化器"><a href="#2-5-优化器" class="headerlink" title="2.5 优化器"></a>2.5 <strong>优化器</strong></h3><p>LoRA 微调通常使用 <strong>AdamW</strong> 优化器，具体代码如下：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.AdamW(lora_model.parameters(), lr=<span class="number">5e-5</span>)</span><br></pre></td></tr></tbody></table></figure><h3 id="2-6-核心代码"><a href="#2-6-核心代码" class="headerlink" title="2.6 核心代码"></a>2.6 <strong>核心代码</strong></h3><p>使用 <code>peft</code> 库实现 LoRA：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> peft <span class="keyword">import</span> LoraConfig, get_peft_model</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForSeq2SeqLM</span><br><span class="line"></span><br><span class="line">model_name = <span class="string">"gpt2"</span></span><br><span class="line">model = AutoModelForSeq2SeqLM.from_pretrained(model_name)</span><br><span class="line"></span><br><span class="line">lora_config = LoraConfig(</span><br><span class="line">    r=<span class="number">8</span>,                </span><br><span class="line">    lora_alpha=<span class="number">32</span>,      </span><br><span class="line">    target_modules=[<span class="string">"q_proj"</span>, <span class="string">"v_proj"</span>],  </span><br><span class="line">    lora_dropout=<span class="number">0.1</span>,   </span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">lora_model = get_peft_model(model, lora_config)</span><br><span class="line">lora_model.train()</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="3-Alignment（对齐技术）"><a href="#3-Alignment（对齐技术）" class="headerlink" title="3. Alignment（对齐技术）"></a>3. <strong>Alignment（对齐技术）</strong></h2><p>在引入KL散度之前，我们首先需要明确LLM对齐（Alignment）是如何实现的，以及背后的原理和数学公式。</p><h3 id="1-什么是模型对齐（Alignment）？"><a href="#1-什么是模型对齐（Alignment）？" class="headerlink" title="1. 什么是模型对齐（Alignment）？"></a><strong>1. 什么是模型对齐（Alignment）？</strong></h3><p>模型对齐的核心目标是让语言模型的输出符合人类的期望或偏好。通常，模型最初通过大规模监督学习（SFT，Supervised Fine-Tuning）训练，生成具有基础能力的模型。接下来，通过对齐技术，进一步调整模型，使其生成的内容更符合人类偏好或避免产生有害、错误的信息。</p><p><strong>对齐的核心机制</strong>：</p><ul><li><strong>正样本</strong>：符合人类预期的输出（如正确回答）。</li><li><strong>负样本</strong>：不符合人类预期的输出（如错误回答）。</li></ul><p>通过使用成对偏好数据或标签（正确/错误），对模型的输出进行进一步微调，使模型能够生成更多的正样本，同时减少负样本的生成概率。</p><hr><h3 id="2-模型对齐的数学原理"><a href="#2-模型对齐的数学原理" class="headerlink" title="2. 模型对齐的数学原理"></a><strong>2. 模型对齐的数学原理</strong></h3><p>在对齐过程中，模型会通过<strong>策略模型</strong>（Policy Model）来生成输出，策略模型通常是经过SFT训练的语言模型，用来在给定输入下生成输出。为了优化模型的输出，使其更加符合人类偏好，常常使用以下损失函数和优化方法：</p><h4 id="2-1-策略模型"><a href="#2-1-策略模型" class="headerlink" title="2.1 策略模型"></a><strong>2.1 策略模型</strong></h4><p>假设当前模型的策略为 $\pi_\theta$，它表示在给定输入 $x$ 时，模型生成输出 $y$ 的概率：<br>$$<br>\pi_\theta(y|x)<br>$$<br>策略模型的目标是通过调整参数 $\theta$，提高生成正确输出（正样本）的概率，降低生成错误输出（负样本）的概率。</p><h4 id="2-2-提高正样本概率与降低负样本概率的机制"><a href="#2-2-提高正样本概率与降低负样本概率的机制" class="headerlink" title="2.2 提高正样本概率与降低负样本概率的机制"></a><strong>2.2 提高正样本概率与降低负样本概率的机制</strong></h4><p>为了实现这个目标，通常使用带有偏好比较或标签的损失函数进行优化：</p><ol><li><p><strong>正样本的优化</strong>：通过增加正样本的损失权重，使得模型生成正样本的概率更高。</p><ul><li>正样本的损失函数会引导模型在面对相同问题时，生成更多符合人类期望的答案。</li></ul></li><li><p><strong>负样本的惩罚</strong>：对负样本施加更高的损失权重，模型会学习到减少这些错误输出的概率。</p><ul><li>负样本的损失函数旨在让模型在生成错误答案时感知到更大的惩罚，从而减少这些输出的生成。</li></ul></li></ol><p>在某些方法中，例如DPO和KTO，还会通过计算当前策略模型与参考模型之间的<strong>KL散度</strong>，来防止模型在优化过程中过度偏离原始预训练模型。</p><hr><h3 id="3-损失函数与KL散度的作用"><a href="#3-损失函数与KL散度的作用" class="headerlink" title="3. 损失函数与KL散度的作用"></a><strong>3. 损失函数与KL散度的作用</strong></h3><p>在模型对齐的过程中，损失函数通常包含两部分：</p><ol><li><strong>偏好损失</strong>或<strong>标签损失</strong>，用于优化模型生成符合人类期望的输出。</li><li><strong>KL散度</strong>，用于约束模型不要偏离参考模型。</li></ol><h4 id="3-1-KL散度的作用"><a href="#3-1-KL散度的作用" class="headerlink" title="3.1 KL散度的作用"></a><strong>3.1 KL散度的作用</strong></h4><p>KL散度（Kullback-Leibler Divergence）衡量的是两个概率分布之间的差异。在模型对齐中，KL散度用于限制当前模型 \(\pi_\theta\) 和参考模型 \(\pi_{\text{ref}}\) 的分布差异，确保在优化过程中模型的输出不会过度偏离预训练模型。具体公式为：<br>$$<br>\text{KL}(\pi_\theta(y|x) | \pi_{\text{ref}}(y|x)) = \sum_y \pi_\theta(y|x) \log \frac{\pi_\theta(y|x)}{\pi_{\text{ref}}(y|x)}<br>$$</p><ul><li>如果KL散度较大，表示当前模型生成的分布与参考模型有较大的差异，这可能意味着模型生成了不合理的输出。</li><li>通过最小化KL散度，模型能够在保证输出合理性的基础上，进行进一步的优化。</li></ul><h4 id="3-2-损失函数公式"><a href="#3-2-损失函数公式" class="headerlink" title="3.2 损失函数公式"></a><strong>3.2 损失函数公式</strong></h4><p>根据偏好或标签，模型的损失函数可以表达为以下形式：</p><h5 id="DPO中的损失函数："><a href="#DPO中的损失函数：" class="headerlink" title="DPO中的损失函数："></a><strong>DPO中的损失函数</strong>：</h5><p>$$<br>L_{DPO} = -\mathbb{E}_{x, y_w, y_l \sim D} [\log \sigma(\beta (\log(\pi_\theta(y_w|x)) - \log(\pi_\theta(y_l|x))))]<br>$$</p><ul><li>$y_w$：偏好较高的答案。</li><li>$y_l$：偏好较低的答案。</li></ul><p>DPO中可以引入KL散度作为正则化项：<br>$$<br>L_{DPO} = -\log \sigma(\log(\pi_\theta(y_w|x)) - \log(\pi_\theta(y_l|x)) + \beta \cdot \text{KL}(\pi_\theta | \pi_{\text{ref}}))<br>$$<br>通过控制KL散度，模型的输出不会偏离参考模型太多。</p><h5 id="KTO中的损失函数："><a href="#KTO中的损失函数：" class="headerlink" title="KTO中的损失函数："></a><strong>KTO中的损失函数</strong>：</h5><p>KTO的损失函数基于前景理论，并将KL散度作为核心部分，表达为：<br>$$<br>L_{KTO} = \lambda_U \cdot \sigma(\beta \cdot (\text{KL}(\pi_\theta(\text{Answer 2}) | \pi_{\text{ref}}) - r_{\theta}(\text{Answer 2})))<br>$$</p><ul><li>$r_{\theta}(x, y)$：当前策略对负样本（错误答案）的置信度。</li><li>KL散度用于衡量当前模型与参考模型的差异，确保模型在减少负样本生成的同时，不偏离原始参考模型。</li></ul><p>通过增加负样本的损失（即增加 $\lambda_U$ 的值），模型会降低负样本的置信度，使未来生成类似错误答案的概率变小。</p><hr><h3 id="4-如何优化模型"><a href="#4-如何优化模型" class="headerlink" title="4. 如何优化模型"></a><strong>4. 如何优化模型</strong></h3><p>通过上面介绍的损失函数，模型的优化通常是通过<strong>梯度下降</strong>（Gradient Descent）来完成的。损失函数的梯度反映了模型输出与期望输出之间的差异，优化目标是最小化损失函数。</p><p><strong>梯度更新公式</strong>：<br>$$<br>\theta_{\text{new}} = \theta_{\text{old}} - \eta \nabla_{\theta} L<br>$$<br>其中：</p><ul><li>$\eta$ 是学习率，决定每次参数更新的步长。</li><li>$\nabla_{\theta} L$ 是损失函数对模型参数的梯度，表示当前参数对损失的贡献。</li></ul><p>通过不断迭代，模型会逐渐提高生成正样本的概率，减少负样本的生成概率，最终实现模型对齐。</p><ul><li>模型对齐（Alignment）的核心目标是通过偏好或标签数据，优化模型的输出，使其符合人类期望。</li><li><strong>策略模型</strong>（$\pi_\theta$）生成输出，KL散度用于控制模型与参考模型的偏离程度，避免模型在优化过程中产生不合理的偏差。</li><li><strong>正样本的概率</strong>通过损失函数的优化逐步提升，<strong>负样本的概率</strong>通过增加损失权重和降低置信度来减少。</li><li>梯度下降用于更新模型参数，最终实现模型对齐</li></ul><hr><h2 id="4-DPO（Direct-Preference-Optimization）"><a href="#4-DPO（Direct-Preference-Optimization）" class="headerlink" title="4. DPO（Direct Preference Optimization）"></a>4. <strong>DPO（Direct Preference Optimization）</strong></h2><h3 id="4-1-原理"><a href="#4-1-原理" class="headerlink" title="4.1 原理"></a>4.1 <strong>原理</strong></h3><p>DPO 通过直接优化模型输出的偏好函数，使模型的输出更加符合人类偏好。它比较模型的不同输出，并通过偏好函数评估这两个输出哪个更好，从而指导模型参数的优化。</p><h3 id="4-2-损失函数"><a href="#4-2-损失函数" class="headerlink" title="4.2 损失函数"></a>4.2 <strong>损失函数</strong></h3><p>DPO 使用偏好损失函数（Preference Loss），用于比较两个输出的优劣：</p><p>$$<br>\mathcal{L}_{\text{DPO}} = \log(1 + \exp(-\sigma \cdot (\hat{y}_a - \hat{y}_b) \cdot p))<br>$$</p><ul><li>$ \hat{y}_a $ 和 $ \hat{y}_b $ 是模型对两个样本的预测值。</li><li>$ p $ 是人类偏好（1 表示偏好 $a$，-1 表示偏好 $b$）。</li><li>$ \sigma $ 是平滑参数。</li></ul><h3 id="4-3-优化器"><a href="#4-3-优化器" class="headerlink" title="4.3 优化器"></a>4.3 <strong>优化器</strong></h3><p>DPO 通常使用 <strong>AdamW</strong> 优化器，适用于大规模参数模型的优化，代码如下：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.AdamW(model.parameters(), lr=<span class="number">1e-5</span>)</span><br></pre></td></tr></tbody></table></figure><h3 id="4-4-核心代码"><a href="#4-4-核心代码" class="headerlink" title="4.4 核心代码"></a>4.4 <strong>核心代码</strong></h3><p>以下是 DPO 的训练步骤：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preference_loss</span>(<span class="params">output_a, output_b, human_preference</span>):</span><br><span class="line">    preference = human_preference(output_a, output_b)</span><br><span class="line">    loss = torch.log(<span class="number">1</span> + torch.exp(-preference * (output_a - output_b)))</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dpo_training_step</span>(<span class="params">model, data_a, data_b, human_preference</span>):</span><br><span class="line">    output_a = model(data_a)</span><br><span class="line">    output_b = model(data_b)</span><br><span class="line">    </span><br><span class="line">    loss = preference_loss(output_a, output_b, human_preference)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> batch_a, batch_b <span class="keyword">in</span> training_data:</span><br><span class="line">    dpo_training_step(model, batch_a, batch_b, human_preference_function)</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="5-KTO（Kahneman-Tversky-Optimization）"><a href="#5-KTO（Kahneman-Tversky-Optimization）" class="headerlink" title="5. KTO（Kahneman-Tversky Optimization）"></a>5. <strong>KTO（Kahneman-Tversky Optimization）</strong></h2><h3 id="5-1-原理"><a href="#5-1-原理" class="headerlink" title="5.1 原理"></a>5.1 <strong>原理</strong></h3><p>KTO 基于 Kahneman 和 Tversky 的前景理论（Prospect Theory），通过非对称效用函数衡量模型的增益和损失，旨在优化模型的表现，尤其在风险和收益不对称的场景下。效用函数定义如下：</p><p>$$<br>\mathcal{U}(x) =<br>\begin{cases}<br>x^{\alpha}, &amp; x \geq 0 \<br>-\lambda (-x)^{\alpha}, &amp; x &lt; 0<br>\end{cases}<br>$$</p><ul><li>$x$ 是模型预测与真实值的差异。</li><li>$\alpha$ 是非线性系数，通常为 0</li></ul><p>.88。</p><ul><li>$\lambda$ 是损失的惩罚权重，通常为 2.25。</li></ul><h3 id="5-2-损失函数"><a href="#5-2-损失函数" class="headerlink" title="5.2 损失函数"></a>5.2 <strong>损失函数</strong></h3><p>KTO 的损失函数基于前景理论的效用函数，用于惩罚模型的预测误差：</p><p>$$<br>\mathcal{L}<em>{\text{KTO}} = -\mathbb{E}[\mathcal{U}(y</em>{\text{pred}} - y_{\text{true}})]<br>$$</p><h3 id="5-3-优化器"><a href="#5-3-优化器" class="headerlink" title="5.3 优化器"></a>5.3 <strong>优化器</strong></h3><p>KTO 常使用 <strong>AdamW</strong> 优化器，以确保训练过程的稳定性：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.AdamW(model.parameters(), lr=<span class="number">3e-5</span>)</span><br></pre></td></tr></tbody></table></figure><h3 id="5-4-核心代码"><a href="#5-4-核心代码" class="headerlink" title="5.4 核心代码"></a>5.4 <strong>核心代码</strong></h3><p>以下是 KTO 损失函数的计算代码：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">prospect_utility</span>(<span class="params">value, alpha=<span class="number">0.88</span>, lambda_=<span class="number">2.25</span></span>):</span><br><span class="line">    <span class="keyword">if</span> value &gt;= <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> torch.<span class="built_in">pow</span>(value, alpha)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> -lambda_ * torch.<span class="built_in">pow</span>(-value, alpha)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">kto_loss</span>(<span class="params">predictions, targets</span>):</span><br><span class="line">    value = predictions - targets</span><br><span class="line">    utility = prospect_utility(value)</span><br><span class="line">    loss = -torch.mean(utility)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">    predictions = model(data)</span><br><span class="line">    loss = kto_loss(predictions, targets)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h3><table><thead><tr><th>方法</th><th>损失函数</th><th>优化器</th></tr></thead><tbody><tr><td><strong>SFT</strong></td><td>交叉熵损失</td><td>AdamW，RMSprop，SGD</td></tr><tr><td><strong>LoRA</strong></td><td>交叉熵损失</td><td>AdamW，RMSprop，SGD</td></tr><tr><td><strong>DPO</strong></td><td>偏好损失函数： $\log(1 + \exp(-\sigma (\hat{y}_a - \hat{y}_b)p))$</td><td>AdamW</td></tr><tr><td><strong>KTO</strong></td><td>前景理论效用函数： $-\mathbb{E}[\mathcal{U}(y_{\text{pred}} - y_{\text{true}})]$</td><td>AdamW</td></tr></tbody></table><p>通过本文档的整理，读者能够清晰理解 SFT、LoRA、DPO 和 KTO 等技术的原理、具体实现步骤、损失函数设计和优化器选择，特别是在 LLAMA3 这种大规模预训练模型的微调场景下的实际应用。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;LoRA-DPO-KTO-与-SFT-技术详解&quot;&gt;&lt;a href=&quot;#LoRA-DPO-KTO-与-SFT-技术详解&quot; class=&quot;headerlink&quot; title=&quot;LoRA, DPO, KTO 与 SFT 技术详解&quot;&gt;&lt;/a&gt;&lt;strong&gt;LoRA, D</summary>
      
    
    
    
    <category term="NLP Insights" scheme="https://chenhuiyu.github.io/categories/NLP-Insights/"/>
    
    
    <category term="LLM" scheme="https://chenhuiyu.github.io/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>使用压缩有限状态机进行本地 LLM 的快速 JSON 解码</title>
    <link href="https://chenhuiyu.github.io/2024/08/13/NLP%20Insights/%E4%BD%BF%E7%94%A8%E5%8E%8B%E7%BC%A9%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA%E8%BF%9B%E8%A1%8C%E6%9C%AC%E5%9C%B0%20LLM%20%E7%9A%84%E5%BF%AB%E9%80%9F%20JSON%20%E8%A7%A3%E7%A0%81/"/>
    <id>https://chenhuiyu.github.io/2024/08/13/NLP%20Insights/%E4%BD%BF%E7%94%A8%E5%8E%8B%E7%BC%A9%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA%E8%BF%9B%E8%A1%8C%E6%9C%AC%E5%9C%B0%20LLM%20%E7%9A%84%E5%BF%AB%E9%80%9F%20JSON%20%E8%A7%A3%E7%A0%81/</id>
    <published>2024-08-13T08:12:10.000Z</published>
    <updated>2024-08-13T08:30:07.807Z</updated>
    
    <content type="html"><![CDATA[<h2 id="使用压缩有限状态机进行本地-LLM-的快速-JSON-解码"><a href="#使用压缩有限状态机进行本地-LLM-的快速-JSON-解码" class="headerlink" title="使用压缩有限状态机进行本地 LLM 的快速 JSON 解码"></a>使用压缩有限状态机进行本地 LLM 的快速 JSON 解码</h2><p><strong>作者</strong>: Liangsheng Yin, Ying Sheng, Lianmin Zheng<br><strong>日期</strong>: 2024 年 2 月 5 日</p><hr><p>本文内容基于 LMSYS Org 发布的一篇博客文章，原文链接：<a href="https://lmsys.org/blog/2024-02-05-compressed-fsm/">LMSYS Org 博客</a>。相关的代码库可以在以下链接找到：<a href="https://github.com/sgl-project/sglang/tree/main?tab=readme-ov-file#json-decoding">SGLang 代码库</a>。</p><p>让一个 LLM 始终生成符合特定模式的有效 JSON 或 YAML，对于许多应用来说是一个关键特性。在这篇博客文章中，我们介绍了一种显著加速这种约束解码的优化方法。我们的方法利用了压缩的有限状态机，并且兼容任何正则表达式，因此可以适用于任何 JSON 或 YAML 模式。与现有系统逐步解码一个标记的方式不同，我们的方法分析了正则表达式的有限状态机，压缩了单一的转换路径，并在可能的情况下一次性解码多个标记。与最先进的系统（guidance + llama.cpp，outlines + vLLM）相比，我们的方法可以将延迟减少最多 2 倍，并提高吞吐量最多 2.5 倍。这一优化还使得约束解码比普通解码更快。你可以在 SGLang 上试用它。</p><p><img src="https://lmsys.org/images/blog/compressed_fsm/demo.gif" alt="图1：SGLang和Outlines + vLLM在JSON解码中的比较"></p><p>图一展示了 SGLang 和 Outlines + vLLM 在 JSON 解码任务中的性能比较。这是一个动态对比，目的是展示两者在相同任务下的速度差异。SGLang 采用了一种新的跳跃前进解码算法，通过压缩有限状态机来加速解码过程。相比之下，Outlines + vLLM 使用了传统的逐步解码方法。图中的动画演示了 SGLang 在处理多字符（或标记）解码时的优势，显著减少了解码时间。</p><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>JSON 是数据交换中最重要的格式之一。要求 LLM 始终生成有效的 JSON 可以使 LLM 的输出以结构化方式轻松解析。认识到其重要性，OpenAI 引入了 JSON 模式，它约束模型始终返回有效的 JSON 对象。然而，通常需要更细粒度的控制，以确保生成的 JSON 对象符合特定的模式，例如：</p><img src="/2024/08/13/NLP%20Insights/%E4%BD%BF%E7%94%A8%E5%8E%8B%E7%BC%A9%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA%E8%BF%9B%E8%A1%8C%E6%9C%AC%E5%9C%B0%20LLM%20%E7%9A%84%E5%BF%AB%E9%80%9F%20JSON%20%E8%A7%A3%E7%A0%81/image-1.png" class="" title="图2：遵循JSON模式的约束生成示例"><p>图二展示了一个受限生成的例子，利用大语言模型（LLMs）来生成符合特定 JSON 模式的对象。在这个例子中，左侧的 JSON 模式定义了一个对象，其中包含了 name、age 和 house 三个属性，分别是字符串和整数类型。右侧则显示了受限生成的输出对象，模型通过约束生成技术，生成了符合这些属性的具体实例，如“Harry”的名字、15 岁的年龄以及属于“Gryffindor”的房子。这展示了 LLMs 在生成结构化数据时的能力，同时确保了生成内容符合预定的格式。</p><p>对于本地 LLM，有两种主要方法来引导模型生成符合特定模式的 JSON 对象。</p><h3 id="方法-1：基于有限状态机"><a href="#方法-1：基于有限状态机" class="headerlink" title="方法 1：基于有限状态机"></a>方法 1：基于有限状态机</h3><p>这种方法涉及将 JSON 模式转换为正则表达式。然后，我们可以基于正则表达式构建一个有限状态机（FSM）。FSM 用于引导 LLM 的生成。在 FSM 的每个状态中，我们可以计算允许的转换并识别可接受的下一个标记。这使我们能够在解码过程中跟踪当前状态，并通过对输出应用 logit 偏差来过滤掉无效的标记。你可以在 outlines 论文中了解更多关于这种方法的信息。</p><img src="/2024/08/13/NLP%20Insights/%E4%BD%BF%E7%94%A8%E5%8E%8B%E7%BC%A9%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA%E8%BF%9B%E8%A1%8C%E6%9C%AC%E5%9C%B0%20LLM%20%E7%9A%84%E5%BF%AB%E9%80%9F%20JSON%20%E8%A7%A3%E7%A0%81/image-2.png" class="" title="图3：基于FSM和Logits屏蔽的约束解码。在第一次约束解码过程中，仅允许age。在第二次过程中，由于正则表达式需要数字，因此允许0和1，但LLM更有可能采样1"><p>图三展示了如何利用有限状态机（FSM）来实现受限解码。在这个过程中，首先将 JSON 模式转换为正则表达式，然后利用 FSM 来引导 LLM 的生成。在图中，FSM 状态图展示了 age 字段的受限生成过程，其中只有合法的数字（如 0-9）会被允许。每个状态的转换由正则表达式的规则定义，确保生成的 JSON 数据始终有效。这种方法通过在生成过程中施加限制，来控制 LLM 生成特定的输出。</p><p>FSM 方法利用广义的正则表达式来定义低层次规则，可以应用于广泛的语法，例如 JSON 模式、IP 地址和电子邮件。</p><h4 id="限制："><a href="#限制：" class="headerlink" title="限制："></a>限制：</h4><p>由于 FSM 是在标记级别构建的，因此它只能在每一步通过一个标记来转换状态。因此，它一次只能解码一个标记，导致解码速度较慢。</p><h3 id="方法-2：基于交织"><a href="#方法-2：基于交织" class="headerlink" title="方法 2：基于交织"></a>方法 2：基于交织</h3><p>除了将整个 JSON 模式转换为正则表达式之外，另一种方法是使用基于交织的解码。在这种方法中，给定的 JSON 模式可以分解为几个部分，每个部分包含一个分块预填充部分或一个约束解码部分。这些不同的部分由推理系统交织执行。由于分块预填充可以在一个前向传递中处理多个标记，它比逐标记解码更快。</p><p>Guidance 提供了一套基于交织解码的语法规则，使用 llama.cpp 作为后端。</p><img src="/2024/08/13/NLP%20Insights/%E4%BD%BF%E7%94%A8%E5%8E%8B%E7%BC%A9%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA%E8%BF%9B%E8%A1%8C%E6%9C%AC%E5%9C%B0%20LLM%20%E7%9A%84%E5%BF%AB%E9%80%9F%20JSON%20%E8%A7%A3%E7%A0%81/image-3.png" class="" title="图4：Guidance中的交织JSON解码"><p>图四展示了 Guidance 框架中的交织语法，如何利用交织语法来进行 JSON 的解码。图中的代码片段定义了一个函数，使用 Guidance 语法生成一个包含 name、age 和 house 的 JSON 对象。交织语法通过将不同部分的解码与预填充部分交替进行，能够提高解码速度。图下方展示了这一过程的工作原理，绿色和蓝色的条形代表不同部分的处理过程，展示了交织解码在不同阶段的执行情况。</p><h4 id="限制：-1"><a href="#限制：-1" class="headerlink" title="限制："></a>限制：</h4><ul><li>基于交织的方法需要自定义语法，使其不如单个正则表达式灵活和表达力强。</li><li>由于解码和分块预填充段之间可能存在冲突，处理标记边界时存在困难。</li><li>解释器与后端之间的频繁通信带来了额外的开销。</li></ul><h3 id="我们的方法：使用压缩有限状态机的跳跃前进解码"><a href="#我们的方法：使用压缩有限状态机的跳跃前进解码" class="headerlink" title="我们的方法：使用压缩有限状态机的跳跃前进解码"></a>我们的方法：使用压缩有限状态机的跳跃前进解码</h3><p>通过引入基于压缩有限状态机的新解码算法——跳跃前进解码，我们可以结合 FSM 和交织方法的优点。</p><p>在由 JSON 模式转换的正则表达式引导的解码过程中，当我们达到特定节点时，可以预测即将到来的字符串：</p><p>在图 3 中，解码开始时，根据正则表达式，我们可以预见到接下来的字符串是：</p><figure class="highlight json"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">{</span></span><br><span class="line">  <span class="attr">"name"</span><span class="punctuation">:</span></span><br></pre></td></tr></tbody></table></figure><p>然后进入实际的解码部分。<br>同样，当 LLM 在为角色填写房子属性时输出了 G，我们可以自信地预测下一个字符串将是 ryffindor，从而完成整个字符串为 Gryffindor。</p><p>这正是跳跃前进解码算法加速解码的方式。在跳跃前进算法中，我们检查给定正则表达式的有限状态机，识别所有单一的转换边，并将连续的转换路径压缩为单一路径。我们可以直接预填充（扩展）这些单一路径，跳过逐标记解码，直到下一个分支点。</p><img src="/2024/08/13/NLP%20Insights/%E4%BD%BF%E7%94%A8%E5%8E%8B%E7%BC%A9%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA%E8%BF%9B%E8%A1%8C%E6%9C%AC%E5%9C%B0%20LLM%20%E7%9A%84%E5%BF%AB%E9%80%9F%20JSON%20%E8%A7%A3%E7%A0%81/image-4.png" class="" title="图5：跳跃前进解码与压缩FSM和普通解码的比较"><p>图五展示了跳跃前进解码与普通解码的对比。跳跃前进解码利用压缩的有限状态机，通过提前预测并预填充可能的字符串，减少了逐标记解码的次数。例如，在为 house 字段生成值时，模型在解码过程中直接跳跃并预填充了“Gryffindor”这个字符串，而无需逐字符生成。图中的流程展示了如何通过这种方法提高解码效率，同时避免了不必要的重复计算。<br>图五展示了<strong>压缩有限状态机的跳跃前进解码</strong>与<strong>普通解码</strong>的对比，特别是在生成 JSON 数据时的性能差异。为了更详细地理解这张图，我们需要分步骤分析图中的各个部分。</p><ol><li><p><strong>输入提示</strong>（左侧的绿色部分）：提示模型生成一个符合 JSON 模式的对象。这里的 JSON 对象包括“name”、“age”和“house”三个属性，分别代表名字、年龄和学院。</p></li><li><p><strong>跳跃前进解码过程</strong>（中间部分的蓝色和橙色方块）：</p><ul><li><strong>橙色方块</strong>代表需要约束解码的部分。例如，生成“name”属性时，模型通过跳跃前进解码算法可以直接生成完整的字符串“Harry”。</li><li><strong>蓝色方块</strong>代表模型在跳跃前进过程中逐字符（或逐标记）解码的部分。这种解码方式在遇到非确定性时（例如多个可能的值）才会出现。</li></ul></li><li><p><strong>普通解码过程</strong>（中间部分的蓝色方块）：普通解码需要逐字符或逐标记地生成整个 JSON 对象。相比之下，普通解码方式在处理每一个字符或标记时都需要进行预测和选择，显著降低了解码速度。</p></li><li><p><strong>对比结果</strong>（右侧部分）：</p><ul><li><strong>跳跃前进解码</strong>生成的 JSON 对象展示在最上方，这种方法通过预测并预填充可能的字符串，大大加速了解码过程。例如，在生成“Gryffindor”这个字符串时，模型直接跳过了逐字符生成的步骤。</li><li><strong>普通解码</strong>生成的 JSON 对象展示在最下方，这种方法逐字符解码，虽然能够保证生成的准确性，但效率较低，尤其是在处理长字符串或复杂结构时。</li></ul></li></ol><h3 id="详细解读："><a href="#详细解读：" class="headerlink" title="详细解读："></a>详细解读：</h3><ol><li><p><strong>跳跃前进解码的工作原理</strong>：</p><ul><li>在解码的过程中，模型使用压缩后的有限状态机（FSM）来预测和识别即将生成的字符串。如果模型能在当前上下文中准确预测出接下来要生成的字符串，那么它可以跳过这些字符串的逐标记解码，直接生成整个字符串（例如“Gryffindor”）。</li><li>这种方法利用了正则表达式的结构特点，将连续的转换路径压缩成一个单一路径，从而避免了不必要的逐标记解码步骤。</li></ul></li><li><p><strong>普通解码的限制</strong>：</p><ul><li>普通解码方法需要逐步解码每一个字符或标记，因此在处理复杂的 JSON 对象时效率较低。每一步都需要模型重新计算可能的输出，并从中选择最优解，这会大幅增加解码时间。</li></ul></li><li><p><strong>性能差异</strong>：</p><ul><li>由于跳跃前进解码减少了逐字符解码的次数，并且利用了 FSM 的压缩特性，它在时间和计算资源上的开销都显著低于普通解码。尤其在需要生成大量数据或处理复杂结构时，跳跃前进解码的优势更加明显。</li></ul></li></ol><p>SGLang 的 RadixAttention 机制极大地简化了跳跃前进解码算法的实现。当执行跳跃前进时，我们可以简单地终止当前请求并排入新请求。SGLang 运行时的 RadixAttention 和高效的扩展原语将自动重用前一组标记的 KV 缓存，从而避免冗余计算。</p><h2 id="标记边界处理"><a href="#标记边界处理" class="headerlink" title="标记边界处理"></a>标记边界处理</h2><p>在实现约束解码时，由于字符与标记之间复杂的可能映射关系，处理标记边界总是很棘手。</p><p>在 LLM 解码过程中，它可能更倾向（意味着概率更高）于将多个字符组合成一个标记。例如，在 JSON 解码的上下文中解码”Hello”时，LLM 可能会输出如下标记：<br>“ He llo “，</p><p>而不是解码最后的” ，它总是倾向于将其与后续字符组合成更常见的标记”， 这种效果可能导致一些奇怪的行为。例如，在上述情况下，如果正则表达式设置为”[\w\d\s]*“（不包含最后的”， ），这可能会导致无限解码，因为 LLM 想要停止于”，但该标记是不允许的。</p><p>此外，在跳跃前进解码过程中，我们发现对跳跃前进部分使用不同的标记策略可能会导致后续标记的 logit 分布不同。简单地将标记化的跳跃前进部分附加到当前的标记序列中可能会产生意外的结果。</p><p>为了解决这些问题，我们提出了以下解决方案：</p><ul><li>我们在跳跃前进阶段实施了重新标记化机制。这包括附加字符串而不是标记，然后重新标记整个文本。这种方法有效地解决了大多数标记化问题，并且仅导致计算开销增加约 4%。</li><li><strong>建议</strong>使用综合正则表达式引导整个解码过程，而不是使用多个连接的正则表达式。这种方法确保 FSM 和 LLM 都了解整个解码过程，从而尽量减少与边界相关的问题。<br>你还可以在这篇博客文章中阅读一些额外的讨论。</li></ul><h2 id="基准测试结果"><a href="#基准测试结果" class="headerlink" title="基准测试结果"></a>基准测试结果</h2><p>我们在两个任务上对我们的跳跃前进解码进行了基准测试：</p><ol><li>使用简短的提示生成 JSON 格式的角色数据。</li><li>从长文档中提取城市信息并以 JSON 格式输出。</li></ol><p>我们在 NVIDIA A10 GPU（24GB）上测试了 llama-7B，使用了 vllm v0.2.7，guidance v0.1.0，outlines v0.2.5 和 llama.cpp v0.2.38（Python 绑定）。下图显示了这些方法的吞吐量（使用每个系统支持的最大批次大小）和延迟（批次大小为 1）：</p><img src="/2024/08/13/NLP%20Insights/%E4%BD%BF%E7%94%A8%E5%8E%8B%E7%BC%A9%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA%E8%BF%9B%E8%A1%8C%E6%9C%AC%E5%9C%B0%20LLM%20%E7%9A%84%E5%BF%AB%E9%80%9F%20JSON%20%E8%A7%A3%E7%A0%81/image-5.png" class="" title="图6：基准测试结果"><p>结果表明，使用我们的解码算法的 SGLang 显著优于所有其他系统。它可以将延迟减少最多 2 倍，并将吞吐量提高最多 2.5 倍。在角色生成任务中，即使不使用跳跃前进的 SGLang 也比 Outlines+vLLM 实现了更高的吞吐量；我们怀疑这是由于 Outlines 中的某些开销所致。</p><h2 id="用例"><a href="#用例" class="headerlink" title="用例"></a>用例</h2><p>我们已经与 Boson.ai 测试了这个功能两周，他们正在将这个功能引入他们的生产用例中，因为它保证了更高的解码吞吐量和可靠的响应。</p><p>此外，另一位用户使用此功能通过视觉语言模型 LLaVA 从图像中提取结构化信息。</p><img src="/2024/08/13/NLP%20Insights/%E4%BD%BF%E7%94%A8%E5%8E%8B%E7%BC%A9%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA%E8%BF%9B%E8%A1%8C%E6%9C%AC%E5%9C%B0%20LLM%20%E7%9A%84%E5%BF%AB%E9%80%9F%20JSON%20%E8%A7%A3%E7%A0%81/image-6.png" class="" title="图7：使用SGLang和LLaVA从图像中提取结构化信息">]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;使用压缩有限状态机进行本地-LLM-的快速-JSON-解码&quot;&gt;&lt;a href=&quot;#使用压缩有限状态机进行本地-LLM-的快速-JSON-解码&quot; class=&quot;headerlink&quot; title=&quot;使用压缩有限状态机进行本地 LLM 的快速 JSON 解码&quot;&gt;&lt;/a</summary>
      
    
    
    
    <category term="NLP Insights" scheme="https://chenhuiyu.github.io/categories/NLP-Insights/"/>
    
    
    <category term="SGLang" scheme="https://chenhuiyu.github.io/tags/SGLang/"/>
    
    <category term="Structured LLM" scheme="https://chenhuiyu.github.io/tags/Structured-LLM/"/>
    
  </entry>
  
  <entry>
    <title>Detailed Steps for Running Fine-tuned Gemma-2-2b-it with vLLM</title>
    <link href="https://chenhuiyu.github.io/2024/08/07/NLP%20Insights/Running%20Fine-tuned%20Gemma-2-2b-it%20with%20vLLM/"/>
    <id>https://chenhuiyu.github.io/2024/08/07/NLP%20Insights/Running%20Fine-tuned%20Gemma-2-2b-it%20with%20vLLM/</id>
    <published>2024-08-07T02:30:00.000Z</published>
    <updated>2024-08-07T11:24:45.021Z</updated>
    
    <content type="html"><![CDATA[<p>In this post, I will share the steps to run the fine-tuned Gemma-2-2b-it model using vLLM. This guide will cover the installation process, environment configuration, and common troubleshooting tips.</p><h2 id="Installation-and-Verification-of-vLLM"><a href="#Installation-and-Verification-of-vLLM" class="headerlink" title="Installation and Verification of vLLM"></a>Installation and Verification of vLLM</h2><p>First, ensure that you have installed and verified vLLM version 0.5.3.</p><ol><li><p>Install vLLM:</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!pip install vllm==0.5.3</span><br></pre></td></tr></tbody></table></figure></li><li><p>Verify the installation:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> vllm</span><br><span class="line"><span class="built_in">print</span>(vllm.__version__)</span><br><span class="line"><span class="comment"># Output: 0.5.3</span></span><br></pre></td></tr></tbody></table></figure></li></ol><h2 id="Installing-Flashinfer"><a href="#Installing-Flashinfer" class="headerlink" title="Installing Flashinfer"></a>Installing Flashinfer</h2><p>Follow these steps to install Flashinfer, ensuring compatibility with your torch version and CUDA.</p><ol><li><p>Check the torch version and CUDA compatibility:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="built_in">print</span>(torch.__version__)  <span class="comment"># Should output: 2.3.1+cu121</span></span><br><span class="line"><span class="built_in">print</span>(torch.version.cuda) <span class="comment"># Should output: 12.1</span></span><br></pre></td></tr></tbody></table></figure></li><li><p>Install Flashinfer:<br>According to the documentation, Gemma runs on version 0.0.8. vLLM requires FlashInfer v0.0.8 (refer to <a href="https://github.com/vllm-project/vllm/issues/7060">vLLM Version and Flashinfer Documentation</a> for details on Gemma 2).</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!pip install flashinfer==0.0.8 -i https://flashinfer.ai/whl/cu121/torch2.3/</span><br></pre></td></tr></tbody></table></figure></li></ol><h2 id="Updating-Environment-Variables-for-vLLM-Backend"><a href="#Updating-Environment-Variables-for-vLLM-Backend" class="headerlink" title="Updating Environment Variables for vLLM Backend"></a>Updating Environment Variables for vLLM Backend</h2><p>Ensure that Flashinfer is set as the attention mechanism backend for vLLM:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">"VLLM_ATTENTION_BACKEND"</span>] = <span class="string">"FLASHINFER"</span></span><br></pre></td></tr></tbody></table></figure><h2 id="Testing-vLLM"><a href="#Testing-vLLM" class="headerlink" title="Testing vLLM"></a>Testing vLLM</h2><p>Here is the test code to generate text using vLLM:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> vllm <span class="keyword">import</span> LLM, SamplingParams</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">llm = LLM(model=<span class="string">"gemma-2-2b-model"</span>, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">sampling_params = SamplingParams(</span><br><span class="line">    temperature=<span class="number">0.8</span>,</span><br><span class="line">    max_tokens=<span class="number">512</span>,</span><br><span class="line">    top_p=<span class="number">0.95</span>,</span><br><span class="line">    top_k=<span class="number">1</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Example test data</span></span><br><span class="line">test_data = [{<span class="string">"text"</span>: <span class="string">"Input test text 1"</span>}, {<span class="string">"text"</span>: <span class="string">"Input test text 2"</span>}]</span><br><span class="line"></span><br><span class="line">prompts = [</span><br><span class="line">    test_data[random.randint(<span class="number">0</span>, <span class="built_in">len</span>(test_data) - <span class="number">1</span>)][<span class="string">"text"</span>],</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">outputs = llm.generate(</span><br><span class="line">    prompts,</span><br><span class="line">    sampling_params</span><br><span class="line">)</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><p>By following these steps, you should be able to successfully run the fine-tuned Gemma-2-2b-it model.</p><h2 id="Common-Errors-and-Solutions"><a href="#Common-Errors-and-Solutions" class="headerlink" title="Common Errors and Solutions"></a>Common Errors and Solutions</h2><p>Here are some common errors you might encounter and their solutions:</p><ol><li><p><strong>RuntimeError: <code>CHECK_EQ(paged_kv_indptr.size(0), batch_size + 1) failed. 1 vs 257</code></strong></p><ul><li><strong>Cause</strong>: Incorrect Flashinfer version.</li><li><strong>Solution</strong>: Ensure you have installed the correct version of Flashinfer.</li></ul></li><li><p><strong>TypeError: <code>'NoneType' object is not callable</code></strong></p><ul><li><strong>Cause</strong>: Flashinfer is not installed.</li><li><strong>Solution</strong>: Install Flashinfer following the steps above.</li></ul></li><li><p><strong>ValueError: <code>Please use Flashinfer backend for models with logits_soft_cap (i.e., Gemma-2). Otherwise, the output might be wrong. Set Flashinfer backend by export VLLM_ATTENTION_BACKEND=FLASHINFER.</code></strong></p><ul><li><strong>Cause</strong>: Flashinfer backend is not set.</li><li><strong>Solution</strong>: Set the environment variable <code>VLLM_ATTENTION_BACKEND</code> to <code>FLASHINFER</code>.</li></ul></li></ol><p>By following these detailed steps and solutions, you should be able to successfully run and debug the fine-tuned Gemma-2-2b-it model. If you encounter any issues, refer to the relevant documentation or seek help from the community.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;In this post, I will share the steps to run the fine-tuned Gemma-2-2b-it model using vLLM. This guide will cover the installation process</summary>
      
    
    
    
    <category term="NLP Insights" scheme="https://chenhuiyu.github.io/categories/NLP-Insights/"/>
    
    
    <category term="vLLM" scheme="https://chenhuiyu.github.io/tags/vLLM/"/>
    
    <category term="Gemma-2-2b-it" scheme="https://chenhuiyu.github.io/tags/Gemma-2-2b-it/"/>
    
  </entry>
  
  <entry>
    <title>使用vLLM运行微调后的Gemma-2</title>
    <link href="https://chenhuiyu.github.io/2024/08/07/NLP%20Insights/%E4%BD%BF%E7%94%A8vLLM%E8%BF%90%E8%A1%8C%E5%BE%AE%E8%B0%83%E5%90%8E%E7%9A%84Gemma-2/"/>
    <id>https://chenhuiyu.github.io/2024/08/07/NLP%20Insights/%E4%BD%BF%E7%94%A8vLLM%E8%BF%90%E8%A1%8C%E5%BE%AE%E8%B0%83%E5%90%8E%E7%9A%84Gemma-2/</id>
    <published>2024-08-07T02:30:00.000Z</published>
    <updated>2024-08-07T11:30:32.047Z</updated>
    
    <content type="html"><![CDATA[<h1 id="使用vLLM运行微调后的Gemma-2-2b-it的详细步骤"><a href="#使用vLLM运行微调后的Gemma-2-2b-it的详细步骤" class="headerlink" title="使用vLLM运行微调后的Gemma-2-2b-it的详细步骤"></a>使用vLLM运行微调后的Gemma-2-2b-it的详细步骤</h1><p>在这里分享一下我运行微调后的Gemma-2-2b-it模型并使用vLLM的步骤，希望对其他人有所帮助。本文将详细介绍安装过程、环境配置以及常见问题的解决方法。</p><h2 id="安装和验证vLLM"><a href="#安装和验证vLLM" class="headerlink" title="安装和验证vLLM"></a>安装和验证vLLM</h2><p>首先，确保安装并验证vLLM的版本是0.5.3。</p><ol><li><p>安装vLLM：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install vllm==0.5.3</span><br></pre></td></tr></tbody></table></figure></li><li><p>验证安装：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> vllm</span><br><span class="line"><span class="built_in">print</span>(vllm.__version__)</span><br><span class="line"><span class="comment"># 输出: 0.5.3</span></span><br></pre></td></tr></tbody></table></figure></li></ol><h2 id="安装Flashinfer"><a href="#安装Flashinfer" class="headerlink" title="安装Flashinfer"></a>安装Flashinfer</h2><p>按照以下步骤安装Flashinfer，并确保您的torch版本和CUDA兼容性。</p><ol><li><p>检查torch版本和CUDA兼容性：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="built_in">print</span>(torch.__version__)  <span class="comment"># 应输出: 2.3.1+cu121</span></span><br><span class="line"><span class="built_in">print</span>(torch.version.cuda) <span class="comment"># 应输出: 12.1</span></span><br></pre></td></tr></tbody></table></figure></li><li><p>安装Flashinfer：<br>根据文档，Gemma运行在版本0.08。vLLM需要FlashInfer v0.0.8（请参阅<a href="https://github.com/vllm-project/vllm/issues/7060">vLLM版本和Flashinfer文档</a>中关于Gemma 2的部分）。</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!pip install flashinfer==0.0.8 -i https://flashinfer.ai/whl/cu121/torch2.3/</span><br></pre></td></tr></tbody></table></figure></li></ol><h2 id="更新环境中的VLLM后端变量"><a href="#更新环境中的VLLM后端变量" class="headerlink" title="更新环境中的VLLM后端变量"></a>更新环境中的VLLM后端变量</h2><p>确保设置Flashinfer为vLLM的注意力机制后端：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">"VLLM_ATTENTION_BACKEND"</span>] = <span class="string">"FLASHINFER"</span></span><br></pre></td></tr></tbody></table></figure><h2 id="测试vLLM"><a href="#测试vLLM" class="headerlink" title="测试vLLM"></a>测试vLLM</h2><p>以下是使用vLLM生成文本的测试代码：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> vllm <span class="keyword">import</span> LLM, SamplingParams</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">llm = LLM(model=<span class="string">"gemma-2-2b-model"</span>, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">sampling_params = SamplingParams(</span><br><span class="line">    temperature=<span class="number">0.8</span>,</span><br><span class="line">    max_tokens=<span class="number">512</span>,</span><br><span class="line">    top_p=<span class="number">0.95</span>,</span><br><span class="line">    top_k=<span class="number">1</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例测试数据</span></span><br><span class="line">test_data = [{<span class="string">"text"</span>: <span class="string">"输入测试文本1"</span>}, {<span class="string">"text"</span>: <span class="string">"输入测试文本2"</span>}]</span><br><span class="line"></span><br><span class="line">prompts = [</span><br><span class="line">    test_data[random.randint(<span class="number">0</span>, <span class="built_in">len</span>(test_data) - <span class="number">1</span>)][<span class="string">"text"</span>],</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">outputs = llm.generate(</span><br><span class="line">    prompts,</span><br><span class="line">    sampling_params</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预期输出:</span></span><br><span class="line"><span class="comment"># Processed prompts: 100%|██████████| 1/1 [00:01&lt;00:00,  1.24s/it, est. speed input: 991.44 toks/s, output: 87.79 toks/s]</span></span><br></pre></td></tr></tbody></table></figure><p>通过上述步骤，您应该能够成功运行微调后的Gemma-2-2b-it模型。</p><h2 id="常见错误及解决方法"><a href="#常见错误及解决方法" class="headerlink" title="常见错误及解决方法"></a>常见错误及解决方法</h2><p>在运行过程中，可能会遇到以下常见错误：</p><ol><li><p><strong>RuntimeError: <code>CHECK_EQ(paged_kv_indptr.size(0), batch_size + 1) failed. 1 vs 257</code></strong></p><ul><li><strong>原因</strong>：Flashinfer版本错误。</li><li><strong>解决方法</strong>：请确保安装了正确版本的Flashinfer。</li></ul></li><li><p><strong>TypeError: <code>'NoneType' object is not callable</code></strong></p><ul><li><strong>原因</strong>：没有安装Flashinfer。</li><li><strong>解决方法</strong>：按照上述步骤安装Flashinfer。</li></ul></li><li><p><strong>ValueError: <code>Please use Flashinfer backend for models with logits_soft_cap (i.e., Gemma-2). Otherwise, the output might be wrong. Set Flashinfer backend by export VLLM_ATTENTION_BACKEND=FLASHINFER.</code></strong></p><ul><li><strong>原因</strong>：未设置Flashinfer后端。</li><li><strong>解决方法</strong>：设置环境变量<code>VLLM_ATTENTION_BACKEND</code>为<code>FLASHINFER</code>。</li></ul></li></ol><p>通过上述详细步骤和解决方法，您应该能够成功运行并调试微调后的Gemma-2-2b-it模型。如果您在任何一步遇到问题，请参考相应的文档或在社区中寻求帮助。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;使用vLLM运行微调后的Gemma-2-2b-it的详细步骤&quot;&gt;&lt;a href=&quot;#使用vLLM运行微调后的Gemma-2-2b-it的详细步骤&quot; class=&quot;headerlink&quot; title=&quot;使用vLLM运行微调后的Gemma-2-2b-it的详细步骤&quot;&gt;</summary>
      
    
    
    
    <category term="NLP Insights" scheme="https://chenhuiyu.github.io/categories/NLP-Insights/"/>
    
    
    <category term="vLLM" scheme="https://chenhuiyu.github.io/tags/vLLM/"/>
    
    <category term="Gemma-2" scheme="https://chenhuiyu.github.io/tags/Gemma-2/"/>
    
  </entry>
  
</feed>
