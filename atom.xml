<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>黑头呆鱼进化之旅</title>
  
  <subtitle>只身打码过草原</subtitle>
  <link href="https://chenhuiyu.github.io/atom.xml" rel="self"/>
  
  <link href="https://chenhuiyu.github.io/"/>
  <updated>2024-08-13T08:25:43.392Z</updated>
  <id>https://chenhuiyu.github.io/</id>
  
  <author>
    <name>Huiyu Chen</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>使用压缩有限状态机进行本地 LLM 的快速 JSON 解码</title>
    <link href="https://chenhuiyu.github.io/2024/08/13/NLP%20Insights/%E4%BD%BF%E7%94%A8%E5%8E%8B%E7%BC%A9%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA%E8%BF%9B%E8%A1%8C%E6%9C%AC%E5%9C%B0%20LLM%20%E7%9A%84%E5%BF%AB%E9%80%9F%20JSON%20%E8%A7%A3%E7%A0%81/"/>
    <id>https://chenhuiyu.github.io/2024/08/13/NLP%20Insights/%E4%BD%BF%E7%94%A8%E5%8E%8B%E7%BC%A9%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA%E8%BF%9B%E8%A1%8C%E6%9C%AC%E5%9C%B0%20LLM%20%E7%9A%84%E5%BF%AB%E9%80%9F%20JSON%20%E8%A7%A3%E7%A0%81/</id>
    <published>2024-08-13T08:12:10.000Z</published>
    <updated>2024-08-13T08:25:43.392Z</updated>
    
    <content type="html"><![CDATA[<h2 id="使用压缩有限状态机进行本地-LLM-的快速-JSON-解码"><a href="#使用压缩有限状态机进行本地-LLM-的快速-JSON-解码" class="headerlink" title="使用压缩有限状态机进行本地 LLM 的快速 JSON 解码"></a>使用压缩有限状态机进行本地 LLM 的快速 JSON 解码</h2><p><strong>作者</strong>: Liangsheng Yin, Ying Sheng, Lianmin Zheng<br><strong>日期</strong>: 2024 年 2 月 5 日</p><hr><p>本文内容基于 LMSYS Org 发布的一篇博客文章，原文链接：<a href="https://lmsys.org/blog/2024-02-05-compressed-fsm/">LMSYS Org 博客</a>。相关的代码库可以在以下链接找到：<a href="https://github.com/sgl-project/sglang/tree/main?tab=readme-ov-file#json-decoding">SGLang 代码库</a>。</p><p>让一个 LLM 始终生成符合特定模式的有效 JSON 或 YAML，对于许多应用来说是一个关键特性。在这篇博客文章中，我们介绍了一种显著加速这种约束解码的优化方法。我们的方法利用了压缩的有限状态机，并且兼容任何正则表达式，因此可以适用于任何 JSON 或 YAML 模式。与现有系统逐步解码一个标记的方式不同，我们的方法分析了正则表达式的有限状态机，压缩了单一的转换路径，并在可能的情况下一次性解码多个标记。与最先进的系统（guidance + llama.cpp，outlines + vLLM）相比，我们的方法可以将延迟减少最多 2 倍，并提高吞吐量最多 2.5 倍。这一优化还使得约束解码比普通解码更快。你可以在 SGLang 上试用它。</p><img src="/2024/08/13/NLP%20Insights/%E4%BD%BF%E7%94%A8%E5%8E%8B%E7%BC%A9%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA%E8%BF%9B%E8%A1%8C%E6%9C%AC%E5%9C%B0%20LLM%20%E7%9A%84%E5%BF%AB%E9%80%9F%20JSON%20%E8%A7%A3%E7%A0%81/image.png" class="" title="图1：SGLang和Outlines + vLLM在JSON解码中的比较"><p>图一展示了 SGLang 和 Outlines + vLLM 在 JSON 解码任务中的性能比较。这是一个动态对比，目的是展示两者在相同任务下的速度差异。SGLang 采用了一种新的跳跃前进解码算法，通过压缩有限状态机来加速解码过程。相比之下，Outlines + vLLM 使用了传统的逐步解码方法。图中的动画演示了 SGLang 在处理多字符（或标记）解码时的优势，显著减少了解码时间。</p><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>JSON 是数据交换中最重要的格式之一。要求 LLM 始终生成有效的 JSON 可以使 LLM 的输出以结构化方式轻松解析。认识到其重要性，OpenAI 引入了 JSON 模式，它约束模型始终返回有效的 JSON 对象。然而，通常需要更细粒度的控制，以确保生成的 JSON 对象符合特定的模式，例如：</p><p><img src="/image-1.png" alt="图2：遵循JSON模式的约束生成示例"><br>图二展示了一个受限生成的例子，利用大语言模型（LLMs）来生成符合特定 JSON 模式的对象。在这个例子中，左侧的 JSON 模式定义了一个对象，其中包含了 name、age 和 house 三个属性，分别是字符串和整数类型。右侧则显示了受限生成的输出对象，模型通过约束生成技术，生成了符合这些属性的具体实例，如“Harry”的名字、15 岁的年龄以及属于“Gryffindor”的房子。这展示了 LLMs 在生成结构化数据时的能力，同时确保了生成内容符合预定的格式。</p><p>对于本地 LLM，有两种主要方法来引导模型生成符合特定模式的 JSON 对象。</p><h3 id="方法-1：基于有限状态机"><a href="#方法-1：基于有限状态机" class="headerlink" title="方法 1：基于有限状态机"></a>方法 1：基于有限状态机</h3><p>这种方法涉及将 JSON 模式转换为正则表达式。然后，我们可以基于正则表达式构建一个有限状态机（FSM）。FSM 用于引导 LLM 的生成。在 FSM 的每个状态中，我们可以计算允许的转换并识别可接受的下一个标记。这使我们能够在解码过程中跟踪当前状态，并通过对输出应用 logit 偏差来过滤掉无效的标记。你可以在 outlines 论文中了解更多关于这种方法的信息。</p><p><img src="/image-2.png" alt="图3：基于FSM和Logits屏蔽的约束解码。在第一次约束解码过程中，仅允许age。在第二次过程中，由于正则表达式需要数字，因此允许0和1，但LLM更有可能采样1"><br>图三展示了如何利用有限状态机（FSM）来实现受限解码。在这个过程中，首先将 JSON 模式转换为正则表达式，然后利用 FSM 来引导 LLM 的生成。在图中，FSM 状态图展示了 age 字段的受限生成过程，其中只有合法的数字（如 0-9）会被允许。每个状态的转换由正则表达式的规则定义，确保生成的 JSON 数据始终有效。这种方法通过在生成过程中施加限制，来控制 LLM 生成特定的输出。</p><p>FSM 方法利用广义的正则表达式来定义低层次规则，可以应用于广泛的语法，例如 JSON 模式、IP 地址和电子邮件。</p><h4 id="限制："><a href="#限制：" class="headerlink" title="限制："></a>限制：</h4><p>由于 FSM 是在标记级别构建的，因此它只能在每一步通过一个标记来转换状态。因此，它一次只能解码一个标记，导致解码速度较慢。</p><h3 id="方法-2：基于交织"><a href="#方法-2：基于交织" class="headerlink" title="方法 2：基于交织"></a>方法 2：基于交织</h3><p>除了将整个 JSON 模式转换为正则表达式之外，另一种方法是使用基于交织的解码。在这种方法中，给定的 JSON 模式可以分解为几个部分，每个部分包含一个分块预填充部分或一个约束解码部分。这些不同的部分由推理系统交织执行。由于分块预填充可以在一个前向传递中处理多个标记，它比逐标记解码更快。</p><p>Guidance 提供了一套基于交织解码的语法规则，使用 llama.cpp 作为后端。</p><p><img src="/image-3.png" alt="图4：Guidance中的交织JSON解码"><br>图四展示了 Guidance 框架中的交织语法，如何利用交织语法来进行 JSON 的解码。图中的代码片段定义了一个函数，使用 Guidance 语法生成一个包含 name、age 和 house 的 JSON 对象。交织语法通过将不同部分的解码与预填充部分交替进行，能够提高解码速度。图下方展示了这一过程的工作原理，绿色和蓝色的条形代表不同部分的处理过程，展示了交织解码在不同阶段的执行情况。</p><h4 id="限制：-1"><a href="#限制：-1" class="headerlink" title="限制："></a>限制：</h4><ul><li>基于交织的方法需要自定义语法，使其不如单个正则表达式灵活和表达力强。</li><li>由于解码和分块预填充段之间可能存在冲突，处理标记边界时存在困难。</li><li>解释器与后端之间的频繁通信带来了额外的开销。</li></ul><h3 id="我们的方法：使用压缩有限状态机的跳跃前进解码"><a href="#我们的方法：使用压缩有限状态机的跳跃前进解码" class="headerlink" title="我们的方法：使用压缩有限状态机的跳跃前进解码"></a>我们的方法：使用压缩有限状态机的跳跃前进解码</h3><p>通过引入基于压缩有限状态机的新解码算法——跳跃前进解码，我们可以结合 FSM 和交织方法的优点。</p><p>在由 JSON 模式转换的正则表达式引导的解码过程中，当我们达到特定节点时，可以预测即将到来的字符串：</p><p>在图 3 中，解码开始时，根据正则表达式，我们可以预见到接下来的字符串是：</p><figure class="highlight json"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">{</span></span><br><span class="line">  <span class="attr">"name"</span><span class="punctuation">:</span></span><br></pre></td></tr></tbody></table></figure><p>然后进入实际的解码部分。<br>同样，当 LLM 在为角色填写房子属性时输出了 G，我们可以自信地预测下一个字符串将是 ryffindor，从而完成整个字符串为 Gryffindor。</p><p>这正是跳跃前进解码算法加速解码的方式。在跳跃前进算法中，我们检查给定正则表达式的有限状态机，识别所有单一的转换边，并将连续的转换路径压缩为单一路径。我们可以直接预填充（扩展）这些单一路径，跳过逐标记解码，直到下一个分支点。</p><p><img src="/image-4.png" alt="图5：跳跃前进解码与压缩FSM和普通解码的比较"><br>图五展示了跳跃前进解码与普通解码的对比。跳跃前进解码利用压缩的有限状态机，通过提前预测并预填充可能的字符串，减少了逐标记解码的次数。例如，在为 house 字段生成值时，模型在解码过程中直接跳跃并预填充了“Gryffindor”这个字符串，而无需逐字符生成。图中的流程展示了如何通过这种方法提高解码效率，同时避免了不必要的重复计算。<br>图五展示了<strong>压缩有限状态机的跳跃前进解码</strong>与<strong>普通解码</strong>的对比，特别是在生成 JSON 数据时的性能差异。为了更详细地理解这张图，我们需要分步骤分析图中的各个部分。</p><ol><li><p><strong>输入提示</strong>（左侧的绿色部分）：提示模型生成一个符合 JSON 模式的对象。这里的 JSON 对象包括“name”、“age”和“house”三个属性，分别代表名字、年龄和学院。</p></li><li><p><strong>跳跃前进解码过程</strong>（中间部分的蓝色和橙色方块）：</p><ul><li><strong>橙色方块</strong>代表需要约束解码的部分。例如，生成“name”属性时，模型通过跳跃前进解码算法可以直接生成完整的字符串“Harry”。</li><li><strong>蓝色方块</strong>代表模型在跳跃前进过程中逐字符（或逐标记）解码的部分。这种解码方式在遇到非确定性时（例如多个可能的值）才会出现。</li></ul></li><li><p><strong>普通解码过程</strong>（中间部分的蓝色方块）：普通解码需要逐字符或逐标记地生成整个 JSON 对象。相比之下，普通解码方式在处理每一个字符或标记时都需要进行预测和选择，显著降低了解码速度。</p></li><li><p><strong>对比结果</strong>（右侧部分）：</p><ul><li><strong>跳跃前进解码</strong>生成的 JSON 对象展示在最上方，这种方法通过预测并预填充可能的字符串，大大加速了解码过程。例如，在生成“Gryffindor”这个字符串时，模型直接跳过了逐字符生成的步骤。</li><li><strong>普通解码</strong>生成的 JSON 对象展示在最下方，这种方法逐字符解码，虽然能够保证生成的准确性，但效率较低，尤其是在处理长字符串或复杂结构时。</li></ul></li></ol><h3 id="详细解读："><a href="#详细解读：" class="headerlink" title="详细解读："></a>详细解读：</h3><ol><li><p><strong>跳跃前进解码的工作原理</strong>：</p><ul><li>在解码的过程中，模型使用压缩后的有限状态机（FSM）来预测和识别即将生成的字符串。如果模型能在当前上下文中准确预测出接下来要生成的字符串，那么它可以跳过这些字符串的逐标记解码，直接生成整个字符串（例如“Gryffindor”）。</li><li>这种方法利用了正则表达式的结构特点，将连续的转换路径压缩成一个单一路径，从而避免了不必要的逐标记解码步骤。</li></ul></li><li><p><strong>普通解码的限制</strong>：</p><ul><li>普通解码方法需要逐步解码每一个字符或标记，因此在处理复杂的 JSON 对象时效率较低。每一步都需要模型重新计算可能的输出，并从中选择最优解，这会大幅增加解码时间。</li></ul></li><li><p><strong>性能差异</strong>：</p><ul><li>由于跳跃前进解码减少了逐字符解码的次数，并且利用了 FSM 的压缩特性，它在时间和计算资源上的开销都显著低于普通解码。尤其在需要生成大量数据或处理复杂结构时，跳跃前进解码的优势更加明显。</li></ul></li></ol><p>SGLang 的 RadixAttention 机制极大地简化了跳跃前进解码算法的实现。当执行跳跃前进时，我们可以简单地终止当前请求并排入新请求。SGLang 运行时的 RadixAttention 和高效的扩展原语将自动重用前一组标记的 KV 缓存，从而避免冗余计算。</p><h2 id="标记边界处理"><a href="#标记边界处理" class="headerlink" title="标记边界处理"></a>标记边界处理</h2><p>在实现约束解码时，由于字符与标记之间复杂的可能映射关系，处理标记边界总是很棘手。</p><p>在 LLM 解码过程中，它可能更倾向（意味着概率更高）于将多个字符组合成一个标记。例如，在 JSON 解码的上下文中解码”Hello”时，LLM 可能会输出如下标记：<br>“ He llo “，</p><p>而不是解码最后的” ，它总是倾向于将其与后续字符组合成更常见的标记”， 这种效果可能导致一些奇怪的行为。例如，在上述情况下，如果正则表达式设置为”[\w\d\s]*“（不包含最后的”， ），这可能会导致无限解码，因为 LLM 想要停止于”，但该标记是不允许的。</p><p>此外，在跳跃前进解码过程中，我们发现对跳跃前进部分使用不同的标记策略可能会导致后续标记的 logit 分布不同。简单地将标记化的跳跃前进部分附加到当前的标记序列中可能会产生意外的结果。</p><p>为了解决这些问题，我们提出了以下解决方案：</p><ul><li>我们在跳跃前进阶段实施了重新标记化机制。这包括附加字符串而不是标记，然后重新标记整个文本。这种方法有效地解决了大多数标记化问题，并且仅导致计算开销增加约 4%。</li><li><strong>建议</strong>使用综合正则表达式引导整个解码过程，而不是使用多个连接的正则表达式。这种方法确保 FSM 和 LLM 都了解整个解码过程，从而尽量减少与边界相关的问题。<br>你还可以在这篇博客文章中阅读一些额外的讨论。</li></ul><h2 id="基准测试结果"><a href="#基准测试结果" class="headerlink" title="基准测试结果"></a>基准测试结果</h2><p>我们在两个任务上对我们的跳跃前进解码进行了基准测试：</p><ol><li>使用简短的提示生成 JSON 格式的角色数据。</li><li>从长文档中提取城市信息并以 JSON 格式输出。</li></ol><p>我们在 NVIDIA A10 GPU（24GB）上测试了 llama-7B，使用了 vllm v0.2.7，guidance v0.1.0，outlines v0.2.5 和 llama.cpp v0.2.38（Python 绑定）。下图显示了这些方法的吞吐量（使用每个系统支持的最大批次大小）和延迟（批次大小为 1）：</p><p><img src="/image-5.png" alt="图6：基准测试结果"></p><p>结果表明，使用我们的解码算法的 SGLang 显著优于所有其他系统。它可以将延迟减少最多 2 倍，并将吞吐量提高最多 2.5 倍。在角色生成任务中，即使不使用跳跃前进的 SGLang 也比 Outlines+vLLM 实现了更高的吞吐量；我们怀疑这是由于 Outlines 中的某些开销所致。</p><h2 id="用例"><a href="#用例" class="headerlink" title="用例"></a>用例</h2><p>我们已经与 Boson.ai 测试了这个功能两周，他们正在将这个功能引入他们的生产用例中，因为它保证了更高的解码吞吐量和可靠的响应。</p><p>此外，另一位用户使用此功能通过视觉语言模型 LLaVA 从图像中提取结构化信息。</p><p><img src="/image-6.png" alt="图7：使用SGLang和LLaVA从图像中提取结构化信息"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;使用压缩有限状态机进行本地-LLM-的快速-JSON-解码&quot;&gt;&lt;a href=&quot;#使用压缩有限状态机进行本地-LLM-的快速-JSON-解码&quot; class=&quot;headerlink&quot; title=&quot;使用压缩有限状态机进行本地 LLM 的快速 JSON 解码&quot;&gt;&lt;/a</summary>
      
    
    
    
    <category term="NLP Insights" scheme="https://chenhuiyu.github.io/categories/NLP-Insights/"/>
    
    
    <category term="SGLang" scheme="https://chenhuiyu.github.io/tags/SGLang/"/>
    
    <category term="Structured LLM" scheme="https://chenhuiyu.github.io/tags/Structured-LLM/"/>
    
  </entry>
  
  <entry>
    <title>Detailed Steps for Running Fine-tuned Gemma-2-2b-it with vLLM</title>
    <link href="https://chenhuiyu.github.io/2024/08/07/NLP%20Insights/Running%20Fine-tuned%20Gemma-2-2b-it%20with%20vLLM/"/>
    <id>https://chenhuiyu.github.io/2024/08/07/NLP%20Insights/Running%20Fine-tuned%20Gemma-2-2b-it%20with%20vLLM/</id>
    <published>2024-08-07T02:30:00.000Z</published>
    <updated>2024-08-07T11:24:45.021Z</updated>
    
    <content type="html"><![CDATA[<p>In this post, I will share the steps to run the fine-tuned Gemma-2-2b-it model using vLLM. This guide will cover the installation process, environment configuration, and common troubleshooting tips.</p><h2 id="Installation-and-Verification-of-vLLM"><a href="#Installation-and-Verification-of-vLLM" class="headerlink" title="Installation and Verification of vLLM"></a>Installation and Verification of vLLM</h2><p>First, ensure that you have installed and verified vLLM version 0.5.3.</p><ol><li><p>Install vLLM:</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!pip install vllm==0.5.3</span><br></pre></td></tr></tbody></table></figure></li><li><p>Verify the installation:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> vllm</span><br><span class="line"><span class="built_in">print</span>(vllm.__version__)</span><br><span class="line"><span class="comment"># Output: 0.5.3</span></span><br></pre></td></tr></tbody></table></figure></li></ol><h2 id="Installing-Flashinfer"><a href="#Installing-Flashinfer" class="headerlink" title="Installing Flashinfer"></a>Installing Flashinfer</h2><p>Follow these steps to install Flashinfer, ensuring compatibility with your torch version and CUDA.</p><ol><li><p>Check the torch version and CUDA compatibility:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="built_in">print</span>(torch.__version__)  <span class="comment"># Should output: 2.3.1+cu121</span></span><br><span class="line"><span class="built_in">print</span>(torch.version.cuda) <span class="comment"># Should output: 12.1</span></span><br></pre></td></tr></tbody></table></figure></li><li><p>Install Flashinfer:<br>According to the documentation, Gemma runs on version 0.0.8. vLLM requires FlashInfer v0.0.8 (refer to <a href="https://github.com/vllm-project/vllm/issues/7060">vLLM Version and Flashinfer Documentation</a> for details on Gemma 2).</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!pip install flashinfer==0.0.8 -i https://flashinfer.ai/whl/cu121/torch2.3/</span><br></pre></td></tr></tbody></table></figure></li></ol><h2 id="Updating-Environment-Variables-for-vLLM-Backend"><a href="#Updating-Environment-Variables-for-vLLM-Backend" class="headerlink" title="Updating Environment Variables for vLLM Backend"></a>Updating Environment Variables for vLLM Backend</h2><p>Ensure that Flashinfer is set as the attention mechanism backend for vLLM:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">"VLLM_ATTENTION_BACKEND"</span>] = <span class="string">"FLASHINFER"</span></span><br></pre></td></tr></tbody></table></figure><h2 id="Testing-vLLM"><a href="#Testing-vLLM" class="headerlink" title="Testing vLLM"></a>Testing vLLM</h2><p>Here is the test code to generate text using vLLM:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> vllm <span class="keyword">import</span> LLM, SamplingParams</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">llm = LLM(model=<span class="string">"gemma-2-2b-model"</span>, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">sampling_params = SamplingParams(</span><br><span class="line">    temperature=<span class="number">0.8</span>,</span><br><span class="line">    max_tokens=<span class="number">512</span>,</span><br><span class="line">    top_p=<span class="number">0.95</span>,</span><br><span class="line">    top_k=<span class="number">1</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Example test data</span></span><br><span class="line">test_data = [{<span class="string">"text"</span>: <span class="string">"Input test text 1"</span>}, {<span class="string">"text"</span>: <span class="string">"Input test text 2"</span>}]</span><br><span class="line"></span><br><span class="line">prompts = [</span><br><span class="line">    test_data[random.randint(<span class="number">0</span>, <span class="built_in">len</span>(test_data) - <span class="number">1</span>)][<span class="string">"text"</span>],</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">outputs = llm.generate(</span><br><span class="line">    prompts,</span><br><span class="line">    sampling_params</span><br><span class="line">)</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><p>By following these steps, you should be able to successfully run the fine-tuned Gemma-2-2b-it model.</p><h2 id="Common-Errors-and-Solutions"><a href="#Common-Errors-and-Solutions" class="headerlink" title="Common Errors and Solutions"></a>Common Errors and Solutions</h2><p>Here are some common errors you might encounter and their solutions:</p><ol><li><p><strong>RuntimeError: <code>CHECK_EQ(paged_kv_indptr.size(0), batch_size + 1) failed. 1 vs 257</code></strong></p><ul><li><strong>Cause</strong>: Incorrect Flashinfer version.</li><li><strong>Solution</strong>: Ensure you have installed the correct version of Flashinfer.</li></ul></li><li><p><strong>TypeError: <code>'NoneType' object is not callable</code></strong></p><ul><li><strong>Cause</strong>: Flashinfer is not installed.</li><li><strong>Solution</strong>: Install Flashinfer following the steps above.</li></ul></li><li><p><strong>ValueError: <code>Please use Flashinfer backend for models with logits_soft_cap (i.e., Gemma-2). Otherwise, the output might be wrong. Set Flashinfer backend by export VLLM_ATTENTION_BACKEND=FLASHINFER.</code></strong></p><ul><li><strong>Cause</strong>: Flashinfer backend is not set.</li><li><strong>Solution</strong>: Set the environment variable <code>VLLM_ATTENTION_BACKEND</code> to <code>FLASHINFER</code>.</li></ul></li></ol><p>By following these detailed steps and solutions, you should be able to successfully run and debug the fine-tuned Gemma-2-2b-it model. If you encounter any issues, refer to the relevant documentation or seek help from the community.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;In this post, I will share the steps to run the fine-tuned Gemma-2-2b-it model using vLLM. This guide will cover the installation process</summary>
      
    
    
    
    <category term="NLP Insights" scheme="https://chenhuiyu.github.io/categories/NLP-Insights/"/>
    
    
    <category term="vLLM" scheme="https://chenhuiyu.github.io/tags/vLLM/"/>
    
    <category term="Gemma-2-2b-it" scheme="https://chenhuiyu.github.io/tags/Gemma-2-2b-it/"/>
    
  </entry>
  
  <entry>
    <title>使用vLLM运行微调后的Gemma-2</title>
    <link href="https://chenhuiyu.github.io/2024/08/07/NLP%20Insights/%E4%BD%BF%E7%94%A8vLLM%E8%BF%90%E8%A1%8C%E5%BE%AE%E8%B0%83%E5%90%8E%E7%9A%84Gemma-2/"/>
    <id>https://chenhuiyu.github.io/2024/08/07/NLP%20Insights/%E4%BD%BF%E7%94%A8vLLM%E8%BF%90%E8%A1%8C%E5%BE%AE%E8%B0%83%E5%90%8E%E7%9A%84Gemma-2/</id>
    <published>2024-08-07T02:30:00.000Z</published>
    <updated>2024-08-07T11:30:32.047Z</updated>
    
    <content type="html"><![CDATA[<h1 id="使用vLLM运行微调后的Gemma-2-2b-it的详细步骤"><a href="#使用vLLM运行微调后的Gemma-2-2b-it的详细步骤" class="headerlink" title="使用vLLM运行微调后的Gemma-2-2b-it的详细步骤"></a>使用vLLM运行微调后的Gemma-2-2b-it的详细步骤</h1><p>在这里分享一下我运行微调后的Gemma-2-2b-it模型并使用vLLM的步骤，希望对其他人有所帮助。本文将详细介绍安装过程、环境配置以及常见问题的解决方法。</p><h2 id="安装和验证vLLM"><a href="#安装和验证vLLM" class="headerlink" title="安装和验证vLLM"></a>安装和验证vLLM</h2><p>首先，确保安装并验证vLLM的版本是0.5.3。</p><ol><li><p>安装vLLM：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install vllm==0.5.3</span><br></pre></td></tr></tbody></table></figure></li><li><p>验证安装：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> vllm</span><br><span class="line"><span class="built_in">print</span>(vllm.__version__)</span><br><span class="line"><span class="comment"># 输出: 0.5.3</span></span><br></pre></td></tr></tbody></table></figure></li></ol><h2 id="安装Flashinfer"><a href="#安装Flashinfer" class="headerlink" title="安装Flashinfer"></a>安装Flashinfer</h2><p>按照以下步骤安装Flashinfer，并确保您的torch版本和CUDA兼容性。</p><ol><li><p>检查torch版本和CUDA兼容性：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="built_in">print</span>(torch.__version__)  <span class="comment"># 应输出: 2.3.1+cu121</span></span><br><span class="line"><span class="built_in">print</span>(torch.version.cuda) <span class="comment"># 应输出: 12.1</span></span><br></pre></td></tr></tbody></table></figure></li><li><p>安装Flashinfer：<br>根据文档，Gemma运行在版本0.08。vLLM需要FlashInfer v0.0.8（请参阅<a href="https://github.com/vllm-project/vllm/issues/7060">vLLM版本和Flashinfer文档</a>中关于Gemma 2的部分）。</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!pip install flashinfer==0.0.8 -i https://flashinfer.ai/whl/cu121/torch2.3/</span><br></pre></td></tr></tbody></table></figure></li></ol><h2 id="更新环境中的VLLM后端变量"><a href="#更新环境中的VLLM后端变量" class="headerlink" title="更新环境中的VLLM后端变量"></a>更新环境中的VLLM后端变量</h2><p>确保设置Flashinfer为vLLM的注意力机制后端：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">"VLLM_ATTENTION_BACKEND"</span>] = <span class="string">"FLASHINFER"</span></span><br></pre></td></tr></tbody></table></figure><h2 id="测试vLLM"><a href="#测试vLLM" class="headerlink" title="测试vLLM"></a>测试vLLM</h2><p>以下是使用vLLM生成文本的测试代码：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> vllm <span class="keyword">import</span> LLM, SamplingParams</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">llm = LLM(model=<span class="string">"gemma-2-2b-model"</span>, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">sampling_params = SamplingParams(</span><br><span class="line">    temperature=<span class="number">0.8</span>,</span><br><span class="line">    max_tokens=<span class="number">512</span>,</span><br><span class="line">    top_p=<span class="number">0.95</span>,</span><br><span class="line">    top_k=<span class="number">1</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例测试数据</span></span><br><span class="line">test_data = [{<span class="string">"text"</span>: <span class="string">"输入测试文本1"</span>}, {<span class="string">"text"</span>: <span class="string">"输入测试文本2"</span>}]</span><br><span class="line"></span><br><span class="line">prompts = [</span><br><span class="line">    test_data[random.randint(<span class="number">0</span>, <span class="built_in">len</span>(test_data) - <span class="number">1</span>)][<span class="string">"text"</span>],</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">outputs = llm.generate(</span><br><span class="line">    prompts,</span><br><span class="line">    sampling_params</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预期输出:</span></span><br><span class="line"><span class="comment"># Processed prompts: 100%|██████████| 1/1 [00:01&lt;00:00,  1.24s/it, est. speed input: 991.44 toks/s, output: 87.79 toks/s]</span></span><br></pre></td></tr></tbody></table></figure><p>通过上述步骤，您应该能够成功运行微调后的Gemma-2-2b-it模型。</p><h2 id="常见错误及解决方法"><a href="#常见错误及解决方法" class="headerlink" title="常见错误及解决方法"></a>常见错误及解决方法</h2><p>在运行过程中，可能会遇到以下常见错误：</p><ol><li><p><strong>RuntimeError: <code>CHECK_EQ(paged_kv_indptr.size(0), batch_size + 1) failed. 1 vs 257</code></strong></p><ul><li><strong>原因</strong>：Flashinfer版本错误。</li><li><strong>解决方法</strong>：请确保安装了正确版本的Flashinfer。</li></ul></li><li><p><strong>TypeError: <code>'NoneType' object is not callable</code></strong></p><ul><li><strong>原因</strong>：没有安装Flashinfer。</li><li><strong>解决方法</strong>：按照上述步骤安装Flashinfer。</li></ul></li><li><p><strong>ValueError: <code>Please use Flashinfer backend for models with logits_soft_cap (i.e., Gemma-2). Otherwise, the output might be wrong. Set Flashinfer backend by export VLLM_ATTENTION_BACKEND=FLASHINFER.</code></strong></p><ul><li><strong>原因</strong>：未设置Flashinfer后端。</li><li><strong>解决方法</strong>：设置环境变量<code>VLLM_ATTENTION_BACKEND</code>为<code>FLASHINFER</code>。</li></ul></li></ol><p>通过上述详细步骤和解决方法，您应该能够成功运行并调试微调后的Gemma-2-2b-it模型。如果您在任何一步遇到问题，请参考相应的文档或在社区中寻求帮助。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;使用vLLM运行微调后的Gemma-2-2b-it的详细步骤&quot;&gt;&lt;a href=&quot;#使用vLLM运行微调后的Gemma-2-2b-it的详细步骤&quot; class=&quot;headerlink&quot; title=&quot;使用vLLM运行微调后的Gemma-2-2b-it的详细步骤&quot;&gt;</summary>
      
    
    
    
    <category term="NLP Insights" scheme="https://chenhuiyu.github.io/categories/NLP-Insights/"/>
    
    
    <category term="vLLM" scheme="https://chenhuiyu.github.io/tags/vLLM/"/>
    
    <category term="Gemma-2" scheme="https://chenhuiyu.github.io/tags/Gemma-2/"/>
    
  </entry>
  
  <entry>
    <title>如何准确计算固定长度模型的困惑度（PPL）</title>
    <link href="https://chenhuiyu.github.io/2024/04/17/NLP%20Insights/%20%E5%9B%B0%E6%83%91%E5%BA%A6(PPL)/"/>
    <id>https://chenhuiyu.github.io/2024/04/17/NLP%20Insights/%20%E5%9B%B0%E6%83%91%E5%BA%A6(PPL)/</id>
    <published>2024-04-17T04:00:00.000Z</published>
    <updated>2024-04-17T09:09:36.393Z</updated>
    
    <content type="html"><![CDATA[<h1 id="如何计算固定长度模型的困惑度（PPL）"><a href="#如何计算固定长度模型的困惑度（PPL）" class="headerlink" title="如何计算固定长度模型的困惑度（PPL）"></a>如何计算固定长度模型的困惑度（PPL）</h1><p>困惑度（PPL）是评估语言模型最常用的指标之一。在深入探讨之前，我们应该注意这个指标特别适用于传统语言模型（有时被称为自回归或因果语言模型），而对于像 BERT 这样的 masked language models 则没有明确定义（见<a href="https://huggingface.co/docs/transformers/main/en/model_summary">模型总结</a>）。</p><p>困惑度被定义为序列的指数化平均负对数似然。如果我们有一个标记化序列 (X = (x_0, x_1, \dots, x_t))，那么 (X) 的困惑度为，</p><p>[<br>\text{PPL}(X) = \exp \left{ -\frac{1}{t}\sum*{i=1}^t \log p<em>\theta (x</em>i|x*{&lt;i}) \right}<br>]</p><p>其中 (\log p<em>\theta (x_i|x</em>{&lt;i})) 是第 i 个标记的对数似然，条件是根据我们的模型前面的标记 (x_{&lt;i})。直观上，它可以被认为是评估模型在语料库中指定标记集合上预测均匀性的能力。重要的是，这意味着标记化程序直接影响模型的困惑度，这在比较不同模型时应始终考虑。</p><p>这也相当于数据和模型预测之间的交叉熵的指数化。想要了解更多关于困惑度及其与每字符位数（BPC）和数据压缩的关系的直觉，可以查看这篇在 The Gradient 上的<a href="https://thegradient.pub/understanding-evaluation-metrics-for-language-models/">精彩博客文章</a>。</p><h2 id="Calculating-PPL-with-fixed-length-models"><a href="#Calculating-PPL-with-fixed-length-models" class="headerlink" title="Calculating PPL with fixed-length models"></a>Calculating PPL with fixed-length models</h2><p>如果我们不受模型上下文大小的限制，我们会通过自回归地分解序列并在每一步都基于整个前序子序列来条件化，从而评估模型的困惑度，如下图所示。</p><img width="600" alt="Full decomposition of a sequence with unlimited context length" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/ppl_full.gif"><p>然而，在处理近似模型时，我们通常受到模型可以处理的标记数量的限制。例如，<a href="https://huggingface.co/docs/transformers/main/en/model_doc/gpt2">GPT-2</a>的最大版本有固定的 1024 个标记长度，所以当 (t) 大于 1024 时，我们无法直接计算 (p<em>\theta(x_t|x</em>{&lt;t}))。</p><p>相反，序列通常被分解成等于模型最大输入大小的子序列。如果模型的最大输入大小是 (k)，那么我们通过只条件化前 (k-1) 个标记（而不是整个上下文）来近似计算一个标记 (x_t) 的似然。在评估模型序列的困惑度时，一种诱人但次优的方法是将序列分解成不相交的块，并独立地累加每个段的分解对数似然。</p><img width="600" alt="Suboptimal PPL not taking advantage of full available context" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/ppl_chunked.gif"><p>这种计算很快，因为每个段的困惑度可以在一次前向传递中计算出来，但这是一个较差的完全分解困惑度的近似，并且通常会产生更高（更差）的 PPL，因为模型在大多数预测步骤中的上下文较少。</p><p>相反，应该使用滑动窗口策略来评估固定长度模型的 PPL。这涉及到重复滑动上下文窗口，使模型在做出每个预测时拥有更多的上下文。</p><img width="600" alt="Sliding window PPL taking advantage of all available context" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/ppl_sliding.gif"><ol><li><p><strong>无限上下文分解：</strong> 如果没有对模型输入长度的限制，我们可以在每一步都使用整个前序子序列来预测下一个标记。这样可以最准确地评估模型的性能，因为每次预测都考虑了所有先前的信息。</p></li><li><p><strong>固定长度限制：</strong> 实际中，大多数模型如 GPT-2 有固定的输入长度限制（例如 1024 个标记）。当序列长度超过这个限制时，不能直接计算每个标记的条件概率，因为不能将整个序列作为条件。</p></li><li><p><strong>分块近似：</strong> 一种处理长序列的方法是将序列分解成多个与模型最大输入长度相等的子序列。每个子序列单独评估，但这种方法可能会因为没有使用完整的上下文而导致更高的困惑度。</p></li><li><p><strong>滑动窗口策略：</strong> 为了更好地利用可用的上下文，可以使用滑动窗口策略。这种方法通过不断移动上下文窗口来尝试在每次预测时为模型提供更多的上下文信息，从而更接近于使用完整上下文的理想情况。</p></li><li><p><strong>跨步滑动窗口：</strong> 一个实际的折中方法是使用跨步滑动窗口，这样可以在保证一定效率的同时，为每次模型预测提供足够的上下文，从而改善困惑度的计算和模型预测的准确性。</p></li></ol><p>这些方法都是为了解决因模型输入长度限制而不能直接评估整个序列的问题，试图通过不同的技术使评估更加准确，同时考虑到计算资源的有效使用。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;如何计算固定长度模型的困惑度（PPL）&quot;&gt;&lt;a href=&quot;#如何计算固定长度模型的困惑度（PPL）&quot; class=&quot;headerlink&quot; title=&quot;如何计算固定长度模型的困惑度（PPL）&quot;&gt;&lt;/a&gt;如何计算固定长度模型的困惑度（PPL）&lt;/h1&gt;&lt;p&gt;困惑</summary>
      
    
    
    
    <category term="NLP Insights" scheme="https://chenhuiyu.github.io/categories/NLP-Insights/"/>
    
    
    <category term="Language Modeling" scheme="https://chenhuiyu.github.io/tags/Language-Modeling/"/>
    
    <category term="Perplexity" scheme="https://chenhuiyu.github.io/tags/Perplexity/"/>
    
  </entry>
  
  <entry>
    <title>【Python题解】2834. 找出美丽数组的最小和</title>
    <link href="https://chenhuiyu.github.io/2024/03/08/Code%20Chronicles/2834.%20%E6%89%BE%E5%87%BA%E7%BE%8E%E4%B8%BD%E6%95%B0%E7%BB%84%E7%9A%84%E6%9C%80%E5%B0%8F%E5%92%8C/"/>
    <id>https://chenhuiyu.github.io/2024/03/08/Code%20Chronicles/2834.%20%E6%89%BE%E5%87%BA%E7%BE%8E%E4%B8%BD%E6%95%B0%E7%BB%84%E7%9A%84%E6%9C%80%E5%B0%8F%E5%92%8C/</id>
    <published>2024-03-08T15:31:44.000Z</published>
    <updated>2024-03-08T14:24:59.239Z</updated>
    
    <content type="html"><![CDATA[<h1 id="2834-找出美丽数组的最小和"><a href="#2834-找出美丽数组的最小和" class="headerlink" title="2834. 找出美丽数组的最小和"></a>2834. 找出美丽数组的最小和</h1><blockquote><p>Problem: <a href="https://leetcode.cn/problems/find-the-minimum-possible-sum-of-a-beautiful-array/description/">2834. 找出美丽数组的最小和</a></p></blockquote><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><p>给定两个正整数 <code>n</code> 和 <code>target</code>，目标是找到一个长度为 <code>n</code> 的数组，满足以下条件：</p><ul><li>数组由两两不同的正整数组成。</li><li>不存在两个不同下标 <code>i</code> 和 <code>j</code> 使得 <code>nums[i] + nums[j] == target</code>。<br>返回符合条件的美丽数组所可能具备的最小和，并对结果进行 <code>10^9 + 7</code> 取模。</li></ul><h3 id="测试用例"><a href="#测试用例" class="headerlink" title="测试用例"></a>测试用例</h3><p>示例 1：</p><ul><li>输入：n = 2, target = 3</li><li>输出：4</li></ul><p>示例 2：</p><ul><li>输入：n = 3, target = 3</li><li>输出：8</li></ul><p>示例 3：</p><ul><li>输入：n = 1, target = 1</li><li>输出：1</li></ul><h2 id="原始思路"><a href="#原始思路" class="headerlink" title="原始思路"></a>原始思路</h2><h3 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h3><p>初始方案是从最小的数字开始，逐个检查每个数字是否可以被添加到数组中，同时确保不会存在两个数字之和等于 <code>target</code>。</p><ul><li>从 <code>1</code> 开始逐个尝试添加数字到数组。</li><li>对于每个数字，检查是否与数组中已有的数字相加会得到 <code>target</code>。</li><li>如果不会，将其添加到数组中。</li><li>继续此过程，直到数组长度达到 <code>n</code>。</li></ul><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">minimumPossibleSum</span>(<span class="params">n, target</span>):</span><br><span class="line">    selected_nums = <span class="built_in">set</span>([<span class="number">1</span>])</span><br><span class="line">    total_sum = <span class="number">1</span></span><br><span class="line">    current_num = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> <span class="built_in">len</span>(selected_nums) &lt; n:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">all</span>((current_num + num != target) <span class="keyword">for</span> num <span class="keyword">in</span> selected_nums):</span><br><span class="line">            selected_nums.add(current_num)</span><br><span class="line">            total_sum += current_num</span><br><span class="line">        current_num += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> total_sum % (<span class="number">10</span>**<span class="number">9</span> + <span class="number">7</span>)</span><br></pre></td></tr></tbody></table></figure><h3 id="复杂度分析"><a href="#复杂度分析" class="headerlink" title="复杂度分析"></a>复杂度分析</h3><ul><li>时间复杂度：O(n^2)，因为每个数字的添加都需要遍历已选择的数字集合。</li><li>空间复杂度：O(n)，用于存储选择的数字集合。</li></ul><h2 id="贪心优化"><a href="#贪心优化" class="headerlink" title="贪心优化"></a>贪心优化</h2><h3 id="方案-1"><a href="#方案-1" class="headerlink" title="方案"></a>方案</h3><ul><li><p><strong>优化策略</strong>：</p><ul><li><strong>避免集合的使用</strong>：<ul><li>引入“避免”集合，存储所有与已选数字相加得到 <code>target</code> 的数字。</li><li>这样可以快速检查新数字是否会导致和为 <code>target</code> 的情况。</li></ul></li><li><strong>直接检查</strong>：<ul><li>每次选择一个新数字时，仅检查它是否在“避免”集合中。</li><li>不在集合中的数字被认为是安全的，可以直接添加。</li></ul></li><li><strong>动态更新避免集合</strong>：<ul><li>当新数字被添加到美丽数组时，相应的 <code>target - 新数字</code> 也被添加到“避免”集合中。</li><li>这确保任何可能与新数字组成 <code>target</code> 的数字在未来都会被避免。</li></ul></li></ul></li><li><p><strong>优化后的时间复杂度</strong>：</p><ul><li>每个数字只需进行一次集合检查。</li><li>时间复杂度降低为 O(n)，显著提高了算法效率。</li></ul></li></ul><h3 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">minimumPossibleSum_optimized</span>(<span class="params">n, target</span>):</span><br><span class="line">    selected_nums = <span class="built_in">set</span>()</span><br><span class="line">    avoid_nums = <span class="built_in">set</span>()</span><br><span class="line">    total_sum = <span class="number">0</span></span><br><span class="line">    current_num = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> <span class="built_in">len</span>(selected_nums) &lt; n:</span><br><span class="line">        <span class="keyword">if</span> current_num <span class="keyword">not</span> <span class="keyword">in</span> avoid_nums:</span><br><span class="line">            selected_nums.add(current_num)</span><br><span class="line">            total_sum += current_num</span><br><span class="line">            avoid_nums.add(target - current_num)</span><br><span class="line">        current_num += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> total_sum % (<span class="number">10</span>**<span class="number">9</span> + <span class="number">7</span>)</span><br></pre></td></tr></tbody></table></figure><h3 id="复杂度分析-1"><a href="#复杂度分析-1" class="headerlink" title="复杂度分析"></a>复杂度分析</h3><ul><li>时间复杂度：O(n)，因为每个数字只需检查一次。</li><li>空间复杂度：O(n)，用于存储选择的数字和避免数字集合。</li></ul><h2 id="数学方法"><a href="#数学方法" class="headerlink" title="数学方法"></a>数学方法</h2><h3 id="方案-2"><a href="#方案-2" class="headerlink" title="方案"></a>方案</h3><p>针对上述问题，我们采用了一种更高效的数学方法来解决这个问题。该方法通过分析问题的数学本质，减少了必要的计算量，特别适用于处理大规模数据。</p><ol><li><p><strong>问题分解</strong>：</p><ul><li>首先，我们将问题分解为两部分。由于数组中的数字都是唯一的，且两个不同的数字之和不能等于 <code>target</code>，我们首先从最小的数字开始选择，直到我们不能再选择更多的数字而不违反和的规则。</li></ul></li><li><p><strong>选择前半部分的数字</strong>：</p><ul><li>在 <code>1</code> 到 <code>target-1</code> 的范围内，某些数字不能同时出现。例如，如果 <code>target</code> 是 <code>6</code>，则 <code>1</code> 和 <code>5</code>、<code>2</code> 和 <code>4</code> 不能同时出现，因为它们的和等于 <code>6</code>。但是，<code>3</code>（当 <code>target</code> 是偶数）或 <code>3</code> 和 <code>2</code>（当 <code>target</code> 是奇数）是可以被选择的。</li><li>这意味着我们可以自由选择从 <code>1</code> 到 <code>m</code> 的数字，其中 <code>m = min(⌊target/2⌋, n)</code>。对于这部分数字，我们可以直接使用等差数列的求和公式来计算它们的总和，即 <code>m * (m + 1) / 2</code>。</li></ul></li><li><p><strong>选择后半部分的数字</strong>：</p><ul><li>一旦我们选择了前 <code>m</code> 个数字，剩下需要选择的数字的数量就是 <code>n - m</code>。由于我们已经选择了 <code>1</code> 到 <code>m</code>，我们现在需要从 <code>target</code> 开始选择剩下的数字。</li><li>如果 <code>n</code> 大于 <code>m</code>，那么我们将从 <code>target</code> 开始连续选择 <code>n - m</code> 个数字。这些数字的总和可以用等差数列的求和公式来计算，公式为：<code>(2 * target + n - m - 1) * (n - m) / 2</code>。</li></ul></li><li><p><strong>计算总和并取模</strong>：</p><ul><li>我们将两部分的和相加，并对结果进行 <code>10^9 + 7</code> 取模，以得到最终答案。</li></ul></li></ol><h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><ul><li>第一部分和：从 <code>1</code> 到 <code>min(target // 2, n)</code> 的和。</li><li>第二部分和（如果需要）：从 <code>target</code> 开始，选择的 <code>n - min(target // 2, n)</code> 个数字的和。</li><li>将这两部分的和相加，即得到符合条件的美丽数组的最小和。</li></ul><ol><li><p><strong>选择小于 <code>target // 2</code> 的数字</strong>：</p><ul><li>当我们从 1 开始逐渐增加数字，直到 <code>target // 2</code>，这些数字不可能与数组中的其他数字相加得到 <code>target</code>。</li><li>例如，如果 <code>target</code> 是 10，那么 <code>target // 2</code> 是 5。在这种情况下，1 到 5 之间的任何两个数字相加都不会等于 10。</li><li>因此，这部分的选择是安全的，并且由于我们需要最小和，所以我们从 1 开始逐一增加。</li></ul></li><li><p>**当 <code>n</code> 大于 <code>target // 2</code>**：</p><ul><li>如果 <code>n</code> 大于 <code>target // 2</code>，这意味着仅仅选择小于 <code>target // 2</code> 的数字不足以填满数组。</li><li>在这种情况下，我们需要继续选择更多的数字，但为了避免和为 <code>target</code> 的组合，我们需要从 <code>target</code> 本身开始选择。</li><li>我们继续逐一增加，直到数组长度达到 <code>n</code>。</li></ul></li><li><p><strong>计算总和</strong>：</p><ul><li>第一部分是从 1 到 <code>min(target // 2, n)</code> 的和。</li><li>第二部分（如果需要）是从 <code>target</code> 开始，选择剩下的 <code>n - min(target // 2, n)</code> 个数字。</li><li>最后，将这两部分的和加起来，就是我们要找的最小和。</li></ul></li></ol><p>这是一个通过数学方法来解决问题的典型例子，它避免了复杂的编程逻辑，提供了一种更简洁高效的解决方案。</p><h3 id="代码-2"><a href="#代码-2" class="headerlink" title="代码"></a>代码</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">minimumPossibleSum_math_approach</span>(<span class="params">n, target</span>):</span><br><span class="line">    MOD = <span class="number">10</span>**<span class="number">9</span> + <span class="number">7</span></span><br><span class="line">    m = <span class="built_in">min</span>(target // <span class="number">2</span>, n)</span><br><span class="line">    first_half_sum = m * (m + <span class="number">1</span>) // <span class="number">2</span></span><br><span class="line">    remaining = n - m</span><br><span class="line">    second_half_sum = (<span class="number">2</span> * target + remaining - <span class="number">1</span>) * remaining // <span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> (first_half_sum + second_half_sum) % MOD</span><br></pre></td></tr></tbody></table></figure><h3 id="复杂度分析-2"><a href="#复杂度分析-2" class="headerlink" title="复杂度分析"></a>复杂度分析</h3><ul><li>时间复杂度：O(1)，因为结果是通过直接计算得出的。</li><li>空间复杂度：O(1)，只使用了固定数量的变量。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;2834-找出美丽数组的最小和&quot;&gt;&lt;a href=&quot;#2834-找出美丽数组的最小和&quot; class=&quot;headerlink&quot; title=&quot;2834. 找出美丽数组的最小和&quot;&gt;&lt;/a&gt;2834. 找出美丽数组的最小和&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;Pro</summary>
      
    
    
    
    <category term="Code Chronicles" scheme="https://chenhuiyu.github.io/categories/Code-Chronicles/"/>
    
    
    <category term="Python" scheme="https://chenhuiyu.github.io/tags/Python/"/>
    
    <category term="Leetcode" scheme="https://chenhuiyu.github.io/tags/Leetcode/"/>
    
    <category term="每日一题" scheme="https://chenhuiyu.github.io/tags/%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>跟着GPT老师学小聊：如何做一个好的捧哏</title>
    <link href="https://chenhuiyu.github.io/2024/03/05/Life%20Reflections/%E8%B7%9F%E7%9D%80GPT%E8%80%81%E5%B8%88%E5%AD%A6%E5%B0%8F%E8%81%8A/"/>
    <id>https://chenhuiyu.github.io/2024/03/05/Life%20Reflections/%E8%B7%9F%E7%9D%80GPT%E8%80%81%E5%B8%88%E5%AD%A6%E5%B0%8F%E8%81%8A/</id>
    <published>2024-03-05T14:31:00.000Z</published>
    <updated>2024-03-05T14:37:40.133Z</updated>
    
    <content type="html"><![CDATA[<p>英语小聊 (small talk) 是日常生活中不可或缺的交流形式，它不仅有助于打破沉默，也能在轻松的氛围中促进理解和友谊。在这篇文章中，我们将结合 10 个主题，提供相关的词汇、短语，并分享生活故事的开场白。同时，学习如何成为一名优秀的捧哏，通过提问和接话，让对话更加流畅和有趣。</p><h3 id="1-旅游体验-Travel-Experiences"><a href="#1-旅游体验-Travel-Experiences" class="headerlink" title="1. 旅游体验 (Travel Experiences)"></a>1. 旅游体验 (Travel Experiences)</h3><ul><li>词汇：Itinerary (行程), off the beaten path (人迹罕至), picturesque (如画的), excursion (远足), landmark (地标)。</li><li>短语：Cultural immersion (文化沉浸), travel off the beaten path (走偏僻的路), soak up the atmosphere (沉浸在气氛中)。</li><li>故事开场： “One place I really enjoyed visiting was…” (我非常喜欢去的一个地方是…)</li><li>追问： “That sounds amazing! What was the most unforgettable part of your trip?” (听起来太棒了！你旅行中最难忘的部分是什么？)</li><li>接话： “I’ve heard that place is beautiful. Did you take a lot of photos?” (我听说那个地方很美。你拍了很多照片吗？)</li></ul><h3 id="2-食物与美食-Food-and-Cuisine"><a href="#2-食物与美食-Food-and-Cuisine" class="headerlink" title="2. 食物与美食 (Food and Cuisine)"></a>2. 食物与美食 (Food and Cuisine)</h3><ul><li>词汇：Gastronomy (美食学), palate (味觉), savory (可口的), gourmet (美食家), culinary (烹饪的)。</li><li>短语：Acquired taste (后天品味), comfort food (安慰食物), fusion cuisine (融合菜肴), culinary delights (烹饪乐趣)。</li><li>故事开场： “I recently tried cooking…” (我最近尝试烹饪…)</li><li>追问： “Oh, how did it turn out? What ingredients did you use?” (哦，结果怎样？你用了哪些食材？)</li><li>接话： “I love trying new recipes too. Do you have any recommendations?” (我也喜欢尝试新食谱。你有什么推荐吗？)</li></ul><h3 id="3-爱好与兴趣-Hobbies-and-Interests"><a href="#3-爱好与兴趣-Hobbies-and-Interests" class="headerlink" title="3. 爱好与兴趣 (Hobbies and Interests)"></a>3. 爱好与兴趣 (Hobbies and Interests)</h3><ul><li>词汇：Amateur (业余爱好者), pastime (消遣), dabble (涉猎), proficiency (熟练), knack (诀窍)。</li><li>短语：Pursue a hobby (追求一个爱好), hone skills (磨练技能), leisure activities (休闲活动), broaden horizons (开阔视野)。</li><li>故事开场： “In my free time, I like to…” (在我空闲的时候，我喜欢…)</li><li>追问： “That’s interesting! How did you get started with that hobby?” (真有趣！你是怎么开始这个爱好的？)</li><li>接话： “It sounds like a great way to relax. I’ve been looking for a new hobby myself.” (听起来是放松的好方式。我自己也在找新的爱好。)</li></ul><h3 id="4-电影、电视节目和书籍-Movies-TV-Shows-and-Books"><a href="#4-电影、电视节目和书籍-Movies-TV-Shows-and-Books" class="headerlink" title="4. 电影、电视节目和书籍 (Movies, TV Shows, and Books)"></a>4. 电影、电视节目和书籍 (Movies, TV Shows, and Books)</h3><ul><li>词汇：Plot (情节), genre (类型), protagonist (主角), cliffhanger (悬念), screenplay (剧本)。</li><li>短语：Twist in the tale (故事的转折), page-turner (扣人心弦的书), critically acclaimed (广受好评), binge-watch (连续看剧)。</li><li>故事开场： “I watched a movie recently, and I found it…” (我最近看了一部电影，我觉得它…)</li><li>追问： “What did you like most about it? Any particular scene or character?” (你最喜欢它的哪个部分？有特别喜欢的场景或角色吗？)</li><li>接话： “I’ve been looking for something good to watch/read. Would you recommend it?” (我一直在找好看/好读的东西。你会推荐它吗？)</li></ul><h3 id="5-当前事件-Current-Events"><a href="#5-当前事件-Current-Events" class="headerlink" title="5. 当前事件 (Current Events)"></a>5. 当前事件 (Current Events)</h3><ul><li>词汇：Geopolitics (地缘政治), humanitarian (人道主义的), legislation (立法), diplomacy (外交), fiscal (财政的)。</li><li>短语：Political turmoil (政治动荡), economic sanctions (经济制裁), diplomatic relations (外交关系), social unrest (社会动乱)。</li><li>故事开场： “I read an interesting news article about…” (我读到了一个有趣的新闻文章，关于…)</li><li>追问： “That does sound interesting. How do you think it will affect us?” (那确实很有趣。你认为它会如何影响我们？)</li><li>接话： “I read something similar. It’s fascinating how quickly things are changing.” (我读过类似的东西。事物变化之快真是令人着迷。)</li></ul><h3 id="6-日常生活与日程-Daily-Life-and-Routine"><a href="#6-日常生活与日程-Daily-Life-and-Routine" class="headerlink" title="6. 日常生活与日程 (Daily Life and Routine)"></a>6. 日常生活与日程 (Daily Life and Routine)</h3><ul><li>词汇：Mundane (平凡的), routine (日常的), chores (杂务), errand (差事), regimen (规律)。</li><li>短语：Daily grind (日常琐事), run errands (做杂事), stick to a routine (遵守日常), day-to-day life (日常生活)。</li><li>故事开场： “A typical day for me involves…” (我的典型一天包括…)</li><li>追问： “Sounds like a busy day. What do you enjoy most in your daily routine?” (听起来是忙碌的一天。你最喜欢日常生活中的哪个部分？)</li><li>接话： “I can relate to that. My mornings are pretty similar. Do you have any morning rituals?” (我能理解。我的早晨也差不多。你有什么晨间仪式吗？)</li></ul><h3 id="7-语言学习-Language-Learning"><a href="#7-语言学习-Language-Learning" class="headerlink" title="7. 语言学习 (Language Learning)"></a>7. 语言学习 (Language Learning)</h3><ul><li>词汇：Fluency (流利), proficiency (精通), bilingual (双语的), immersion (沉浸式), linguistics (语言学)。</li><li>短语：Gain proficiency (提高熟练度), language barrier (语言障碍), mother tongue (母语), pick up a language (学习一种语言)。</li><li>故事开场： “One challenge I face in learning English is…” (我在学习英语时面临的一个挑战是…)</li><li>追问： “I see. What strategies are you using to overcome that challenge?” (我明白了。你用什么策略来克服这个挑战？)</li><li>接话： “Learning a language can be tough. I’m also trying to improve my [language].” (学习一门语言可能很难。我也在努力提高我的[语言]水平。)</li></ul><h3 id="8-文化差异-Cultural-Differences"><a href="#8-文化差异-Cultural-Differences" class="headerlink" title="8. 文化差异 (Cultural Differences)"></a>8. 文化差异 (Cultural Differences)</h3><ul><li>词汇：Etiquette (礼仪), customs (风俗), heritage (遗产), assimilate (同化), diversity (多样性)。</li><li>短语：Cultural exchange (文化交流), societal norms (社会规范), cross-cultural (跨文化), traditional values (传统价值)。</li><li>故事开场： “One thing I find different here compared to my home country is…” (我发现这里和我的祖国相比有一点不同是…)</li><li>追问： “That’s quite interesting. How do you feel about that difference?” (这很有趣。你对这种差异有什么感觉？)</li><li>接话： “Cultural differences are so intriguing. I’ve noticed something similar when I traveled to [country].” (文化差异真的很有趣。我在去[国家]旅行时也注意到了类似的事情。)</li></ul><h3 id="9-科技与趋势-Technology-and-Trends"><a href="#9-科技与趋势-Technology-and-Trends" class="headerlink" title="9. 科技与趋势 (Technology and Trends)"></a>9. 科技与趋势 (Technology and Trends)</h3><ul><li>词汇：Innovative (创新的), cutting-edge (尖端的), algorithm (算法), virtual reality (虚拟现实), automation (自动化)。</li><li>短语：Stay ahead of the curve (保持领先), technological advancements (技术进步), digital age (数字时代), the latest trend (最新趋势)。</li><li>故事开场： “I’m curious about how…” (我对…感到好奇)</li><li>追问： “Why does that interest you? Have you tried it out yourself?” (为什么那会引起你的兴趣？你自己试过了吗？)</li><li>接话： “Technology is advancing so fast. I’m also curious about [specific technology or trend].” (科技进步太快了。我也对[特定科技或趋势]感到好奇。)</li></ul><h3 id="10-个人发展-Personal-Development"><a href="#10-个人发展-Personal-Development" class="headerlink" title="10. 个人发展 (Personal Development)"></a>10. 个人发展 (Personal Development)</h3><ul><li>词汇：Self-improvement (自我提升), mindfulness (正念), resilience (韧性), aspiration (抱负), introspection (反省)。</li><li>短语：Set goals (设定目标), personal growth(个人成长), step out of comfort zone (走出舒适区), life-long learning (终身学习)。</li><li>故事开场： “I’ve been trying to…” (我一直在尝试…)</li><li>追问： “That’s a great goal. How are you planning to achieve it?” (那是个很好的目标。你打算如何实现它？)</li><li>接话： “Self-improvement is so important. I’m also working on [your goal or habit].” (自我提升非常重要。我也在努力[你的目标或习惯]。)</li></ul><p>成为一个好的捧哏不仅能使对话更加深入和有意义，还能展现你的倾听和理解能力。这些问题和评论可以帮助你在各种话题中更好地参与和维持英语对话。试试看，你会发现每次小聊都充满新的发现和乐趣！🌟🗣️💬</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;英语小聊 (small talk) 是日常生活中不可或缺的交流形式，它不仅有助于打破沉默，也能在轻松的氛围中促进理解和友谊。在这篇文章中，我们将结合 10 个主题，提供相关的词汇、短语，并分享生活故事的开场白。同时，学习如何成为一名优秀的捧哏，通过提问和接话，让对话更加流畅</summary>
      
    
    
    
    <category term="Life Reflections" scheme="https://chenhuiyu.github.io/categories/Life-Reflections/"/>
    
    
    <category term="Language Learning" scheme="https://chenhuiyu.github.io/tags/Language-Learning/"/>
    
    <category term="Small Talk" scheme="https://chenhuiyu.github.io/tags/Small-Talk/"/>
    
  </entry>
  
  <entry>
    <title>【Python题解】100226. 在带权树网络中统计可连接服务器对数目</title>
    <link href="https://chenhuiyu.github.io/2024/03/04/Code%20Chronicles/%E3%80%90Python%E9%A2%98%E8%A7%A3%E3%80%91100226.%20%E5%9C%A8%E5%B8%A6%E6%9D%83%E6%A0%91%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%BB%9F%E8%AE%A1%E5%8F%AF%E8%BF%9E%E6%8E%A5%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AF%B9%E6%95%B0%E7%9B%AE/"/>
    <id>https://chenhuiyu.github.io/2024/03/04/Code%20Chronicles/%E3%80%90Python%E9%A2%98%E8%A7%A3%E3%80%91100226.%20%E5%9C%A8%E5%B8%A6%E6%9D%83%E6%A0%91%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%BB%9F%E8%AE%A1%E5%8F%AF%E8%BF%9E%E6%8E%A5%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AF%B9%E6%95%B0%E7%9B%AE/</id>
    <published>2024-03-04T15:31:44.000Z</published>
    <updated>2024-03-05T14:36:12.984Z</updated>
    
    <content type="html"><![CDATA[<h3 id="题目：100226-在带权树网络中统计可连接服务器对数目"><a href="#题目：100226-在带权树网络中统计可连接服务器对数目" class="headerlink" title="题目：100226. 在带权树网络中统计可连接服务器对数目"></a>题目：100226. 在带权树网络中统计可连接服务器对数目</h3><h4 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h4><p>你被给定一个未定根的加权树，它有 <code>n</code> 个顶点，代表从 0 到 <code>n - 1</code> 编号的服务器，一个数组 <code>edges</code>，其中 <code>edges[i] = [ai, bi, weighti]</code> 代表顶点 <code>ai</code> 和 <code>bi</code> 之间的双向边，边的权重为 <code>weighti</code>。你还被给定一个整数 <code>signalSpeed</code>。</p><p>如果满足以下条件，两个服务器 <code>a</code> 和 <code>b</code> 可以通过服务器 <code>c</code> 连接：</p><ul><li><code>a &lt; b</code>，<code>a != c</code> 且 <code>b != c</code>。</li><li>从 <code>c</code> 到 <code>a</code> 的距离可被 <code>signalSpeed</code> 整除。</li><li>从 <code>c</code> 到 <code>b</code> 的距离可被 <code>signalSpeed</code> 整除。</li><li>从 <code>c</code> 到 <code>b</code> 和从 <code>c</code> 到 <code>a</code> 的路径不共享任何边。</li></ul><p>返回一个整数数组 <code>count</code>，长度为 <code>n</code>，其中 <code>count[i]</code> 是通过服务器 <code>i</code> 可连接的服务器对数。</p><h4 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h4><p><strong>示例 1</strong>:</p><p>输入: <code>edges = [[0,1,1],[1,2,5],[2,3,13],[3,4,9],[4,5,2]]</code>, <code>signalSpeed = 1</code><br>输出: <code>[0,4,6,6,4,0]</code><br>解释: 由于 <code>signalSpeed</code> 为 1，<code>count[c]</code> 等于从 <code>c</code> 出发且不共享任何边的路径对数。<br>在给定的路径图中，<code>count[c]</code> 等于 <code>c</code> 左侧的服务器数乘以 <code>c</code> 右侧的服务器数。</p><p><strong>示例 2</strong>:</p><p>输入: <code>edges = [[0,6,3],[6,5,3],[0,3,1],[3,2,7],[3,1,6],[3,4,2]]</code>, <code>signalSpeed = 3</code><br>输出: <code>[2,0,0,0,0,0,2]</code><br>解释: 通过服务器 0，有 2 对可连接服务器：(4, 5) 和 (4, 6)。<br>通过服务器 6，有 2 对可连接服务器：(4, 5) 和 (0, 5)。<br>可以证明，除了 0 和 6 之外的服务器无法连接任何两个服务器。</p><h4 id="限制条件"><a href="#限制条件" class="headerlink" title="限制条件"></a>限制条件</h4><ul><li><code>2 &lt;= n &lt;= 1000</code></li><li><code>edges.length == n - 1</code></li><li><code>edges[i].length == 3</code></li><li><code>0 &lt;= ai, bi &lt; n</code></li><li><code>1 &lt;= weighti &lt;= 10^6</code></li><li><code>1 &lt;= signalSpeed &lt;= 10^6</code></li><li>输入保证 <code>edges</code> 表示一个有效的树。</li></ul><hr><h3 id="问题概述"><a href="#问题概述" class="headerlink" title="问题概述"></a>问题概述</h3><p>给定一个表示服务器网络的树结构。每个服务器通过带权重的边与其他服务器连接。目标是计算在树中的每个服务器通过的可连接服务器对的数量，这些条件由<code>signalSpeed</code>定义。</p><h3 id="可连接服务器的条件"><a href="#可连接服务器的条件" class="headerlink" title="可连接服务器的条件"></a>可连接服务器的条件</h3><p>如果满足以下条件，两个服务器 <code>a</code> 和 <code>b</code> 可以通过服务器 <code>c</code> 连接：</p><ul><li><code>a &lt; b</code></li><li><code>a</code> 和 <code>b</code> 都不同于 <code>c</code>。</li><li>从 <code>c</code> 到 <code>a</code> 和从 <code>c</code> 到 <code>b</code> 的距离都能被 <code>signalSpeed</code> 整除。</li><li>从 <code>c</code> 到 <code>a</code> 和从 <code>c</code> 到 <code>b</code> 的路径不共享任何边。</li></ul><h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><ol><li><p><strong>树表示</strong>：树使用邻接表表示，每个服务器连接到其邻居以及连接边的权重。</p></li><li><p><strong>深度优先搜索（DFS）</strong>：</p><ul><li>使用修改后的 DFS 算法从每个服务器开始遍历树。</li><li>该算法计算所有其他服务器与当前服务器的距离。</li><li>DFS 确保在路径中不考虑共享边。</li></ul></li><li><p><strong>计算可连接对</strong>：</p><ul><li>对于每个服务器 <code>c</code>，该算法识别通过移除 <code>c</code> 形成的所有可能的子树。</li><li>它计算每个子树中与 <code>c</code> 的距离能被 <code>signalSpeed</code> 整除的服务器数量。</li><li>通过服务器 <code>c</code> 的可连接对的总数是通过考虑来自不同子树的对的所有可能组合计算出来的。</li></ul></li></ol><h3 id="Python-实现"><a href="#Python-实现" class="headerlink" title="Python 实现"></a>Python 实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">count_connectable_servers</span>(<span class="params">edges, signal_speed</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">build_tree</span>(<span class="params">edges</span>):</span><br><span class="line">        tree = defaultdict(<span class="built_in">list</span>)</span><br><span class="line">        <span class="keyword">for</span> a, b, weight <span class="keyword">in</span> edges:</span><br><span class="line">            tree[a].append((b, weight))</span><br><span class="line">            tree[b].append((a, weight))</span><br><span class="line">        <span class="keyword">return</span> tree</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">dfs_count_nodes</span>(<span class="params">server, parent, distance</span>):</span><br><span class="line">        <span class="keyword">if</span> distance % signal_speed == <span class="number">0</span>:</span><br><span class="line">            count[<span class="number">0</span>] += <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> neighbor, weight <span class="keyword">in</span> tree[server]:</span><br><span class="line">            <span class="keyword">if</span> neighbor != parent:</span><br><span class="line">                dfs_count_nodes(neighbor, server, distance + weight)</span><br><span class="line"></span><br><span class="line">    n = <span class="built_in">len</span>(edges) + <span class="number">1</span></span><br><span class="line">    tree = build_tree(edges)</span><br><span class="line">    counts = [<span class="number">0</span>] * n</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        subtree_counts = []</span><br><span class="line">        <span class="keyword">for</span> neighbor, weight <span class="keyword">in</span> tree[c]:</span><br><span class="line">            count = [<span class="number">0</span>]</span><br><span class="line">            dfs_count_nodes(neighbor, c, weight)</span><br><span class="line">            subtree_counts.append(count[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(subtree_counts)):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i + <span class="number">1</span>, <span class="built_in">len</span>(subtree_counts)):</span><br><span class="line">                counts[c] += subtree_counts[i] * subtree_counts[j]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> counts</span><br></pre></td></tr></tbody></table></figure><h3 id="复杂度分析"><a href="#复杂度分析" class="headerlink" title="复杂度分析"></a>复杂度分析</h3><ul><li><strong>时间复杂度</strong>：该算法对树中的每个服务器执行一次 DFS。由于每条边在每次 DFS 中被访问一次，并且有 <code>n</code> 个服务器，所以总体时间复杂度为 O(n^2)，其中 <code>n</code> 是服务器的数量。</li><li><strong>空间复杂度</strong>：由于存储树结构和在 DFS 过程中使用的辅助数据结构，空间复杂度为 O(n)。</li></ul><h3 id="测试用例"><a href="#测试用例" class="headerlink" title="测试用例"></a>测试用例</h3><p>该解决方案已经通过提供的示例和其他自定义测试用例进行了测试，以确保其正确性。</p><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>这个解决方案有效地计算了在树形网络中，每个服务器通过的可连接服务器对的数量，考虑到了与 <code>signalSpeed</code> 和连接规则相关的给定限制。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;题目：100226-在带权树网络中统计可连接服务器对数目&quot;&gt;&lt;a href=&quot;#题目：100226-在带权树网络中统计可连接服务器对数目&quot; class=&quot;headerlink&quot; title=&quot;题目：100226. 在带权树网络中统计可连接服务器对数目&quot;&gt;&lt;/a&gt;题</summary>
      
    
    
    
    <category term="Code Chronicles" scheme="https://chenhuiyu.github.io/categories/Code-Chronicles/"/>
    
    
    <category term="Python" scheme="https://chenhuiyu.github.io/tags/Python/"/>
    
    <category term="Leetcode" scheme="https://chenhuiyu.github.io/tags/Leetcode/"/>
    
    <category term="双周赛" scheme="https://chenhuiyu.github.io/tags/%E5%8F%8C%E5%91%A8%E8%B5%9B/"/>
    
  </entry>
  
  <entry>
    <title>【Python题解】100232. 超过阈值的最少操作数 II</title>
    <link href="https://chenhuiyu.github.io/2024/03/03/Code%20Chronicles/%E3%80%90Python%E9%A2%98%E8%A7%A3%E3%80%91100232.%20%E8%B6%85%E8%BF%87%E9%98%88%E5%80%BC%E7%9A%84%E6%9C%80%E5%B0%91%E6%93%8D%E4%BD%9C%E6%95%B0%20II/"/>
    <id>https://chenhuiyu.github.io/2024/03/03/Code%20Chronicles/%E3%80%90Python%E9%A2%98%E8%A7%A3%E3%80%91100232.%20%E8%B6%85%E8%BF%87%E9%98%88%E5%80%BC%E7%9A%84%E6%9C%80%E5%B0%91%E6%93%8D%E4%BD%9C%E6%95%B0%20II/</id>
    <published>2024-03-03T15:32:44.000Z</published>
    <updated>2024-03-05T14:36:18.743Z</updated>
    
    <content type="html"><![CDATA[<h3 id="题目：100232-超过阈值的最少操作数-II"><a href="#题目：100232-超过阈值的最少操作数-II" class="headerlink" title="题目：100232. 超过阈值的最少操作数 II"></a>题目：100232. 超过阈值的最少操作数 II</h3><h3 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h3><p><strong>超过阈值的最少操作数 II</strong></p><p>给定一个从 0 开始的整数数组 <code>nums</code> 和一个整数 <code>k</code>。你可以进行如下操作：</p><ul><li>选择 <code>nums</code> 中最小的两个整数 <code>x</code> 和 <code>y</code>。</li><li>将 <code>x</code> 和 <code>y</code> 从 <code>nums</code> 中删除。</li><li>将 <code>min(x, y) * 2 + max(x, y)</code> 添加到数组中的任意位置。</li></ul><p>注意，只有当 <code>nums</code> 至少包含两个元素时，你才可以执行以上操作。</p><p>目标是使数组中的所有元素都大于或等于 <code>k</code>。请返回实现此目标所需的最少操作次数。</p><h4 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h4><ol><li><p>示例 1：</p><ul><li>输入：<code>nums = [2,11,10,1,3], k = 10</code></li><li>输出：<code>2</code></li><li>解释：第一次操作中，我们删除元素 1 和 2，然后添加 1 _ 2 + 2 到 <code>nums</code> 中，<code>nums</code> 变为 <code>[4, 11, 10, 3]</code>。第二次操作中，我们删除元素 3 和 4，然后添加 3 _ 2 + 4 到 <code>nums</code> 中，<code>nums</code> 变为 <code>[10, 11, 10]</code>。此时，数组中的所有元素都大于等于 10，所以我们停止操作。需要的最少操作次数为 2。</li></ul></li><li><p>示例 2：</p><ul><li>输入：<code>nums = [1,1,2,4,9], k = 20</code></li><li>输出：<code>4</code></li><li>解释：第一次操作后，<code>nums</code> 变为 <code>[2, 4, 9, 3]</code>。第二次操作后，<code>nums</code> 变为 <code>[7, 4, 9]</code>。第三次操作后，<code>nums</code> 变为 <code>[15, 9]</code>。第四次操作后，<code>nums</code> 变为 <code>[33]</code>。此时，数组中的所有元素都大于等于 20，所以我们停止操作。需要的最少操作次数为 4。</li></ul></li></ol><h4 id="提示"><a href="#提示" class="headerlink" title="提示"></a>提示</h4><ul><li><code>2 &lt;= nums.length &lt;= 2 * 10^5</code></li><li><code>1 &lt;= nums[i] &lt;= 10^9</code></li><li><code>1 &lt;= k &lt;= 10^9</code></li><li>输入保证答案一定存在，也就是说一定存在一个操作序列使数组中所有元素都大于等于 <code>k</code>。</li></ul><h3 id="解决策略"><a href="#解决策略" class="headerlink" title="解决策略"></a>解决策略</h3><p>这个问题可以通过贪心算法和最小堆来高效解决。我们每次从数组中选择最小的两个数进行操作，这样可以最快地增加数的总和，更快地达到或超过阈值 <code>k</code>。步骤如下：</p><ol><li><p><strong>初始化</strong>: 将数组转换成最小堆，以便快速找到最小的两个数。</p></li><li><p><strong>执行操作</strong>: 反复执行以下步骤，直到数组中的所有元素都大于或等于 <code>k</code>：</p><ul><li>从堆中弹出最小的两个元素 <code>x</code> 和 <code>y</code>。</li><li>将 <code>2 * min(x, y) + max(x, y)</code> 添加回堆中。</li><li>记录操作次数。</li></ul></li><li><p><strong>返回结果</strong>: 当所有元素都大于或等于 <code>k</code> 时，返回操作次数。</p></li></ol><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> heapq</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">min_operations_to_reach_k</span>(<span class="params">nums, k</span>):</span><br><span class="line">    <span class="comment"># 将数组转换成最小堆</span></span><br><span class="line">    heapq.heapify(nums)</span><br><span class="line">    operations = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 当最小的元素小于k时，继续操作</span></span><br><span class="line">    <span class="keyword">while</span> nums[<span class="number">0</span>] &lt; k:</span><br><span class="line">        <span class="comment"># 如果数组中只剩一个元素，则无法进行操作</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(nums) &lt; <span class="number">2</span>:</span><br><span class="line">            <span class="keyword">return</span> -<span class="number">1</span>  <span class="comment"># 返回-1表示无法达到目标</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 弹出最小的两个元素</span></span><br><span class="line">        x = heapq.heappop(nums)</span><br><span class="line">        y = heapq.heappop(nums)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将新元素插入堆中</span></span><br><span class="line">        heapq.heappush(nums, <span class="number">2</span> * <span class="built_in">min</span>(x, y) + <span class="built_in">max</span>(x, y))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 增加操作计数</span></span><br><span class="line">        operations += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> operations</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试代码</span></span><br><span class="line"><span class="built_in">print</span>(min_operations_to_reach_k([<span class="number">2</span>, <span class="number">11</span>, <span class="number">10</span>, <span class="number">1</span>, <span class="number">3</span>], <span class="number">10</span>))  <span class="comment"># 应为2</span></span><br><span class="line"><span class="built_in">print</span>(min_operations_to_reach_k([<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">9</span>], <span class="number">20</span>))  <span class="comment"># 应为4</span></span><br></pre></td></tr></tbody></table></figure><p>这段代码使用 Python 的 <code>heapq</code> 模块来有效管理最小堆，确保每次都能快速找到最小的两个数。通过这种方法，我们可以高效地解决这个问题。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;题目：100232-超过阈值的最少操作数-II&quot;&gt;&lt;a href=&quot;#题目：100232-超过阈值的最少操作数-II&quot; class=&quot;headerlink&quot; title=&quot;题目：100232. 超过阈值的最少操作数 II&quot;&gt;&lt;/a&gt;题目：100232. 超过阈值的</summary>
      
    
    
    
    <category term="Code Chronicles" scheme="https://chenhuiyu.github.io/categories/Code-Chronicles/"/>
    
    
    <category term="Python" scheme="https://chenhuiyu.github.io/tags/Python/"/>
    
    <category term="Leetcode" scheme="https://chenhuiyu.github.io/tags/Leetcode/"/>
    
    <category term="双周赛" scheme="https://chenhuiyu.github.io/tags/%E5%8F%8C%E5%91%A8%E8%B5%9B/"/>
    
  </entry>
  
  <entry>
    <title>【Python题解】100231. 超过阈值的最少操作数 I</title>
    <link href="https://chenhuiyu.github.io/2024/03/02/Code%20Chronicles/%E3%80%90Python%E9%A2%98%E8%A7%A3%E3%80%91100231.%20%E8%B6%85%E8%BF%87%E9%98%88%E5%80%BC%E7%9A%84%E6%9C%80%E5%B0%91%E6%93%8D%E4%BD%9C%E6%95%B0%20I/"/>
    <id>https://chenhuiyu.github.io/2024/03/02/Code%20Chronicles/%E3%80%90Python%E9%A2%98%E8%A7%A3%E3%80%91100231.%20%E8%B6%85%E8%BF%87%E9%98%88%E5%80%BC%E7%9A%84%E6%9C%80%E5%B0%91%E6%93%8D%E4%BD%9C%E6%95%B0%20I/</id>
    <published>2024-03-02T15:34:44.000Z</published>
    <updated>2024-03-02T15:36:00.540Z</updated>
    
    <content type="html"><![CDATA[<h1 id="超过阈值的最少操作数-I"><a href="#超过阈值的最少操作数-I" class="headerlink" title="超过阈值的最少操作数 I"></a>超过阈值的最少操作数 I</h1><h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><p>给定一个下标从 0 开始的整数数组 <code>nums</code> 和一个整数 <code>k</code>。在每次操作中，你可以删除 <code>nums</code> 中的最小元素。目标是通过最少的操作次数使数组中的所有元素都大于或等于 <code>k</code>。需要返回实现此目标所需的最少操作次数。</p><h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><p><strong>示例 1:</strong></p><ul><li>输入：<code>nums = [2, 11, 10, 1, 3], k = 10</code></li><li>输出：<code>3</code></li><li>解释：<ul><li>第一次操作后，<code>nums</code> 变为 <code>[2, 11, 10, 3]</code>。</li><li>第二次操作后，<code>nums</code> 变为 <code>[11, 10, 3]</code>。</li><li>第三次操作后，<code>nums</code> 变为 <code>[11, 10]</code>。</li><li>此时，数组中的所有元素都大于等于 <code>10</code>，所以停止操作。</li><li>使数组中所有元素都大于等于 <code>10</code> 需要的最少操作次数为 <code>3</code>。</li></ul></li></ul><p><strong>示例 2:</strong></p><ul><li>输入：<code>nums = [1, 1, 2, 4, 9], k = 1</code></li><li>输出：<code>0</code></li><li>解释：数组中的所有元素都大于等于 <code>1</code>，所以不需要对 <code>nums</code> 做任何操作。</li></ul><p><strong>示例 3:</strong></p><ul><li>输入：<code>nums = [1, 1, 2, 4, 9], k = 9</code></li><li>输出：<code>4</code></li><li>解释：<code>nums</code> 中只有一个元素大于等于 <code>9</code>，所以需要执行 <code>4</code> 次操作。</li></ul><h3 id="提示"><a href="#提示" class="headerlink" title="提示"></a>提示</h3><ul><li><code>1 &lt;= nums.length &lt;= 50</code></li><li><code>1 &lt;= nums[i] &lt;= 10^9</code></li><li><code>1 &lt;= k &lt;= 10^9</code></li><li>输入保证至少有一个满足 <code>nums[i] &gt;= k</code> 的下标 <code>i</code> 存在。</li></ul><h2 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h2><p>要解决这个问题，首先需要对数组 <code>nums</code> 进行排序。排序后，数组中的最小元素将位于数组的起始位置。从数组的最小端开始，每遇到一个小于 <code>k</code> 的元素，就将其删除，并将操作计数器加一。当遇到第一个大于或等于 <code>k</code> 的元素时，停止删除操作。</p><p>给定一个从 0 开始的整数数组 <code>nums</code> 和一个整数 <code>k</code>，目的是通过最少的操作次数使数组中的所有元素都大于或等于 <code>k</code>。操作定义为删除数组中的最小元素。</p><h3 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h3><ol><li>对数组 <code>nums</code> 进行排序。</li><li>初始化操作计数器为 <code>0</code>。</li><li>遍历排序后的数组，对于每个小于 <code>k</code> 的元素，增加操作计数。</li><li>当遇到第一个大于或等于 <code>k</code> 的元素时，停止遍历。</li><li>返回操作计数。</li></ol><h3 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h3><h4 id="算法思路"><a href="#算法思路" class="headerlink" title="算法思路"></a>算法思路</h4><ol><li><strong>排序数组</strong>：首先将数组排序，这样可以确保我们总是在执行操作时删除最小的元素。</li><li><strong>计数操作</strong>：从数组的最小端开始，对于每个小于 <code>k</code> 的元素，我们将其删除，并增加操作计数器。</li><li><strong>停止条件</strong>：当数组中所有剩余的元素都大于或等于 <code>k</code> 时，操作结束。</li></ol><h4 id="算法步骤-1"><a href="#算法步骤-1" class="headerlink" title="算法步骤"></a>算法步骤</h4><ol><li>对数组 <code>nums</code> 进行排序。</li><li>初始化一个操作计数器。</li><li>遍历排序后的数组，对于每个小于 <code>k</code> 的元素，增加操作计数。</li><li>当遇到第一个大于或等于 <code>k</code> 的元素时，停止遍历。</li><li>返回操作计数。</li></ol><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">min_operations</span>(<span class="params">nums, k</span>):</span><br><span class="line">    <span class="comment"># Sort the array</span></span><br><span class="line">    nums.sort()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Count the number of operations</span></span><br><span class="line">    operations = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> num <span class="keyword">in</span> nums:</span><br><span class="line">        <span class="keyword">if</span> num &lt; k:</span><br><span class="line">            operations += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> operations</span><br></pre></td></tr></tbody></table></figure><h3 id="测试用例"><a href="#测试用例" class="headerlink" title="测试用例"></a>测试用例</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(min_operations([<span class="number">2</span>, <span class="number">11</span>, <span class="number">10</span>, <span class="number">1</span>, <span class="number">3</span>], <span class="number">10</span>))  <span class="comment"># 应输出: 3</span></span><br><span class="line"><span class="built_in">print</span>(min_operations([<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">9</span>], <span class="number">1</span>))     <span class="comment"># 应输出: 0</span></span><br><span class="line"><span class="built_in">print</span>(min_operations([<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">9</span>], <span class="number">9</span>))     <span class="comment"># 应输出: 4</span></span><br></pre></td></tr></tbody></table></figure><h3 id="复杂度分析"><a href="#复杂度分析" class="headerlink" title="复杂度分析"></a>复杂度分析</h3><ul><li><strong>时间复杂度</strong>：O(n log n)，主要由排序步骤决定。</li><li><strong>空间复杂度</strong>：O(n) 或 O(1)，取决于所使用的排序算法。如果使用原地排序算法，空间复杂度可以降低到 O(1)。</li></ul><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>这个解决方案通过排序和简单的线性遍历，有效地找出了将数组中所有元素变得大于或等于 <code>k</code> 所需的最少操作次数。它适用于不同的输入场景，并且在时间和空间效率方面表现良好。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;超过阈值的最少操作数-I&quot;&gt;&lt;a href=&quot;#超过阈值的最少操作数-I&quot; class=&quot;headerlink&quot; title=&quot;超过阈值的最少操作数 I&quot;&gt;&lt;/a&gt;超过阈值的最少操作数 I&lt;/h1&gt;&lt;h2 id=&quot;题目&quot;&gt;&lt;a href=&quot;#题目&quot; class=&quot;</summary>
      
    
    
    
    <category term="Code Chronicles" scheme="https://chenhuiyu.github.io/categories/Code-Chronicles/"/>
    
    
    <category term="Python" scheme="https://chenhuiyu.github.io/tags/Python/"/>
    
    <category term="Leetcode" scheme="https://chenhuiyu.github.io/tags/Leetcode/"/>
    
    <category term="双周赛" scheme="https://chenhuiyu.github.io/tags/%E5%8F%8C%E5%91%A8%E8%B5%9B/"/>
    
  </entry>
  
  <entry>
    <title>【Python题解】2369. 检查数组是否存在有效划分</title>
    <link href="https://chenhuiyu.github.io/2024/03/01/Code%20Chronicles/2369.%20%E6%A3%80%E6%9F%A5%E6%95%B0%E7%BB%84%E6%98%AF%E5%90%A6%E5%AD%98%E5%9C%A8%E6%9C%89%E6%95%88%E5%88%92%E5%88%86/"/>
    <id>https://chenhuiyu.github.io/2024/03/01/Code%20Chronicles/2369.%20%E6%A3%80%E6%9F%A5%E6%95%B0%E7%BB%84%E6%98%AF%E5%90%A6%E5%AD%98%E5%9C%A8%E6%9C%89%E6%95%88%E5%88%92%E5%88%86/</id>
    <published>2024-03-01T07:02:44.000Z</published>
    <updated>2024-03-01T16:03:29.414Z</updated>
    
    <content type="html"><![CDATA[<h1 id="【Python-题解】2369-检查数组的有效划分方法"><a href="#【Python-题解】2369-检查数组的有效划分方法" class="headerlink" title="【Python 题解】2369. 检查数组的有效划分方法"></a>【Python 题解】2369. 检查数组的有效划分方法</h1><h2 id="题目概述"><a href="#题目概述" class="headerlink" title="题目概述"></a>题目概述</h2><p>Leetcode 的题目 2369 要求我们检查一个整数数组<code>nums</code>是否可以划分为一个或多个满足特定条件的连续子数组。有效的划分条件包括：</p><ul><li>子数组由两个相等的元素组成，如 <code>[2, 2]</code>。</li><li>子数组由三个相等的元素组成，如 <code>[3, 3, 3]</code>。</li><li>子数组由三个连续递增的元素组成，且相邻元素之间差值为 1，如 <code>[4, 5, 6]</code>。</li></ul><p>如果数组至少存在一种有效划分，则返回<code>true</code>；否则返回<code>false</code>。</p><h2 id="示例分析"><a href="#示例分析" class="headerlink" title="示例分析"></a>示例分析</h2><ul><li>示例 1：<code>nums = [4, 4, 4, 5, 6]</code>可以划分为 <code>[4, 4]</code> 和 <code>[4, 5, 6]</code>，因此返回<code>true</code>。</li><li>示例 2：<code>nums = [1, 1, 1, 2]</code>不满足任何划分条件，因此返回<code>false</code>。</li></ul><h2 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h2><p>采用<strong>动态规划</strong>策略，我们定义一个布尔数组<code>dp</code>，其中<code>dp[i]</code>表示数组的前<code>i</code>个元素是否可以有效地划分。遍历数组，对于每个位置<code>i</code>，尝试以下三种划分方式：</p><ol><li>子数组由最后两个相等的元素组成。</li><li>子数组由最后三个相等的元素组成。</li><li>子数组由最后三个连续递增的元素组成。</li></ol><h2 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h2><ol><li>初始化 dp[0] 为 true，表示空数组可以有效划分。</li><li>对于每个 i（从 2 开始），检查是否可以通过上述三种方式之一将数组划分到 i。</li><li>最终，dp[n]（其中 n 是数组的长度）给出了整个数组是否可以有效划分的答案。</li><li>如果任何一种方式可行，则将<code>dp[i]</code>设置为<code>true</code>。</li></ol><p>具体来讲<br>初始化：</p><ol><li>初始化 dp[0]为 true，表示空数组是可以被有效划分的。</li><li>状态转移：对于数组中的每个元素 nums[i]（从第二个元素开始），考虑以下几种情况：<ul><li>如果 nums[i]与 nums[i-1]相等，检查 dp[i-2]是否为 true。如果是，表示[nums[i-1], nums[i]]可以形成有效划分，设置 dp[i]为 true。</li><li>如果 nums[i]、nums[i-1]和 nums[i-2]三者相等，同样检查 dp[i-3]。如果 dp[i-3]为 true，表示[nums[i-2], nums[i-1], nums[i]]可以形成有效划分，设置 dp[i]为 true。</li><li>如果 nums[i]、nums[i-1]和 nums[i-2]形成连续递增序列（即 nums[i]-1 == nums[i-1]且 nums[i-1]-1 == nums[i-2]），检查 dp[i-3]。若为 true，则设置 dp[i]为 true。</li></ul></li><li>结果判断：<ul><li>最终，检查 dp[n]的值（其中 n 是数组 nums 的长度）。如果 dp[n]为 true，则表示整个数组可以被有效划分；否则，不可以。</li></ul></li></ol><h2 id="动态规划实现"><a href="#动态规划实现" class="headerlink" title="动态规划实现"></a>动态规划实现</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">validPartition</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        n = <span class="built_in">len</span>(nums)</span><br><span class="line">        dp = [<span class="literal">False</span>] * (n + <span class="number">1</span>)</span><br><span class="line">        dp[<span class="number">0</span>] = <span class="literal">True</span>  <span class="comment"># 空数组视为有效划分</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, n + <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> nums[i - <span class="number">1</span>] == nums[i - <span class="number">2</span>]:</span><br><span class="line">                dp[i] = dp[i] <span class="keyword">or</span> dp[i - <span class="number">2</span>]</span><br><span class="line">            <span class="keyword">if</span> i &gt; <span class="number">2</span> <span class="keyword">and</span> nums[i - <span class="number">1</span>] == nums[i - <span class="number">2</span>] == nums[i - <span class="number">3</span>]:</span><br><span class="line">                dp[i] = dp[i] <span class="keyword">or</span> dp[i - <span class="number">3</span>]</span><br><span class="line">            <span class="keyword">if</span> i &gt; <span class="number">2</span> <span class="keyword">and</span> nums[i - <span class="number">1</span>] - <span class="number">1</span> == nums[i - <span class="number">2</span>] <span class="keyword">and</span> nums[i - <span class="number">2</span>] - <span class="number">1</span> == nums[i - <span class="number">3</span>]:</span><br><span class="line">                dp[i] = dp[i] <span class="keyword">or</span> dp[i - <span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> dp[n]</span><br></pre></td></tr></tbody></table></figure><h2 id="算法复杂度分析"><a href="#算法复杂度分析" class="headerlink" title="算法复杂度分析"></a>算法复杂度分析</h2><ul><li><strong>时间复杂度</strong>：O(N)，其中 N 是数组<code>nums</code>的长度，需要遍历整个数组来填充动态规划数组。</li><li><strong>空间复杂度</strong>：O(N)，用于存储动态规划数组<code>dp</code>，记录每个位置的划分情况。</li></ul><p>通过动态规划，我们提供了一种高效且直观的解决方案，确保算法的优化性能，适用于类似的数组划分问题。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;【Python-题解】2369-检查数组的有效划分方法&quot;&gt;&lt;a href=&quot;#【Python-题解】2369-检查数组的有效划分方法&quot; class=&quot;headerlink&quot; title=&quot;【Python 题解】2369. 检查数组的有效划分方法&quot;&gt;&lt;/a&gt;【Pyt</summary>
      
    
    
    
    <category term="Code Chronicles" scheme="https://chenhuiyu.github.io/categories/Code-Chronicles/"/>
    
    
    <category term="Python" scheme="https://chenhuiyu.github.io/tags/Python/"/>
    
    <category term="Leetcode" scheme="https://chenhuiyu.github.io/tags/Leetcode/"/>
    
    <category term="每日一题" scheme="https://chenhuiyu.github.io/tags/%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"/>
    
    <category term="动态规划" scheme="https://chenhuiyu.github.io/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/"/>
    
  </entry>
  
  <entry>
    <title>Gorilla LLM 大语言模型简介</title>
    <link href="https://chenhuiyu.github.io/2024/02/28/NLP%20Insights/Gorilla:%20Large%20Language%20Model%20Connected%20with%20Massive%20APIs/"/>
    <id>https://chenhuiyu.github.io/2024/02/28/NLP%20Insights/Gorilla:%20Large%20Language%20Model%20Connected%20with%20Massive%20APIs/</id>
    <published>2024-02-28T07:19:18.000Z</published>
    <updated>2024-02-28T07:19:23.979Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Gorilla-LLM-大语言模型简介"><a href="#Gorilla-LLM-大语言模型简介" class="headerlink" title="Gorilla LLM 大语言模型简介"></a>Gorilla LLM 大语言模型简介</h1><p>🦍 Gorilla: Large Language Model Connected with Massive APIs<br>Link: <a href="https://gorilla.cs.berkeley.edu/blogs/7_open_functions_v2.html">https://gorilla.cs.berkeley.edu/blogs/7_open_functions_v2.html</a></p><ul><li>Berkeley 功能调用排行榜<a href="https://gorilla.cs.berkeley.edu/leaderboard.html">Berkeley 功能调用排行榜</a></li><li>在线体验模型：<a href="https://gorilla.cs.berkeley.edu/leaderboard.html">Gorilla OpenFunctions-v2 网络演示</a></li><li>项目详情：<a href="https://github.com/ShishirPatil/gorilla/tree/main/openfunctions">GitHub</a></li><li>模型（7B 参数）在 HuggingFace 上的页面：<a href="https://huggingface.co/gorilla-llm/gorilla-openfunctions-v2">gorilla-llm/gorilla-openfunctions-v2</a></li></ul><h2 id="1-伯克利函数调用排行榜"><a href="#1-伯克利函数调用排行榜" class="headerlink" title="1. 伯克利函数调用排行榜"></a>1. 伯克利函数调用排行榜</h2><p>自 2022 年底以来，大语言模型（LLMs）凭借其执行通用任务的强大能力，成为众人关注的焦点。不仅限于聊天应用，将这些模型应用于开发各类 AI 应用和软件（如 Langchain, Llama Index, AutoGPT, Voyager）已成为一种趋势。GPT, Gemini, Llama, Mistral 等模型通过与外部世界的交互，如函数调用和执行，展现了其巨大潜力。</p><p>我们推出了<strong>伯克利函数调用排行榜（BFCL）</strong>，这是首个全面且可执行的 LLMs 函数调用评估。与之前的评估如 Anyscale 函数调用数据集不同，我们考虑了更多形式的函数调用、不同场景下的调用，以及函数调用的可执行性。我们根据实际应用场景构建了这个数据集，涵盖了大多数用户可能遇到的函数调用用例，例如在 AI 智能体或企业工作流程中的应用。为此，我们的评估数据集包含了丰富的类别，覆盖了多种语言。同时，我们还发布了 Gorilla-Openfunctions-v2 模型，这是目前最先进的开源模型，能够处理多种编程语言的函数调用，包括并行和多重函数调用。此外，我们还提供了一项特殊的调试功能，即当提供的函数不符合任务要求时，模型会输出“错误消息”。</p><p><a href="https://gorilla.cs.berkeley.edu/leaderboard#api-explorer">https://gorilla.cs.berkeley.edu/leaderboard#api-explorer</a></p><h3 id="伯克利函数调用排行榜-🏆"><a href="#伯克利函数调用排行榜-🏆" class="headerlink" title="伯克利函数调用排行榜 🏆"></a>伯克利函数调用排行榜 🏆</h3><p>伯克利函数调用</p><p>排行榜（BFCL）旨在全面研究不同 LLMs 在函数调用能力上的表现。它包含了 2000 个包含多种编程语言（Python, Java, JavaScript, REST API）的问题-函数-答案对，覆盖了多样化的应用领域和复杂的用例（如多重函数调用和并行函数调用）。我们还研究了函数相关性检测，以确定模型对不适合的函数如何作出反应（在这种情况下会提供“错误消息”）。具体来说，BFCL 包括了 100 个 Java、50 个 JavaScript、70 个 REST API、100 个 SQL 和 1680 个 Python 的各种简单、并行、多重、可执行函数调用场景以及函数相关性检测。</p><p>排行榜显示，OpenAI 的 GPT-4 在函数调用评估中仍领先，而 Gorilla OpenFunctions-v2（来自 Gorilla LLM）的表现几乎与之媲美。其后是 Mistral-medium 模型（来自 Mistral AI）和 Claude-2.1（来自 Anthropic）。这说明，一个经过微调的开源模型在函数调用任务上也可以达到与专有模型相近的水平，而无需进行复杂的链接。</p><p>我们致力于涵盖真实世界的用例和多样的语言。未来，我们将继续扩展测试领域，并探索更多创新用例。</p><p><img src="https://gorilla.cs.berkeley.edu/assets/img/blog_post_8_Leaderboard.jpg" alt="https://gorilla.cs.berkeley.edu/assets/img/blog_post_8_Leaderboard.jpg"></p><p><em>LLMs 在伯克利函数调用排行榜（BFCL）上的表现</em></p><p>为了更深入地分析和可视化结果，我们提供了一个交互式的六边形工具，供用户比较不同模型的性能。我们将测试分为 9 个类别，包括函数不相关性检测、AST 树检查和执行函数调用检查，用于简单、多重、并行多功能场景。通过这个工具，我们可以清楚地看到各个测试中模型的表现。在简单单一函数调用方面，专有模型和开源模型表现类似。但在涉及多重和并行函数调用时，GPT 系列模型的表现超过了开源模型。</p><p><img src="https://gorilla.cs.berkeley.edu/assets/img/blog_post_8_Wagon.gif" alt="https://gorilla.cs.berkeley.edu/assets/img/blog_post_8_Wagon.gif"></p><p><em>使用伯克利函数调用排行榜（BFCL）六边形图进行的详细分析</em></p><h3 id="数据集组成"><a href="#数据集组成" class="headerlink" title="数据集组成"></a>数据集组成</h3><p>Gorilla OpenFunctions 的评估数据集已从最初的 100 个条目扩展到 1900 个。评估数据集在以下方面展现了多样性：</p><ul><li>函数文档领域</li><li>函数文档和函数调用问答对的数量</li><li>不同编程语言的数据类型</li></ul><p>我们的评估 JSON 函数是从不同网站来源抓取和生成的。我们特意包含了像数学代数、体育足球、金融抵押等领域。我们在评估中包括了 40 个子领域的函数，这使我们能够了解模型性能在数据丰富的领域（如计算和云）以及体育、法律等小众领域的表现。</p><p><img src="https://gorilla.cs.berkeley.edu/assets/img/blog_post_8_data_composition.png" alt="https://gorilla.cs.berkeley.edu/assets/img/blog_post_8_data_composition.png"></p><p><em>伯克利函数调用排行榜（BFCL）数据组成</em></p><h3 id="评估类别-📊"><a href="#评估类别-📊" class="headerlink" title="评估类别 📊"></a>评估类别 📊</h3><p>我们将评估主要分为两大类：</p><ul><li>Python：简单函数、多重函数、并行函数、并行多重函数</li><li>非 Python：函数相关性检测、REST API、SQL、Java、JavaScript</li></ul><h3 id="Python-评估"><a href="#Python-评估" class="headerlink" title="Python 评估"></a>Python 评估</h3><ul><li><strong>简单函数：</strong>这一类别包含了最常见的格式：用户提供一个 JSON 函数文档，模型只调用其中一个函数。</li><li><strong>多重函数：</strong>这一类别包含了需要从 2-4 个 JSON 函数文档中选择并调用一个函数的用户问题。模型需要根据用户提供的上下文选择最合适的函数。</li><li><strong>并行函数：</strong>并行函数定义为使用一个用户查询同时调用多个函数。模型需要判断需要调用多少个函数，问题可以是单个或多个句子。</li><li><strong>并行多重函数：</strong>并行多重函数是并行函数和多重函数的结合，即模型被提供了多个函数文档，每个文档中的函数可能被调用一次或多次。</li></ul><p>每个类别都有相应的可执行类别。在这部分，我们根据一些免费的 REST API 端点（例如获取天气）和直接计算的函数（例如线性回归）编写了函数代码。可执行类别旨在判断函数调用生成是否能够在实际应用中使用。</p><h3 id="非-Python-评估"><a href="#非-Python-评估" class="headerlink" title="非 Python 评估"></a>非 Python 评估</h3><p>除了上述主要类别外，我们还包含了更具体的类别来评估模型在不同场景下的表现，并测试其对不相关问题和函数文档的应对能力。</p><ul><li>函数相关性检测：这一类别设计了一个场景，其中提供的任何函数都不相关，也不应该被调用。我们期望模型的输出是没有函数调用。这个场景帮助我们了解模型是否会在缺乏生成函数代码的信息时产生错误。</li><li><strong>REST API</strong>：现实世界中的大多数 API 调用都是 REST API 调用。Python 主要通过 requests.get(), requests.post(), requests.delete()等方法在 python requests 库中完成 REST API 调用。</li><li><strong>GET 请求：</strong>GET 请求是现实世界中最常用的。因此，我们包括了真实世界的 GET 请求来测试模型生成可执行 REST API 调用的能力。我们的评估包括两种变体：一种是需要在 URL 中传递参数的，另一种是需要将参数作为键/值对放入 requests.get()的 params 和/或 headers 中。模型需要根据情况决定如何调用。</li><li><strong>SQL：</strong>SQL 评估数据包括我们定制的 sql.execute 函数，其中包含 sql_keyword, table_name, columns 和 conditions。这些参数提供了构建简单 SQL 查询的必要信息。我们希望通过函数调用可靠地构建和使用 SQL 查询，而不是专门训练一个 SQL 模型。我们的评估数据集限制了场景，仅支持简单的关键词，如“SELECT”, “INSERT INTO”, “UPDATE”, “DELETE”, “CREATE”。</li><li><strong>Java + JavaScript：</strong>尽管大多数编程语言的函数调用格式相同，但每种编程语言都有其特有的类型。例如，C 有指针类型，Java 有 HashMap 类型。这个测试类别的目的是了解函数调用模型如何扩展到不仅仅是 JSON 和 Python 类型，还包括所有特定于语言的类型。</li></ul><p>这些类别使我们能够看到不同模型在 API 调用的流行用例中的表现，并为我们提供了关于函数调用模型潜力的洞察。</p><h3 id="评估指标-📈"><a href="#评估指标-📈" class="headerlink" title="评估指标 📈"></a>评估指标 📈</h3><p>我们使用两种流行的方法来评估模型生成答案的准确性：AST 检查器和执行检查器。理想情况下，应使用执行检查器，但由于并非所有结果都容易执行（如 Java 函数），我们使用 AST 作为执行检查器的补充。</p><ul><li>抽象语法树（AST）检查器</li><li>执行检查器</li></ul><p>AST 检查：对于可执行的函数调用答案，我们使用 AST 树进行解析。</p><p>示例：<code>[calculate_triangle_area(base=10, height=5)]</code></p><p>解析：<code>Module(body=[Expr(value=List(elts=[Call(func=Name(id='calculate_triangle_area', ctx=Load()), args=[], keywords=[keyword(arg='base', value=Constant(value=10)), keyword(arg='height', value=Constant(value=5))])], ctx=Load()))], type_ignores=[]) [calculate_triangle_area(base=10, height=5)]</code></p><p>我们从 AST 中提取变量，并检查每个参数是否在可能的答案中找到并精确匹配。对于每个可能的答案，应接受的答案包括：</p><ul><li>布尔值：<ul><li>我们检查布尔值的直接匹配，不允许对布尔值的字符串版本有宽容。</li></ul></li><li>整数、浮点数：<ul><li>答案应该是唯一的，例如 [1]</li></ul></li><li>列表：<ul><li>我们检查精确匹配，因此任何顺序的列表都应匹配。[1,2,3]==[2,3,1]</li></ul></li><li>字典：<ul><li>为简化，我们跳过检查递归 AST 字典结构。</li></ul></li><li>字符串：<ul><li>可能的日期 “20th June”, “2023-06-20”, “06/20/2023”, “Jun.20,2023”</li><li>可能的位置 [“New York City”, “NYC”]</li><li>可能的任何东西 [“Manchester United”, “Man United”, “Man U”, “MUFC”]</li></ul></li></ul><p>以下是一些可能的答案示例：</p><ul><li><code>{"calculate_triangle_area": {"base": [10], "height": [5], "unit": ["units", "unit"]}}</code></li><li><code>{"predict_house_price": {"bedrooms": [3], "bathrooms": [2], "area": [1800], "location": ["San Francisco", "San Francisco, CA"]}}</code></li></ul><p>这种检查机制适用于除了 executable_*和 REST 之外的所有内容。</p><p>可执行检查：对于 executable_*和 REST，我们有相应的函数，可以为每个问题执行。因此，在模型生成答案后，我们将直接执行这些答案。有两种类型的匹配：</p><ul><li>确定性的可执行输出：我们根据我们人类执行的结果检查精确匹配。</li><li>非确定性和现实世界相关的可执行输出：我们检查其响应类型和响应 JSON 键的一致性，看看值是否是我们期望看到的。</li></ul><h3 id="提示"><a href="#提示" class="headerlink" title="提示"></a>提示</h3><p>我们提供了用于评估我们的专有和开源模型的所有提示。对于函数调用模型，我们没有提供任何系统提示，而是直接启用函数调用模式并放置函数定义。对于聊天模型，我们提供了明确的系统消息。</p><ol><li><p>对于所有函数调用模型，我们直接启用函数调用模式并放置函数定义。</p></li><li><p>对于聊天模型，我们提供了明确的系统消息：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">SYSTEM_PROMPT_FOR_CHAT_MODEL = """"</span><br><span class="line">你是一个编写函数的专家。你会收到一个问题和一系列可能的函数。</span><br><span class="line">根据问题，你需要进行一个或多个函数/工具调用来实现目的。</span><br><span class="line">如果没有一个函数可以使用，请指出。如果给定问题缺少函数所需的参数，</span><br><span class="line">也请指出。你应该只在工具调用部分返回函数调用。</span><br><span class="line">"""</span><br><span class="line"></span><br><span class="line">SYSTEM_PROMPT_FOR_CHAT_MODEL = """"</span><br><span class="line">You are an expert in composing functions. You are given a question and a set of possible functions.</span><br><span class="line">Based on the question, you will need to make one or more function/tool calls to achieve the purpose.</span><br><span class="line">If none of the function can be used, point it out. If the given question lacks the parameters required by the function,</span><br><span class="line">also point it out. You should only return the function call in tools call sections.</span><br><span class="line">"""</span><br></pre></td></tr></tbody></table></figure><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">USER_MESSAGE_FOR_CHAT_MODEL = "Questions:{user_prompt}\\n这里是一系列你可以调用的JSON格式函数列表:\\n{functions}. 如果你决定返回函数调用，不得包含其他文本。"</span><br><span class="line">USER_MESSAGE_FOR_CHAT_MODEL = "Questions:{user_prompt}\nHere is a list of functions in JSON format that you can invoke:\n{functions}. Should you decide to return the function call(s), NO other text MUST be included."</span><br></pre></td></tr></tbody></table></figure></li></ol><h3 id="常见错误"><a href="#常见错误" class="headerlink" title="常见错误"></a>常见错误</h3><p>通过我们的基准测试 BFCL，我们能够识别 LLMs 在生成函数调用时所犯的一些常见错误。这些错误揭示了当前模型的局限性，并为如何改进</p><p>它们提供了洞察。</p><ol><li><p>GPT 的函数文档难以格式化，其类型在现实世界场景中受到限制。例如，我们需要将 float 手动转换为 number，以使函数与 OpenAI 兼容。此外，数字相比 float 在精度和类型一致性方面信息传递较少。</p><p>在 Gorilla Openfunctions-v2 中，我们通过不限制参数类型来提高函数文档的灵活性。换言之，用户可以提供 Tuple、Float，甚至 Java 中的特定类型，如 HashMap 和 LinkedList。</p></li><li><p>GPT 在需要某种隐式转换的参数场景中表现不佳。例如，当参数不是直接在用户问题中给出时。</p><figure class="highlight json"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">"Function"</span><span class="punctuation">:</span></span><br><span class="line"><span class="punctuation">{</span></span><br><span class="line">    <span class="attr">"name"</span><span class="punctuation">:</span> <span class="string">"finance.predict_future_value"</span><span class="punctuation">,</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="attr">"parameters"</span><span class="punctuation">:</span></span><br><span class="line">    <span class="punctuation">{</span></span><br><span class="line">        <span class="attr">"type"</span><span class="punctuation">:</span> <span class="string">"object"</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">"properties"</span><span class="punctuation">:</span></span><br><span class="line">        <span class="punctuation">{</span></span><br><span class="line">            <span class="attr">"present_value"</span><span class="punctuation">:</span></span><br><span class="line">            <span class="punctuation">{</span></span><br><span class="line">                <span class="attr">"type"</span><span class="punctuation">:</span> <span class="string">"number"</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">"description"</span><span class="punctuation">:</span> <span class="string">"The present value of the investment."</span></span><br><span class="line">            <span class="punctuation">}</span><span class="punctuation">,</span></span><br><span class="line"></span><br><span class="line">            <span class="attr">"annual_interest_rate"</span><span class="punctuation">:</span></span><br><span class="line">            <span class="punctuation">{</span></span><br><span class="line">                <span class="attr">"type"</span><span class="punctuation">:</span> <span class="string">"number"</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">"description"</span><span class="punctuation">:</span> <span class="string">"The annual interest rate of the investment."</span></span><br><span class="line">            <span class="punctuation">}</span><span class="punctuation">,</span></span><br><span class="line"></span><br><span class="line">            <span class="attr">"compounding_periods_per_year"</span><span class="punctuation">:</span></span><br><span class="line">            <span class="punctuation">{</span></span><br><span class="line">                <span class="attr">"type"</span><span class="punctuation">:</span> <span class="string">"integer"</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">"description"</span><span class="punctuation">:</span> <span class="string">"The number of times that interest is compounded per year."</span><span class="punctuation">,</span></span><br><span class="line">            <span class="punctuation">}</span><span class="punctuation">,</span></span><br><span class="line">            <span class="attr">"time_years"</span><span class="punctuation">:</span></span><br><span class="line">            <span class="punctuation">{</span></span><br><span class="line">                <span class="attr">"type"</span><span class="punctuation">:</span> <span class="string">"integer"</span><span class="punctuation">,</span></span><br><span class="line">                <span class="attr">"description"</span><span class="punctuation">:</span> <span class="string">"The investment horizon in years."</span></span><br><span class="line">            <span class="punctuation">}</span></span><br><span class="line">            ...</span><br><span class="line">        <span class="punctuation">}</span></span><br><span class="line">        <span class="attr">"required"</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">"present_value"</span><span class="punctuation">,</span> <span class="string">"annual_interest_rate"</span><span class="punctuation">,</span> <span class="string">"time_years"</span><span class="punctuation">]</span></span><br><span class="line">    <span class="punctuation">}</span></span><br><span class="line"><span class="punctuation">}</span></span><br></pre></td></tr></tbody></table></figure><p>Questions : Predict the future value of a $5000 investment with an annual interest rate of 5% in 3 years with monthly compounding.</p><figure class="highlight json"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">GPT<span class="number">-4</span> output<span class="punctuation">:</span></span><br><span class="line"><span class="punctuation">[</span><span class="punctuation">{</span></span><br><span class="line">    <span class="attr">"name"</span><span class="punctuation">:</span> <span class="string">"finance.predict_future_value"</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">"parameters"</span><span class="punctuation">:</span></span><br><span class="line">    <span class="punctuation">{</span></span><br><span class="line">        <span class="attr">"present_value"</span><span class="punctuation">:</span> <span class="number">5000</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">"annual_interest_rate"</span><span class="punctuation">:</span> <span class="number">5</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">"compounding_periods_per_year"</span><span class="punctuation">:</span> <span class="number">12</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">"time_years"</span><span class="punctuation">:</span> <span class="number">3</span></span><br><span class="line">    <span class="punctuation">}</span></span><br><span class="line"><span class="punctuation">}</span><span class="punctuation">]</span></span><br></pre></td></tr></tbody></table></figure><figure class="highlight json"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Gorilla-openfunctions-v2 output<span class="punctuation">:</span></span><br><span class="line"><span class="punctuation">[</span><span class="punctuation">{</span></span><br><span class="line">    <span class="attr">"name"</span><span class="punctuation">:</span> <span class="string">"finance.predict_future_value"</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">"parameters"</span><span class="punctuation">:</span></span><br><span class="line">    <span class="punctuation">{</span></span><br><span class="line">        <span class="attr">"present_value"</span><span class="punctuation">:</span> <span class="number">5000</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">"annual_interest_rate"</span><span class="punctuation">:</span> <span class="number">0.05</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">"compounding_periods_per_year"</span><span class="punctuation">:</span> <span class="number">12</span><span class="punctuation">,</span></span><br><span class="line">        <span class="attr">"time_years"</span><span class="punctuation">:</span> <span class="number">3</span></span><br><span class="line">    <span class="punctuation">}</span></span><br><span class="line"><span class="punctuation">}</span><span class="punctuation">]</span></span><br></pre></td></tr></tbody></table></figure></li><li><p>聊天模型倾向于生成格式错误的函数调用，其中参数可以提取但无法执行。</p><p>mistral-medium 生成的结果示例如下：<code>solve_quadratic_equation(a=2, b=6, c=5)</code>。通过 gorilla-openfunctions-v2，我们能够直接输出<code>solve_quadratic_equation(a=3, b=2, c=1)</code>，该结果在接收后即可执行。</p></li><li><p>REST API 调用不一致：例如，某些情况下模型可能无法正确生成 API 调用的 URL 或参数。</p></li></ol><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>我们通过 Gorilla Open Functions 排行榜对 LLMs 函数调用进行了全面和系统性的评估。研究表明，在不涉及复杂规划和链式函数调用的简单函数调用方面，经过微调的开源模型可以与专有模型相媲美。此外，我们还推出了 Gorilla Open Functions v2，这是一个开源模型，可以帮助用户通过函数调用构建 AI 应用，并实现与 json 兼容的输出交互。</p><p>我们希望您喜欢这篇博客文章。欢迎您在<a href="https://discord.gg/SwTyuTAxX3">Discord</a>、<a href="https://twitter.com/shishirpatil_/status/1661780076277678082">Twitter (#GorillaLLM)</a>和<a href="https://github.com/ShishirPatil/gorilla/">GitHub</a>上分享您的想法。</p><p>如果您想引用 Gorilla：</p><figure class="highlight json"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">@misc<span class="punctuation">{</span>berkeley-function-calling-leaderboard<span class="punctuation">,</span></span><br><span class="line">  title=<span class="punctuation">{</span>Berkeley Function Calling Leaderboard<span class="punctuation">}</span><span class="punctuation">,</span></span><br><span class="line">  author=<span class="punctuation">{</span>Fanjia Yan and Huanzhi Mao and Charlie Cheng-Jie Ji and Tianjun Zhang and Shishir G. Patil and Ion Stoica and Joseph E. Gonzalez<span class="punctuation">}</span><span class="punctuation">,</span></span><br><span class="line">  howpublished=<span class="punctuation">{</span>\url<span class="punctuation">{</span>https<span class="punctuation">:</span><span class="comment">//gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html}},</span></span><br><span class="line">  year=<span class="punctuation">{</span><span class="number">2024</span><span class="punctuation">}</span><span class="punctuation">,</span></span><br><span class="line"><span class="punctuation">}</span></span><br></pre></td></tr></tbody></table></figure><h2 id="2-Gorilla-OpenFunctions-v2"><a href="#2-Gorilla-OpenFunctions-v2" class="headerlink" title="2. Gorilla OpenFunctions v2"></a>2. Gorilla OpenFunctions v2</h2><p><img src="https://gorilla.cs.berkeley.edu/assets/img/blog_post_7_demo.gif" alt="https://gorilla.cs.berkeley.edu/assets/img/blog_post_7_demo.gif"></p><p><em>Gorilla OpenFunctions-v2！在开源模型中技术领先（SoTA），与商业模型媲美。</em></p><p>Gorilla OpenFunctions 的最新版本——版本 2，带来了大语言模型（LLM）在开源社区中函数调用方面的重大进展。作为前一版本的升级替代，Gorilla OpenFunctions-v2 不仅保持了开源精神，还引入了令人兴奋的新功能。这包括支持 Python、Java、JavaScript 和 REST API 等</p><p>多种编程语言——这在开源和闭源模型中都是首次；同时具备处理多个和并行函数调用的能力，以及判断函数相关性的能力。这次更新巩固了 gorilla-openfunctions-v2 在 LLM 领域中函数调用能力的领先地位。而且，这种即插即用的更新方式使得 OpenFunctions 可以轻松集成到各种应用中，从社交媒体平台如 Instagram 到送货服务如 Doordash，还有包括 Google Calendar 和 Stripe 等实用工具。</p><h3 id="新功能速览-🚀"><a href="#新功能速览-🚀" class="headerlink" title="新功能速览!! 🚀"></a>新功能速览!! 🚀</h3><p>我们在 OpenFunctions-v2 中推出的五个激动人心的新功能包括：</p><p><img src="https://gorilla.cs.berkeley.edu/assets/img/blog_post_7_open_function_v2_features.png" alt="https://gorilla.cs.berkeley.edu/assets/img/blog_post_7_open_function_v2_features.png"></p><ul><li><strong>支持更多数据类型：</strong> Gorilla Open Functions v2 现在能支持多种语言，并扩展了对函数调用中参数类型的支持。例如，对于 Python，支持的类型包括<code>[string, number, boolean, list, tuple, dict, any]</code>；Java 和 Javascript 同样支持丰富的类型。相比之下，OpenAI 和许多其他公司仅支持 JSON 模式，即<code>[string, number, integer, object, array, boolean]</code>。这种对类型的原生支持意味着您现在可以更方便地使用 openfunctions-v2。</li><li><strong>支持并行和多功能：</strong> 可以处理并行和多功能调用。在多功能场景中，用户可以在不确定哪个功能最合适时输入多个功能；Gorilla 模型将从中选择一个或多个（或不选择）来响应用户的请求。在并行功能中，可以通过多次调用同一功能来响应用户的提示。Gorilla 模型不仅同时支持这两种模式，还能将它们的优势结合起来。</li><li><strong>功能相关性检测：</strong> 在没有提供功能或相关功能的情况下减少错误响应。Gorilla openfunctions v2 现在能自动判断提供给模型的功能是否能够解决用户的问题。识别到这一点后，LLM 会向用户展示一个错误信息，提供更多帮助。</li><li><strong>增强的 RESTful API 能力：</strong> 提升了格式化 RESTful API 调用的能力。RESTful API 广泛应用于网络中，为许多流行的软件服务（如 Slack、PayPal 等）提供支持。我们的模型经过特殊训练，能够高质量地处理 RESTful API 调用。</li></ul><p>快速链接：</p><ul><li>其他功能调用模型的表现：<a href="https://gorilla.cs.berkeley.edu/leaderboard.html">Berkeley 功能调用排行榜</a></li><li>在线体验模型：<a href="https://gorilla.cs.berkeley.edu/leaderboard.html">Gorilla OpenFunctions-v2 网络演示</a></li><li>项目详情：<a href="https://github.com/ShishirPatil/gorilla/tree/main/openfunctions">GitHub</a></li><li>模型（7B 参数）在 HuggingFace 上的页面：<a href="https://huggingface.co/gorilla-llm/gorilla-openfunctions-v2">gorilla-llm/gorilla-openfunctions-v2</a></li></ul><h3 id="在您的应用中集成-OpenFunctions-v2-🔨"><a href="#在您的应用中集成-OpenFunctions-v2-🔨" class="headerlink" title="在您的应用中集成 OpenFunctions-v2 🔨"></a>在您的应用中集成 OpenFunctions-v2 🔨</h3><p>使用 Gorilla OpenFunctions-v2 非常简单：</p><ol><li>为了便于快速原型开发，我们提供了一个托管的 Gorilla Openfunctions-v2 模型供推理使用。您也可以在本地运行它，或通过 HuggingFace 的页面自行托管。以下示例展示了如何调用托管的 gorilla openfunctions v2 模型：</li></ol><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> openai</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_gorilla_response</span>(<span class="params">prompt=<span class="string">""</span>, model=<span class="string">"gorilla-openfunctions-v2"</span>, functions=[]</span>):</span><br><span class="line">    openai.api_key = <span class="string">"EMPTY"</span>  <span class="comment"># 由UC Berkeley免费托管 ❤️</span></span><br><span class="line">    openai.api_base = <span class="string">"&lt;http://luigi.millennium.berkeley.edu:8000/v1&gt;"</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">    completion = openai.ChatCompletion.create(</span><br><span class="line">        model=<span class="string">"gorilla-openfunctions-v2"</span>,</span><br><span class="line">        temperature=<span class="number">0.0</span>,</span><br><span class="line">        messages=[{<span class="string">"role"</span>: <span class="string">"user"</span>, <span class="string">"content"</span>: prompt}],</span><br><span class="line">        functions=functions,</span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># completion.choices[0].message.content, 函数调用的字符串格式</span></span><br><span class="line">    <span class="comment"># completion.choices[0].message.functionsl, 函数调用的Json格式</span></span><br><span class="line">    <span class="keyword">return</span> completion.choices[<span class="number">0</span>]</span><br></pre></td></tr></tbody></table></figure><ol><li>向模型提问：<br><code>波士顿和旧金山的天气怎么样？</code></li><li>格式化您的功能调用：模型将根据您的请求返回功能调用。</li></ol><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">query = <span class="string">"波士顿和旧金山的天气怎么样？"</span></span><br><span class="line">functions = [</span><br><span class="line">    {</span><br><span class="line">        <span class="string">"name"</span>: <span class="string">"get_current_weather"</span>,</span><br><span class="line">        <span class="string">"description"</span>: <span class="string">"获取指定地点的当前天气"</span>,</span><br><span class="line">        <span class="string">"parameters"</span>: {</span><br><span class="line">            <span class="string">"type"</span>: <span class="string">"object"</span>,</span><br><span class="line">            <span class="string">"properties"</span>: {</span><br><span class="line">                <span class="string">"location"</span>: {</span><br><span class="line">                    <span class="string">"type"</span>: <span class="string">"string"</span>,</span><br><span class="line">                    <span class="string">"description"</span>: <span class="string">"城市和州，比如旧金山，CA"</span>,</span><br><span class="line">                },</span><br><span class="line">                <span class="string">"unit"</span>: {<span class="string">"type"</span>: <span class="string">"string"</span>, <span class="string">"enum"</span>: [<span class="string">"celsius"</span>, <span class="string">"fahrenheit"</span>]},</span><br><span class="line">            },</span><br><span class="line">            <span class="string">"required"</span>: [<span class="string">"location"</span>],</span><br><span class="line">        },</span><br><span class="line">    }</span><br><span class="line">]</span><br></pre></td></tr></tbody></table></figure><ol><li>获取您的功能调用：模型将根据您的请求返回一个 Python 功能调用。<br>这为开发人员和非开发人员提供了便利，使他们能够利用复杂功能而无需编写大量代码。</li></ol><p>输入：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">get_gorilla_response(prompt=query, functions=[functions])</span><br></pre></td></tr></tbody></table></figure><p>输出：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[get_current_weather(location=<span class="string">'Boston, MA'</span>), get_current_weather(location=<span class="string">'San Francisco, CA'</span>)]</span><br></pre></td></tr></tbody></table></figure><p>通过上面的示例，您可以利用 Gorilla OpenFunctions-v2 生成格式良好的输出，或用您自己的定义调用函数！然后，您可以在您的应用程序和聊天机器人中自由地使用这些功能！</p><p>注意：Gorilla 目前仅支持<code>openai==0.28.1</code>版本的托管端点。我们很快将升级以支持<code>openai==1.xx</code>版本，届时<code>functions</code>将被<code>tool_calls</code>替换。</p><h3 id="Berkeley-功能调用排行榜上的表现-🔥"><a href="#Berkeley-功能调用排行榜上的表现-🔥" class="headerlink" title="Berkeley 功能调用排行榜上的表现 🔥"></a>Berkeley 功能调用排行榜上的表现 🔥</h3><p><img src="https://gorilla.cs.berkeley.edu/assets/img/blog_post_7_open_function_v2_summary.png" alt="https://gorilla.cs.berkeley.edu/assets/img/blog_post_7_open_function_v2_summary.png"></p><p><em>我们在 Berkeley 功能调用排行榜上进行了全面和详尽的评估，我们的模型与目前技术最先进的 GPT-4-1106 预览版以及 GPT-4 和 GPT-3.5-turbo 功能调用特性进行了对比。此外，我们还将我们的模型与其他开源模型进行了比较，展示了其优越性能。我们的评估涵盖了来自不同领域（包括旅游、金融、安排会议等）和语言（java、javascript、python、restAPI）的 2000 多个不同的查询和 API 文档对。</em></p><p>要深入了解我们的模型在每个类别中的表现，请参阅下面 Berkeley 功能调用排行榜中的详细表格。与目前技术最先进的 GPT-4 功能调用相比，Gorilla OpenFunctions-v2 在 Python 中的简单功能调用类别表现更优，但在涉及多个和并行功能的功能调用上表现不如 GPT-4。这一新特性对我们和整个开源社区来说仍是一个令人兴奋的研究领域。值得一提的是，我们的模型提供了非常稳定的可执行功能调用 - 这些功能调用是通过实际执行来评估的，无需任何干预。不出所料，经过训练的 Gorilla 模型在除 Python 以外的编程语言（如 Java、Javascript 和 REST API）上的功能调用上胜过了 GPT-4。对于 REST API，我们的模型提供了更稳定的输出，其中包括了所有必需的字段，包括<strong>url</strong>、<strong>params</strong>和<strong>header</strong>，使我们的模型非常适合立即采用。</p><p><img src="https://gorilla.cs.berkeley.edu/assets/img/blog_post_7_open_function_v2_leaderboard.png" alt="https://gorilla.cs.berkeley.edu/assets/img/blog_post_7_open_function_v2_leaderboard.png"></p><p><em>左侧是 GPT-4 生成的，右侧是 openfunctions-v2 生成的。从上面的错误中可以看出，当 GPT-4 功能调用处理涉及复杂参数结构（例如字典中的字典）并带有默认值的功能时，该模型往往会遇到麻烦，尤其是在解析默认值方面。与其说是一种边缘情况，不如说上面的示例是 REST API 的一个常见范例。</em></p><h3 id="OpenFunctions-数据组成与训练-🍕"><a href="#OpenFunctions-数据组成与训练-🍕" class="headerlink" title="OpenFunctions 数据组成与训练 🍕"></a>OpenFunctions 数据组成与训练 🍕</h3><p>Gorilla openfunctions v2 是一个基于\[deepseek-coder-7b-instruct-v1.5\]大语言模型进一步训练的 7B 参数模型。为了训练该模型，我们从三个不同来源收集了共计 65,283 个问题-功能-答案对：Python 包（19,353）、Java 存储库（16,586）、Javascript 存储库（4,245）、公共 API（6,009）以及来自各种云提供商的命令行工具（19,090）。数据组成如下图所示。</p><p>在数据收集之后，我们进行了四次数据增强，以提高我们的训练数据集的多样性。首先，我们更改了函数名称，这对于确保模型不会“记住”API 映射至关重要。其次，我们添加了随机选择的、数量不等的函数，使我们的数据集与并行函数兼容。这样我们就可以从简单的函数中生成多功能数据集。第三，我们采用扰动提示的策略来生成并行功能的场景，并将其扩展到同时包括多功能和并行功能。最后，我们还包含了一些功能在输入时不足以解决任务的数据集部分，我们将这些标记为“相关性检测”场景。与大多数 LLM 训练一样，我们对每种数据增强的程度进行了广泛的变化，以训练出一个健壮的模型。</p><ul><li><strong>函数名称变换：</strong> 我们通过使用不同的函数名称来增强原始的问题-功能-答案对，避免模型记住函数名称和问题之间的相关性（例如，“uber”API 用于交通）。<br><code>query + [{'name': 'func1', 'description': 'order takeout'}] -&gt; ans1 =&gt; query + [{'name': 'func2', 'description': 'order takeout'}] -&gt; [ans2]</code></li><li><strong>并行功能变换：</strong> 为了处理选择多个功能来回答用户请求的更复杂情况，我们更改了原始问题以要求多个输出。<br><code>query + [{'name': 'func1', 'description': 'order takeout'}] -&gt; ans1 =&gt; query + [{'name': 'func1', 'description': 'order takeout'}, {'name': 'func2', 'description': 'get weather'}] -&gt; [ans1]</code></li><li><strong>多功能变换：</strong> 在训练中包含多个功能调用的原始功能变换，使模型学习选择使用哪个功能调用。<br><code>query1 + [{'name': 'func1', 'description': 'order takeout'}] -&gt; ans1 =&gt; query2 + [{'name': 'func1', 'description': 'order takeout'}] -&gt; [ans1, ans2]</code></li><li><strong>并行多功能变换：</strong> 上述并行和多功能变换的结合。<br><code>query1 + [{'name': 'func1', 'description': 'order takeout'}] -&gt; ans1 =&gt; query2 + [{'name': 'func1', 'description': 'order takeout'}, {'name': 'func2', 'description': 'get weather'}] -&gt; [ans1, ans2]</code></li><li><strong>功能相关性检测变换：</strong> 我们还包含了一些在输入时提供的功能无法解决任务的数据集部分。我们称之为“相关性检测”。<br><code>query1 + [{'name': 'func1', 'description': 'order takeout'}] -&gt; ans1 =&gt; query2 + [{'name': 'func1', 'description': 'order takeout'}] -&gt; [Error, the function cannot solve the question.]</code></li></ul><p>在整个数据增强之后，我们还使用 Rouge 得分进行了数据去重，这已经成为标准做法。</p><h3 id="结论-1"><a href="#结论-1" class="headerlink" title="结论"></a>结论</h3><p>我们很高兴发布<code>gorilla-openfunctions-v2</code>，这是一个在 Deepseek-Coder-7B-Instruct-v1.5 大语言模型基础上训练的 7B 参数模型。它接收用户的提示和多个 API 调用，并返回带有正确参数的功能。OpenFunctions 扩展了对 Python、Java 和 JavaScript 以及 RESTful API 中参数类型的原生支持。欲了解更多信息，请查看我们在 Berkeley 功能调用排行榜上的博客评估，以及我们 GitHub 页面上的模型。博客中的所有结果都是使用<code>gorilla-openfunctions-v2</code>生成的。</p><p>Licensing:</p><p>Gorilla OpenFunctions v2 is distributed under the Apache 2.0 license. This software incorporates elements from the Deepseek model. Consequently, the licensing of Gorilla OpenFunctions v2 adheres to the Apache 2.0 license, with additional terms as outlined in Appendix A of the Deepseek license.</p><hr><p>我们希望您喜欢这篇博客文章。欢迎您在<a href="https://discord.gg/3apqwwME">Discord</a>、Twitter (#GorillaLLM)和<a href="https://github.com/ShishirPatil/gorilla/">GitHub</a>上与我们分享您的想法。</p><p>如果您想引用 Gorilla：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@inproceedings{gorilla-openfunctions-v2,</span></span><br><span class="line">  title={Gorilla OpenFunctions v2},</span><br><span class="line">  author={Charlie Cheng-Jie Ji, Huanzhi Mao, Fanjia Yan, Shishir G. Patil, Tianjun Zhang, Ion Stoica, Joseph E. Gonzalez},</span><br><span class="line">  year={<span class="number">2024</span>},</span><br><span class="line">  howpublished={\url{https://gorilla.cs.berkeley.edu//blogs/7_open_functions_v2.html}},</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Gorilla-LLM-大语言模型简介&quot;&gt;&lt;a href=&quot;#Gorilla-LLM-大语言模型简介&quot; class=&quot;headerlink&quot; title=&quot;Gorilla LLM 大语言模型简介&quot;&gt;&lt;/a&gt;Gorilla LLM 大语言模型简介&lt;/h1&gt;&lt;p&gt;🦍</summary>
      
    
    
    
    <category term="NLP Insights" scheme="https://chenhuiyu.github.io/categories/NLP-Insights/"/>
    
    
    <category term="LLM" scheme="https://chenhuiyu.github.io/tags/LLM/"/>
    
    <category term="Agent" scheme="https://chenhuiyu.github.io/tags/Agent/"/>
    
    <category term="Tool Use" scheme="https://chenhuiyu.github.io/tags/Tool-Use/"/>
    
    <category term="Gorilla" scheme="https://chenhuiyu.github.io/tags/Gorilla/"/>
    
  </entry>
  
  <entry>
    <title>FastChat Training Script Code Analysis - Train.py 【FastChat Series Part 1】</title>
    <link href="https://chenhuiyu.github.io/2024/02/27/NLP%20Insights/FastChat%20Training%20Script%20Code%20Analysis%20-%20Train.py%20%E3%80%90FastChat%20Series%20Part%201%E3%80%91/"/>
    <id>https://chenhuiyu.github.io/2024/02/27/NLP%20Insights/FastChat%20Training%20Script%20Code%20Analysis%20-%20Train.py%20%E3%80%90FastChat%20Series%20Part%201%E3%80%91/</id>
    <published>2024-02-26T17:43:18.000Z</published>
    <updated>2024-02-26T18:00:42.331Z</updated>
    
    <content type="html"><![CDATA[<h1 id="FastChat-Training-Script-Code-Analysis-Train-py-【FastChat-Series-Part-1】"><a href="#FastChat-Training-Script-Code-Analysis-Train-py-【FastChat-Series-Part-1】" class="headerlink" title="FastChat Training Script Code Analysis - Train.py 【FastChat Series Part 1】"></a>FastChat Training Script Code Analysis - Train.py 【FastChat Series Part 1】</h1><p>In this article, we delve into the train.py script of FastChat (<a href="https://github.com/lm-sys/FastChat">https://github.com/lm-sys/FastChat</a>) (<a href="https://github.com/lm-sys/FastChat/blob/main/fastchat/train/train.py">https://github.com/lm-sys/FastChat/blob/main/fastchat/train/train.py</a>), a key component for training and optimizing large language models (LLMs). FastChat is an advanced open-source platform focused on developing, deploying, and evaluating chatbots based on LLMs. The platform not only supports top-tier models like Vicuna and MT-Bench but also includes a distributed multi-model service system equipped with a Web UI and RESTful API compatible with OpenAI, enabling efficient training and evaluation of models.</p><p>We provide a detailed analysis of the train.py script’s source code. This script is a training script for natural language processing models based on the transformers library, covering critical steps such as data preprocessing, model training, and saving. Our goal is to offer a detailed explanation of each class and function in train.py, including their functionality and role in the overall training process.</p><h2 id="1-Importing-Modules"><a href="#1-Importing-Modules" class="headerlink" title="1. Importing Modules"></a>1. Importing Modules</h2><h3 id="1-Built-in-Modules"><a href="#1-Built-in-Modules" class="headerlink" title="1. Built-in Modules"></a>1. Built-in Modules</h3><p>These are standard library modules that come with Python and don’t require additional installation.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> dataclasses <span class="keyword">import</span> dataclass, field</span><br></pre></td></tr></tbody></table></figure><p>Imports Python’s <code>dataclasses</code> module for creating classes with default values.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br></pre></td></tr></tbody></table></figure><p>Imports the <code>json</code> module for handling JSON format data.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br></pre></td></tr></tbody></table></figure><p>Imports the <code>math</code> module for mathematical operations.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pathlib</span><br></pre></td></tr></tbody></table></figure><p>Imports the <code>pathlib</code> module for handling file paths.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Dict</span>, <span class="type">Optional</span>, <span class="type">Sequence</span></span><br></pre></td></tr></tbody></table></figure><p>Imports the <code>typing</code> module for type annotations.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></tbody></table></figure><h3 id="2-Dependency-Libraries"><a href="#2-Dependency-Libraries" class="headerlink" title="2. Dependency Libraries"></a>2. Dependency Libraries</h3><p>These are external libraries typically installed via a package manager like pip.<br>Imports the <code>numpy</code> library, commonly used for scientific computing.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></tbody></table></figure><p>Imports <code>PyTorch</code>, a popular deep learning framework.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br></pre></td></tr></tbody></table></figure><p>Imports <code>Dataset</code> from <code>torch</code> for creating custom datasets.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> transformers</span><br></pre></td></tr></tbody></table></figure><p>Imports the <code>transformers</code> library, a popular natural language processing library.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Trainer</span><br></pre></td></tr></tbody></table></figure><p>Imports <code>Trainer</code> from <code>transformers</code> for training models.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers.trainer_pt_utils <span class="keyword">import</span> LabelSmoother</span><br></pre></td></tr></tbody></table></figure><p>Imports <code>LabelSmoother</code> from <code>transformers</code> for label smoothing.</p><h3 id="3-Project-Specific-Functions"><a href="#3-Project-Specific-Functions" class="headerlink" title="3. Project-Specific Functions"></a>3. Project-Specific Functions</h3><p>These are functions or classes custom-implemented in the Fast Chat project.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fastchat.conversation <span class="keyword">import</span> SeparatorStyle</span><br></pre></td></tr></tbody></table></figure><p>Imports <code>SeparatorStyle</code> from the <code>fastchat</code> package for defining conversation separator styles. The <code>SeparatorStyle</code> class is an enumeration class created using Python’s <code>enum</code> module, defining a series of separator styles. Enumerations are a programming concept used to define a named set of constants, making code clearer and more maintainable.</p><p>In the <code>SeparatorStyle</code> class, each member represents a specific style of separator. These styles are often used in text processing, especially in scenarios where different sections or elements need to be distinguished. For instance, in handling dialog or textual data, different methods might be needed to differentiate between user input and machine responses.</p><p>Regarding the use of the <code>auto()</code> function:</p><ul><li><code>auto()</code> is a special function provided by Python’s <code>enum</code> module. It automatically assigns a unique value to each member in an enumeration class.</li><li>Without using <code>auto()</code>, you would need to manually assign a unique value to each enumeration member. <code>auto()</code> simplifies this process by letting Python handle the assignment of these values automatically.</li><li>The values assigned by <code>auto()</code> are usually integers, starting from 1 and increasing sequentially.</li></ul><p>In the case of the <code>SeparatorStyle</code> class, <code>auto()</code> is used to automatically assign a unique integer value to each type of separator style. For example, <code>ADD_COLON_SINGLE</code>, <code>ADD_COLON_TWO</code>, etc., will be given different integer values.</p><p>The names of each enumeration member (such as <code>ADD_COLON_SINGLE</code>, <code>NO_COLON_SINGLE</code>, etc.) typically describe the characteristics of that separator style. For instance, <code>ADD_COLON_SINGLE</code> might represent adding a colon as a separator after a certain element, whereas <code>NO_COLON_SINGLE</code> means no colon is added.</p><p>This approach makes referencing and handling these separator styles in the code more convenient and clear. For example, different separator styles can be chosen based on different scenarios or requirements without having to remember their specific values.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fastchat.model.model_adapter <span class="keyword">import</span> get_conversation_template</span><br></pre></td></tr></tbody></table></figure><p>Imports <code>get_conversation_template</code> from the <code>fastchat</code> package for obtaining conversation templates. In this code segment, the call logic primarily involves obtaining the default conversation template for a specific model. The call chain is as follows:</p><ol><li><p><strong>Starting Call - <code>get_conversation_template(model_path: str)</code></strong></p><ul><li>This function is the starting point of the call chain. It accepts a parameter <code>model_path</code>, specifying the path of the model.</li><li>The purpose of this function is to obtain the default conversation template for the given model path.</li></ul></li><li><p><strong>Call <code>get_model_adapter(model_path: str)</code></strong></p><ul><li>The <code>get_conversation_template</code> function first calls <code>get_model_adapter</code>, passing in the model path.</li><li>The purpose of <code>get_model_adapter</code> is to find and return a suitable <code>BaseModelAdapter</code> object for the provided model path.</li><li>This function first tries to match the basename of <code>model_path</code>. If no match is found, it tries the full path.</li><li>If a suitable adapter is found, it is returned; otherwise, a <code>ValueError</code> is thrown.</li></ul></li><li><p><strong>Execute <code>BaseModelAdapter.get_default_conv_template(model_path: str)</code></strong></p><ul><li>Once the appropriate model adapter is obtained, <code>get_conversation_template</code> retrieves the default conversation template by calling the <code>get_default_conv_template</code> method of that adapter.</li><li>Note that this method is defined in the <code>BaseModelAdapter</code> class but might be overridden in subclasses.</li></ul></li><li><p><strong>Call <code>get_conv_template(name: str)</code></strong></p><ul><li>Inside the <code>get_default_conv_template</code> method, it calls the <code>get_conv_template</code> function, usually passing a predefined template name like <code>"one_shot"</code>.</li><li>The purpose of <code>get_conv_template</code> is to retrieve a specified name’s template from the global registry of conversation templates <code>conv_templates</code>.</li></ul></li><li><p><strong>Obtain and Return a <code>Conversation</code> Object</strong></p><ul><li>The <code>get_conv_template</code> function returns an instance of the <code>Conversation</code> class, usually copied from the <code>conv_templates</code> dictionary.</li><li>Finally, this <code>Conversation</code> instance is returned to the original call site of <code>get_conversation_template</code>.</li></ul></li></ol><p>Summarizing the call chain:</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">get_conversation_template(model_path)</span><br><span class="line">  -&gt; get_model_adapter(model_path)</span><br><span class="line">  -&gt; [BaseModelAdapter].get_default_conv_template(model_path)</span><br><span class="line">    -&gt; get_conv_template(name)</span><br><span class="line">      -&gt; Return Conversation Object</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><p>In this process, the code navigates through a series of function calls to find a suitable model adapter based on the provided model path and retrieve a specific conversation template from it. This design pattern allows flexibility in providing different conversation templates for different models, enhancing the reusability and extensibility of the code.</p><hr><h2 id="2-Configuration-Classes"><a href="#2-Configuration-Classes" class="headerlink" title="2. Configuration Classes"></a>2. Configuration Classes</h2><p>These classes are defined using Python’s <code>dataclass</code> decorator and are mainly used for storing configurations and parameters. These classes usually do not contain complex methods or logic but are used to define and store data structures. These classes include:</p><ul><li><code>ModelArguments</code>: Stores parameters related to the model, like model path, trust in remote code, etc.</li><li><code>DataArguments</code>: Stores parameters related to data, like data path, evaluation data path, and whether to use lazy preprocessing.</li><li><code>TrainingArguments</code>: Stores parameters related to training, like cache directory, optimizer type, model maximum length, etc. This class extends <code>transformers.TrainingArguments</code> and adds some custom parameters.</li></ul><p>These classes are mainly used to simplify and organize parameter management in the code, making parameter modification and access more convenient.</p><h3 id="1-ModelArguments-Class"><a href="#1-ModelArguments-Class" class="headerlink" title="1. ModelArguments Class"></a>1. ModelArguments Class</h3><h4 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ModelArguments</span>:</span><br><span class="line">    model_name_or_path: <span class="type">Optional</span>[<span class="built_in">str</span>] = field(default=<span class="string">"facebook/opt-125m"</span>)</span><br><span class="line">    trust_remote_code: <span class="built_in">bool</span> = field(</span><br><span class="line">        default=<span class="literal">False</span>,</span><br><span class="line">        metadata={</span><br><span class="line">            <span class="string">"help"</span>: <span class="string">"Whether or not to allow for custom models defined on the Hub in their own modeling files"</span></span><br><span class="line">        },</span><br><span class="line">    )</span><br><span class="line">    padding_side: <span class="built_in">str</span> = field(</span><br><span class="line">        default=<span class="string">"right"</span>, metadata={<span class="string">"help"</span>: <span class="string">"The padding side in tokenizer"</span>}</span><br><span class="line">    )</span><br></pre></td></tr></tbody></table></figure><h4 id="Explanation"><a href="#Explanation" class="headerlink" title="Explanation"></a>Explanation</h4><p><code>ModelArguments</code> is a data class (<code>dataclass</code>) used for storing model-related configuration parameters.<br><strong>Attributes:</strong></p><ol><li><code>model_name_or_path</code>: Specifies the name or path of the pretrained model.</li><li><code>trust_remote_code</code>: Whether to allow custom models that have their modeling files defined on the Hub.</li><li><code>padding_side</code>: Specifies the padding side in the tokenizer, typically right or left padding.</li></ol><details><summary> Introduction to `@dataclass` decorator, click to expand </summary>`@dataclass` is a decorator used to automate the generation of special methods like `__init__()`, `__repr__()`, `__eq__()` etc., thus simplifying the writing of data classes. This decorator is part of Python 3.7 and is in the `dataclasses` module.<p>When you use <code>@dataclass</code> before a class definition, Python automatically adds some special methods based on the fields defined in the class. This is very useful for creating classes that store a small amount of data but do not need complex methods.</p><p>Specifically, using <code>@dataclass</code>:</p><ol><li><p><strong>Automatically generates a constructor (<code>__init__</code> method)</strong>: Python creates an <code>__init__</code> method automatically based on the fields defined in the class, so you don’t need to manually write this method to initialize your class instances.</p></li><li><p><strong>Automatically generates a <code>__repr__</code> method</strong>: This makes printing the class instances provide a more readable string representation, usually including the class name and its fields and their values.</p></li><li><p><strong>Automatically generates an <code>__eq__</code> method</strong>: This allows you to use the <code>==</code> operator to compare two instances of the class, comparing the values of the instance fields.</p></li><li><p><strong>Support for type annotations</strong>: When defining fields, you can use type annotations, which not only help with clarity of code but can also be checked for type correctness using some tools.</p></li></ol><p>In the case of the <code>ModelArguments</code> class, the <code>@dataclass</code> decorator will generate the above-mentioned methods. This means you can easily create an instance of <code>ModelArguments</code>, and when printing or comparing these instances, you will get the expected behavior.</p><p>For example, when you create an instance of <code>ModelArguments</code>:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">args = ModelArguments()</span><br></pre></td></tr></tbody></table></figure><p>This will call the automatically generated <code>__init__</code> method, using the default values “facebook/opt-125m” for <code>model_name_or_path</code>, <code>False</code> for <code>trust_remote_code</code>, and “right” for <code>padding_side</code>.</p><p>When you print this instance:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(args)</span><br></pre></td></tr></tbody></table></figure><p>This will call the automatically generated <code>__repr__</code> method, showing a detailed view of the class instance, like <code>ModelArguments(model_name_or_path="facebook/opt-125m", trust_remote_code=False, padding_side="right")</code>.</p><p>Thus, the <code>@dataclass</code> decorator simplifies the process of creating classes, making the code more concise and maintainable.</p><p>Overall, the <code>@dataclass</code> decorator is a convenient tool provided by Python for quickly creating classes mainly used for storing data.</p></details><h3 id="2-DataArguments-Class"><a href="#2-DataArguments-Class" class="headerlink" title="2. DataArguments Class"></a>2. DataArguments Class</h3><h4 id="Code-1"><a href="#Code-1" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DataArguments</span>:</span><br><span class="line">    data_path: <span class="built_in">str</span> = field(</span><br><span class="line">        default=<span class="literal">None</span>, metadata={<span class="string">"help"</span>: <span class="string">"Path to the training data."</span>}</span><br><span class="line">    )</span><br><span class="line">    eval_data_path: <span class="built_in">str</span> = field(</span><br><span class="line">        default=<span class="literal">None</span>, metadata={<span class="string">"help"</span>: <span class="string">"Path to the evaluation data."</span>}</span><br><span class="line">    )</span><br><span class="line">    lazy_preprocess: <span class="built_in">bool</span> = <span class="literal">False</span></span><br></pre></td></tr></tbody></table></figure><h4 id="Explanation-1"><a href="#Explanation-1" class="headerlink" title="Explanation"></a>Explanation</h4><p><strong>DataArguments Class</strong></p><ul><li><code>DataArguments</code> is also a data class used for storing data-related configuration parameters.</li><li>Attributes:<ul><li><code>data_path</code>: Path to the training data.</li><li><code>eval_data_path</code>: Path to the evaluation data.</li><li><code>lazy_preprocess</code>: Whether to use lazy loading for data preprocessing, i.e., load and process data as needed.</li></ul></li></ul><h3 id="3-TrainingArguments-Class"><a href="#3-TrainingArguments-Class" class="headerlink" title="3. TrainingArguments Class"></a>3. TrainingArguments Class</h3><h4 id="Code-2"><a href="#Code-2" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TrainingArguments</span>(transformers.TrainingArguments):</span><br><span class="line">    cache_dir: <span class="type">Optional</span>[<span class="built_in">str</span>] = field(default=<span class="literal">None</span>)</span><br><span class="line">    optim: <span class="built_in">str</span> = field(default=<span class="string">"adamw_torch"</span>)</span><br><span class="line">    model_max_length: <span class="built_in">int</span> = field(</span><br><span class="line">        default=<span class="number">512</span>,</span><br><span class="line">        metadata={</span><br><span class="line">            <span class="string">"help"</span>: <span class="string">"Maximum sequence length. Sequences will be right padded (and possibly truncated)."</span></span><br><span class="line">        },</span><br><span class="line">    )</span><br></pre></td></tr></tbody></table></figure><h4 id="Explanation-2"><a href="#Explanation-2" class="headerlink" title="Explanation"></a>Explanation</h4><p><code>TrainingArguments</code> class extends <code>transformers.TrainingArguments</code>.</p><ol><li><p><strong>TrainingArguments Class</strong></p><ul><li><code>TrainingArguments</code> is a data class that, by extending <code>transformers.TrainingArguments</code>, gains the capability to handle training parameters.</li><li>Attributes defined in <code>TrainingArguments</code>:<ul><li><code>cache_dir</code>: Specifies the directory path for caching the model and tokenizer.</li><li><code>optim</code>: Defines the type of optimizer to use, like <code>'adamw_torch'</code>.</li><li><code>model_max_length</code>: Specifies the maximum sequence length the model can handle.</li></ul></li></ul></li><li><p><strong>transformers.TrainingArguments Class</strong></p><ul><li><code>transformers.TrainingArguments</code> is a class in the transformers library that is used for configuring various parameters in the model training process.</li><li>This class contains a plethora of attributes for controlling the training process, such as:<ul><li><code>output_dir</code>: Specifies the directory to save the model and training results.</li><li><code>num_train_epochs</code>: Number of training epochs.</li><li><code>per_device_train_batch_size</code>: Batch size per device for training.</li><li><code>save_steps</code>: Steps interval for saving the model.</li><li><code>evaluation_strategy</code>: Strategy for evaluating the model, like at the end of each epoch.</li><li><code>learning_rate</code>: Learning rate.</li><li><code>warmup_steps</code>: Steps used for warmup in the learning rate schedule.</li></ul></li><li><code>transformers.TrainingArguments</code> also</li></ul></li></ol><p> contains many other parameters for fine-tuning the training process, including logging, model saving strategies, learning rate scheduling, and more.</p><p>By extending <code>transformers.TrainingArguments</code>, the <code>TrainingArguments</code> class not only inherits all these training parameter configurations but can also add some custom training parameters, like in this case <code>cache_dir</code>, <code>optim</code>, and <code>model_max_length</code>. This approach enhances code reusability and flexibility, allowing you to adjust and extend training configurations as per the specific requirements of your project.</p><h2 id="3-Functional-Utility-Functions"><a href="#3-Functional-Utility-Functions" class="headerlink" title="3. Functional Utility Functions"></a>3. Functional Utility Functions</h2><h3 id="1-rank0-print-args"><a href="#1-rank0-print-args" class="headerlink" title="1. rank0_print(*args)"></a>1. rank0_print(*args)</h3><h4 id="Code-3"><a href="#Code-3" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">local_rank = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">rank0_print</span>(<span class="params">*args</span>):</span><br><span class="line">    <span class="keyword">if</span> local_rank == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(*args)</span><br></pre></td></tr></tbody></table></figure><h4 id="Explanation-3"><a href="#Explanation-3" class="headerlink" title="Explanation"></a>Explanation</h4><p>Defines a global variable local_rank for distributed training.<br>Defines a function rank0_print to print information only if local_rank is 0, used for controlling output in distributed training. This way, repetitive printing of the same information across multiple nodes is avoided, making the output clearer and more concise.</p><ul><li>Used to print information only on the main node (rank 0) in a distributed training environment.</li><li>Parameters: A variable number of arguments for printing.</li></ul><h3 id="2-trainer-save-model-safe-trainer-transformers-Trainer"><a href="#2-trainer-save-model-safe-trainer-transformers-Trainer" class="headerlink" title="2. trainer_save_model_safe(trainer: transformers.Trainer)"></a>2. <code>trainer_save_model_safe(trainer: transformers.Trainer)</code></h3><h4 id="Code-4"><a href="#Code-4" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">trainer_save_model_safe</span>(<span class="params">trainer: transformers.Trainer</span>):</span><br><span class="line">    <span class="keyword">from</span> torch.distributed.fsdp <span class="keyword">import</span> FullyShardedDataParallel <span class="keyword">as</span> FSDP</span><br><span class="line">    <span class="keyword">from</span> torch.distributed.fsdp <span class="keyword">import</span> StateDictType, FullStateDictConfig</span><br><span class="line"></span><br><span class="line">    save_policy = FullStateDictConfig(offload_to_cpu=<span class="literal">True</span>, rank0_only=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">with</span> FSDP.state_dict_type(</span><br><span class="line">        trainer.model, StateDictType.FULL_STATE_DICT, save_policy</span><br><span class="line">    ):</span><br><span class="line">        trainer.save_model()</span><br></pre></td></tr></tbody></table></figure><p>The function <code>trainer_save_model_safe(trainer: transformers.Trainer)</code> aims to safely save models trained with the PyTorch distributed framework. Let’s delve into the details of this function and its key components.</p><h4 id="Explanation-4"><a href="#Explanation-4" class="headerlink" title="Explanation"></a>Explanation</h4><ol><li><p>Parameters:</p><ul><li><code>trainer</code>: An instance of <code>transformers.Trainer</code>. This class is one of the core components of the Hugging Face Transformers library, used for training and evaluating models.</li></ul></li><li><p>Functionality:</p><ul><li>The main purpose of this function is to safely save models in a distributed training environment. It particularly considers the model saving strategy when using Fully Sharded Data Parallel (FSDP).</li></ul></li><li><p>FSDP</p><ul><li><strong>FullyShardedDataParallel (FSDP)</strong><ul><li>This is a component of PyTorch’s distributed training framework. FSDP helps reduce memory usage on each GPU by sharding model parameters across multiple GPUs, allowing the training of larger models.</li><li>In this context, FSDP is primarily used for handling and saving model states in distributed training.</li></ul></li><li><strong>StateDictType</strong><ul><li>This is an enumeration type that defines how to save the model’s state dictionary. In FSDP environments, saving and loading model states might require special handling.</li></ul></li><li><strong>FullStateDictConfig</strong><ul><li>This class configures parameters for saving the full state dictionary. It’s part of FSDP’s functionality and is used to control how the model state is saved.</li></ul></li></ul></li><li><p>Function Implementation</p><ul><li><strong>Setting Save Policy</strong><ul><li><code>save_policy = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)</code> creates a save policy. Here, two key parameters are specified:<ul><li><code>offload_to_cpu</code>: Offload model parameters to CPU before saving the state dictionary, which helps reduce GPU memory usage.</li><li><code>rank0_only</code>: Save the model only on rank 0 (usually the main node). In distributed training, this avoids saving the same model copy on every node, saving storage space.</li></ul></li></ul></li><li><strong>Saving the Model</strong><ul><li>Using the <code>with FSDP.state_dict_type(trainer.model, StateDictType.FULL_STATE_DICT, save_policy)</code> context manager, the type and policy for saving the model’s state dictionary are set.</li><li>Within this context, <code>trainer.save_model()</code> is called to save the model. Due to the <code>save_policy</code>, the model is saved securely following the specified configuration.</li></ul></li></ul></li></ol><p>The function <code>trainer_save_model_safe</code> encapsulates a safe model saving logic, particularly for scenarios involving PyTorch’s FSDP in distributed training. It ensures that only a complete model state is saved on one node and offloads model parameters to CPU before saving, optimizing memory usage and storage efficiency. This is crucial for training large models and managing large-scale distributed training environments.</p><h3 id="3-preprocess-sources-tokenizer-transformers-PreTrainedTokenizer-gt-Dict"><a href="#3-preprocess-sources-tokenizer-transformers-PreTrainedTokenizer-gt-Dict" class="headerlink" title="3.preprocess(sources,tokenizer: transformers.PreTrainedTokenizer) -> Dict"></a>3.<code>preprocess(sources,tokenizer: transformers.PreTrainedTokenizer) -&gt; Dict</code></h3><h4 id="Code-5"><a href="#Code-5" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess</span>(<span class="params"></span></span><br><span class="line"><span class="params">    sources,</span></span><br><span class="line"><span class="params">    tokenizer: transformers.PreTrainedTokenizer,</span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="type">Dict</span>:</span><br><span class="line">    conv = get_conversation_template(<span class="string">"vicuna"</span>)</span><br><span class="line">    roles = {<span class="string">"human"</span>: conv.roles[<span class="number">0</span>], <span class="string">"gpt"</span>: conv.roles[<span class="number">1</span>]}</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Apply prompt templates</span></span><br><span class="line">    conversations = []</span><br><span class="line">    <span class="keyword">for</span> i, source <span class="keyword">in</span> <span class="built_in">enumerate</span>(sources):</span><br><span class="line">        <span class="keyword">if</span> roles[source[<span class="number">0</span>][<span class="string">"from"</span>]] != conv.roles[<span class="number">0</span>]:</span><br><span class="line">            <span class="comment"># Skip the first one if it is not from human</span></span><br><span class="line">            source = source[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">        conv.messages = []</span><br><span class="line">        <span class="keyword">for</span> j, sentence <span class="keyword">in</span> <span class="built_in">enumerate</span>(source):</span><br><span class="line">            role = roles[sentence[<span class="string">"from"</span>]]</span><br><span class="line">            <span class="keyword">assert</span> role == conv.roles[j % <span class="number">2</span>], <span class="string">f"<span class="subst">{i}</span>"</span></span><br><span class="line">            conv.append_message(role, sentence[<span class="string">"value"</span>])</span><br><span class="line">        conversations.append(conv.get_prompt())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Tokenize conversations</span></span><br><span class="line">    input_ids = tokenizer(</span><br><span class="line">        conversations,</span><br><span class="line">        return_tensors=<span class="string">"pt"</span>,</span><br><span class="line">        padding=<span class="string">"max_length"</span>,</span><br><span class="line">        max_length=tokenizer.model_max_length,</span><br><span class="line">        truncation=<span class="literal">True</span>,</span><br><span class="line">    ).input_ids</span><br><span class="line">    targets = input_ids.clone()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> conv.sep_style == SeparatorStyle.ADD_COLON_TWO</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Mask targets. Only compute loss on the assistant outputs.</span></span><br><span class="line">    sep = conv.sep + conv.roles[<span class="number">1</span>] + <span class="string">": "</span></span><br><span class="line">    <span class="keyword">for</span> conversation, target <span class="keyword">in</span> <span class="built_in">zip</span>(conversations, targets):</span><br><span class="line">        total_len = <span class="built_in">int</span>(target.ne(tokenizer.pad_token_id).<span class="built_in">sum</span>())</span><br><span class="line"></span><br><span class="line">        turns = conversation.split(conv.sep2)</span><br><span class="line">        cur_len = <span class="number">1</span></span><br><span class="line">        target[:cur_len] = IGNORE_TOKEN_ID</span><br><span class="line">        <span class="keyword">for</span> i, turn <span class="keyword">in</span> <span class="built_in">enumerate</span>(turns):</span><br><span class="line">            <span class="keyword">if</span> turn == <span class="string">""</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            turn_len = <span class="built_in">len</span>(tokenizer(turn).input_ids)</span><br><span class="line"></span><br><span class="line">            parts = turn.split(sep)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(parts) != <span class="number">2</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            parts[<span class="number">0</span>] += sep</span><br><span class="line">            <span class="comment"># "-2" is hardcoded for the Llama tokenizer to make the offset correct.</span></span><br><span class="line">            instruction_len = <span class="built_in">len</span>(tokenizer(parts[<span class="number">0</span>]).input_ids) - <span class="number">2</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i != <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> tokenizer.legacy:</span><br><span class="line">                <span class="comment"># The legacy and non-legacy modes handle special tokens differently</span></span><br><span class="line">                instruction_len -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Ignore the user instructions</span></span><br><span class="line">            target[cur_len : cur_len + instruction_len] = IGNORE_TOKEN_ID</span><br><span class="line">            cur_len += turn_len</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i != <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> tokenizer.legacy:</span><br><span class="line">                <span class="comment"># The legacy and non-legacy modes handle special tokens differently</span></span><br><span class="line">                cur_len -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        target[cur_len:] = IGNORE_TOKEN_ID</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="literal">False</span>:  <span class="comment"># Inspect and check the correctness of masking</span></span><br><span class="line">            z = target.clone()</span><br><span class="line">            z = torch.where(z == IGNORE_TOKEN_ID, tokenizer.unk_token_id, z)</span><br><span class="line">            rank0_print(tokenizer.decode(z))</span><br><span class="line">            exit()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> cur_len &lt; tokenizer.model_max_length:</span><br><span class="line">           </span><br><span class="line"></span><br><span class="line"> <span class="keyword">if</span> cur_len != total_len:</span><br><span class="line">                target[:] = IGNORE_TOKEN_ID</span><br><span class="line">                rank0_print(</span><br><span class="line">                    <span class="string">f"WARNING: tokenization mismatch: <span class="subst">{cur_len}</span> vs. <span class="subst">{total_len}</span>."</span></span><br><span class="line">                    <span class="string">f" #turn = <span class="subst">{<span class="built_in">len</span>(turns) - <span class="number">1</span>}</span>. (ignored)"</span></span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">dict</span>(</span><br><span class="line">        input_ids=input_ids,</span><br><span class="line">        labels=targets,</span><br><span class="line">        attention_mask=input_ids.ne(tokenizer.pad_token_id),</span><br><span class="line">    )</span><br></pre></td></tr></tbody></table></figure><p>The function <code>preprocess(sources, tokenizer: transformers.PreTrainedTokenizer) -&gt; Dict</code> is intended for preprocessing dialogue data to be suitable for training machine learning models. This function can be broken down into several main parts for a more detailed explanation:</p><h4 id="1-Obtaining-Conversation-Templates-and-Role-Definitions"><a href="#1-Obtaining-Conversation-Templates-and-Role-Definitions" class="headerlink" title="1. Obtaining Conversation Templates and Role Definitions"></a>1. Obtaining Conversation Templates and Role Definitions</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conv = get_conversation_template(<span class="string">"vicuna"</span>)</span><br><span class="line">roles = {<span class="string">"human"</span>: conv.roles[<span class="number">0</span>], <span class="string">"gpt"</span>: conv.roles[<span class="number">1</span>]}</span><br></pre></td></tr></tbody></table></figure><ul><li><strong>Functionality</strong>: Initializes conversation templates and defines the roles of dialogue participants.</li><li><strong>Implementation</strong>:<ul><li><code>conv = get_conversation_template("vicuna")</code> obtains the conversation template for a specified model (e.g., “vicuna”).</li><li>The <code>roles</code> dictionary maps “human” and “gpt” to the roles defined in the conversation template.</li></ul></li><li><strong>Example</strong>:<ul><li>If the conversation template is for “vicuna”, then <code>roles</code> might map “human” to “user” and “gpt” to “assistant”. For example, <code>{'human': 'USER', 'gpt': 'ASSISTANT'}</code>.</li></ul></li></ul><h4 id="2-Applying-Prompt-Templates"><a href="#2-Applying-Prompt-Templates" class="headerlink" title="2. Applying Prompt Templates"></a>2. Applying Prompt Templates</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Apply prompt templates</span></span><br><span class="line">conversations = []</span><br><span class="line"><span class="keyword">for</span> i, source <span class="keyword">in</span> <span class="built_in">enumerate</span>(sources):</span><br><span class="line">    <span class="keyword">if</span> roles[source[<span class="number">0</span>][<span class="string">"from"</span>]] != conv.roles[<span class="number">0</span>]:</span><br><span class="line">        <span class="comment"># Skip the first one if it is not from human</span></span><br><span class="line">        source = source[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">    conv.messages = []</span><br><span class="line">    <span class="keyword">for</span> j, sentence <span class="keyword">in</span> <span class="built_in">enumerate</span>(source):</span><br><span class="line">        role = roles[sentence[<span class="string">"from"</span>]]</span><br><span class="line">        <span class="keyword">assert</span> role == conv.roles[j % <span class="number">2</span>], <span class="string">f"<span class="subst">{i}</span>"</span></span><br><span class="line">        conv.append_message(role, sentence[<span class="string">"value"</span>])</span><br><span class="line">    conversations.append(conv.get_prompt())</span><br></pre></td></tr></tbody></table></figure><ul><li><strong>Functionality</strong>: Applies prompt templates to source data to construct dialogues.</li><li><strong>Implementation</strong>:<ul><li>Iterates through <code>sources</code> (original dialogue data), transforming each dialogue source into a conversation in template format.</li><li>If the first part of a dialogue is not initiated by the “human” role, it skips that part.</li><li>Assigns a role to each sentence and adds it to the conversation template.</li><li>Ultimately, each processed dialogue is added to the <code>conversations</code> list.</li></ul></li><li><strong>Example</strong>:<ul><li>Suppose we have a source which is the first item in dummy input: <code>python source = [{'from': 'human', 'value': 'Who are you?'}, {'from': 'gpt', 'value': 'I am Vicuna, a language model trained by researchers from Large Model Systems Organization (LMSYS).'}, {'from': 'human', 'value': 'Have a nice day!'}, {'from': 'gpt', 'value': 'You too!'}]</code></li><li><code>conversations</code> under the Vicuna template, using <code>SeparatorStyle.ADD_COLON_TWO</code> as the separator style, might look like [“A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user’s questions. USER: Who are you? ASSISTANT: I am Vicuna, a language model trained by researchers from Large Model Systems Organization (LMSYS).USER: Have a nice day! ASSISTANT: You too!“]</li><li><details><summary> Implementation of get_prompt </summary>The `get_prompt` method implementation varies depending on the `SeparatorStyle`. Below is a table detailing the `get_prompt` method for various styles, along with English examples:<table><thead><tr><th>Separator Style (<code>SeparatorStyle</code>)</th><th>Description</th><th>Example</th></tr></thead><tbody><tr><td><code>ADD_COLON_SINGLE</code></td><td>Adds a colon and separator after each message.</td><td>USER: Hello there!\nASSISTANT: Hi, how can I help?\n</td></tr><tr><td><code>ADD_COLON_TWO</code></td><td>Uses two alternating separators, usually between different roles.</td><td>USER: What’s the weather?\nASSISTANT: It’s sunny today.\n\n</td></tr><tr><td><code>ADD_COLON_SPACE_SINGLE</code></td><td>Adds a colon, space, and separator after each message.</td><td>USER: Can you book a flight?\nASSISTANT: Sure, where to?\n</td></tr><tr><td><code>NO_COLON_SINGLE</code></td><td>Messages directly follow roles without a colon, followed by a separator.</td><td>USERWhat are you doing?\nASSISTANTI’m here to assist you.\n</td></tr><tr><td><code>NO_COLON_TWO</code></td><td>No colons, with two alternating separators.</td><td>USERHow’s the project going?\nASSISTANTIt’s on track.\n\n</td></tr><tr><td><code>ADD_NEW_LINE_SINGLE</code></td><td>Each message is preceded by a newline, followed by a separator.</td><td>USER\nHow can I reset my password?\nASSISTANT\nYou can reset it via email.\n</td></tr><tr><td><code>RWKV</code></td><td>Special format, usually for specific models.</td><td>USER: What is AI?\n\nASSISTANT: AI stands for Artificial Intelligence.\n\n</td></tr><tr><td><code>LLAMA2</code></td><td>Special label format for specific models.</td><td>[INST] USER How does blockchain work?\nASSISTANT It is a distributed ledger.\n\n</td></tr><tr><td><code>CHATGLM</code></td><td>Specific format for <code>CHATGLM</code> model.</td><td>[Round 1]\nUSER: Tell me a joke.\nASSISTANT: Why did the chicken cross the road?\n</td></tr><tr><td><code>CHATML</code></td><td>Similar to <code>CHATGLM</code>, but with newlines before and after each message.</td><td>USER\nDo you like music?\n\nASSISTANT\nYes, I enjoy many genres.\n\n</td></tr><tr><td><code>CHATGLM3</code></td><td>Format for <code>CHATGLM3</code> model.</td><td>USER\nCan you play chess?\nASSISTANTYes, I can play.\n</td></tr><tr><td><code>CHATINTERN</code></td><td>Format for <code>CHATINTERN</code> model, using special markers.</td><td><s>USER:Where is the nearest ATM?<s>\nASSISTANT:It’s next to the post office.\n</s></s></td></tr><tr><td><code>DOLLY</code></td><td>Specific format for <code>DOLLY</code> model.</td><td>USER:\nWhat is quantum computing?\nASSISTANT:\nIt involves computation using quantum-mechanical phenomena.\n\n</td></tr><tr><td><code>PHOENIX</code></td><td>For <code>PHOENIX</code> model, messages are wrapped in special markers.</td><td>USER: <s>How to bake a cake?</s>\nASSISTANT: <s>You need flour, sugar, and eggs.</s>\n</td></tr><tr><td><code>ROBIN</code></td><td>Similar to <code>ADD_NEW_LINE_SINGLE</code>, but with a newline after roles.</td><td>USER:\nIs AI dangerous?\nASSISTANT:\nIt depends on how it’s used.\n</td></tr><tr><td></td><td></td><td></td></tr></tbody></table></details></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;FastChat-Training-Script-Code-Analysis-Train-py-【FastChat-Series-Part-1】&quot;&gt;&lt;a href=&quot;#FastChat-Training-Script-Code-Analysis-Train-py-</summary>
      
    
    
    
    <category term="NLP Insights" scheme="https://chenhuiyu.github.io/categories/NLP-Insights/"/>
    
    
    <category term="FastChat" scheme="https://chenhuiyu.github.io/tags/FastChat/"/>
    
    <category term="LLM" scheme="https://chenhuiyu.github.io/tags/LLM/"/>
    
    <category term="Train" scheme="https://chenhuiyu.github.io/tags/Train/"/>
    
  </entry>
  
  <entry>
    <title>FastChat 训练脚本代码逐行解析-Train.py 【FastChat 系列第 1 篇】</title>
    <link href="https://chenhuiyu.github.io/2024/02/27/NLP%20Insights/FastChat%20%E8%AE%AD%E7%BB%83%E8%84%9A%E6%9C%AC%E4%BB%A3%E7%A0%81%E9%80%90%E8%A1%8C%E8%A7%A3%E6%9E%90-Train.py%20%E3%80%90FastChat%20%E7%B3%BB%E5%88%97%E7%AC%AC%201%20%E7%AF%87%E3%80%91/"/>
    <id>https://chenhuiyu.github.io/2024/02/27/NLP%20Insights/FastChat%20%E8%AE%AD%E7%BB%83%E8%84%9A%E6%9C%AC%E4%BB%A3%E7%A0%81%E9%80%90%E8%A1%8C%E8%A7%A3%E6%9E%90-Train.py%20%E3%80%90FastChat%20%E7%B3%BB%E5%88%97%E7%AC%AC%201%20%E7%AF%87%E3%80%91/</id>
    <published>2024-02-26T17:43:18.000Z</published>
    <updated>2024-02-27T02:33:36.395Z</updated>
    
    <content type="html"><![CDATA[<h1 id="FastChat-训练脚本代码逐行解析-Train-py-【FastChat-系列第-1-篇】"><a href="#FastChat-训练脚本代码逐行解析-Train-py-【FastChat-系列第-1-篇】" class="headerlink" title="FastChat 训练脚本代码逐行解析-Train.py 【FastChat 系列第 1 篇】"></a>FastChat 训练脚本代码逐行解析-Train.py 【FastChat 系列第 1 篇】</h1><p>在本文中，我们将深入探讨 <a href="https://github.com/lm-sys/FastChat">FastChat</a> 的 <code>train.py</code> 脚本，这是一个用于训练和优化大型语言模型的关键组件。FastChat 是一个先进的开源平台，专注于开发、部署和评估基于大型语言模型（LLM）的聊天机器人。该平台不仅提供对顶尖模型如 Vicuna 和 MT-Bench 的支持，还包括一个分布式的多模型服务系统，配备了 Web UI 和与 OpenAI 兼容的 RESTful API，使用户能够高效地训练和评估他们的模型。</p><p>本文的深入分析将聚焦于 <code>train.py</code> 脚本的源代码。这个脚本是基于 transformers 库的自然语言处理模型训练脚本，涵盖了数据预处理、模型训练和保存等关键步骤。我们旨在提供对 <code>train.py</code> 中每个类和函数的详细解释，包括它们的功能和在整个训练过程中的作用。</p><h2 id="1-导入模块"><a href="#1-导入模块" class="headerlink" title="1. 导入模块"></a>1. 导入模块</h2><h3 id="1-内置模块"><a href="#1-内置模块" class="headerlink" title="1. 内置模块"></a>1. 内置模块</h3><p>这些是 Python 自带的标准库模块，无需额外安装。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> dataclasses <span class="keyword">import</span> dataclass, field</span><br></pre></td></tr></tbody></table></figure><p>导入 Python 的<code>dataclasses</code>模块，用于创建带有默认值的类。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br></pre></td></tr></tbody></table></figure><p>导入<code>json</code>模块，用于处理 JSON 格式的数据。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br></pre></td></tr></tbody></table></figure><p>导入<code>math</code>模块，用于数学运算。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pathlib</span><br></pre></td></tr></tbody></table></figure><p>导入<code>pathlib</code>模块，用于处理文件路径。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Dict</span>, <span class="type">Optional</span>, <span class="type">Sequence</span></span><br></pre></td></tr></tbody></table></figure><p>导入<code>typing</code>模块，用于类型注解。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></tbody></table></figure><h3 id="2-依赖库"><a href="#2-依赖库" class="headerlink" title="2. 依赖库"></a>2. 依赖库</h3><p>这些是外部安装的依赖库，通常通过包管理器如 pip 安装。<br>导入<code>numpy</code>库，一个常用的科学计算库。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></tbody></table></figure><p>导入<code>PyTorch</code>，一个流行的深度学习框架。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br></pre></td></tr></tbody></table></figure><p>从<code>torch</code>中导入<code>Dataset</code>，用于创建自定义数据集。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> transformers</span><br></pre></td></tr></tbody></table></figure><p>导入<code>transformers</code>库，一个流行的自然语言处理库。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Trainer</span><br></pre></td></tr></tbody></table></figure><p>从<code>transformers</code>中导入<code>Trainer</code>，用于训练模型。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers.trainer_pt_utils <span class="keyword">import</span> LabelSmoother</span><br></pre></td></tr></tbody></table></figure><p>从<code>transformers</code>中导入<code>LabelSmoother</code>，用于标签平滑。</p><h3 id="3-项目特定函数"><a href="#3-项目特定函数" class="headerlink" title="3. 项目特定函数"></a>3. 项目特定函数</h3><p>这些是在 Fast Chat 项目中自定义实现的函数或类。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fastchat.conversation <span class="keyword">import</span> SeparatorStyle</span><br><span class="line"></span><br><span class="line"><span class="comment">## code from FastChat/fastchat/conversation.py</span></span><br><span class="line">    <span class="string">"""Separator styles."""</span></span><br><span class="line"></span><br><span class="line">    ADD_COLON_SINGLE = auto()</span><br><span class="line">    ADD_COLON_TWO = auto()</span><br><span class="line">    ADD_COLON_SPACE_SINGLE = auto()</span><br><span class="line">    NO_COLON_SINGLE = auto()</span><br><span class="line">    NO_COLON_TWO = auto()</span><br><span class="line">    ADD_NEW_LINE_SINGLE = auto()</span><br><span class="line">    LLAMA2 = auto()</span><br><span class="line">    CHATGLM = auto()</span><br><span class="line">    CHATML = auto()</span><br><span class="line">    CHATINTERN = auto()</span><br><span class="line">    DOLLY = auto()</span><br><span class="line">    RWKV = auto()</span><br><span class="line">    PHOENIX = auto()</span><br><span class="line">    ROBIN = auto()</span><br><span class="line">    FALCON_CHAT = auto()</span><br><span class="line">    CHATGLM3 = auto()</span><br><span class="line">    DEEPSEEK_CHAT = auto()</span><br><span class="line">    METAMATH = auto()</span><br><span class="line">    YUAN2 = auto()</span><br></pre></td></tr></tbody></table></figure><p>从<code>fastchat</code>包导入<code>SeparatorStyle</code>，用于定义对话分隔符风格。<code>SeparatorStyle</code> 类是一个使用 Python 的 <code>enum</code> 模块创建的枚举类，用于定义一系列的分隔符样式。枚举（Enumeration）是一种编程概念，用于定义一组命名的常数，使代码更加清晰和易于维护。</p><p>在 <code>SeparatorStyle</code> 类中，每个成员代表一种特定的分隔符样式。这些样式通常用于文本处理中，特别是在需要区分不同部分或元素的情况下。例如，在处理对话或文本数据时，可能需要不同的方式来区分用户输入和机器回复。</p><p>关于 <code>auto()</code> 函数的使用：</p><ul><li><code>auto()</code> 是 Python <code>enum</code> 模块提供的一个特殊函数。它在枚举类中自动分配一个唯一的值给每个成员。</li><li>在不使用 <code>auto()</code> 的情况下，你需要手动为每个枚举成员指定一个唯一的值。使用 <code>auto()</code> 可以简化这个过程，让 Python 自动处理这些值的分配。</li><li><code>auto()</code> 分配的值通常是整数，从 1 开始依次递增。</li></ul><p>具体到 <code>SeparatorStyle</code> 类，<code>auto()</code> 被用来为每种分隔符样式自动分配一个唯一的整数值。例如，<code>ADD_COLON_SINGLE</code>、<code>ADD_COLON_TWO</code> 等将分别被赋予不同的整数值。</p><p>每个枚举成员的名称（如 <code>ADD_COLON_SINGLE</code>、<code>NO_COLON_SINGLE</code> 等）通常描述了该分隔符样式的特点。例如，<code>ADD_COLON_SINGLE</code> 可能表示在某个元素后添加一个冒号作为分隔符，而 <code>NO_COLON_SINGLE</code> 则表示不添加冒号。</p><p>这种方式使得在代码中引用和处理这些分隔符样式变得更加方便和清晰。例如，可以根据不同的场景或需求选择使用不同的分隔符样式，而无需记住它们对应的具体值。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fastchat.model.model_adapter <span class="keyword">import</span> get_conversation_template</span><br><span class="line"></span><br><span class="line"><span class="comment">## code from FastChat/fastchat/model/model_adapter.py</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_conversation_template</span>(<span class="params">model_path: <span class="built_in">str</span></span>) -&gt; Conversation:</span><br><span class="line">    <span class="string">"""Get the default conversation template."""</span></span><br><span class="line">    adapter = get_model_adapter(model_path)</span><br><span class="line">    <span class="keyword">return</span> adapter.get_default_conv_template(model_path)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## code from FastChat/fastchat/model/model_adapter.py</span></span><br><span class="line"><span class="meta">@cache</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_model_adapter</span>(<span class="params">model_path: <span class="built_in">str</span></span>) -&gt; BaseModelAdapter:</span><br><span class="line">    <span class="string">"""Get a model adapter for a model_path."""</span></span><br><span class="line">    model_path_basename = os.path.basename(os.path.normpath(model_path))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Try the basename of model_path at first</span></span><br><span class="line">    <span class="keyword">for</span> adapter <span class="keyword">in</span> model_adapters:</span><br><span class="line">        <span class="keyword">if</span> adapter.<span class="keyword">match</span>(model_path_basename) <span class="keyword">and</span> <span class="built_in">type</span>(adapter) != BaseModelAdapter:</span><br><span class="line">            <span class="keyword">return</span> adapter</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Then try the full path</span></span><br><span class="line">    <span class="keyword">for</span> adapter <span class="keyword">in</span> model_adapters:</span><br><span class="line">        <span class="keyword">if</span> adapter.<span class="keyword">match</span>(model_path):</span><br><span class="line">            <span class="keyword">return</span> adapter</span><br><span class="line"></span><br><span class="line">    <span class="keyword">raise</span> ValueError(<span class="string">f"No valid model adapter for <span class="subst">{model_path}</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## code from FastChat/fastchat/model/model_adapter.py</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_default_conv_template</span>(<span class="params">self, model_path: <span class="built_in">str</span></span>) -&gt; Conversation:</span><br><span class="line">        <span class="keyword">return</span> get_conv_template(<span class="string">"one_shot"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## code from FastChat/fastchat/conversation.py</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_conv_template</span>(<span class="params">name: <span class="built_in">str</span></span>) -&gt; Conversation:</span><br><span class="line">    <span class="string">"""Get a conversation template."""</span></span><br><span class="line">    <span class="keyword">return</span> conv_templates[name].copy()</span><br><span class="line"></span><br><span class="line"><span class="comment">## code from FastChat/fastchat/conversation.py</span></span><br><span class="line"><span class="comment"># A global registry for all conversation templates</span></span><br><span class="line">conv_templates: <span class="type">Dict</span>[<span class="built_in">str</span>, Conversation] = {}</span><br></pre></td></tr></tbody></table></figure><p>从<code>fastchat</code>包导入<code>get_conversation_template</code>，用于获取对话模板。<br>在这段代码中，调用逻辑主要涉及到获取特定模型的默认对话模板。调用链路如下：</p><ol><li><p><strong>起始调用 - <code>get_conversation_template(model_path: str)</code></strong></p><ul><li>这个函数是调用链的起点。它接收一个参数 <code>model_path</code>，用于指定模型的路径。</li><li>这个函数的目的是获取给定模型路径的默认对话模板。</li></ul></li><li><p><strong>调用 <code>get_model_adapter(model_path: str)</code></strong></p><ul><li><code>get_conversation_template</code> 函数首先调用 <code>get_model_adapter</code>，传入模型路径。</li><li><code>get_model_adapter</code> 的目的是根据提供的模型路径，找到并返回一个适合该模型的 <code>BaseModelAdapter</code> 对象。</li><li>这个函数首先尝试匹配 <code>model_path</code> 的基本名称（basename），如果没有找到匹配项，它会尝试匹配完整的路径。</li><li>如果找到合适的适配器，则返回该适配器；如果没有找到，则抛出一个 <code>ValueError</code>。</li></ul></li><li><p><strong>执行 <code>BaseModelAdapter.get_default_conv_template(model_path: str)</code></strong></p><ul><li>在获取到适当的模型适配器后，<code>get_conversation_template</code> 通过调用该适配器的 <code>get_default_conv_template</code> 方法来获取默认的对话模板。</li><li>注意这个方法在 <code>BaseModelAdapter</code> 类中定义，但可能在子类中被重写。</li></ul></li><li><p><strong>调用 <code>get_conv_template(name: str)</code></strong></p><ul><li>在 <code>get_default_conv_template</code> 方法内部，它调用 <code>get_conv_template</code> 函数，通常传入一个预定义的模板名称，比如 <code>"one_shot"</code>。</li><li><code>get_conv_template</code> 的作用是从全局注册的对话模板字典 <code>conv_templates</code> 中获取指定名称的模板。</li></ul></li><li><p><strong>获取并返回 <code>Conversation</code> 对象</strong></p><ul><li><code>get_conv_template</code> 函数返回 <code>Conversation</code> 类的一个实例，这通常是从 <code>conv_templates</code> 字典中复制得到的。</li><li>最终，这个 <code>Conversation</code> 实例被返回到最初调用 <code>get_conversation_template</code> 的地方。</li></ul></li></ol><p>总结调用链路：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">get_conversation_template(model_path)</span><br><span class="line">  -&gt; get_model_adapter(model_path)</span><br><span class="line">  -&gt; [BaseModelAdapter].get_default_conv_template(model_path)</span><br><span class="line">    -&gt; get_conv_template(name)</span><br><span class="line">      -&gt; 返回 Conversation 对象</span><br></pre></td></tr></tbody></table></figure><p>在这个过程中，代码通过一系列函数调用，根据提供的模型路径，找到相应的模型适配器，并从中获取特定的对话模板。这种设计模式允许灵活地为不同的模型提供不同的对话模板，从而提高了代码的可重用性和可扩展性。</p><hr><h2 id="2-配置类"><a href="#2-配置类" class="headerlink" title="2. 配置类"></a>2. 配置类</h2><p>这些类是使用 Python 的 <code>dataclass</code> 装饰器定义的，主要用于存储配置和参数。这些类通常不包含复杂的方法或逻辑，而是用于定义和存储数据结构。这些类包括：</p><ul><li><code>ModelArguments</code>: 存储与模型相关的参数，如模型路径、远程代码信任等。</li><li><code>DataArguments</code>: 存储与数据相关的参数，如数据路径、评估数据路径以及是否使用懒加载预处理。</li><li><code>TrainingArguments</code>: 存储与训练相关的参数，如缓存目录、优化器类型、模型最大长度等。这个类继承自 <code>transformers.TrainingArguments</code>，增加了一些自定义参数。</li></ul><p>这些类主要用于简化和组织代码中的参数管理，使得参数的修改和访问更加方便。</p><h3 id="1-ModelArguments-类"><a href="#1-ModelArguments-类" class="headerlink" title="1. ModelArguments 类"></a>1. ModelArguments 类</h3><h4 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ModelArguments</span>:</span><br><span class="line">    model_name_or_path: <span class="type">Optional</span>[<span class="built_in">str</span>] = field(default=<span class="string">"facebook/opt-125m"</span>)</span><br><span class="line">    trust_remote_code: <span class="built_in">bool</span> = field(</span><br><span class="line">        default=<span class="literal">False</span>,</span><br><span class="line">        metadata={</span><br><span class="line">            <span class="string">"help"</span>: <span class="string">"Whether or not to allow for custom models defined on the Hub in their own modeling files"</span></span><br><span class="line">        },</span><br><span class="line">    )</span><br><span class="line">    padding_side: <span class="built_in">str</span> = field(</span><br><span class="line">        default=<span class="string">"right"</span>, metadata={<span class="string">"help"</span>: <span class="string">"The padding side in tokenizer"</span>}</span><br><span class="line">    )</span><br></pre></td></tr></tbody></table></figure><h4 id="Explanation"><a href="#Explanation" class="headerlink" title="Explanation"></a>Explanation</h4><p><code>ModelArguments</code> 是一个数据类（<code>dataclass</code>），用于存储与模型相关的配置参数。<br><strong>属性：</strong></p><ol><li><code>model_name_or_path</code>: 指定预训练模型的名称或路径。</li><li><code>trust_remote_code</code>: 是否允许使用自定义模型，这些模型在 Hub 上有自己的模型文件。</li><li><code>padding_side</code>: 指定在分词器（<code>tokenizer</code>）中使用的填充方式，通常是左填充或右填充。</li></ol><details><summary> `@dataclass`装饰器的介绍，点击展开</summary>`@dataclass` 是一个装饰器，用于自动化生成特殊方法，如 `__init__()`、`__repr__()`、`__eq__()` 等，从而简化数据类的编写。这个装饰器是 Python 3.7 中引入的一部分，属于 `dataclasses` 模块。<p>当你在一个类定义前使用 <code>@dataclass</code> 装饰器时，Python 会自动为这个类添加一些由属性定义的特殊方法。这对于创建存储少量数据但不需要复杂方法的类非常有用。</p><p>具体来说，使用 <code>@dataclass</code> 时：</p><ol><li><p><strong>自动生成构造函数（<code>__init__</code> 方法）</strong>：Python 会根据类中定义的字段自动创建一个 <code>__init__</code> 方法，这样你就不需要手动编写这个方法来初始化类的实例了。</p></li><li><p><strong>自动生成 <code>__repr__</code> 方法</strong>：这使得打印类的实例时能够得到更具可读性的字符串表示，通常包含类名和其中的字段及其值。</p></li><li><p><strong>自动生成 <code>__eq__</code> 方法</strong>：这使得可以使用 <code>==</code> 操作符来比较两个类的实例，比较的是实例中字段的值。</p></li><li><p><strong>支持类型注解</strong>：在定义字段时，你可以使用类型注解，这不仅有助于代码清晰性，还可以通过一些工具进行类型检查。</p></li></ol><p>在<code>ModelArguments</code>类的例子中，<code>@dataclass</code>装饰器会为这个类生成上述的方法。这意味着你可以很方便地创建<code>ModelArguments</code>的实例，并在打印或比较这些实例时得到预期的行为。</p><p>例如，当你创建一个<code>ModelArguments</code>实例时：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">args = ModelArguments()</span><br></pre></td></tr></tbody></table></figure><p>这将调用自动生成的<code>__init__</code>方法，使用默认值”facebook/opt-125m”为<code>model_name_or_path</code>、<code>False</code>为<code>trust_remote_code</code>和”right”为<code>padding_side</code>。</p><p>当你打印这个实例：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(args)</span><br></pre></td></tr></tbody></table></figure><p>这将调用自动生成的<code>__repr__</code>方法，显示类实例的详细信息，如<code>ModelArguments(model_name_or_path="facebook/opt-125m", trust_remote_code=False, padding_side="right")</code>。</p><p>这样，<code>@dataclass</code>装饰器简化了类的创建过程，使得代码更加简洁和易于维护。</p><p>总的来说，<code>@dataclass</code> 装饰器是 Python 提供的一个便捷工具，用于快速创建主要用于存储数据的类。</p></details><h3 id="2-DataArguments-类"><a href="#2-DataArguments-类" class="headerlink" title="2. DataArguments 类"></a>2. DataArguments 类</h3><h4 id="Code-1"><a href="#Code-1" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DataArguments</span>:</span><br><span class="line">    data_path: <span class="built_in">str</span> = field(</span><br><span class="line">        default=<span class="literal">None</span>, metadata={<span class="string">"help"</span>: <span class="string">"Path to the training data."</span>}</span><br><span class="line">    )</span><br><span class="line">    eval_data_path: <span class="built_in">str</span> = field(</span><br><span class="line">        default=<span class="literal">None</span>, metadata={<span class="string">"help"</span>: <span class="string">"Path to the evaluation data."</span>}</span><br><span class="line">    )</span><br><span class="line">    lazy_preprocess: <span class="built_in">bool</span> = <span class="literal">False</span></span><br></pre></td></tr></tbody></table></figure><h4 id="Explanation-1"><a href="#Explanation-1" class="headerlink" title="Explanation"></a>Explanation</h4><p><strong>DataArguments 类</strong></p><ul><li><code>DataArguments</code> 也是一个数据类，用于存储数据相关的配置参数。</li><li>属性：<ul><li><code>data_path</code>: 训练数据的路径。</li><li><code>eval_data_path</code>: 评估数据的路径。</li><li><code>lazy_preprocess</code>: 是否在数据预处理时使用延迟加载，即在需要时才加载和处理数据。</li></ul></li></ul><h3 id="3-TrainingArguments-类"><a href="#3-TrainingArguments-类" class="headerlink" title="3. TrainingArguments 类"></a>3. TrainingArguments 类</h3><h4 id="Code-2"><a href="#Code-2" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TrainingArguments</span>(transformers.TrainingArguments):</span><br><span class="line">    cache_dir: <span class="type">Optional</span>[<span class="built_in">str</span>] = field(default=<span class="literal">None</span>)</span><br><span class="line">    optim: <span class="built_in">str</span> = field(default=<span class="string">"adamw_torch"</span>)</span><br><span class="line">    model_max_length: <span class="built_in">int</span> = field(</span><br><span class="line">        default=<span class="number">512</span>,</span><br><span class="line">        metadata={</span><br><span class="line">            <span class="string">"help"</span>: <span class="string">"Maximum sequence length. Sequences will be right padded (and possibly truncated)."</span></span><br><span class="line">        },</span><br><span class="line">    )</span><br></pre></td></tr></tbody></table></figure><h4 id="Explanation-2"><a href="#Explanation-2" class="headerlink" title="Explanation"></a>Explanation</h4><p><code>TrainingArguments</code> 类继承自 <code>transformers.TrainingArguments</code>。。</p><ol><li><p><strong>TrainingArguments 类</strong></p><ul><li><code>TrainingArguments</code> 是一个数据类，它通过继承 <code>transformers.TrainingArguments</code>，获得了处理训练参数的能力。</li><li>在 <code>TrainingArguments</code> 中定义的属性：<ul><li><code>cache_dir</code>: 用于指定模型和分词器缓存的目录路径。</li><li><code>optim</code>: 定义了要使用的优化器类型，例如 <code>'adamw_torch'</code>。</li><li><code>model_max_length</code>: 指定模型能处理的最大序列长度。</li></ul></li></ul></li><li><p><strong>transformers.TrainingArguments 类</strong></p><ul><li><code>transformers.TrainingArguments</code> 是 <code>transformers</code> 库中的一个类，用于配置模型训练过程中的各种参数。</li><li>这个类包含大量的属性，用于控制训练过程，例如：<ul><li><code>output_dir</code>: 指定保存模型和训练结果的目录。</li><li><code>num_train_epochs</code>: 训练的轮数（epochs）。</li><li><code>per_device_train_batch_size</code>: 每个设备上的训练批次大小。</li><li><code>save_steps</code>: 保存模型的步数间隔。</li><li><code>evaluation_strategy</code>: 评估模型的策略，如在每个 epoch 结束时进行评估。</li><li><code>learning_rate</code>: 学习率。</li><li><code>warmup_steps</code>: 在学习率调度中用于预热的步数。</li></ul></li><li><code>transformers.TrainingArguments</code> 还包含了许多其他参数，用于微调训练过程，包括日志记录、模型保存策略、学习率调度等。</li></ul></li></ol><p>通过继承 <code>transformers.TrainingArguments</code>，<code>TrainingArguments</code> 类不仅继承了所有这些训练参数的配置能力，而且还可以添加一些自定义的训练参数，如本例中的 <code>cache_dir</code>、<code>optim</code> 和 <code>model_max_length</code>。这种做法提高了代码的可复用性和灵活性，使得您可以根据项目的具体需求调整和扩展训练配置。</p><h2 id="3-功能型函数-Functional-Utility-Functions"><a href="#3-功能型函数-Functional-Utility-Functions" class="headerlink" title="3.功能型函数 (Functional Utility Functions)"></a>3.功能型函数 (Functional Utility Functions)</h2><h3 id="1-rank0-print-args"><a href="#1-rank0-print-args" class="headerlink" title="1. rank0_print(*args)"></a>1. rank0_print(*args)</h3><h4 id="Code-3"><a href="#Code-3" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">local_rank = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">rank0_print</span>(<span class="params">*args</span>):</span><br><span class="line">    <span class="keyword">if</span> local_rank == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(*args)</span><br></pre></td></tr></tbody></table></figure><h4 id="Explanation-3"><a href="#Explanation-3" class="headerlink" title="Explanation"></a>Explanation</h4><p>定义一个全局变量 local_rank，用于分布式训练。<br>定义一个函数 rank0_print，只在 local_rank 为 0 时打印信息，用于分布式训练中的信息输出控制。这样可以避免在多个节点上重复打印相同的信息，使得输出更加清晰和简洁。</p><ul><li>用于只在分布式训练环境中的主节点（rank 0）上打印信息。</li><li>参数：可变数量的参数，用于打印。</li></ul><h3 id="2-trainer-save-model-safe-trainer-transformers-Trainer"><a href="#2-trainer-save-model-safe-trainer-transformers-Trainer" class="headerlink" title="2. trainer_save_model_safe(trainer: transformers.Trainer)"></a>2. <code>trainer_save_model_safe(trainer: transformers.Trainer)</code></h3><h4 id="Code-4"><a href="#Code-4" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">trainer_save_model_safe</span>(<span class="params">trainer: transformers.Trainer</span>):</span><br><span class="line">    <span class="keyword">from</span> torch.distributed.fsdp <span class="keyword">import</span> FullyShardedDataParallel <span class="keyword">as</span> FSDP</span><br><span class="line">    <span class="keyword">from</span> torch.distributed.fsdp <span class="keyword">import</span> StateDictType, FullStateDictConfig</span><br><span class="line"></span><br><span class="line">    save_policy = FullStateDictConfig(offload_to_cpu=<span class="literal">True</span>, rank0_only=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">with</span> FSDP.state_dict_type(</span><br><span class="line">        trainer.model, StateDictType.FULL_STATE_DICT, save_policy</span><br><span class="line">    ):</span><br><span class="line">        trainer.save_model()</span><br></pre></td></tr></tbody></table></figure><p>函数 <code>trainer_save_model_safe(trainer: transformers.Trainer)</code> 旨在安全地保存使用 PyTorch 分布式框架训练的模型。让我们详细了解此函数及其涉及的关键组件。</p><h4 id="Explanation-4"><a href="#Explanation-4" class="headerlink" title="Explanation"></a>Explanation</h4><ol><li>参数：</li></ol><ul><li><code>trainer</code>: <code>transformers.Trainer</code> 的实例。这个类是 Hugging Face Transformers 库的核心组件之一，用于训练和评估模型。</li></ul><ol start="2"><li>功能：</li></ol><ul><li>此函数的主要目的是在分布式训练环境中安全地保存模型。它特别考虑了使用 <code>FullyShardedDataParallel</code> (FSDP) 进行训练时的模型保存策略。</li></ul><ol start="3"><li>FSDP</li></ol><ul><li><strong>FullyShardedDataParallel (FSDP)</strong><ul><li>这是 PyTorch 分布式训练框架的一个组件。FSDP 通过将模型参数分片到多个 GPU 上来减少每个 GPU 的内存占用，从而实现更大模型的训练。</li><li>在此场景中，FSDP 主要用于处理和保存分布式训练中的模型状态。</li></ul></li><li><strong>StateDictType</strong><ul><li>这是一个枚举类型，定义了如何保存模型的状态字典（state dict）。在 FSDP 环境中，保存和加载模型状态可能需要特殊的处理。</li></ul></li><li><strong>FullStateDictConfig</strong><ul><li>这个类用于配置保存完整状态字典时的参数。它是 FSDP 功能的一部分，用于控制如何保存模型状态。</li></ul></li></ul><ol start="4"><li>函数实现</li></ol><ul><li><strong>设置保存策略</strong><ul><li><code>save_policy = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)</code> 创建了一个保存策略。这里指定两个关键参数：<ul><li><code>offload_to_cpu</code>: 在保存状态字典之前，将模型参数卸载到 CPU，这有助于减少 GPU 内存的使用。</li><li><code>rank0_only</code>: 只在 rank 0（通常是主节点）上保存模型。在分布式训练中，这可以避免每个节点都保存相同的模型副本，节省存储空间。</li></ul></li></ul></li><li><strong>保存模型</strong><ul><li>使用 <code>with FSDP.state_dict_type(trainer.model, StateDictType.FULL_STATE_DICT, save_policy)</code> 上下文管理器设置模型保存的状态字典类型和策略。</li><li>在这个上下文内，调用 <code>trainer.save_model()</code> 来保存模型。由于使用了 <code>save_policy</code>，模型将根据上述配置安全地保存。</li></ul></li></ul><p>函数 <code>trainer_save_model_safe</code> 封装了一个安全的模型保存逻辑，特别是针对使用 PyTorch 的 FSDP 进行分布式训练的场景。它确保了只在一个节点上保存完整的模型状态，并且在保存之前将模型参数转移到 CPU，从而优化内存使用和存储效率。这对于训练大型模型和管理大规模分布式训练环境至关重要。</p><h3 id="3-preprocess-sources-tokenizer-transformers-PreTrainedTokenizer-gt-Dict"><a href="#3-preprocess-sources-tokenizer-transformers-PreTrainedTokenizer-gt-Dict" class="headerlink" title="3.preprocess(sources,tokenizer: transformers.PreTrainedTokenizer) -> Dict"></a>3.<code>preprocess(sources,tokenizer: transformers.PreTrainedTokenizer) -&gt; Dict</code></h3><h4 id="Code-5"><a href="#Code-5" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess</span>(<span class="params"></span></span><br><span class="line"><span class="params">    sources,</span></span><br><span class="line"><span class="params">    tokenizer: transformers.PreTrainedTokenizer,</span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="type">Dict</span>:</span><br><span class="line">    conv = get_conversation_template(<span class="string">"vicuna"</span>)</span><br><span class="line">    roles = {<span class="string">"human"</span>: conv.roles[<span class="number">0</span>], <span class="string">"gpt"</span>: conv.roles[<span class="number">1</span>]}</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Apply prompt templates</span></span><br><span class="line">    conversations = []</span><br><span class="line">    <span class="keyword">for</span> i, source <span class="keyword">in</span> <span class="built_in">enumerate</span>(sources):</span><br><span class="line">        <span class="keyword">if</span> roles[source[<span class="number">0</span>][<span class="string">"from"</span>]] != conv.roles[<span class="number">0</span>]:</span><br><span class="line">            <span class="comment"># Skip the first one if it is not from human</span></span><br><span class="line">            source = source[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">        conv.messages = []</span><br><span class="line">        <span class="keyword">for</span> j, sentence <span class="keyword">in</span> <span class="built_in">enumerate</span>(source):</span><br><span class="line">            role = roles[sentence[<span class="string">"from"</span>]]</span><br><span class="line">            <span class="keyword">assert</span> role == conv.roles[j % <span class="number">2</span>], <span class="string">f"<span class="subst">{i}</span>"</span></span><br><span class="line">            conv.append_message(role, sentence[<span class="string">"value"</span>])</span><br><span class="line">        conversations.append(conv.get_prompt())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Tokenize conversations</span></span><br><span class="line">    input_ids = tokenizer(</span><br><span class="line">        conversations,</span><br><span class="line">        return_tensors=<span class="string">"pt"</span>,</span><br><span class="line">        padding=<span class="string">"max_length"</span>,</span><br><span class="line">        max_length=tokenizer.model_max_length,</span><br><span class="line">        truncation=<span class="literal">True</span>,</span><br><span class="line">    ).input_ids</span><br><span class="line">    targets = input_ids.clone()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> conv.sep_style == SeparatorStyle.ADD_COLON_TWO</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Mask targets. Only compute loss on the assistant outputs.</span></span><br><span class="line">    sep = conv.sep + conv.roles[<span class="number">1</span>] + <span class="string">": "</span></span><br><span class="line">    <span class="keyword">for</span> conversation, target <span class="keyword">in</span> <span class="built_in">zip</span>(conversations, targets):</span><br><span class="line">        total_len = <span class="built_in">int</span>(target.ne(tokenizer.pad_token_id).<span class="built_in">sum</span>())</span><br><span class="line"></span><br><span class="line">        turns = conversation.split(conv.sep2)</span><br><span class="line">        cur_len = <span class="number">1</span></span><br><span class="line">        target[:cur_len] = IGNORE_TOKEN_ID</span><br><span class="line">        <span class="keyword">for</span> i, turn <span class="keyword">in</span> <span class="built_in">enumerate</span>(turns):</span><br><span class="line">            <span class="keyword">if</span> turn == <span class="string">""</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            turn_len = <span class="built_in">len</span>(tokenizer(turn).input_ids)</span><br><span class="line"></span><br><span class="line">            parts = turn.split(sep)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(parts) != <span class="number">2</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            parts[<span class="number">0</span>] += sep</span><br><span class="line">            <span class="comment"># "-2" is hardcoded for the Llama tokenizer to make the offset correct.</span></span><br><span class="line">            instruction_len = <span class="built_in">len</span>(tokenizer(parts[<span class="number">0</span>]).input_ids) - <span class="number">2</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i != <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> tokenizer.legacy:</span><br><span class="line">                <span class="comment"># The legacy and non-legacy modes handle special tokens differently</span></span><br><span class="line">                instruction_len -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Ignore the user instructions</span></span><br><span class="line">            target[cur_len : cur_len + instruction_len] = IGNORE_TOKEN_ID</span><br><span class="line">            cur_len += turn_len</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i != <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> tokenizer.legacy:</span><br><span class="line">                <span class="comment"># The legacy and non-legacy modes handle special tokens differently</span></span><br><span class="line">                cur_len -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        target[cur_len:] = IGNORE_TOKEN_ID</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="literal">False</span>:  <span class="comment"># Inspect and check the correctness of masking</span></span><br><span class="line">            z = target.clone()</span><br><span class="line">            z = torch.where(z == IGNORE_TOKEN_ID, tokenizer.unk_token_id, z)</span><br><span class="line">            rank0_print(tokenizer.decode(z))</span><br><span class="line">            exit()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> cur_len &lt; tokenizer.model_max_length:</span><br><span class="line">            <span class="keyword">if</span> cur_len != total_len:</span><br><span class="line">                target[:] = IGNORE_TOKEN_ID</span><br><span class="line">                rank0_print(</span><br><span class="line">                    <span class="string">f"WARNING: tokenization mismatch: <span class="subst">{cur_len}</span> vs. <span class="subst">{total_len}</span>."</span></span><br><span class="line">                    <span class="string">f" #turn = <span class="subst">{<span class="built_in">len</span>(turns) - <span class="number">1</span>}</span>. (ignored)"</span></span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">dict</span>(</span><br><span class="line">        input_ids=input_ids,</span><br><span class="line">        labels=targets,</span><br><span class="line">        attention_mask=input_ids.ne(tokenizer.pad_token_id),</span><br><span class="line">    )</span><br></pre></td></tr></tbody></table></figure><p>函数 <code>preprocess(sources, tokenizer: transformers.PreTrainedTokenizer) -&gt; Dict</code> 用于预处理对话数据，使其适用于机器学习模型的训练。这个函数可以分为几个主要部分进行详细介绍：</p><h4 id="1-获取对话模板和角色定义"><a href="#1-获取对话模板和角色定义" class="headerlink" title="1. 获取对话模板和角色定义"></a>1. 获取对话模板和角色定义</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conv = get_conversation_template(<span class="string">"vicuna"</span>)</span><br><span class="line">roles = {<span class="string">"human"</span>: conv.roles[<span class="number">0</span>], <span class="string">"gpt"</span>: conv.roles[<span class="number">1</span>]}</span><br></pre></td></tr></tbody></table></figure><ul><li><strong>功能</strong>: 初始化对话模板和定义对话参与者的角色。</li><li><strong>实现</strong>:<ul><li><code>conv = get_conversation_template("vicuna")</code> 获取指定模型（如 “vicuna”）的对话模板。</li><li><code>roles</code> 字典将 “human” 和 “gpt” 分别映射到对话模板中定义的角色。</li></ul></li><li><strong>示例</strong>:<ul><li>如果对话模板是 “vicuna”，则 <code>roles</code> 可能是 <code>{"human": "user", "gpt": "assistant"}</code>。<ul><li><code>conv = get_conversation_template("vicuna")</code> 得到的模板如下：<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Conversation(name=<span class="string">'vicuna_v1.1'</span>, system_template=<span class="string">'{system_message}'</span>, system_message=<span class="string">"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."</span>, roles=(<span class="string">'USER'</span>, <span class="string">'ASSISTANT'</span>), messages=[], offset=<span class="number">0</span>, sep_style=&lt;SeparatorStyle.ADD_COLON_TWO: <span class="number">2</span>&gt;, sep=<span class="string">' '</span>, sep2=<span class="string">'&lt;/s&gt;'</span>, stop_str=<span class="literal">None</span>, stop_token_ids=<span class="literal">None</span>)</span><br></pre></td></tr></tbody></table></figure></li><li><code>roles</code> 将 “human” 映射到 “USER”，将 “gpt” 映射到 “ASSISTANT”。<code>{'human': 'USER', 'gpt': 'ASSISTANT'}</code></li></ul></li></ul></li></ul><h4 id="2-prompt-模板"><a href="#2-prompt-模板" class="headerlink" title="2. prompt 模板"></a>2. prompt 模板</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Apply prompt templates</span></span><br><span class="line">conversations = []</span><br><span class="line"><span class="keyword">for</span> i, source <span class="keyword">in</span> <span class="built_in">enumerate</span>(sources):</span><br><span class="line">    <span class="keyword">if</span> roles[source[<span class="number">0</span>][<span class="string">"from"</span>]] != conv.roles[<span class="number">0</span>]:</span><br><span class="line">        <span class="comment"># Skip the first one if it is not from human</span></span><br><span class="line">        source = source[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">    conv.messages = []</span><br><span class="line">    <span class="keyword">for</span> j, sentence <span class="keyword">in</span> <span class="built_in">enumerate</span>(source):</span><br><span class="line">        role = roles[sentence[<span class="string">"from"</span>]]</span><br><span class="line">        <span class="keyword">assert</span> role == conv.roles[j % <span class="number">2</span>], <span class="string">f"<span class="subst">{i}</span>"</span></span><br><span class="line">        conv.append_message(role, sentence[<span class="string">"value"</span>])</span><br><span class="line">    conversations.append(conv.get_prompt())</span><br></pre></td></tr></tbody></table></figure><ul><li><p><strong>功能</strong>: 为源数据应用提示模板，构建对话。</p></li><li><p><strong>实现</strong>:</p><ul><li>遍历 <code>sources</code>（原始对话数据），将每个对话源转换为模板格式的对话。</li><li>如果对话的第一部分不是 “human” 角色发起，则跳过该部分。</li><li>为每个句子指定角色，并将其添加到对话模板中。</li><li>最终，每个处理后的对话被添加到 <code>conversations</code> 列表中。</li></ul></li><li><p><strong>示例</strong>:</p><ul><li>假如我们的 source 是 dummy input 中的第一条数据:<br><code>python source = [{'from': 'human', 'value': 'Who are you?'}, {'from': 'gpt', 'value': 'I am Vicuna, a language model trained by researchers from Large Model Systems Organization (LMSYS).'}, {'from': 'human', 'value': 'Have a nice day!'}, {'from': 'gpt', 'value': 'You too!'}]</code></li><li><code>conversations</code> 在 Vicuna template 下,我们会使用<code>SeparatorStyle.ADD_COLON_TWO</code>作为分隔符风格，构成的数据可能是 [“A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user’s questions. USER: Who are you? ASSISTANT: I am Vicuna, a language model trained by researchers from Large Model Systems Organization (LMSYS).USER: Have a nice day! ASSISTANT: You too!“]</li><li><details><summary> get_prompt的实现 </summary>`get_prompt` 方法的实现根据不同的 `SeparatorStyle` 有着不同的行为。下面是一个表格，详细介绍了各种风格的 `get_prompt` 方法，以及对应的英文示例：<table><thead><tr><th>分隔符风格 (<code>SeparatorStyle</code>)</th><th>描述</th><th>示例</th></tr></thead><tbody><tr><td><code>ADD_COLON_SINGLE</code></td><td>在每个消息后加冒号和分隔符。</td><td>USER: Hello there!\nASSISTANT: Hi, how can I help?\n</td></tr><tr><td><code>ADD_COLON_TWO</code></td><td>使用两种分隔符交替，通常在不同角色之间切换。</td><td>USER: What’s the weather?\nASSISTANT: It’s sunny today.\n\n</td></tr><tr><td><code>ADD_COLON_SPACE_SINGLE</code></td><td>消息后加冒号、空格和分隔符。</td><td>USER: Can you book a flight?\nASSISTANT: Sure, where to?\n</td></tr><tr><td><code>NO_COLON_SINGLE</code></td><td>消息直接跟在角色后，不加冒号，后接分隔符。</td><td>USERWhat are you doing?\nASSISTANTI’m here to assist you.\n</td></tr><tr><td><code>NO_COLON_TWO</code></td><td>无冒号，使用两种分隔符交替。</td><td>USERHow’s the project going?\nASSISTANTIt’s on track.\n\n</td></tr><tr><td><code>ADD_NEW_LINE_SINGLE</code></td><td>每条消息前换行，消息后加分隔符。</td><td>USER\nHow can I reset my password?\nASSISTANT\nYou can reset it via email.\n</td></tr><tr><td><code>RWKV</code></td><td>特殊格式，通常用于特定模型。</td><td>USER: What is AI?\n\nASSISTANT: AI stands for Artificial Intelligence.\n\n</td></tr><tr><td><code>LLAMA2</code></td><td>特殊标签格式，针对特定模型。</td><td>[INST] USER How does blockchain work?\nASSISTANT It is a distributed ledger.\n\n</td></tr><tr><td><code>CHATGLM</code></td><td>特定于 <code>CHATGLM</code> 模型的格式。</td><td>[Round 1]\nUSER: Tell me a joke.\nASSISTANT: Why did the chicken cross the road?\n</td></tr><tr><td><code>CHATML</code></td><td>类似 <code>CHATGLM</code>，但每条消息前后都有换行。</td><td>USER\nDo you like music?\n\nASSISTANT\nYes, I enjoy many genres.\n\n</td></tr><tr><td><code>CHATGLM3</code></td><td>适用于 <code>CHATGLM3</code> 模型的格式。</td><td>USER\nCan you play chess?\nASSISTANTYes, I can play.\n</td></tr><tr><td><code>CHATINTERN</code></td><td>适用于 <code>CHATINTERN</code> 模型的格式，使用特殊标记。</td><td><s>USER:Where is the nearest ATM?<s>\nASSISTANT:It’s next to the post office.\n</s></s></td></tr><tr><td><code>DOLLY</code></td><td>特定于 <code>DOLLY</code> 模型的格式。</td><td>USER:\nWhat is quantum computing?\nASSISTANT:\nIt involves computation using quantum-mechanical phenomena.\n\n</td></tr><tr><td><code>PHOENIX</code></td><td>适用于 <code>PHOENIX</code> 模型，消息被特殊标记包裹。</td><td>USER: <s>How to bake a cake?</s>\nASSISTANT: <s>You need flour, sugar, and eggs.</s>\n</td></tr><tr><td><code>ROBIN</code></td><td>类似 <code>ADD_NEW_LINE_SINGLE</code>，但角色后有换行。</td><td>USER:\nIs AI dangerous?\nASSISTANT:\nIt depends on how it’s used.\n</td></tr><tr><td><code>FALCON_CHAT</code></td><td>类似 <code>ADD_COLON_SINGLE</code>，但可适用于 <code>FALCON</code> 模型。</td><td>USER: What is the capital of France?\nASSISTANT: It’s Paris.\n</td></tr><tr><td><code>METAMATH</code></td><td>对话中使用特殊前缀和后缀，适用于 <code>METAMATH</code> 模型。</td><td>USER:\nWhat is 2+2?\n: It’s 4\n</td></tr><tr><td><code>DEEPSEEK_CHAT</code></td><td>适用于 <code>DEEPSEEK</code> 模型的特定格式。</td><td>USER: What’s your favorite color?\nASSISTANT: I like blue.\n\n</td></tr><tr><td><code>YUAN2</code></td><td>适用于 <code>YUAN2</code> 模型，特殊的分隔符应用。</td><td>How are you today?<n>I’m fine, thank you!<n></n></n></td></tr></tbody></table>每种风格都有其特定的格式，这在处理与不同模型或任务相关的对话数据时非常重要。通过 <code>get_prompt</code> 方法的不同实现，可以灵活地适应各种需求，使对话生成或处理更加准确和高效。</details></li></ul></li></ul><h4 id="3-对话的分词"><a href="#3-对话的分词" class="headerlink" title="3. 对话的分词"></a>3. 对话的分词</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Tokenize conversations</span></span><br><span class="line">input_ids = tokenizer(</span><br><span class="line">    conversations,</span><br><span class="line">    return_tensors=<span class="string">"pt"</span>,</span><br><span class="line">    padding=<span class="string">"max_length"</span>,</span><br><span class="line">    max_length=tokenizer.model_max_length,</span><br><span class="line">    truncation=<span class="literal">True</span>,</span><br><span class="line">).input_ids</span><br><span class="line">targets = input_ids.clone()</span><br></pre></td></tr></tbody></table></figure><ul><li><p><strong>功能</strong>: 文本对话首先被分词处理，转换成模型能够处理的数值序列。然后，这些序列被克隆以形成初始的训练目标。这样做的目的是为了在训练过程中提供一个基准，指导模型学习生成正确的输出。在后续步骤中，这些目标可能会根据特定的训练目标进行调整。</p></li><li><p><strong>实现</strong>：</p><ul><li><code>tokenizer</code> 函数接收文本列表（这里是 <code>conversations</code>），并返回一个包含数值化表示的 <code>input_ids</code>。<ul><li><code>return_tensors="pt"</code> 指定返回的数据类型为 PyTorch 张量。</li><li><code>padding="max_length"</code> 和 <code>max_length=tokenizer.model_max_length</code> 确保所有输入长度统一，不足的部分使用填充。</li><li><code>truncation=True</code> 表示如果输入过长，将其截断到最大长度。</li></ul></li><li>在训练期间，模型需要知道期望的输出以计算损失和进行反向传播。这些期望的输出被称为 “targets”。<code>targets = input_ids.clone()</code> 表示创建 <code>input_ids</code> 的一个副本作为初始的目标。<ul><li>之所以需要克隆 <code>input_ids</code>，是因为在许多语言模型训练任务中（特别是像自回归模型这样的生成任务），模型的目标输出往往与输入非常相似，但在某些细节上存在差异。</li><li>在后续步骤中，这个 <code>targets</code> 可能会根据特定的训练需求进一步修改或掩码（例如，在对话任务中，可能只对模型生成的回复部分计算损失，而不是整个对话）。</li></ul></li></ul></li></ul><h4 id="4-目标掩码"><a href="#4-目标掩码" class="headerlink" title="4. 目标掩码"></a>4. 目标掩码</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Mask targets. Only compute loss on the assistant outputs.</span></span><br><span class="line">sep = conv.sep + conv.roles[<span class="number">1</span>] + <span class="string">": "</span></span><br><span class="line"><span class="keyword">for</span> conversation, target <span class="keyword">in</span> <span class="built_in">zip</span>(conversations, targets):</span><br><span class="line">    total_len = <span class="built_in">int</span>(target.ne(tokenizer.pad_token_id).<span class="built_in">sum</span>())</span><br><span class="line"></span><br><span class="line">    turns = conversation.split(conv.sep2)</span><br><span class="line">    cur_len = <span class="number">1</span></span><br><span class="line">    target[:cur_len] = IGNORE_TOKEN_ID</span><br><span class="line">    <span class="keyword">for</span> i, turn <span class="keyword">in</span> <span class="built_in">enumerate</span>(turns):</span><br><span class="line">        <span class="keyword">if</span> turn == <span class="string">""</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        turn_len = <span class="built_in">len</span>(tokenizer(turn).input_ids)</span><br><span class="line"></span><br><span class="line">        parts = turn.split(sep)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(parts) != <span class="number">2</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        parts[<span class="number">0</span>] += sep</span><br><span class="line">        <span class="comment"># "-2" is hardcoded for the Llama tokenizer to make the offset correct.</span></span><br><span class="line">        instruction_len = <span class="built_in">len</span>(tokenizer(parts[<span class="number">0</span>]).input_ids) - <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> i != <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> tokenizer.legacy:</span><br><span class="line">            <span class="comment"># The legacy and non-legacy modes handle special tokens differently</span></span><br><span class="line">            instruction_len -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Ignore the user instructions</span></span><br><span class="line">        target[cur_len : cur_len + instruction_len] = IGNORE_TOKEN_ID</span><br><span class="line">        cur_len += turn_len</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> i != <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> tokenizer.legacy:</span><br><span class="line">            <span class="comment"># The legacy and non-legacy modes handle special tokens differently</span></span><br><span class="line">            cur_len -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    target[cur_len:] = IGNORE_TOKEN_ID</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="literal">False</span>:  <span class="comment"># Inspect and check the correctness of masking</span></span><br><span class="line">        z = target.clone()</span><br><span class="line">        z = torch.where(z == IGNORE_TOKEN_ID, tokenizer.unk_token_id, z)</span><br><span class="line">        rank0_print(tokenizer.decode(z))</span><br><span class="line">        exit()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> cur_len &lt; tokenizer.model_max_length:</span><br><span class="line">        <span class="keyword">if</span> cur_len != total_len:</span><br><span class="line">            target[:] = IGNORE_TOKEN_ID</span><br><span class="line">            rank0_print(</span><br><span class="line">                <span class="string">f"WARNING: tokenization mismatch: <span class="subst">{cur_len}</span> vs. <span class="subst">{total_len}</span>."</span></span><br><span class="line">                <span class="string">f" #turn = <span class="subst">{<span class="built_in">len</span>(turns) - <span class="number">1</span>}</span>. (ignored)"</span></span><br><span class="line">            )</span><br></pre></td></tr></tbody></table></figure><ul><li><p><strong>功能</strong>: 对目标输出进行掩码处理，以便模型只对特定输出计算损失。目标是对生成的 targets（即模型的输出标签）进行掩码处理。这是为了确保在训练过程中只对助手（assistant）的输出计算损失，而不是整个对话。</p></li><li><p><strong>实现</strong>：</p><ul><li><code>sep = conv.sep + conv.roles[1] + ": "</code> 定义了用于识别助手回复的分隔符。在这个例子中，<code>sep</code> 可能是 “\n\nAssistant: “。</li><li>循环遍历每个处理后的对话 (<code>conversation</code>) 及其对应的目标 (<code>target</code>)。</li><li><code>total_len</code> 是当前目标序列中非填充（padding）部分的长度。</li><li><code>turns</code> 是将对话根据 <code>conv.sep2</code> 分隔成不同轮次的列表。</li></ul></li></ul><ol><li>对每个轮次进行处理</li></ol><ul><li>每个轮次（turn）包含用户和助手的消息。</li><li>使用 <code>tokenizer(turn)</code> 将每个轮次的文本转换为模型能理解的 ID 序列。</li><li>通过 <code>parts = turn.split(sep)</code> 分离用户和助手的消息。</li><li><code>instruction_len</code> 是用户消息部分的长度（在某些情况下需要调整，比如 <code>-2</code> 是为了适应特定的分词器）。</li></ul><ol start="2"><li>掩码目标</li></ol><ul><li><code>target[cur_len : cur_len + instruction_len] = IGNORE_TOKEN_ID</code> 将用户消息部分的目标 ID 替换为 <code>IGNORE_TOKEN_ID</code>，这意味着在计算损失时会忽略这部分。</li><li><code>cur_len</code> 用于跟踪当前处理到的位置。</li><li>每处理完一个轮次，更新 <code>cur_len</code>。</li></ul><ol start="3"><li>最终处理</li></ol><ul><li><code>target[cur_len:] = IGNORE_TOKEN_ID</code> 确保在最后一个轮次之后的所有内容都被忽略。</li><li>如果 <code>cur_len</code> 小于 <code>tokenizer.model_max_length</code>，但不等于 <code>total_len</code>，则表示有不一致性，此时会发出警告，并将整个目标序列设置为 <code>IGNORE_TOKEN_ID</code>。</li></ul><h4 id="5-返回处理后的数据"><a href="#5-返回处理后的数据" class="headerlink" title="5. 返回处理后的数据"></a>5. 返回处理后的数据</h4><ul><li><strong>功能</strong>: 返回预处理后的数据，包括输入 ID、目标标签和注意力掩码。</li><li><strong>实现</strong>:<ul><li>返回一个字典，包含 <code>input_ids</code>（模型输入）、<code>labels</code>（训练目标）和 <code>attention_mask</code>（指示哪些部分是有效输入的掩码）。</li></ul></li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>这个 <code>preprocess</code> 函数通过将原始文本数据转换为模型可以理解的格式，为训练准备数据。它涵盖了从文本处理到分词，再到目标掩码的整个预处理流程。这个过程对于任何基于对话的自然语言处理任务至关重要，特别是在需要模型专注于对特定部分的响应时。</p><h2 id="4-数据集类"><a href="#4-数据集类" class="headerlink" title="4. 数据集类"></a>4. 数据集类</h2><p>这些类继承自 PyTorch 的 <code>Dataset</code> 类，并且是为特定的数据处理任务定制的。这些类包含具体的方法来处理和准备数据，以便用于模型训练。这些类包括：</p><ul><li><code>SupervisedDataset</code>: 用于有监督学习的数据集。它处理原始数据，将其转换为适合模型训练的格式。</li><li><code>LazySupervisedDataset</code>: 类似于 <code>SupervisedDataset</code>，但使用懒加载方式处理数据。这意味着数据只在需要时才被加载和处理，这对于处理大型数据集特别有用。</li></ul><p>这些类通常包含 <code>__init__</code>, <code>__len__</code>, 和 <code>__getitem__</code> 方法，分别用于初始化数据集、获取数据集大小和检索特定索引的数据。这样的设计模式使得数据集可以轻松地与 PyTorch 的 DataLoader 配合使用，从而实现高效的数据加载和批处理。</p><h3 id="1-SupervisedDataset-类"><a href="#1-SupervisedDataset-类" class="headerlink" title="1. SupervisedDataset 类"></a>1. SupervisedDataset 类</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SupervisedDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="string">"""Dataset for supervised fine-tuning."""</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, raw_data, tokenizer: transformers.PreTrainedTokenizer</span>):</span><br><span class="line">        <span class="built_in">super</span>(SupervisedDataset, self).__init__()</span><br><span class="line"></span><br><span class="line">        rank0_print(<span class="string">"Formatting inputs..."</span>)</span><br><span class="line">        sources = [example[<span class="string">"conversations"</span>] <span class="keyword">for</span> example <span class="keyword">in</span> raw_data]</span><br><span class="line">        data_dict = preprocess(sources, tokenizer)</span><br><span class="line"></span><br><span class="line">        self.input_ids = data_dict[<span class="string">"input_ids"</span>]</span><br><span class="line">        self.labels = data_dict[<span class="string">"labels"</span>]</span><br><span class="line">        self.attention_mask = data_dict[<span class="string">"attention_mask"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.input_ids)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, i</span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, torch.Tensor]:</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">dict</span>(</span><br><span class="line">            input_ids=self.input_ids[i],</span><br><span class="line">            labels=self.labels[i],</span><br><span class="line">            attention_mask=self.attention_mask[i],</span><br><span class="line">        )</span><br></pre></td></tr></tbody></table></figure><p><code>SupervisedDataset</code> 类是一个用于有监督学习的数据集类，特别是为了微调（fine-tuning）任务设计。这个类继承自 PyTorch 的 <code>Dataset</code> 类，并重写了其方法以适应特定的数据处理需求。下面是对这个类的详细介绍：<code>SupervisedDataset</code> 类提供了一种结构化和高效的方法来处理和加载用于有监督学习的对话数据。它遵循 PyTorch 数据集（<code>Dataset</code>）的标准结构，使得与 PyTorch 的数据加载器（<code>DataLoader</code>）等其他组件兼容，从而方便在训练循环中使用。通过预处理步骤，该类确保数据以适当的格式提供给模型，以便进行有效的训练。</p><ul><li><strong>类名</strong>: <code>SupervisedDataset</code></li><li><strong>继承</strong>: <code>Dataset</code>（来自 PyTorch）</li><li><strong>目的</strong>: 用于有监督的模型微调任务。</li></ul><h4 id="1-1-init-self-raw-data-tokenizer-transformers-PreTrainedTokenizer"><a href="#1-1-init-self-raw-data-tokenizer-transformers-PreTrainedTokenizer" class="headerlink" title="1.1 __init__(self, raw_data, tokenizer: transformers.PreTrainedTokenizer)"></a>1.1 <code>__init__(self, raw_data, tokenizer: transformers.PreTrainedTokenizer)</code></h4><ul><li><strong>调用时机</strong>：创建 <code>SupervisedDataset</code> 类的实例时。这通常发生在准备训练数据集的阶段，当你创建数据加载器（DataLoader）之前。</li><li><strong>功能</strong>：初始化数据集实例，处理原始对话数据，并将其转换为模型可以理解的格式。</li><li><strong>参数</strong>：<ul><li><code>raw_data</code>：包含对话数据的列表或类似结构。</li><li><code>tokenizer</code>：一个预训练的分词器实例，用于将文本转换为模型可以处理的格式。</li></ul></li><li><strong>返回值</strong>：无返回值，但此方法会设置 <code>input_ids</code>、<code>labels</code> 和 <code>attention_mask</code> 作为类的内部状态。</li><li><strong>实现细节</strong>:<ul><li>使用列表推导式从 <code>raw_data</code> 中提取每个样本的对话内容。</li><li>调用 <code>preprocess</code> 函数处理这些对话，将其转换为适合模型输入的格式。</li><li>从返回的 <code>data_dict</code> 中提取 <code>input_ids</code>（模型输入 ID）、<code>labels</code>（目标标签）和 <code>attention_mask</code>（注意力掩码）。</li></ul></li></ul><h4 id="1-2-len-self"><a href="#1-2-len-self" class="headerlink" title="1.2 __len__(self)"></a>1.2 <code>__len__(self)</code></h4><ul><li><strong>调用时机</strong>：当需要获取数据集大小时，例如在设置数据加载器时，或者在训练循环中迭代数据集时。</li><li><strong>功能</strong>：返回数据集中的样本数量。</li><li><strong>返回值</strong>：一个整数，表示数据集中的样本数量。</li><li><strong>实现</strong>: 直接返回 <code>input_ids</code> 的长度，即样本的数量。</li></ul><h4 id="1-3-getitem-self-i"><a href="#1-3-getitem-self-i" class="headerlink" title="1.3 __getitem__(self, i)"></a>1.3 <code>__getitem__(self, i)</code></h4><ul><li><strong>调用时机</strong>：在数据加载器请求数据集的特定样本时，这通常发生在训练或评估循环的每个迭代中。</li><li><strong>功能</strong>：获取指定索引 <code>i</code> 处的数据样本。</li><li><strong>参数</strong>：<ul><li><code>i</code>：所请求样本的索引。</li></ul></li><li><strong>返回值</strong>：一个字典，包含索引 <code>i</code> 处样本的 <code>input_ids</code>、<code>labels</code> 和 <code>attention_mask</code>。这些是 PyTorch 张量（<code>torch.Tensor</code>），适用于模型的训练或评估。</li></ul><p>在有监督学习的场景中，<code>SupervisedDataset</code> 类扮演着数据预处理和封装的角色，确保数据以正确的格式提供给模型。<code>__init__</code> 方法在数据集实例化时调用，负责数据的初始化和预处理。<code>__len__</code> 和 <code>__getitem__</code> 方法则在训练和评估过程中被频繁调用，分别用于获取数据集的大小和提取特定的数据样本。这些方法的设计和实现使得 <code>SupervisedDataset</code> 类可以无缝地与 PyTorch 的其他数据处理和训练工具集成。</p><h3 id="2-LazySupervisedDataset-类"><a href="#2-LazySupervisedDataset-类" class="headerlink" title="2. LazySupervisedDataset 类"></a>2. LazySupervisedDataset 类</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LazySupervisedDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="string">"""Dataset for supervised fine-tuning."""</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, raw_data, tokenizer: transformers.PreTrainedTokenizer</span>):</span><br><span class="line">        <span class="built_in">super</span>(LazySupervisedDataset, self).__init__()</span><br><span class="line">        self.tokenizer = tokenizer</span><br><span class="line"></span><br><span class="line">        rank0_print(<span class="string">"Formatting inputs...Skip in lazy mode"</span>)</span><br><span class="line">        self.tokenizer = tokenizer</span><br><span class="line">        self.raw_data = raw_data</span><br><span class="line">        self.cached_data_dict = {}</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.raw_data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, i</span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, torch.Tensor]:</span><br><span class="line">        <span class="keyword">if</span> i <span class="keyword">in</span> self.cached_data_dict:</span><br><span class="line">            <span class="keyword">return</span> self.cached_data_dict[i]</span><br><span class="line"></span><br><span class="line">        ret = preprocess([self.raw_data[i][<span class="string">"conversations"</span>]], self.tokenizer)</span><br><span class="line">        ret = <span class="built_in">dict</span>(</span><br><span class="line">            input_ids=ret[<span class="string">"input_ids"</span>][<span class="number">0</span>],</span><br><span class="line">            labels=ret[<span class="string">"labels"</span>][<span class="number">0</span>],</span><br><span class="line">            attention_mask=ret[<span class="string">"attention_mask"</span>][<span class="number">0</span>],</span><br><span class="line">        )</span><br><span class="line">        self.cached_data_dict[i] = ret</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ret</span><br></pre></td></tr></tbody></table></figure><p><code>LazySupervisedDataset</code> 类是另一种数据集实现，用于有监督的模型微调。与 <code>SupervisedDataset</code> 相比，它采用了一种“懒加载”（lazy loading）的策略。以下是对该类的详细解释，以及它与非懒加载版本的比较。</p><h4 id="2-1-init-self-raw-data-tokenizer-transformers-PreTrainedTokenizer"><a href="#2-1-init-self-raw-data-tokenizer-transformers-PreTrainedTokenizer" class="headerlink" title="2.1 __init__(self, raw_data, tokenizer: transformers.PreTrainedTokenizer)"></a>2.1 <code>__init__(self, raw_data, tokenizer: transformers.PreTrainedTokenizer)</code></h4><ul><li><strong>作用</strong>：初始化 <code>LazySupervisedDataset</code> 实例。</li><li><strong>实现细节</strong>：<ul><li>将原始数据 (<code>raw_data</code>) 和分词器 (<code>tokenizer</code>) 保存为类的属性。</li><li>初始化一个空字典 <code>cached_data_dict</code>，用于缓存已处理的数据。</li></ul></li><li><strong>与 <code>SupervisedDataset</code> 的差异</strong>：<ul><li>在 <code>LazySupervisedDataset</code> 中，原始数据不是在初始化时立即处理，而是存储原始形式以便稍后处理。</li><li><code>cached_data_dict</code> 用于缓存按需处理的数据，以避免重复处理。</li></ul></li></ul><h4 id="2-2-len-self"><a href="#2-2-len-self" class="headerlink" title="2.2 __len__(self)"></a>2.2 <code>__len__(self)</code></h4><ul><li><p><strong>作用</strong>：返回数据集中的样本数量。</p></li><li><p><strong>实现</strong>：直接返回原始数据 (<code>raw_data</code>) 的长度。</p></li><li><p><strong>与 <code>SupervisedDataset</code> 的差异</strong>：</p><ul><li><p>在 <code>LazySupervisedDataset</code> 类中，<code>__len__</code> 方法确实返回的是数据集中样本的数量，但是这里的“样本数量”是指原始数据 (<code>raw_data</code>) 中的样本数量，而不是处理后的数据的数量。由于 <code>LazySupervisedDataset</code> 采用懒加载策略，数据在初始化时并未被处理，因此 <code>__len__</code> 方法基于原始数据计算长度是合理的。</p></li><li><p>这意味着即便数据尚未被转换为模型可用的格式，<code>__len__</code> 方法仍能准确反映数据集中待处理样本的数量。这与 <code>SupervisedDataset</code> 的主要区别在于后者在初始化时就对所有数据进行预处理，因此其 <code>__len__</code> 方法返回的是已处理数据的数量。而在 <code>LazySupervisedDataset</code> 中，数据处理是按需进行的，因此 <code>__len__</code> 返回的是原始数据中的样本数。</p></li></ul></li></ul><h4 id="getitem-self-i"><a href="#getitem-self-i" class="headerlink" title="__getitem__(self, i)"></a><code>__getitem__(self, i)</code></h4><ul><li><strong>作用</strong>：按需获取并处理指定索引 <code>i</code> 处的数据样本。</li><li><strong>实现细节</strong>：<ul><li>首先检查索引 <code>i</code> 是否在缓存 <code>cached_data_dict</code> 中。</li><li>如果是，则直接返回缓存的数据；如果不是，则处理原始数据中索引 <code>i</code> 处的样本，并将处理后的结果添加到缓存中。</li><li>返回一个包含 <code>input_ids</code>、<code>labels</code> 和 <code>attention_mask</code> 的字典。</li></ul></li><li><strong>与 <code>SupervisedDataset</code> 的差异</strong>：<ul><li><code>LazySupervisedDataset</code> 在 <code>__getitem__</code> 被调用时才处理数据，而 <code>SupervisedDataset</code> 在初始化时就处理所有数据。</li><li><code>LazySupervisedDataset</code> 使用缓存来避免重复处理同一样本，而 <code>SupervisedDataset</code> 不需要这种机制，因为所有数据在初始化时就已经被处理。</li></ul></li></ul><h4 id="懒加载-vs-非懒加载"><a href="#懒加载-vs-非懒加载" class="headerlink" title="懒加载 vs 非懒加载"></a>懒加载 vs 非懒加载</h4><ul><li><strong>懒加载（Lazy Loading）</strong>：<ul><li><strong>优点</strong>：减少内存占用，因为只有需要时才处理数据。对于大型数据集非常有用。</li><li><strong>缺点</strong>：可能增加训练时的数据加载时间，尤其是当缓存未命中时。</li></ul></li><li><strong>非懒加载（Eager Loading）</strong>：<ul><li><strong>优点</strong>：在训练开始前一次性处理所有数据，可以减少训练过程中的延迟。</li><li><strong>缺点</strong>：需要更多的初始内存来存储处理后的所有数据，对于非常大的数据集可能不实用。</li></ul></li></ul><h3 id="3-make-supervised-data-module-函数"><a href="#3-make-supervised-data-module-函数" class="headerlink" title="3. make_supervised_data_module 函数"></a>3. <code>make_supervised_data_module</code> 函数</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">make_supervised_data_module</span>(<span class="params"></span></span><br><span class="line"><span class="params">    tokenizer: transformers.PreTrainedTokenizer, data_args</span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="type">Dict</span>:</span><br><span class="line">    <span class="string">"""Make dataset and collator for supervised fine-tuning."""</span></span><br><span class="line">    dataset_cls = (</span><br><span class="line">        LazySupervisedDataset <span class="keyword">if</span> data_args.lazy_preprocess <span class="keyword">else</span> SupervisedDataset</span><br><span class="line">    )</span><br><span class="line">    rank0_print(<span class="string">"Loading data..."</span>)</span><br><span class="line"></span><br><span class="line">    train_json = json.load(<span class="built_in">open</span>(data_args.data_path, <span class="string">"r"</span>))</span><br><span class="line">    train_dataset = dataset_cls(train_json, tokenizer=tokenizer)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> data_args.eval_data_path:</span><br><span class="line">        eval_json = json.load(<span class="built_in">open</span>(data_args.eval_data_path, <span class="string">"r"</span>))</span><br><span class="line">        eval_dataset = dataset_cls(eval_json, tokenizer=tokenizer)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        eval_dataset = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">dict</span>(train_dataset=train_dataset, eval_dataset=eval_dataset)</span><br></pre></td></tr></tbody></table></figure><p>函数 <code>make_supervised_data_module</code> 的目的是为有监督的模型微调创建数据集和数据整理器（collator）。这个函数根据提供的参数构建适合训练和评估的数据集。下面是对这个函数的超级详细解释：</p><h3 id="函数签名"><a href="#函数签名" class="headerlink" title="函数签名"></a>函数签名</h3><ul><li><strong>函数名</strong>：<code>make_supervised_data_module</code></li><li><strong>参数</strong>：<ul><li><code>tokenizer</code>: <code>transformers.PreTrainedTokenizer</code> 的实例，用于文本的分词处理。</li><li><code>data_args</code>: 包含数据相关设置的对象，通常包括数据文件路径等信息。</li></ul></li><li><strong>返回值</strong>：一个字典，包含训练和评估数据集。</li></ul><h3 id="函数实现细节"><a href="#函数实现细节" class="headerlink" title="函数实现细节"></a>函数实现细节</h3><h4 id="1-选择数据集类"><a href="#1-选择数据集类" class="headerlink" title="1. 选择数据集类"></a>1. 选择数据集类</h4><ul><li>根据 <code>data_args.lazy_preprocess</code> 的值选择使用 <code>LazySupervisedDataset</code> 还是 <code>SupervisedDataset</code> 类。<ul><li>如果 <code>data_args.lazy_preprocess</code> 为 <code>True</code>，则使用 <code>LazySupervisedDataset</code> 实现懒加载。</li><li>否则，使用 <code>SupervisedDataset</code> 进行预加载。</li></ul></li><li>这一选择影响数据的加载方式，即数据是一次性全部加载并预处理，还是按需加载和处理。</li></ul><h4 id="2-加载训练数据"><a href="#2-加载训练数据" class="headerlink" title="2. 加载训练数据"></a>2. 加载训练数据</h4><ul><li>使用 <code>json.load(open(data_args.data_path, "r"))</code> 加载训练数据。<ul><li>这里假设训练数据以 JSON 格式存储，并且 <code>data_args.data_path</code> 包含了数据文件的路径。</li></ul></li><li>创建训练数据集 <code>train_dataset</code> 实例，传入加载的训练数据和分词器。</li></ul><h4 id="3-加载评估数据（如果提供）"><a href="#3-加载评估数据（如果提供）" class="headerlink" title="3. 加载评估数据（如果提供）"></a>3. 加载评估数据（如果提供）</h4><ul><li>检查是否提供了评估数据路径 <code>data_args.eval_data_path</code>。<ul><li>如果提供，同样使用 <code>json.load</code> 加载评估数据。</li><li>创建评估数据集 <code>eval_dataset</code> 实例。</li></ul></li><li>如果没有提供评估数据路径，将 <code>eval_dataset</code> 设置为 <code>None</code>。</li></ul><h4 id="4-返回结果"><a href="#4-返回结果" class="headerlink" title="4. 返回结果"></a>4. 返回结果</h4><ul><li>返回一个字典，包含两个键：<code>train_dataset</code> 和 <code>eval_dataset</code>。<ul><li><code>train_dataset</code> 对应训练数据集实例。</li><li><code>eval_dataset</code> 对应评估数据集实例（如果有的话）。</li></ul></li></ul><h2 id="5-训练流程"><a href="#5-训练流程" class="headerlink" title="5. 训练流程"></a>5. 训练流程</h2><p>了解了，我将为 <code>train()</code> 函数中每行代码提供更详细的解释：</p><h3 id="解析命令行参数"><a href="#解析命令行参数" class="headerlink" title="解析命令行参数"></a>解析命令行参数</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parser = transformers.HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))</span><br></pre></td></tr></tbody></table></figure><ul><li>创建一个 <code>HfArgumentParser</code> 实例，这是一个帮助解析命令行参数的工具，特别用于处理 Hugging Face transformers 库中的参数。</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model_args, data_args, training_args = parser.parse_args_into_dataclasses()</span><br></pre></td></tr></tbody></table></figure><ul><li>解析命令行参数并将它们映射到三个数据类 (<code>ModelArguments</code>, <code>DataArguments</code>, <code>TrainingArguments</code>) 的实例中。</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">local_rank = training_args.local_rank</span><br></pre></td></tr></tbody></table></figure><ul><li><code>local_rank</code> 用于标识分布式训练中的进程编号。<code>training_args.local_rank</code> 获取这个编号。</li></ul><h3 id="设置模型配置"><a href="#设置模型配置" class="headerlink" title="设置模型配置"></a>设置模型配置</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">config = transformers.AutoConfig.from_pretrained(</span><br><span class="line">    model_args.model_name_or_path,</span><br><span class="line">    cache_dir=training_args.cache_dir,</span><br><span class="line">    trust_remote_code=model_args.trust_remote_code,</span><br><span class="line">)</span><br></pre></td></tr></tbody></table></figure><ul><li>从预训练模型的配置创建 <code>AutoConfig</code> 实例。它自动加载与特定模型相关的配置。</li><li><code>model_args.model_name_or_path</code>: 指定模型的名称或路径。</li><li><code>cache_dir=training_args.cache_dir</code>: 指定缓存目录。</li><li><code>trust_remote_code=model_args.trust_remote_code</code>: 指定是否信任从远程下载的代码。</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">orig_ctx_len = <span class="built_in">getattr</span>(config, <span class="string">"max_position_embeddings"</span>, <span class="literal">None</span>)</span><br></pre></td></tr></tbody></table></figure><ul><li>使用 <code>getattr</code> 函数从配置中获取 <code>max_position_embeddings</code> 属性，该属性指示模型的最大位置嵌入数（即模型能处理的最大序列长度）。如果不存在该属性，则返回 <code>None</code>。<code>getattr</code> 是一个 Python 内置函数，用于获取对象的属性值。如果属性不存在，返回第三个参数指定的默认值（此处为 <code>None</code>）。</li><li><code>orig_ctx_len</code> 存储模型配置中的 <code>max_position_embeddings</code> 属性值，即模型可以处理的最大位置嵌入数（通常与最大序列长度相关）。</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> orig_ctx_len <span class="keyword">and</span> training_args.model_max_length &gt; orig_ctx_len:</span><br><span class="line">    scaling_factor = <span class="built_in">float</span>(math.ceil(training_args.model_max_length / orig_ctx_len))</span><br><span class="line">    config.rope_scaling = {<span class="string">"type"</span>: <span class="string">"linear"</span>, <span class="string">"factor"</span>: scaling_factor}</span><br></pre></td></tr></tbody></table></figure><ul><li>如果提供的模型最大长度 (<code>training_args.model_max_length</code>) 超过了原始模型的最大长度 (<code>orig_ctx_len</code>)，则计算一个缩放因子以进行位置编码的调整。这通常用于处理超出预训练模型原始设计的序列长度。</li><li><code>rope_scaling</code> 用于调整相对位置编码。</li><li><code>scaling_factor</code> 和 RoPE 缩放<ul><li>如果模型的最大长度超过原始配置的最大长度，<code>scaling_factor</code> 被用来计算缩放因子。</li><li>这涉及到 Rotary Positional Embedding（RoPE）的概念，即在位置嵌入中使用的技术，可以随序列长度线性缩放。</li><li>缩放因子用于调整位置嵌入，使其适应更长的序列。</li></ul></li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">config.use_cache = <span class="literal">False</span></span><br></pre></td></tr></tbody></table></figure><ul><li>禁用模型在前向传播时缓存中间计算结果的功能，这有助于减少内存消耗。这个设置告诉模型在前向传播时不使用或保存缓存。</li></ul><h3 id="加载模型和分词器"><a href="#加载模型和分词器" class="headerlink" title="加载模型和分词器"></a>加载模型和分词器</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = transformers.AutoModelForCausalLM.from_pretrained(</span><br><span class="line">    model_args.model_name_or_path,</span><br><span class="line">    config=config,</span><br><span class="line">    cache_dir=training_args.cache_dir,</span><br><span class="line">    trust_remote_code=model_args.trust_remote_code,</span><br><span class="line">)</span><br></pre></td></tr></tbody></table></figure><ul><li>加载预训练的因果语言模型（Causal Language Model）。这类模型通常用于生成任务。</li><li><code>trust_remote_code</code>这个参数用于确定是否信任从远程（如 Hugging Face Hub）加载的自定义模型代码。</li><li><code>cache_dir=training_args.cache_dir</code><ul><li>指定下载和缓存预训练模型和分词器的目录。</li><li>如果指定，模型和分词器将从这个目录加载，如果不存在，将从远程下载并缓存到此目录。</li></ul></li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tokenizer = transformers.AutoTokenizer.from_pretrained(</span><br><span class="line">    model_args.model_name_or_path,</span><br><span class="line">    cache_dir=training_args.cache_dir,</span><br><span class="line">    model_max_length=training_args.model_max_length,</span><br><span class="line">    padding_side=model_args.padding_side,</span><br><span class="line">    use_fast=<span class="literal">False</span>,</span><br><span class="line">    trust_remote_code=model_args.trust_remote_code,</span><br><span class="line">)</span><br></pre></td></tr></tbody></table></figure><ul><li>加载与模型对应的分词器。</li><li><code>use_fast=False</code>: 表示不使用快速分词器，快速分词器通常是基于 Rust 的分词器，提供更高效的分词处理。</li><li><code>padding_side=model_args.padding_side</code>: 指定填充（padding）应该发生在序列的哪一侧。</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> tokenizer.pad_token != tokenizer.unk_token:</span><br><span class="line">    tokenizer.pad_token = tokenizer.unk_token</span><br></pre></td></tr></tbody></table></figure><ul><li>将分词器的填充令牌设置为未知令牌（<code>unk_token</code>），如果它们不一致的话。这是因为某些模型需要在填充位置使用特定的令牌。</li></ul><h3 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)</span><br></pre></td></tr></tbody></table></figure><ul><li>调用 <code>make_supervised_data_module</code> 函数，为训练和评估准备数据集。这个函数会根据 <code>data_args</code> 中的设置，选择使用懒加载或预加载的方式处理数据。</li></ul><h3 id="初始化并启动训练器"><a href="#初始化并启动训练器" class="headerlink" title="初始化并启动训练器"></a>初始化并启动训练器</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">trainer = Trainer(</span><br><span class="line">    model=model, tokenizer=tokenizer, args=training_args, **data_module</span><br><span class="line">)</span><br></pre></td></tr></tbody></table></figure><ul><li>初始化 <code>Trainer</code> 对象，传入模型、分词器、训练参数以及通过 <code>make_supervised_data_module</code> 函数准备好的数据。</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="built_in">list</span>(pathlib.Path(training_args.output_dir).glob(<span class="string">"checkpoint-*"</span>)):</span><br><span class="line">    trainer.train(resume_from_checkpoint=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    trainer.train()</span><br></pre></td></tr></tbody></table></figure><ul><li>检查是否存在训练检查点，如果存在，则从检查点恢复训练；如果不存在，开始新的训练过程。</li></ul><h3 id="保存模型"><a href="#保存模型" class="headerlink" title="保存模型"></a>保存模型</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model.config.use_cache = <span class="literal">True</span></span><br><span class="line">trainer.save_state()</span><br></pre></td></tr></tbody></table></figure><ul><li>启用模型的缓存并保存训练器的状态。</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> trainer.is_deepspeed_enabled:</span><br><span class="line">    trainer.save_model()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    trainer_save_model_safe(trainer)</span><br></pre></td></tr></tbody></table></figure><ul><li>检查是否启用了 DeepSpeed。如果启用了，则使用 <code>trainer.save_model()</code> 保存模型。如果没有启用 DeepSpeed，则使用 <code>trainer_save_model_safe</code> 安全地保存模型，特别是在使用分布式训练时。</li></ul><h3 id="Trainer-类解释"><a href="#Trainer-类解释" class="headerlink" title="Trainer 类解释"></a>Trainer 类解释</h3><p><code>Trainer</code> 是 Hugging Face Transformers 库提供的一个类，用于封装模型的训练逻辑。以下是对 <code>Trainer</code> 类的功能的详细介绍：</p><ul><li><p><strong>模型训练与评估</strong>：<code>Trainer</code> 类负责设置和执行模型的训练和评估过程。它自动处理数据的批处理、梯度计算、优化器步骤和设备管理等任务。</p></li><li><p><strong>参数</strong>：在初始化时，<code>Trainer</code> 接受多种参数，包括模型（<code>model</code>）、分词器（<code>tokenizer</code>）、训练参数（如学习率、批大小等，通过 <code>training_args</code> 传入）和数据集。</p></li><li><p><strong>灵活性和高级功能</strong>：<code>Trainer</code> 支持多种训练设置，如多 GPU 训练、混合精度训练和 TPU 训练。它还支持自定义回调函数，用于在训练过程中执行特定操作。</p></li><li><p><strong>简化 API</strong>：<code>Trainer</code> 类提供了一个简化的 API，使得用户可以用几行代码配置和运行模型训练。它抽象了许多底层细节，使得用户可以专注于模型的构建和训练策略。</p></li><li><p><strong>检查点和恢复</strong>：<code>Trainer</code> 支持保存和加载检查点，这意味着训练过程可以在中断后从上次保存的状态恢复。</p></li></ul><p>总体来说，<code>Trainer</code> 类是一个功能强大且灵活的工具，为训练复杂的 Transformer 模型提供了便利和高效性。</p><h2 id="6-run-bash"><a href="#6-run-bash" class="headerlink" title="6. run bash"></a>6. run bash</h2><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">torchrun --nproc_per_node=8 --master_port=20001 fastchat/train/train.py \</span><br><span class="line">    --model_name_or_path ~/vicuna-7b-v1.5-16k  \</span><br><span class="line">    --data_path data/dummy_conversation.json \</span><br><span class="line">    --fp16 True \</span><br><span class="line">    --output_dir output_vicuna \</span><br><span class="line">    --num_train_epochs 3 \</span><br><span class="line">    --per_device_train_batch_size 8 \</span><br><span class="line">    --per_device_eval_batch_size 1 \</span><br><span class="line">    --gradient_accumulation_steps 1 \</span><br><span class="line">    --evaluation_strategy <span class="string">"no"</span> \</span><br><span class="line">    --save_strategy <span class="string">"steps"</span> \</span><br><span class="line">    --save_steps 1200 \</span><br><span class="line">    --save_total_limit 10 \</span><br><span class="line">    --learning_rate 2e-5 \</span><br><span class="line">    --weight_decay 0. \</span><br><span class="line">    --warmup_ratio 0.03 \</span><br><span class="line">    --lr_scheduler_type <span class="string">"cosine"</span> \</span><br><span class="line">    --logging_steps 1 \</span><br><span class="line">    --fsdp <span class="string">"full_shard auto_wrap"</span> \</span><br><span class="line">    --fsdp_transformer_layer_cls_to_wrap <span class="string">'LlamaDecoderLayer'</span> \</span><br><span class="line">    --model_max_length 2048 \</span><br><span class="line">    --gradient_checkpointing True \</span><br><span class="line">    --lazy_preprocess True</span><br></pre></td></tr></tbody></table></figure><h3 id="torchrun"><a href="#torchrun" class="headerlink" title="torchrun"></a>torchrun</h3><p><code>torchrun</code> 是 PyTorch 提供的一个命令行工具，用于启动分布式训练。它是 <code>torch.distributed.launch</code> 模块的一部分，旨在简化在多个进程上运行 PyTorch 程序的过程。以下是对 <code>torchrun</code> 中使用的参数的详细解释：</p><ol><li><p><code>--nproc_per_node=8</code></p><ul><li><code>--nproc_per_node</code> 指定每个节点（在这种情况下通常是一台机器）上要启动的进程数。这里设置为 8，意味着在当前节点上将启动 8 个训练进程。</li><li>作用：用于控制每个节点上的并行度。在多 GPU 系统中，这通常等于 GPU 的数量。</li></ul></li><li><p><code>--master_port=20001</code></p><ul><li><code>--master_port</code> 指定主节点用于通信的端口。这里设置为 20001。</li><li>作用：在分布式训练中，不同进程需要通过网络进行通信。这个参数指定了用于进程间通信的端口。</li></ul></li><li><p><code>fastchat/train/train.py</code></p><ul><li>这不是 <code>torchrun</code> 的参数，而是指定了要执行的 Python 脚本，即训练脚本的路径。</li></ul></li></ol><p>在分布式训练中，<code>torchrun</code> 负责在每个进程中正确地设置环境变量，如 <code>LOCAL_RANK</code>（当前进程在其节点上的排名）、<code>WORLD_SIZE</code>（总进程数）和 <code>RANK</code>（全局进程排名）。这些环境变量对于使用 PyTorch 分布式包（如 <code>torch.distributed</code>）进行有效通信至关重要。</p><h3 id="使用示例"><a href="#使用示例" class="headerlink" title="使用示例"></a>使用示例</h3><p>假设您有一台拥有 8 个 GPU 的机器，您想在所有 GPU 上并行运行训练。使用 <code>torchrun</code>，您的命令可能如下所示：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torchrun --nproc_per_node=8 --master_port=20001 fastchat/train/train.py --其他参数</span><br></pre></td></tr></tbody></table></figure><p>这个命令会在每个 GPU 上启动一个训练进程，每个进程运行 <code>train.py</code> 脚本，并且所有进程能够通过分布式通信有效协作。</p><p><code>torchrun</code> 是分布式训练的关键工具，它简化了在多个进程上启动 PyTorch 程序的流程，特别是在多 GPU 环境中。通过自动设置必要的环境变量，<code>torchrun</code> 使得实现和运行分布式训练变得更加容易和可靠。</p><h3 id="2-参数"><a href="#2-参数" class="headerlink" title="2. 参数"></a>2. 参数</h3><ol><li><p><code>--model_name_or_path</code></p><ul><li>可以是预训练模型的官方名称（如 “bert-base-uncased”）、自定义训练的模型路径或 Hugging Face Model Hub 上的模型。</li><li>作用：指定用于训练的模型。</li></ul></li><li><p><code>--data_path</code></p><ul><li>路径可以是本地文件系统上的路径。</li><li>作用：指定训练使用的数据文件。</li></ul></li><li><p><code>--fp16</code></p><ul><li>可取值为 True 或 False。</li><li>作用：启用或禁用混合精度训练，以提高训练速度和降低显存使用。</li></ul></li><li><p><code>--output_dir</code></p><ul><li>任何有效的文件路径。</li><li>作用：指定输出目录，用于保存训练过程中产生的文件。</li></ul></li><li><p><code>--num_train_epochs</code></p><ul><li>任何正整数。</li><li>作用：指定训练的轮次。</li></ul></li><li><p><code>--per_device_train_batch_size</code> 和 <code>--per_device_eval_batch_size</code></p><ul><li>任何正整数。</li><li>作用：分别指定每个设备上的训练和评估批次大小。</li></ul></li><li><p><code>--gradient_accumulation_steps</code></p><ul><li>任何正整数。</li><li>作用：指定梯度累积的步骤数，用于在有限的显存下增加有效的批次大小。</li></ul></li><li><p><code>--evaluation_strategy</code></p><ul><li>可取值包括 “no”、”steps”、”epoch”。</li><li>作用：指定评估的策略，如每个 epoch 或特定步数后进行评估，或不进行评估。</li></ul></li><li><p><code>--save_strategy</code></p><ul><li>可取值包括 “no”、”steps”、”epoch”。</li><li>作用：指定模型保存的策略。</li></ul></li><li><p><code>--save_steps</code> 和 <code>--save_total_limit</code></p><ul><li><code>--save_steps</code> 取任何正整数。</li><li><code>--save_total_limit</code> 取任何正整数或 None。</li><li>作用：分别指定保存模型的步数间隔和最大保存的检查点数量。</li></ul></li><li><p><code>--learning_rate</code></p><ul><li>任何正浮点数。</li><li>作用：指定优化器的学习率。</li></ul></li><li><p><code>--weight_decay</code></p><ul><li>任何非负浮点数。</li><li>作用：指定权重衰减，用于正则化。</li></ul></li><li><p><code>--warmup_ratio</code></p><ul><li>任何非负浮点数，通常在 0 到 1 之间。</li><li>作用：指定预热的比例，即学习率在初始阶段逐渐增加的过程。</li></ul></li><li><p><code>--lr_scheduler_type</code></p><ul><li>可取值如 “linear”、”cosine”、”cosine_with_restarts”、”polynomial” 等。</li><li>作用：指定学习率调度器的类型。</li></ul></li><li><p><code>--logging_steps</code></p><ul><li>任何正整数。</li><li>作用：指定记录日志的步数间隔。</li></ul></li><li><p><code>--fsdp</code></p><ul><li>可取值如 “full_shard”、”auto_wrap” 等，或它们的组合。</li><li>作用：指定使用全分片数据并行（Fully Sharded Data Parallel）的配置。</li></ul></li><li><p><code>--fsdp_transformer_layer_cls_to_wrap</code></p><ul><li>指定要在 FSDP 中包装的特定层的类名。</li><li>作用：针对大型模型的分布式训练进行优化。</li></ul></li><li><p><code>--model_max_length</code></p><ul><li>任何正整数。</li><li>作用：指定模型处理的最大序列长度。</li></ul></li><li><p><code>--gradient_checkpointing</code></p><ul><li>可取值为 True 或 False。</li><li>作用：启用或禁用梯度检查点，以减少显存使用。</li></ul></li><li><p><code>--lazy_preprocess</code></p><ul><li>可取值为 True 或 False。</li><li>作用：启用或禁用懒加载预处理，即按需加载和处理数据。</li></ul></li></ol><p>这些参数共同构成了一个复杂的训练配置，允许用户根据特定需求灵活调整模型训练过程。</p><h2 id="7-总结"><a href="#7-总结" class="headerlink" title="7. 总结"></a>7. 总结</h2><p>随着本文的结束，我们完成了对 FastChat 平台中 train.py 脚本的深入解析，这只是我们系列技术博客中的第一部分。在这一部分中，我们聚焦于 train.py 脚本的结构和功能，涵盖了从数据预处理到模型训练和保存等关键步骤。通过这次解析，读者不仅能够更好地理解 FastChat 平台的工作原理，还能获得如何有效利用这个工具进行大型语言模型训练的宝贵知识。</p><p>随着我们技术博客系列的不断展开，我们将继续深入探索 FastChat 的其他组件和功能。接下来的文章将进一步拓展我们的讨论范围，涉及到更多高级功能和实际应用场景。我们期望这些内容能够为对 AI 和机器学习感兴趣的读者提供更全面、深入的见解。</p><p>最后，我们鼓励读者持续关注我们的博客，以获取关于 FastChat 及其在大型语言模型训练领域应用的最新信息和分析。无论您是该领域的专家还是初学者，我们相信这个系列将为您提供价值和启发。敬请期待我们下一篇文章的发布，它将为您揭开 FastChat 更多令人兴奋的面纱。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;FastChat-训练脚本代码逐行解析-Train-py-【FastChat-系列第-1-篇】&quot;&gt;&lt;a href=&quot;#FastChat-训练脚本代码逐行解析-Train-py-【FastChat-系列第-1-篇】&quot; class=&quot;headerlink&quot; title</summary>
      
    
    
    
    <category term="NLP Insights" scheme="https://chenhuiyu.github.io/categories/NLP-Insights/"/>
    
    
    <category term="FastChat" scheme="https://chenhuiyu.github.io/tags/FastChat/"/>
    
    <category term="LLM" scheme="https://chenhuiyu.github.io/tags/LLM/"/>
    
    <category term="Train" scheme="https://chenhuiyu.github.io/tags/Train/"/>
    
  </entry>
  
  <entry>
    <title>英语学习日记：De Facto</title>
    <link href="https://chenhuiyu.github.io/2024/02/26/Life%20Reflections/%E8%8B%B1%E8%AF%AD%E5%AD%A6%E4%B9%A0%E6%97%A5%E8%AE%B0%EF%BC%9ADe%20Facto/"/>
    <id>https://chenhuiyu.github.io/2024/02/26/Life%20Reflections/%E8%8B%B1%E8%AF%AD%E5%AD%A6%E4%B9%A0%E6%97%A5%E8%AE%B0%EF%BC%9ADe%20Facto/</id>
    <published>2024-02-26T06:29:00.000Z</published>
    <updated>2024-02-26T06:29:53.822Z</updated>
    
    <content type="html"><![CDATA[<p>大家好！🌐 今天我们要探讨一个在英语对话和写作中常见的短语：<strong>de facto</strong></p><h3 id="理解-“De-Facto”"><a href="#理解-“De-Facto”" class="headerlink" title="理解 “De Facto”"></a>理解 “De Facto”</h3><ol><li><p><strong>含义</strong>：’De facto’ 这个短语用来描述一些实际上存在的事物，即使它们没有被官方认可或法律确立。就像是在说“实际上”或“实践中”，而不是“理论上”或“官方上”。</p></li><li><p><strong>词源</strong>：这个短语有着非常有趣的历史。它源自拉丁语，其中 ‘de’ 意为 ‘来自’，’facto’ 意味着 ‘事实’。随着时间的推移，它被英语采纳，并保留了从拉丁语中原始的精髓。</p></li></ol><h3 id="例句"><a href="#例句" class="headerlink" title="例句"></a>例句</h3><ul><li>🌟 <em>In many organizations, there is a <strong>de facto</strong> leader who isn’t officially the boss but is respected and followed by the team.</em></li><li>🌟 <em>While English is the <strong>de facto</strong> language of international business, it’s not the official language in many countries where it’s widely spoken.</em></li><li>🌟 <em>The museum, though not formally recognized, acts as the <strong>de facto</strong> cultural center of the small town.</em></li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>‘De facto’ 这个术语是一种微妙但强大的方式，用来描述情况的现实，区别于其官方或法律地位。将这样的短语纳入你的语言库不仅丰富了你的词汇，还增强了你表达细微想法的能力。继续探索并拥抱语言的美妙吧！📚💬</p><p>记住，语言是一段旅程，不是终点。祝学习愉快，我们下次见！🚀🌟</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;大家好！🌐 今天我们要探讨一个在英语对话和写作中常见的短语：&lt;strong&gt;de facto&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&quot;理解-“De-Facto”&quot;&gt;&lt;a href=&quot;#理解-“De-Facto”&quot; class=&quot;headerlink&quot; title=&quot;理解</summary>
      
    
    
    
    <category term="Life Reflections" scheme="https://chenhuiyu.github.io/categories/Life-Reflections/"/>
    
    
    <category term="Language Learning" scheme="https://chenhuiyu.github.io/tags/Language-Learning/"/>
    
    <category term="English Vocabulary" scheme="https://chenhuiyu.github.io/tags/English-Vocabulary/"/>
    
  </entry>
  
  <entry>
    <title>De Facto: Unveiling the Power of an Intriguing Phrase</title>
    <link href="https://chenhuiyu.github.io/2024/02/26/Life%20Reflections/De%20Facto:%20Unveiling%20the%20Power%20of%20an%20Intriguing%20Phrase%20/"/>
    <id>https://chenhuiyu.github.io/2024/02/26/Life%20Reflections/De%20Facto:%20Unveiling%20the%20Power%20of%20an%20Intriguing%20Phrase%20/</id>
    <published>2024-02-26T06:25:00.000Z</published>
    <updated>2024-02-26T06:29:42.776Z</updated>
    
    <content type="html"><![CDATA[<p>Hello, language enthusiasts! 🌐 Today, we’re diving into a fascinating phrase that often pops up in English conversations and writings: <strong>de facto</strong>. Let’s explore its meaning, origins, and how to use it effectively in sentences. Whether you’re a language learner or a word nerd, you’ll find this exploration both enlightening and fun! ✨</p><h3 id="Understanding-“De-Facto”"><a href="#Understanding-“De-Facto”" class="headerlink" title="Understanding “De Facto”"></a>Understanding “De Facto”</h3><ol><li><p><strong>Meaning</strong>: The term ‘de facto’ is used to describe something that exists in reality, even if it’s not officially recognized or legally established. It’s like saying “in practice” or “in actuality,” as opposed to “in theory” or “officially.”</p></li><li><p><strong>Origins</strong>: This phrase has an interesting journey. It comes from Latin, where ‘de’ means ‘from’ and ‘facto’ means ‘fact.’ Over time, it’s been adopted into English, retaining its original essence from Latin.</p></li></ol><h3 id="Examples-in-Sentences"><a href="#Examples-in-Sentences" class="headerlink" title="Examples in Sentences"></a>Examples in Sentences</h3><ul><li>🌟 <em>In many organizations, there is a <strong>de facto</strong> leader who isn’t officially the boss but is respected and followed by the team.</em></li><li>🌟 <em>While English is the <strong>de facto</strong> language of international business, it’s not the official language in many countries where it’s widely spoken.</em></li><li>🌟 <em>The museum, though not formally recognized, acts as the <strong>de facto</strong> cultural center of the small town.</em></li></ul><h3 id="Wrapping-Up"><a href="#Wrapping-Up" class="headerlink" title="Wrapping Up"></a>Wrapping Up</h3><p>The term ‘de facto’ is a subtle but powerful way to describe the reality of a situation, distinguishing it from its official or legal status. Incorporating such phrases into your language arsenal not only enriches your vocabulary but also enhances your ability to express nuanced ideas. Keep exploring and embracing the beauty of language! 📚💬</p><p>Remember, language is a journey, not a destination. Happy learning, and see you in the next post! 🚀🌟</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Hello, language enthusiasts! 🌐 Today, we’re diving into a fascinating phrase that often pops up in English conversations and writings: &lt;</summary>
      
    
    
    
    <category term="Life Reflections" scheme="https://chenhuiyu.github.io/categories/Life-Reflections/"/>
    
    
    <category term="Language Learning" scheme="https://chenhuiyu.github.io/tags/Language-Learning/"/>
    
    <category term="English Vocabulary" scheme="https://chenhuiyu.github.io/tags/English-Vocabulary/"/>
    
  </entry>
  
  <entry>
    <title>英语学习日记：探索金融术语 &#39;Giro Date&#39;</title>
    <link href="https://chenhuiyu.github.io/2024/02/21/Life%20Reflections/%E8%8B%B1%E8%AF%AD%E5%AD%A6%E4%B9%A0%E6%97%A5%E8%AE%B0%EF%BC%9A%E6%8E%A2%E7%B4%A2%E9%87%91%E8%9E%8D%E6%9C%AF%E8%AF%AD%20&#39;Giro%20Date&#39;/"/>
    <id>https://chenhuiyu.github.io/2024/02/21/Life%20Reflections/%E8%8B%B1%E8%AF%AD%E5%AD%A6%E4%B9%A0%E6%97%A5%E8%AE%B0%EF%BC%9A%E6%8E%A2%E7%B4%A2%E9%87%91%E8%9E%8D%E6%9C%AF%E8%AF%AD%20&#39;Giro%20Date&#39;/</id>
    <published>2024-02-21T07:33:00.000Z</published>
    <updated>2024-02-21T07:34:04.140Z</updated>
    
    <content type="html"><![CDATA[<p>今天的英语学习之旅中，我遇到了一个有趣的金融术语：“Giro date”。这个探索过程不仅丰富了我的词汇，还加深了我对英语中金融概念的理解。让我和你分享一下我是如何分析这个术语及其在金融世界中的重要性。</p><h3 id="发现词源-🌍"><a href="#发现词源-🌍" class="headerlink" title="发现词源 🌍"></a>发现词源 🌍</h3><ul><li><strong>意大利语根源</strong>: 我的研究发现 ‘Giro’ 来自意大利语单词 “girare”，意味着转账或支付。</li><li><strong>金融语境</strong>: 在金融领域中，’giro’ 通常指的是通过银行或其他金融机构的转账。</li></ul><h3 id="学习其用法-💡"><a href="#学习其用法-💡" class="headerlink" title="学习其用法 💡"></a>学习其用法 💡</h3><ul><li><strong>定义</strong>: 在金融交易中，’Giro date’ 特指支付或结算日期。</li><li><strong>银行业重要性</strong>: 这是一个预定的日期，资金预期在此日期被支付或结算。</li></ul><h3 id="实际应用-📘"><a href="#实际应用-📘" class="headerlink" title="实际应用 📘"></a>实际应用 📘</h3><ul><li><strong>发票支付日期</strong>: “Please ensure that the giro date for the invoice is set to the 25th of this month.”（请确保发票的支付日期设为本月25日。）</li><li><strong>贷款还款日期</strong>: “The giro date for the loan repayment is automatically set for the 1st of each month.”（贷款还款的支付日期自动设定为每月的第一天。）</li></ul><p>这些例子帮助我巩固了对这个术语的理解，展示了它在银行业务、财务管理和账务处理中的用途。</p><h3 id="反思与前行-🌟"><a href="#反思与前行-🌟" class="headerlink" title="反思与前行 🌟"></a>反思与前行 🌟</h3><p>随我继续在英语学习的迷人世界中旅行，每一个术语都解锁了新的知识和理解！📚✨</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;今天的英语学习之旅中，我遇到了一个有趣的金融术语：“Giro date”。这个探索过程不仅丰富了我的词汇，还加深了我对英语中金融概念的理解。让我和你分享一下我是如何分析这个术语及其在金融世界中的重要性。&lt;/p&gt;
&lt;h3 id=&quot;发现词源-🌍&quot;&gt;&lt;a href=&quot;#发现词源</summary>
      
    
    
    
    <category term="Life Reflections" scheme="https://chenhuiyu.github.io/categories/Life-Reflections/"/>
    
    
    <category term="Language Learning" scheme="https://chenhuiyu.github.io/tags/Language-Learning/"/>
    
    <category term="English Vocabulary" scheme="https://chenhuiyu.github.io/tags/English-Vocabulary/"/>
    
  </entry>
  
  <entry>
    <title>English Learning Journey: Unraveling the Term &#39;Giro Date&#39;</title>
    <link href="https://chenhuiyu.github.io/2024/02/21/Life%20Reflections/English%20Learning%20Journey:%20Unraveling%20the%20Term%20&#39;Giro%20Date&#39;/"/>
    <id>https://chenhuiyu.github.io/2024/02/21/Life%20Reflections/English%20Learning%20Journey:%20Unraveling%20the%20Term%20&#39;Giro%20Date&#39;/</id>
    <published>2024-02-21T07:32:00.000Z</published>
    <updated>2024-02-21T10:47:41.097Z</updated>
    
    <content type="html"><![CDATA[<p>Today in my English learning journey, I encountered an intriguing financial term: “Giro date”. This exploration not only expanded my vocabulary but also deepened my understanding of financial concepts in English. Let me share with you how I dissected this term and its relevance in the financial world.</p><h3 id="Discovering-the-Origin-🌍"><a href="#Discovering-the-Origin-🌍" class="headerlink" title="Discovering the Origin 🌍"></a>Discovering the Origin 🌍</h3><ul><li><strong>Italian Roots</strong>: My research revealed that ‘Giro’ originates from the Italian word “girare,” meaning to transfer or pay. </li><li><strong>Financial Context</strong>: In the realm of finance, ‘giro’ typically refers to a bank or institutional transfer.</li></ul><h3 id="Learning-the-Usage-💡"><a href="#Learning-the-Usage-💡" class="headerlink" title="Learning the Usage 💡"></a>Learning the Usage 💡</h3><ul><li><strong>Definition</strong>: In financial transactions, ‘Giro date’ specifically denotes the payment or settlement date.</li><li><strong>Banking Significance</strong>: It’s a predetermined date when funds are expected to be paid or settled.</li></ul><h3 id="Applying-it-in-Context-📘"><a href="#Applying-it-in-Context-📘" class="headerlink" title="Applying it in Context 📘"></a>Applying it in Context 📘</h3><ul><li><strong>Invoice Payment Date</strong>: I practiced using the term in a sentence: “Please ensure that the giro date for the invoice is set to the 25th of this month.”</li><li><strong>Loan Repayment Date</strong>: Another example I came up with was, “The giro date for the loan repayment is automatically set for the 1st of each month.”</li></ul><p>These examples helped cement the term in my mind, illustrating its use in banking, financial management, and account processing.</p><h3 id="Reflections-and-Forward-Steps-🌟"><a href="#Reflections-and-Forward-Steps-🌟" class="headerlink" title="Reflections and Forward Steps 🌟"></a>Reflections and Forward Steps 🌟</h3><p>Understanding ‘Giro date’ was not just about adding a new word to my vocabulary; it was about comprehending a concept that plays a vital role in financial transactions. This learning experience has made me appreciate the nuances of financial English and motivated me to delve deeper into industry-specific terminology.</p><p>Join me as I continue my journey through the fascinating world of English language learning, where every term unlocks new knowledge and understanding! 📚✨</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Today in my English learning journey, I encountered an intriguing financial term: “Giro date”. This exploration not only expanded my voca</summary>
      
    
    
    
    <category term="Life Reflections" scheme="https://chenhuiyu.github.io/categories/Life-Reflections/"/>
    
    
    <category term="Language Learning" scheme="https://chenhuiyu.github.io/tags/Language-Learning/"/>
    
    <category term="English Vocabulary" scheme="https://chenhuiyu.github.io/tags/English-Vocabulary/"/>
    
  </entry>
  
  <entry>
    <title>How to Resolve SSH Key Issues with Multiple Git Services</title>
    <link href="https://chenhuiyu.github.io/2024/02/19/Debugging%20Diaries/How%20to%20Resolve%20SSH%20Key%20Issues%20with%20Multiple%20Git%20Services/"/>
    <id>https://chenhuiyu.github.io/2024/02/19/Debugging%20Diaries/How%20to%20Resolve%20SSH%20Key%20Issues%20with%20Multiple%20Git%20Services/</id>
    <published>2024-02-19T07:49:55.000Z</published>
    <updated>2024-02-27T02:34:40.172Z</updated>
    
    <content type="html"><![CDATA[<h1 id="How-to-Resolve-SSH-Key-Issues-with-Multiple-Git-Services-🗝️"><a href="#How-to-Resolve-SSH-Key-Issues-with-Multiple-Git-Services-🗝️" class="headerlink" title="How to Resolve SSH Key Issues with Multiple Git Services 🗝️"></a>How to Resolve SSH Key Issues with Multiple Git Services 🗝️</h1><p>When using Git with different Git services such as GitHub and GitLab, you may encounter SSH key issues. This article will guide you on how to set up and configure SSH keys so that you can work smoothly with multiple services simultaneously.</p><h2 id="1-Generate-SSH-Keys-🔑"><a href="#1-Generate-SSH-Keys-🔑" class="headerlink" title="1. Generate SSH Keys 🔑"></a>1. Generate SSH Keys 🔑</h2><p>First, generate a separate SSH key for each Git service.</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -b 4096 -C <span class="string">"your_email@example.com"</span></span><br></pre></td></tr></tbody></table></figure><p>When generating the keys, save each key with a different filename, for example, <code>id_rsa_github</code> and <code>id_rsa_gitlab</code>.</p><h2 id="2-Add-SSH-Keys-to-Git-Services-🌐"><a href="#2-Add-SSH-Keys-to-Git-Services-🌐" class="headerlink" title="2. Add SSH Keys to Git Services 🌐"></a>2. Add SSH Keys to Git Services 🌐</h2><p>Log in to your GitHub and GitLab accounts, then add the generated public keys (<code>.pub</code> files) to the SSH key sections of each respective account.</p><h2 id="3-Configure-SSH-⚙️"><a href="#3-Configure-SSH-⚙️" class="headerlink" title="3. Configure SSH ⚙️"></a>3. Configure SSH ⚙️</h2><p>Create or edit the <code>~/.ssh/config</code> file to configure different SSH keys for each service.</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GitHub</span></span><br><span class="line">Host github.com</span><br><span class="line">  HostName github.com</span><br><span class="line">  User git</span><br><span class="line">  IdentityFile ~/.ssh/id_rsa_github</span><br><span class="line"></span><br><span class="line"><span class="comment"># GitLab</span></span><br><span class="line">Host gitlab.com</span><br><span class="line">  HostName gitlab.com</span><br><span class="line">  User git</span><br><span class="line">  IdentityFile ~/.ssh/id_rsa_gitlab</span><br></pre></td></tr></tbody></table></figure><p>If your company uses a custom GitLab instance, add a separate configuration block for it.</p><h2 id="4-Test-SSH-Connections-🧪"><a href="#4-Test-SSH-Connections-🧪" class="headerlink" title="4. Test SSH Connections 🧪"></a>4. Test SSH Connections 🧪</h2><p>Test if you can successfully connect to each service via SSH.</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ssh -T git@github.com</span><br><span class="line">ssh -T git@gitlab.com</span><br></pre></td></tr></tbody></table></figure><h2 id="5-Handling-Common-Errors-❗"><a href="#5-Handling-Common-Errors-❗" class="headerlink" title="5. Handling Common Errors ❗"></a>5. Handling Common Errors ❗</h2><p>If you encounter errors such as “Permission denied (publickey)”, check the following:</p><ul><li>Ensure SSH keys are correctly added to the respective Git services.</li><li>Verify if the <code>~/.ssh/config</code> file is configured correctly.</li><li>Use the <code>ssh-add</code> command to ensure SSH keys are loaded into the SSH Agent.</li></ul><h2 id="6-Common-Issues-and-Solutions-💡"><a href="#6-Common-Issues-and-Solutions-💡" class="headerlink" title="6. Common Issues and Solutions 💡"></a>6. Common Issues and Solutions 💡</h2><ul><li><strong>Multiple GitLab Instances</strong>: If you use a custom GitLab instance along with GitLab.com, ensure they are separately configured in the SSH <code>config</code> file.</li><li><strong>Network Issues</strong>: Check if any network settings (like proxies, VPNs) might affect SSH connections.</li></ul><hr><p>By following the above steps, you should be able to resolve most SSH key-related issues, especially when dealing with multiple Git services. If you have further questions or specific scenarios, feel free to ask in the comments. 🚀✨</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;How-to-Resolve-SSH-Key-Issues-with-Multiple-Git-Services-🗝️&quot;&gt;&lt;a href=&quot;#How-to-Resolve-SSH-Key-Issues-with-Multiple-Git-Services-🗝️</summary>
      
    
    
    
    <category term="Debugging Diaries" scheme="https://chenhuiyu.github.io/categories/Debugging-Diaries/"/>
    
    
    <category term="IssueFix" scheme="https://chenhuiyu.github.io/tags/IssueFix/"/>
    
  </entry>
  
  <entry>
    <title>如何解决多个 Git 服务的 SSH 密钥问题</title>
    <link href="https://chenhuiyu.github.io/2024/02/19/Debugging%20Diaries/%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E5%A4%9A%E4%B8%AA%20Git%20%E6%9C%8D%E5%8A%A1%E7%9A%84%20SSH%20%E5%AF%86%E9%92%A5%E9%97%AE%E9%A2%98/"/>
    <id>https://chenhuiyu.github.io/2024/02/19/Debugging%20Diaries/%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E5%A4%9A%E4%B8%AA%20Git%20%E6%9C%8D%E5%8A%A1%E7%9A%84%20SSH%20%E5%AF%86%E9%92%A5%E9%97%AE%E9%A2%98/</id>
    <published>2024-02-19T07:47:55.000Z</published>
    <updated>2024-02-27T02:34:39.008Z</updated>
    
    <content type="html"><![CDATA[<h1 id="如何解决多个-Git-服务的-SSH-密钥问题-🗝️"><a href="#如何解决多个-Git-服务的-SSH-密钥问题-🗝️" class="headerlink" title="如何解决多个 Git 服务的 SSH 密钥问题 🗝️"></a>如何解决多个 Git 服务的 SSH 密钥问题 🗝️</h1><p>在使用 Git 和不同的 Git 服务（如 GitHub 和 GitLab）时，可能会遇到 SSH 密钥的问题。本文将指导你如何设置和配置 SSH 密钥，以便可以同时与多个服务顺利工作。</p><h2 id="1-生成-SSH-密钥-🔑"><a href="#1-生成-SSH-密钥-🔑" class="headerlink" title="1. 生成 SSH 密钥 🔑"></a>1. 生成 SSH 密钥 🔑</h2><p>首先，为每个 Git 服务生成一个独立的 SSH 密钥。</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -b 4096 -C <span class="string">"your_email@example.com"</span></span><br></pre></td></tr></tbody></table></figure><p>在生成密钥时，将每个密钥保存为不同的文件名，例如 <code>id_rsa_github</code> 和 <code>id_rsa_gitlab</code>。</p><h2 id="2-将-SSH-密钥添加到-Git-服务-🌐"><a href="#2-将-SSH-密钥添加到-Git-服务-🌐" class="headerlink" title="2. 将 SSH 密钥添加到 Git 服务 🌐"></a>2. 将 SSH 密钥添加到 Git 服务 🌐</h2><p>登录到你的 GitHub 和 GitLab 账户，然后将生成的公钥（<code>.pub</code> 文件）添加到各自账户的 SSH 密钥部分。</p><h2 id="3-配置-SSH-⚙️"><a href="#3-配置-SSH-⚙️" class="headerlink" title="3. 配置 SSH ⚙️"></a>3. 配置 SSH ⚙️</h2><p>创建或编辑 <code>~/.ssh/config</code> 文件，为每个服务配置不同的 SSH 密钥。</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GitHub</span></span><br><span class="line">Host github.com</span><br><span class="line">  HostName github.com</span><br><span class="line">  User git</span><br><span class="line">  IdentityFile ~/.ssh/id_rsa_github</span><br><span class="line"></span><br><span class="line"><span class="comment"># GitLab</span></span><br><span class="line">Host gitlab.com</span><br><span class="line">  HostName gitlab.com</span><br><span class="line">  User git</span><br><span class="line">  IdentityFile ~/.ssh/id_rsa_gitlab</span><br></pre></td></tr></tbody></table></figure><p>如果你的公司使用自定义的 GitLab 实例，请为其添加一个单独的配置块。</p><h2 id="4-测试-SSH-连接-🧪"><a href="#4-测试-SSH-连接-🧪" class="headerlink" title="4. 测试 SSH 连接 🧪"></a>4. 测试 SSH 连接 🧪</h2><p>测试是否能成功通过 SSH 连接到每个服务。</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ssh -T git@github.com</span><br><span class="line">ssh -T git@gitlab.com</span><br></pre></td></tr></tbody></table></figure><h2 id="5-处理常见错误-❗"><a href="#5-处理常见错误-❗" class="headerlink" title="5. 处理常见错误 ❗"></a>5. 处理常见错误 ❗</h2><p>如果遇到错误，如 “Permission denied (publickey)”，请检查以下几点：</p><ul><li>确认 SSH 密钥是否已正确添加到相应的 Git 服务。</li><li>检查 <code>~/.ssh/config</code> 文件是否正确配置。</li><li>使用 <code>ssh-add</code> 命令确保 SSH 密钥已加载到 SSH Agent。</li></ul><h2 id="6-常见问题和解决方案-💡"><a href="#6-常见问题和解决方案-💡" class="headerlink" title="6. 常见问题和解决方案 💡"></a>6. 常见问题和解决方案 💡</h2><ul><li><strong>多个 GitLab 实例</strong>：如果你使用了公司的自定义 GitLab 实例和 GitLab.com，请确保 SSH <code>config</code> 文件中它们的配置是分开的。</li><li><strong>网络问题</strong>：检查是否有网络设置（如代理、VPN）可能影响 SSH 连接。</li></ul><hr><p>通过遵循以上步骤，你应该能够解决大部分与 SSH 密钥相关的问题，特别是在处理多个 Git 服务时。如果有进一步的问题或者特殊情况，欢迎在评论中提出。 🚀✨</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;如何解决多个-Git-服务的-SSH-密钥问题-🗝️&quot;&gt;&lt;a href=&quot;#如何解决多个-Git-服务的-SSH-密钥问题-🗝️&quot; class=&quot;headerlink&quot; title=&quot;如何解决多个 Git 服务的 SSH 密钥问题 🗝️&quot;&gt;&lt;/a&gt;如何解决多</summary>
      
    
    
    
    <category term="Debugging Diaries" scheme="https://chenhuiyu.github.io/categories/Debugging-Diaries/"/>
    
    
    <category term="IssueFix" scheme="https://chenhuiyu.github.io/tags/IssueFix/"/>
    
  </entry>
  
  <entry>
    <title>理解大型语言模型中Fine-tuning和Further Pretraining的区别</title>
    <link href="https://chenhuiyu.github.io/2024/02/19/NLP%20Insights/%E7%90%86%E8%A7%A3%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%ADFine-tuning%E5%92%8CFurther%20Pretraining%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
    <id>https://chenhuiyu.github.io/2024/02/19/NLP%20Insights/%E7%90%86%E8%A7%A3%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%ADFine-tuning%E5%92%8CFurther%20Pretraining%E7%9A%84%E5%8C%BA%E5%88%AB/</id>
    <published>2024-02-19T07:10:29.000Z</published>
    <updated>2024-02-19T08:56:59.892Z</updated>
    
    <content type="html"><![CDATA[<h1 id="理解大型语言模型中-Fine-tuning-和-Further-Pretraining-的区别"><a href="#理解大型语言模型中-Fine-tuning-和-Further-Pretraining-的区别" class="headerlink" title="理解大型语言模型中 Fine-tuning 和 Further Pretraining 的区别"></a>理解大型语言模型中 Fine-tuning 和 Further Pretraining 的区别</h1><p>在自然语言处理（NLP）领域，大型语言模型，如 GPT 和 BERT 的出现，彻底改变了我们处理文本分类、情感分析和问答等任务的方式。在这些模型的应用中，Fine-tuning（微调）和 Further Pretraining（进一步预训练）是两种关键技术。虽然它们看起来相似，但实际上服务于 NLP 流程中的不同需求和场景。</p><h2 id="什么是-Fine-tuning？"><a href="#什么是-Fine-tuning？" class="headerlink" title="什么是 Fine-tuning？"></a>什么是 Fine-tuning？</h2><p>Fine-tuning 是指在特定任务的数据集上进一步训练（或“微调”）一个预训练好的模型的过程。这种方法在数据集相对较小但标注良好的情况下特别有效。</p><h3 id="示例场景：情感分析"><a href="#示例场景：情感分析" class="headerlink" title="示例场景：情感分析"></a>示例场景：情感分析</h3><p>假设你有一组电影评论数据，每条评论都标记了正面或负面情感。你想创建一个模型来预测评论的情感。</p><h4 id="Python-代码示例（使用-PyTorch-和-HuggingFace-的-Transformers）"><a href="#Python-代码示例（使用-PyTorch-和-HuggingFace-的-Transformers）" class="headerlink" title="Python 代码示例（使用 PyTorch 和 HuggingFace 的 Transformers）"></a>Python 代码示例（使用 PyTorch 和 HuggingFace 的 Transformers）</h4><p>This notebook demonstrates the fine-tuning of a BERT model on the IMDB dataset for sentiment analysis. For detailed code implementation, please refer to the following link:<a href="https://colab.research.google.com/drive/15naxP8pNMoCCBMgMSOv4ETDRGL46YR38?usp=sharing">link</a>.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> (</span><br><span class="line">    BertTokenizer,</span><br><span class="line">    BertForSequenceClassification,</span><br><span class="line">    Trainer,</span><br><span class="line">    TrainingArguments,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset, DatasetDict</span><br><span class="line"></span><br><span class="line"><span class="string">"""## 1.加载和准备IMDB数据集样本</span></span><br><span class="line"><span class="string">选取一部分数据用于Fine-tuning。</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载IMDB数据集</span></span><br><span class="line">dataset = load_dataset(<span class="string">'imdb'</span>, split=<span class="string">'train'</span>)</span><br><span class="line">small_dataset = dataset.shuffle(seed=<span class="number">42</span>).select(<span class="built_in">range</span>(<span class="number">10000</span>))  <span class="comment"># 选取前10000个样本</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化tokenizer</span></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">"bert-base-uncased"</span>)</span><br><span class="line"></span><br><span class="line">device = <span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 编码数据集</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">examples</span>):</span><br><span class="line">    <span class="keyword">return</span> tokenizer(examples[<span class="string">"text"</span>], padding=<span class="string">"max_length"</span>, truncation=<span class="literal">True</span>, max_length=<span class="number">512</span>)</span><br><span class="line"></span><br><span class="line">encoded_small_dataset = small_dataset.<span class="built_in">map</span>(encode, batched=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertModel</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">visualize_attention</span>(<span class="params">sentence, model, tokenizer</span>):</span><br><span class="line">    model.to(device)</span><br><span class="line">    <span class="comment"># 将模型设置为评估模式</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将输入文本转换为模型可以理解的形式</span></span><br><span class="line">    inputs = tokenizer(sentence, return_tensors=<span class="string">"pt"</span>).to(device) <span class="comment"># 确保输入也在相同设备</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用模型获取注意力权重</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        outputs = model(**inputs)</span><br><span class="line">    attentions = outputs.attentions</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 选择要可视化的层和头</span></span><br><span class="line">    layer = <span class="number">5</span></span><br><span class="line">    head = <span class="number">1</span></span><br><span class="line">    attention = attentions[layer][<span class="number">0</span>, head].cpu().numpy() <span class="comment"># 将注意力权重移回CPU进行可视化</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设置可视化的tokens</span></span><br><span class="line">    tokens = tokenizer.convert_ids_to_tokens(inputs[<span class="string">"input_ids"</span>][<span class="number">0</span>].cpu()) <span class="comment"># 同样确保tokens在CPU上</span></span><br><span class="line">    <span class="comment"># 绘制注意力矩阵</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">10</span>, <span class="number">10</span>))</span><br><span class="line">    plt.matshow(attention, cmap=<span class="string">'viridis'</span>)</span><br><span class="line">    plt.xticks(<span class="built_in">range</span>(<span class="built_in">len</span>(tokens)), tokens, rotation=<span class="number">90</span>)</span><br><span class="line">    plt.yticks(<span class="built_in">range</span>(<span class="built_in">len</span>(tokens)), tokens)</span><br><span class="line">    plt.colorbar()</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="string">"""## 2. 可视化一个样本句子的注意力权重（未经Fine-tuning）</span></span><br><span class="line"><span class="string">选择数据集中的一个句子并展示其原始BERT模型的注意力权重。</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用未经Fine-tuning的模型</span></span><br><span class="line">model = BertModel.from_pretrained(<span class="string">"bert-base-uncased"</span>, output_attentions=<span class="literal">True</span>)</span><br><span class="line">sample_sentence = <span class="string">"I love this movie, it's fantastic!"</span></span><br><span class="line">visualize_attention(sample_sentence, model, tokenizer)</span><br><span class="line"></span><br><span class="line"><span class="string">"""## 3. Fine-tuning BERT模型</span></span><br><span class="line"><span class="string">在选取的IMDB样本上进行Fine-tuning。</span></span><br><span class="line"><span class="string">### 3.1 准备数据加载器</span></span><br><span class="line"><span class="string">为了训练模型，我们需要创建PyTorch的DataLoader。这将使我们能够在训练过程中有效地加载数据。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">### 3.2 设置Fine-tuning环境</span></span><br><span class="line"><span class="string">初始化模型、优化器以及损失函数。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">### 3.3 Fine-tuning模型</span></span><br><span class="line"><span class="string">执行Fine-tuning的训练循环。执行以上代码将在IMDB数据集的小样本上对BERT模型进行Fine-tuning。这可能需要一些时间，具体取决于您的硬件配置。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据集转换为PyTorch Tensor</span></span><br><span class="line">encoded_small_dataset.set_format(<span class="string">'torch'</span>, columns=[<span class="string">'input_ids'</span>, <span class="string">'attention_mask'</span>, <span class="string">'label'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建数据加载器</span></span><br><span class="line">train_loader = DataLoader(encoded_small_dataset, batch_size=<span class="number">8</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertConfig, BertForSequenceClassification</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载配置并设置输出注意力权重</span></span><br><span class="line">config = BertConfig.from_pretrained(<span class="string">"bert-base-uncased"</span>, output_attentions=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化用于序列分类的BERT模型</span></span><br><span class="line"><span class="comment"># 使用更新后的配置加载模型</span></span><br><span class="line">model = BertForSequenceClassification.from_pretrained(<span class="string">"bert-base-uncased"</span>, config=config)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置优化器</span></span><br><span class="line">optimizer = optim.AdamW(model.parameters(), lr=<span class="number">5e-5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用交叉熵损失函数</span></span><br><span class="line">criterion = torch.nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置训练的轮次</span></span><br><span class="line">epochs = <span class="number">8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    model.train()</span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> train_loader:</span><br><span class="line">        <span class="comment"># 将数据移至GPU</span></span><br><span class="line">        input_ids = batch[<span class="string">'input_ids'</span>].to(device)</span><br><span class="line">        attention_mask = batch[<span class="string">'attention_mask'</span>].to(device)</span><br><span class="line">        labels = batch[<span class="string">'label'</span>].to(device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 模型前向传播</span></span><br><span class="line">        outputs = model(input_ids, attention_mask=attention_mask)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算损失</span></span><br><span class="line">        loss = criterion(outputs.logits, labels)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 反向传播和优化</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        total_loss += loss.item()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"Epoch: <span class="subst">{epoch+<span class="number">1</span>}</span>, Loss: <span class="subst">{total_loss/<span class="built_in">len</span>(train_loader)}</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">"""### 4. 可视化同一句子的注意力权重（经过Fine-tuning）</span></span><br><span class="line"><span class="string">使用Fine-tuning后的模型再次可视化同一句子的注意力权重。您可以重用之前提供的visualize_attention函数：</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">visualize_attention(sample_sentence, model, tokenizer)</span><br></pre></td></tr></tbody></table></figure><p>在这个例子中，BERT 模型在电影评论数据集上进行了 fine-tuning，用于情感分析。</p><h2 id="什么是-Further-Pretraining？"><a href="#什么是-Further-Pretraining？" class="headerlink" title="什么是 Further Pretraining？"></a>什么是 Further Pretraining？</h2><p>Further Pretraining（也称为 Domain-adaptive Pretraining，领域适应性预训练）是在一个新的数据集上继续训练一个预训练模型的过程，这个新的数据集与特定的领域更相关，但不一定为特定任务标注。</p><h3 id="示例场景：法律文档分析"><a href="#示例场景：法律文档分析" class="headerlink" title="示例场景：法律文档分析"></a>示例场景：法律文档分析</h3><p>假设你正在处理法律文档，并希望利用一个在通用文本上训练的语言模型。</p><h4 id="Further-Pretraining-的代码示例"><a href="#Further-Pretraining-的代码示例" class="headerlink" title="Further Pretraining 的代码示例"></a>Further Pretraining 的代码示例</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertModel, BertTokenizer</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载预训练的BERT模型和分词器</span></span><br><span class="line">model = BertModel.from_pretrained(<span class="string">'bert-base-uncased'</span>)</span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">'bert-base-uncased'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备法律文档数据集</span></span><br><span class="line"><span class="comment"># 假设'legal_documents'是法律文档的文本列表</span></span><br><span class="line">encoded_input = tokenizer(legal_documents, padding=<span class="literal">True</span>, truncation=<span class="literal">True</span>, max_length=<span class="number">512</span>, return_tensors=<span class="string">'pt'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 继续预训练模型</span></span><br><span class="line"><span class="comment"># 这一步通常包括掩码语言建模或其他预训练目标</span></span><br><span class="line"><span class="comment"># 这里提供一个概念性示例</span></span><br><span class="line">model.train()</span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> encoded_input:</span><br><span class="line">    outputs = model(**batch)</span><br><span class="line">    <span class="comment"># ... 执行进一步训练步骤</span></span><br></pre></td></tr></tbody></table></figure><p>在这个例子中，BERT 模型在法律文档数据集上进行了进一步的预训练，使其在进行特定法律 NLP 任务的 fine-tuning 之前，更擅长理解法律术语和概念。</p><h2 id="关键区别"><a href="#关键区别" class="headerlink" title="关键区别"></a>关键区别</h2><ul><li><strong>目的</strong>：Fine-tuning 针对具有标签数据的特定任务进行模型调整，而 Further Pretraining 则是使模型更好地适应特定领域或语言风格。</li><li><strong>数据集</strong>：Fine-tuning 使用特定任务的标注数据集。Further Pretraining 使用更大的、特定领域的数据集，这些数据集可能不是为特定任务标注的。</li><li><strong>训练目标</strong>：Fine-tuning 涉及调整模型进行特定预测，而 Further Pretraining 侧重于在新领域中的通用语言理解</li></ul><p>。</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>Fine-tuning 和 Further Pretraining 都是 NLP 领域的强大技术。通过理解它们的区别和应用，我们可以更好地利用大型语言模型来解决各种领域中的多样化和复杂任务。无论你是在构建社交媒体帖子的情感分析模型，还是调整模型以理解法律文档，这些技术都为 NLP 领域的不断发展提供了稳健的解决方案。</p><hr><p><strong>注意</strong>：提供的代码示例是概念性的，需要适当的环境设置，包括必要的库和数据集，才能执行。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;理解大型语言模型中-Fine-tuning-和-Further-Pretraining-的区别&quot;&gt;&lt;a href=&quot;#理解大型语言模型中-Fine-tuning-和-Further-Pretraining-的区别&quot; class=&quot;headerlink&quot; title</summary>
      
    
    
    
    <category term="NLP Insights" scheme="https://chenhuiyu.github.io/categories/NLP-Insights/"/>
    
    
    <category term="LLM" scheme="https://chenhuiyu.github.io/tags/LLM/"/>
    
  </entry>
  
</feed>
