<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>黑头呆鱼进化之旅</title>
  
  <subtitle>只身打码过草原</subtitle>
  <link href="https://chenhuiyu.github.io/atom.xml" rel="self"/>
  
  <link href="https://chenhuiyu.github.io/"/>
  <updated>2024-02-27T02:33:36.395Z</updated>
  <id>https://chenhuiyu.github.io/</id>
  
  <author>
    <name>Huiyu Chen</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>FastChat 训练脚本代码逐行解析-Train.py 【FastChat 系列第 1 篇】</title>
    <link href="https://chenhuiyu.github.io/2024/02/27/NLP%20Insights/FastChat%20%E8%AE%AD%E7%BB%83%E8%84%9A%E6%9C%AC%E4%BB%A3%E7%A0%81%E9%80%90%E8%A1%8C%E8%A7%A3%E6%9E%90-Train.py%20%E3%80%90FastChat%20%E7%B3%BB%E5%88%97%E7%AC%AC%201%20%E7%AF%87%E3%80%91/"/>
    <id>https://chenhuiyu.github.io/2024/02/27/NLP%20Insights/FastChat%20%E8%AE%AD%E7%BB%83%E8%84%9A%E6%9C%AC%E4%BB%A3%E7%A0%81%E9%80%90%E8%A1%8C%E8%A7%A3%E6%9E%90-Train.py%20%E3%80%90FastChat%20%E7%B3%BB%E5%88%97%E7%AC%AC%201%20%E7%AF%87%E3%80%91/</id>
    <published>2024-02-26T17:43:18.000Z</published>
    <updated>2024-02-27T02:33:36.395Z</updated>
    
    <content type="html"><![CDATA[<h1 id="FastChat-训练脚本代码逐行解析-Train-py-【FastChat-系列第-1-篇】"><a href="#FastChat-训练脚本代码逐行解析-Train-py-【FastChat-系列第-1-篇】" class="headerlink" title="FastChat 训练脚本代码逐行解析-Train.py 【FastChat 系列第 1 篇】"></a>FastChat 训练脚本代码逐行解析-Train.py 【FastChat 系列第 1 篇】</h1><p>在本文中，我们将深入探讨 <a href="https://github.com/lm-sys/FastChat">FastChat</a> 的 <code>train.py</code> 脚本，这是一个用于训练和优化大型语言模型的关键组件。FastChat 是一个先进的开源平台，专注于开发、部署和评估基于大型语言模型（LLM）的聊天机器人。该平台不仅提供对顶尖模型如 Vicuna 和 MT-Bench 的支持，还包括一个分布式的多模型服务系统，配备了 Web UI 和与 OpenAI 兼容的 RESTful API，使用户能够高效地训练和评估他们的模型。</p><p>本文的深入分析将聚焦于 <code>train.py</code> 脚本的源代码。这个脚本是基于 transformers 库的自然语言处理模型训练脚本，涵盖了数据预处理、模型训练和保存等关键步骤。我们旨在提供对 <code>train.py</code> 中每个类和函数的详细解释，包括它们的功能和在整个训练过程中的作用。</p><h2 id="1-导入模块"><a href="#1-导入模块" class="headerlink" title="1. 导入模块"></a>1. 导入模块</h2><h3 id="1-内置模块"><a href="#1-内置模块" class="headerlink" title="1. 内置模块"></a>1. 内置模块</h3><p>这些是 Python 自带的标准库模块，无需额外安装。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> dataclasses <span class="keyword">import</span> dataclass, field</span><br></pre></td></tr></tbody></table></figure><p>导入 Python 的<code>dataclasses</code>模块，用于创建带有默认值的类。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br></pre></td></tr></tbody></table></figure><p>导入<code>json</code>模块，用于处理 JSON 格式的数据。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br></pre></td></tr></tbody></table></figure><p>导入<code>math</code>模块，用于数学运算。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pathlib</span><br></pre></td></tr></tbody></table></figure><p>导入<code>pathlib</code>模块，用于处理文件路径。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Dict</span>, <span class="type">Optional</span>, <span class="type">Sequence</span></span><br></pre></td></tr></tbody></table></figure><p>导入<code>typing</code>模块，用于类型注解。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></tbody></table></figure><h3 id="2-依赖库"><a href="#2-依赖库" class="headerlink" title="2. 依赖库"></a>2. 依赖库</h3><p>这些是外部安装的依赖库，通常通过包管理器如 pip 安装。<br>导入<code>numpy</code>库，一个常用的科学计算库。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></tbody></table></figure><p>导入<code>PyTorch</code>，一个流行的深度学习框架。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br></pre></td></tr></tbody></table></figure><p>从<code>torch</code>中导入<code>Dataset</code>，用于创建自定义数据集。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> transformers</span><br></pre></td></tr></tbody></table></figure><p>导入<code>transformers</code>库，一个流行的自然语言处理库。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Trainer</span><br></pre></td></tr></tbody></table></figure><p>从<code>transformers</code>中导入<code>Trainer</code>，用于训练模型。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers.trainer_pt_utils <span class="keyword">import</span> LabelSmoother</span><br></pre></td></tr></tbody></table></figure><p>从<code>transformers</code>中导入<code>LabelSmoother</code>，用于标签平滑。</p><h3 id="3-项目特定函数"><a href="#3-项目特定函数" class="headerlink" title="3. 项目特定函数"></a>3. 项目特定函数</h3><p>这些是在 Fast Chat 项目中自定义实现的函数或类。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fastchat.conversation <span class="keyword">import</span> SeparatorStyle</span><br><span class="line"></span><br><span class="line"><span class="comment">## code from FastChat/fastchat/conversation.py</span></span><br><span class="line">    <span class="string">"""Separator styles."""</span></span><br><span class="line"></span><br><span class="line">    ADD_COLON_SINGLE = auto()</span><br><span class="line">    ADD_COLON_TWO = auto()</span><br><span class="line">    ADD_COLON_SPACE_SINGLE = auto()</span><br><span class="line">    NO_COLON_SINGLE = auto()</span><br><span class="line">    NO_COLON_TWO = auto()</span><br><span class="line">    ADD_NEW_LINE_SINGLE = auto()</span><br><span class="line">    LLAMA2 = auto()</span><br><span class="line">    CHATGLM = auto()</span><br><span class="line">    CHATML = auto()</span><br><span class="line">    CHATINTERN = auto()</span><br><span class="line">    DOLLY = auto()</span><br><span class="line">    RWKV = auto()</span><br><span class="line">    PHOENIX = auto()</span><br><span class="line">    ROBIN = auto()</span><br><span class="line">    FALCON_CHAT = auto()</span><br><span class="line">    CHATGLM3 = auto()</span><br><span class="line">    DEEPSEEK_CHAT = auto()</span><br><span class="line">    METAMATH = auto()</span><br><span class="line">    YUAN2 = auto()</span><br></pre></td></tr></tbody></table></figure><p>从<code>fastchat</code>包导入<code>SeparatorStyle</code>，用于定义对话分隔符风格。<code>SeparatorStyle</code> 类是一个使用 Python 的 <code>enum</code> 模块创建的枚举类，用于定义一系列的分隔符样式。枚举（Enumeration）是一种编程概念，用于定义一组命名的常数，使代码更加清晰和易于维护。</p><p>在 <code>SeparatorStyle</code> 类中，每个成员代表一种特定的分隔符样式。这些样式通常用于文本处理中，特别是在需要区分不同部分或元素的情况下。例如，在处理对话或文本数据时，可能需要不同的方式来区分用户输入和机器回复。</p><p>关于 <code>auto()</code> 函数的使用：</p><ul><li><code>auto()</code> 是 Python <code>enum</code> 模块提供的一个特殊函数。它在枚举类中自动分配一个唯一的值给每个成员。</li><li>在不使用 <code>auto()</code> 的情况下，你需要手动为每个枚举成员指定一个唯一的值。使用 <code>auto()</code> 可以简化这个过程，让 Python 自动处理这些值的分配。</li><li><code>auto()</code> 分配的值通常是整数，从 1 开始依次递增。</li></ul><p>具体到 <code>SeparatorStyle</code> 类，<code>auto()</code> 被用来为每种分隔符样式自动分配一个唯一的整数值。例如，<code>ADD_COLON_SINGLE</code>、<code>ADD_COLON_TWO</code> 等将分别被赋予不同的整数值。</p><p>每个枚举成员的名称（如 <code>ADD_COLON_SINGLE</code>、<code>NO_COLON_SINGLE</code> 等）通常描述了该分隔符样式的特点。例如，<code>ADD_COLON_SINGLE</code> 可能表示在某个元素后添加一个冒号作为分隔符，而 <code>NO_COLON_SINGLE</code> 则表示不添加冒号。</p><p>这种方式使得在代码中引用和处理这些分隔符样式变得更加方便和清晰。例如，可以根据不同的场景或需求选择使用不同的分隔符样式，而无需记住它们对应的具体值。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fastchat.model.model_adapter <span class="keyword">import</span> get_conversation_template</span><br><span class="line"></span><br><span class="line"><span class="comment">## code from FastChat/fastchat/model/model_adapter.py</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_conversation_template</span>(<span class="params">model_path: <span class="built_in">str</span></span>) -&gt; Conversation:</span><br><span class="line">    <span class="string">"""Get the default conversation template."""</span></span><br><span class="line">    adapter = get_model_adapter(model_path)</span><br><span class="line">    <span class="keyword">return</span> adapter.get_default_conv_template(model_path)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## code from FastChat/fastchat/model/model_adapter.py</span></span><br><span class="line"><span class="meta">@cache</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_model_adapter</span>(<span class="params">model_path: <span class="built_in">str</span></span>) -&gt; BaseModelAdapter:</span><br><span class="line">    <span class="string">"""Get a model adapter for a model_path."""</span></span><br><span class="line">    model_path_basename = os.path.basename(os.path.normpath(model_path))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Try the basename of model_path at first</span></span><br><span class="line">    <span class="keyword">for</span> adapter <span class="keyword">in</span> model_adapters:</span><br><span class="line">        <span class="keyword">if</span> adapter.<span class="keyword">match</span>(model_path_basename) <span class="keyword">and</span> <span class="built_in">type</span>(adapter) != BaseModelAdapter:</span><br><span class="line">            <span class="keyword">return</span> adapter</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Then try the full path</span></span><br><span class="line">    <span class="keyword">for</span> adapter <span class="keyword">in</span> model_adapters:</span><br><span class="line">        <span class="keyword">if</span> adapter.<span class="keyword">match</span>(model_path):</span><br><span class="line">            <span class="keyword">return</span> adapter</span><br><span class="line"></span><br><span class="line">    <span class="keyword">raise</span> ValueError(<span class="string">f"No valid model adapter for <span class="subst">{model_path}</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## code from FastChat/fastchat/model/model_adapter.py</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_default_conv_template</span>(<span class="params">self, model_path: <span class="built_in">str</span></span>) -&gt; Conversation:</span><br><span class="line">        <span class="keyword">return</span> get_conv_template(<span class="string">"one_shot"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## code from FastChat/fastchat/conversation.py</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_conv_template</span>(<span class="params">name: <span class="built_in">str</span></span>) -&gt; Conversation:</span><br><span class="line">    <span class="string">"""Get a conversation template."""</span></span><br><span class="line">    <span class="keyword">return</span> conv_templates[name].copy()</span><br><span class="line"></span><br><span class="line"><span class="comment">## code from FastChat/fastchat/conversation.py</span></span><br><span class="line"><span class="comment"># A global registry for all conversation templates</span></span><br><span class="line">conv_templates: <span class="type">Dict</span>[<span class="built_in">str</span>, Conversation] = {}</span><br></pre></td></tr></tbody></table></figure><p>从<code>fastchat</code>包导入<code>get_conversation_template</code>，用于获取对话模板。<br>在这段代码中，调用逻辑主要涉及到获取特定模型的默认对话模板。调用链路如下：</p><ol><li><p><strong>起始调用 - <code>get_conversation_template(model_path: str)</code></strong></p><ul><li>这个函数是调用链的起点。它接收一个参数 <code>model_path</code>，用于指定模型的路径。</li><li>这个函数的目的是获取给定模型路径的默认对话模板。</li></ul></li><li><p><strong>调用 <code>get_model_adapter(model_path: str)</code></strong></p><ul><li><code>get_conversation_template</code> 函数首先调用 <code>get_model_adapter</code>，传入模型路径。</li><li><code>get_model_adapter</code> 的目的是根据提供的模型路径，找到并返回一个适合该模型的 <code>BaseModelAdapter</code> 对象。</li><li>这个函数首先尝试匹配 <code>model_path</code> 的基本名称（basename），如果没有找到匹配项，它会尝试匹配完整的路径。</li><li>如果找到合适的适配器，则返回该适配器；如果没有找到，则抛出一个 <code>ValueError</code>。</li></ul></li><li><p><strong>执行 <code>BaseModelAdapter.get_default_conv_template(model_path: str)</code></strong></p><ul><li>在获取到适当的模型适配器后，<code>get_conversation_template</code> 通过调用该适配器的 <code>get_default_conv_template</code> 方法来获取默认的对话模板。</li><li>注意这个方法在 <code>BaseModelAdapter</code> 类中定义，但可能在子类中被重写。</li></ul></li><li><p><strong>调用 <code>get_conv_template(name: str)</code></strong></p><ul><li>在 <code>get_default_conv_template</code> 方法内部，它调用 <code>get_conv_template</code> 函数，通常传入一个预定义的模板名称，比如 <code>"one_shot"</code>。</li><li><code>get_conv_template</code> 的作用是从全局注册的对话模板字典 <code>conv_templates</code> 中获取指定名称的模板。</li></ul></li><li><p><strong>获取并返回 <code>Conversation</code> 对象</strong></p><ul><li><code>get_conv_template</code> 函数返回 <code>Conversation</code> 类的一个实例，这通常是从 <code>conv_templates</code> 字典中复制得到的。</li><li>最终，这个 <code>Conversation</code> 实例被返回到最初调用 <code>get_conversation_template</code> 的地方。</li></ul></li></ol><p>总结调用链路：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">get_conversation_template(model_path)</span><br><span class="line">  -&gt; get_model_adapter(model_path)</span><br><span class="line">  -&gt; [BaseModelAdapter].get_default_conv_template(model_path)</span><br><span class="line">    -&gt; get_conv_template(name)</span><br><span class="line">      -&gt; 返回 Conversation 对象</span><br></pre></td></tr></tbody></table></figure><p>在这个过程中，代码通过一系列函数调用，根据提供的模型路径，找到相应的模型适配器，并从中获取特定的对话模板。这种设计模式允许灵活地为不同的模型提供不同的对话模板，从而提高了代码的可重用性和可扩展性。</p><hr><h2 id="2-配置类"><a href="#2-配置类" class="headerlink" title="2. 配置类"></a>2. 配置类</h2><p>这些类是使用 Python 的 <code>dataclass</code> 装饰器定义的，主要用于存储配置和参数。这些类通常不包含复杂的方法或逻辑，而是用于定义和存储数据结构。这些类包括：</p><ul><li><code>ModelArguments</code>: 存储与模型相关的参数，如模型路径、远程代码信任等。</li><li><code>DataArguments</code>: 存储与数据相关的参数，如数据路径、评估数据路径以及是否使用懒加载预处理。</li><li><code>TrainingArguments</code>: 存储与训练相关的参数，如缓存目录、优化器类型、模型最大长度等。这个类继承自 <code>transformers.TrainingArguments</code>，增加了一些自定义参数。</li></ul><p>这些类主要用于简化和组织代码中的参数管理，使得参数的修改和访问更加方便。</p><h3 id="1-ModelArguments-类"><a href="#1-ModelArguments-类" class="headerlink" title="1. ModelArguments 类"></a>1. ModelArguments 类</h3><h4 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ModelArguments</span>:</span><br><span class="line">    model_name_or_path: <span class="type">Optional</span>[<span class="built_in">str</span>] = field(default=<span class="string">"facebook/opt-125m"</span>)</span><br><span class="line">    trust_remote_code: <span class="built_in">bool</span> = field(</span><br><span class="line">        default=<span class="literal">False</span>,</span><br><span class="line">        metadata={</span><br><span class="line">            <span class="string">"help"</span>: <span class="string">"Whether or not to allow for custom models defined on the Hub in their own modeling files"</span></span><br><span class="line">        },</span><br><span class="line">    )</span><br><span class="line">    padding_side: <span class="built_in">str</span> = field(</span><br><span class="line">        default=<span class="string">"right"</span>, metadata={<span class="string">"help"</span>: <span class="string">"The padding side in tokenizer"</span>}</span><br><span class="line">    )</span><br></pre></td></tr></tbody></table></figure><h4 id="Explanation"><a href="#Explanation" class="headerlink" title="Explanation"></a>Explanation</h4><p><code>ModelArguments</code> 是一个数据类（<code>dataclass</code>），用于存储与模型相关的配置参数。<br><strong>属性：</strong></p><ol><li><code>model_name_or_path</code>: 指定预训练模型的名称或路径。</li><li><code>trust_remote_code</code>: 是否允许使用自定义模型，这些模型在 Hub 上有自己的模型文件。</li><li><code>padding_side</code>: 指定在分词器（<code>tokenizer</code>）中使用的填充方式，通常是左填充或右填充。</li></ol><details><summary> `@dataclass`装饰器的介绍，点击展开</summary>`@dataclass` 是一个装饰器，用于自动化生成特殊方法，如 `__init__()`、`__repr__()`、`__eq__()` 等，从而简化数据类的编写。这个装饰器是 Python 3.7 中引入的一部分，属于 `dataclasses` 模块。<p>当你在一个类定义前使用 <code>@dataclass</code> 装饰器时，Python 会自动为这个类添加一些由属性定义的特殊方法。这对于创建存储少量数据但不需要复杂方法的类非常有用。</p><p>具体来说，使用 <code>@dataclass</code> 时：</p><ol><li><p><strong>自动生成构造函数（<code>__init__</code> 方法）</strong>：Python 会根据类中定义的字段自动创建一个 <code>__init__</code> 方法，这样你就不需要手动编写这个方法来初始化类的实例了。</p></li><li><p><strong>自动生成 <code>__repr__</code> 方法</strong>：这使得打印类的实例时能够得到更具可读性的字符串表示，通常包含类名和其中的字段及其值。</p></li><li><p><strong>自动生成 <code>__eq__</code> 方法</strong>：这使得可以使用 <code>==</code> 操作符来比较两个类的实例，比较的是实例中字段的值。</p></li><li><p><strong>支持类型注解</strong>：在定义字段时，你可以使用类型注解，这不仅有助于代码清晰性，还可以通过一些工具进行类型检查。</p></li></ol><p>在<code>ModelArguments</code>类的例子中，<code>@dataclass</code>装饰器会为这个类生成上述的方法。这意味着你可以很方便地创建<code>ModelArguments</code>的实例，并在打印或比较这些实例时得到预期的行为。</p><p>例如，当你创建一个<code>ModelArguments</code>实例时：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">args = ModelArguments()</span><br></pre></td></tr></tbody></table></figure><p>这将调用自动生成的<code>__init__</code>方法，使用默认值”facebook/opt-125m”为<code>model_name_or_path</code>、<code>False</code>为<code>trust_remote_code</code>和”right”为<code>padding_side</code>。</p><p>当你打印这个实例：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(args)</span><br></pre></td></tr></tbody></table></figure><p>这将调用自动生成的<code>__repr__</code>方法，显示类实例的详细信息，如<code>ModelArguments(model_name_or_path="facebook/opt-125m", trust_remote_code=False, padding_side="right")</code>。</p><p>这样，<code>@dataclass</code>装饰器简化了类的创建过程，使得代码更加简洁和易于维护。</p><p>总的来说，<code>@dataclass</code> 装饰器是 Python 提供的一个便捷工具，用于快速创建主要用于存储数据的类。</p></details><h3 id="2-DataArguments-类"><a href="#2-DataArguments-类" class="headerlink" title="2. DataArguments 类"></a>2. DataArguments 类</h3><h4 id="Code-1"><a href="#Code-1" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DataArguments</span>:</span><br><span class="line">    data_path: <span class="built_in">str</span> = field(</span><br><span class="line">        default=<span class="literal">None</span>, metadata={<span class="string">"help"</span>: <span class="string">"Path to the training data."</span>}</span><br><span class="line">    )</span><br><span class="line">    eval_data_path: <span class="built_in">str</span> = field(</span><br><span class="line">        default=<span class="literal">None</span>, metadata={<span class="string">"help"</span>: <span class="string">"Path to the evaluation data."</span>}</span><br><span class="line">    )</span><br><span class="line">    lazy_preprocess: <span class="built_in">bool</span> = <span class="literal">False</span></span><br></pre></td></tr></tbody></table></figure><h4 id="Explanation-1"><a href="#Explanation-1" class="headerlink" title="Explanation"></a>Explanation</h4><p><strong>DataArguments 类</strong></p><ul><li><code>DataArguments</code> 也是一个数据类，用于存储数据相关的配置参数。</li><li>属性：<ul><li><code>data_path</code>: 训练数据的路径。</li><li><code>eval_data_path</code>: 评估数据的路径。</li><li><code>lazy_preprocess</code>: 是否在数据预处理时使用延迟加载，即在需要时才加载和处理数据。</li></ul></li></ul><h3 id="3-TrainingArguments-类"><a href="#3-TrainingArguments-类" class="headerlink" title="3. TrainingArguments 类"></a>3. TrainingArguments 类</h3><h4 id="Code-2"><a href="#Code-2" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TrainingArguments</span>(transformers.TrainingArguments):</span><br><span class="line">    cache_dir: <span class="type">Optional</span>[<span class="built_in">str</span>] = field(default=<span class="literal">None</span>)</span><br><span class="line">    optim: <span class="built_in">str</span> = field(default=<span class="string">"adamw_torch"</span>)</span><br><span class="line">    model_max_length: <span class="built_in">int</span> = field(</span><br><span class="line">        default=<span class="number">512</span>,</span><br><span class="line">        metadata={</span><br><span class="line">            <span class="string">"help"</span>: <span class="string">"Maximum sequence length. Sequences will be right padded (and possibly truncated)."</span></span><br><span class="line">        },</span><br><span class="line">    )</span><br></pre></td></tr></tbody></table></figure><h4 id="Explanation-2"><a href="#Explanation-2" class="headerlink" title="Explanation"></a>Explanation</h4><p><code>TrainingArguments</code> 类继承自 <code>transformers.TrainingArguments</code>。。</p><ol><li><p><strong>TrainingArguments 类</strong></p><ul><li><code>TrainingArguments</code> 是一个数据类，它通过继承 <code>transformers.TrainingArguments</code>，获得了处理训练参数的能力。</li><li>在 <code>TrainingArguments</code> 中定义的属性：<ul><li><code>cache_dir</code>: 用于指定模型和分词器缓存的目录路径。</li><li><code>optim</code>: 定义了要使用的优化器类型，例如 <code>'adamw_torch'</code>。</li><li><code>model_max_length</code>: 指定模型能处理的最大序列长度。</li></ul></li></ul></li><li><p><strong>transformers.TrainingArguments 类</strong></p><ul><li><code>transformers.TrainingArguments</code> 是 <code>transformers</code> 库中的一个类，用于配置模型训练过程中的各种参数。</li><li>这个类包含大量的属性，用于控制训练过程，例如：<ul><li><code>output_dir</code>: 指定保存模型和训练结果的目录。</li><li><code>num_train_epochs</code>: 训练的轮数（epochs）。</li><li><code>per_device_train_batch_size</code>: 每个设备上的训练批次大小。</li><li><code>save_steps</code>: 保存模型的步数间隔。</li><li><code>evaluation_strategy</code>: 评估模型的策略，如在每个 epoch 结束时进行评估。</li><li><code>learning_rate</code>: 学习率。</li><li><code>warmup_steps</code>: 在学习率调度中用于预热的步数。</li></ul></li><li><code>transformers.TrainingArguments</code> 还包含了许多其他参数，用于微调训练过程，包括日志记录、模型保存策略、学习率调度等。</li></ul></li></ol><p>通过继承 <code>transformers.TrainingArguments</code>，<code>TrainingArguments</code> 类不仅继承了所有这些训练参数的配置能力，而且还可以添加一些自定义的训练参数，如本例中的 <code>cache_dir</code>、<code>optim</code> 和 <code>model_max_length</code>。这种做法提高了代码的可复用性和灵活性，使得您可以根据项目的具体需求调整和扩展训练配置。</p><h2 id="3-功能型函数-Functional-Utility-Functions"><a href="#3-功能型函数-Functional-Utility-Functions" class="headerlink" title="3.功能型函数 (Functional Utility Functions)"></a>3.功能型函数 (Functional Utility Functions)</h2><h3 id="1-rank0-print-args"><a href="#1-rank0-print-args" class="headerlink" title="1. rank0_print(*args)"></a>1. rank0_print(*args)</h3><h4 id="Code-3"><a href="#Code-3" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">local_rank = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">rank0_print</span>(<span class="params">*args</span>):</span><br><span class="line">    <span class="keyword">if</span> local_rank == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(*args)</span><br></pre></td></tr></tbody></table></figure><h4 id="Explanation-3"><a href="#Explanation-3" class="headerlink" title="Explanation"></a>Explanation</h4><p>定义一个全局变量 local_rank，用于分布式训练。<br>定义一个函数 rank0_print，只在 local_rank 为 0 时打印信息，用于分布式训练中的信息输出控制。这样可以避免在多个节点上重复打印相同的信息，使得输出更加清晰和简洁。</p><ul><li>用于只在分布式训练环境中的主节点（rank 0）上打印信息。</li><li>参数：可变数量的参数，用于打印。</li></ul><h3 id="2-trainer-save-model-safe-trainer-transformers-Trainer"><a href="#2-trainer-save-model-safe-trainer-transformers-Trainer" class="headerlink" title="2. trainer_save_model_safe(trainer: transformers.Trainer)"></a>2. <code>trainer_save_model_safe(trainer: transformers.Trainer)</code></h3><h4 id="Code-4"><a href="#Code-4" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">trainer_save_model_safe</span>(<span class="params">trainer: transformers.Trainer</span>):</span><br><span class="line">    <span class="keyword">from</span> torch.distributed.fsdp <span class="keyword">import</span> FullyShardedDataParallel <span class="keyword">as</span> FSDP</span><br><span class="line">    <span class="keyword">from</span> torch.distributed.fsdp <span class="keyword">import</span> StateDictType, FullStateDictConfig</span><br><span class="line"></span><br><span class="line">    save_policy = FullStateDictConfig(offload_to_cpu=<span class="literal">True</span>, rank0_only=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">with</span> FSDP.state_dict_type(</span><br><span class="line">        trainer.model, StateDictType.FULL_STATE_DICT, save_policy</span><br><span class="line">    ):</span><br><span class="line">        trainer.save_model()</span><br></pre></td></tr></tbody></table></figure><p>函数 <code>trainer_save_model_safe(trainer: transformers.Trainer)</code> 旨在安全地保存使用 PyTorch 分布式框架训练的模型。让我们详细了解此函数及其涉及的关键组件。</p><h4 id="Explanation-4"><a href="#Explanation-4" class="headerlink" title="Explanation"></a>Explanation</h4><ol><li>参数：</li></ol><ul><li><code>trainer</code>: <code>transformers.Trainer</code> 的实例。这个类是 Hugging Face Transformers 库的核心组件之一，用于训练和评估模型。</li></ul><ol start="2"><li>功能：</li></ol><ul><li>此函数的主要目的是在分布式训练环境中安全地保存模型。它特别考虑了使用 <code>FullyShardedDataParallel</code> (FSDP) 进行训练时的模型保存策略。</li></ul><ol start="3"><li>FSDP</li></ol><ul><li><strong>FullyShardedDataParallel (FSDP)</strong><ul><li>这是 PyTorch 分布式训练框架的一个组件。FSDP 通过将模型参数分片到多个 GPU 上来减少每个 GPU 的内存占用，从而实现更大模型的训练。</li><li>在此场景中，FSDP 主要用于处理和保存分布式训练中的模型状态。</li></ul></li><li><strong>StateDictType</strong><ul><li>这是一个枚举类型，定义了如何保存模型的状态字典（state dict）。在 FSDP 环境中，保存和加载模型状态可能需要特殊的处理。</li></ul></li><li><strong>FullStateDictConfig</strong><ul><li>这个类用于配置保存完整状态字典时的参数。它是 FSDP 功能的一部分，用于控制如何保存模型状态。</li></ul></li></ul><ol start="4"><li>函数实现</li></ol><ul><li><strong>设置保存策略</strong><ul><li><code>save_policy = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)</code> 创建了一个保存策略。这里指定两个关键参数：<ul><li><code>offload_to_cpu</code>: 在保存状态字典之前，将模型参数卸载到 CPU，这有助于减少 GPU 内存的使用。</li><li><code>rank0_only</code>: 只在 rank 0（通常是主节点）上保存模型。在分布式训练中，这可以避免每个节点都保存相同的模型副本，节省存储空间。</li></ul></li></ul></li><li><strong>保存模型</strong><ul><li>使用 <code>with FSDP.state_dict_type(trainer.model, StateDictType.FULL_STATE_DICT, save_policy)</code> 上下文管理器设置模型保存的状态字典类型和策略。</li><li>在这个上下文内，调用 <code>trainer.save_model()</code> 来保存模型。由于使用了 <code>save_policy</code>，模型将根据上述配置安全地保存。</li></ul></li></ul><p>函数 <code>trainer_save_model_safe</code> 封装了一个安全的模型保存逻辑，特别是针对使用 PyTorch 的 FSDP 进行分布式训练的场景。它确保了只在一个节点上保存完整的模型状态，并且在保存之前将模型参数转移到 CPU，从而优化内存使用和存储效率。这对于训练大型模型和管理大规模分布式训练环境至关重要。</p><h3 id="3-preprocess-sources-tokenizer-transformers-PreTrainedTokenizer-gt-Dict"><a href="#3-preprocess-sources-tokenizer-transformers-PreTrainedTokenizer-gt-Dict" class="headerlink" title="3.preprocess(sources,tokenizer: transformers.PreTrainedTokenizer) -> Dict"></a>3.<code>preprocess(sources,tokenizer: transformers.PreTrainedTokenizer) -&gt; Dict</code></h3><h4 id="Code-5"><a href="#Code-5" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess</span>(<span class="params"></span></span><br><span class="line"><span class="params">    sources,</span></span><br><span class="line"><span class="params">    tokenizer: transformers.PreTrainedTokenizer,</span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="type">Dict</span>:</span><br><span class="line">    conv = get_conversation_template(<span class="string">"vicuna"</span>)</span><br><span class="line">    roles = {<span class="string">"human"</span>: conv.roles[<span class="number">0</span>], <span class="string">"gpt"</span>: conv.roles[<span class="number">1</span>]}</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Apply prompt templates</span></span><br><span class="line">    conversations = []</span><br><span class="line">    <span class="keyword">for</span> i, source <span class="keyword">in</span> <span class="built_in">enumerate</span>(sources):</span><br><span class="line">        <span class="keyword">if</span> roles[source[<span class="number">0</span>][<span class="string">"from"</span>]] != conv.roles[<span class="number">0</span>]:</span><br><span class="line">            <span class="comment"># Skip the first one if it is not from human</span></span><br><span class="line">            source = source[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">        conv.messages = []</span><br><span class="line">        <span class="keyword">for</span> j, sentence <span class="keyword">in</span> <span class="built_in">enumerate</span>(source):</span><br><span class="line">            role = roles[sentence[<span class="string">"from"</span>]]</span><br><span class="line">            <span class="keyword">assert</span> role == conv.roles[j % <span class="number">2</span>], <span class="string">f"<span class="subst">{i}</span>"</span></span><br><span class="line">            conv.append_message(role, sentence[<span class="string">"value"</span>])</span><br><span class="line">        conversations.append(conv.get_prompt())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Tokenize conversations</span></span><br><span class="line">    input_ids = tokenizer(</span><br><span class="line">        conversations,</span><br><span class="line">        return_tensors=<span class="string">"pt"</span>,</span><br><span class="line">        padding=<span class="string">"max_length"</span>,</span><br><span class="line">        max_length=tokenizer.model_max_length,</span><br><span class="line">        truncation=<span class="literal">True</span>,</span><br><span class="line">    ).input_ids</span><br><span class="line">    targets = input_ids.clone()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> conv.sep_style == SeparatorStyle.ADD_COLON_TWO</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Mask targets. Only compute loss on the assistant outputs.</span></span><br><span class="line">    sep = conv.sep + conv.roles[<span class="number">1</span>] + <span class="string">": "</span></span><br><span class="line">    <span class="keyword">for</span> conversation, target <span class="keyword">in</span> <span class="built_in">zip</span>(conversations, targets):</span><br><span class="line">        total_len = <span class="built_in">int</span>(target.ne(tokenizer.pad_token_id).<span class="built_in">sum</span>())</span><br><span class="line"></span><br><span class="line">        turns = conversation.split(conv.sep2)</span><br><span class="line">        cur_len = <span class="number">1</span></span><br><span class="line">        target[:cur_len] = IGNORE_TOKEN_ID</span><br><span class="line">        <span class="keyword">for</span> i, turn <span class="keyword">in</span> <span class="built_in">enumerate</span>(turns):</span><br><span class="line">            <span class="keyword">if</span> turn == <span class="string">""</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            turn_len = <span class="built_in">len</span>(tokenizer(turn).input_ids)</span><br><span class="line"></span><br><span class="line">            parts = turn.split(sep)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(parts) != <span class="number">2</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            parts[<span class="number">0</span>] += sep</span><br><span class="line">            <span class="comment"># "-2" is hardcoded for the Llama tokenizer to make the offset correct.</span></span><br><span class="line">            instruction_len = <span class="built_in">len</span>(tokenizer(parts[<span class="number">0</span>]).input_ids) - <span class="number">2</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i != <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> tokenizer.legacy:</span><br><span class="line">                <span class="comment"># The legacy and non-legacy modes handle special tokens differently</span></span><br><span class="line">                instruction_len -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Ignore the user instructions</span></span><br><span class="line">            target[cur_len : cur_len + instruction_len] = IGNORE_TOKEN_ID</span><br><span class="line">            cur_len += turn_len</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i != <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> tokenizer.legacy:</span><br><span class="line">                <span class="comment"># The legacy and non-legacy modes handle special tokens differently</span></span><br><span class="line">                cur_len -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        target[cur_len:] = IGNORE_TOKEN_ID</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="literal">False</span>:  <span class="comment"># Inspect and check the correctness of masking</span></span><br><span class="line">            z = target.clone()</span><br><span class="line">            z = torch.where(z == IGNORE_TOKEN_ID, tokenizer.unk_token_id, z)</span><br><span class="line">            rank0_print(tokenizer.decode(z))</span><br><span class="line">            exit()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> cur_len &lt; tokenizer.model_max_length:</span><br><span class="line">            <span class="keyword">if</span> cur_len != total_len:</span><br><span class="line">                target[:] = IGNORE_TOKEN_ID</span><br><span class="line">                rank0_print(</span><br><span class="line">                    <span class="string">f"WARNING: tokenization mismatch: <span class="subst">{cur_len}</span> vs. <span class="subst">{total_len}</span>."</span></span><br><span class="line">                    <span class="string">f" #turn = <span class="subst">{<span class="built_in">len</span>(turns) - <span class="number">1</span>}</span>. (ignored)"</span></span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">dict</span>(</span><br><span class="line">        input_ids=input_ids,</span><br><span class="line">        labels=targets,</span><br><span class="line">        attention_mask=input_ids.ne(tokenizer.pad_token_id),</span><br><span class="line">    )</span><br></pre></td></tr></tbody></table></figure><p>函数 <code>preprocess(sources, tokenizer: transformers.PreTrainedTokenizer) -&gt; Dict</code> 用于预处理对话数据，使其适用于机器学习模型的训练。这个函数可以分为几个主要部分进行详细介绍：</p><h4 id="1-获取对话模板和角色定义"><a href="#1-获取对话模板和角色定义" class="headerlink" title="1. 获取对话模板和角色定义"></a>1. 获取对话模板和角色定义</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conv = get_conversation_template(<span class="string">"vicuna"</span>)</span><br><span class="line">roles = {<span class="string">"human"</span>: conv.roles[<span class="number">0</span>], <span class="string">"gpt"</span>: conv.roles[<span class="number">1</span>]}</span><br></pre></td></tr></tbody></table></figure><ul><li><strong>功能</strong>: 初始化对话模板和定义对话参与者的角色。</li><li><strong>实现</strong>:<ul><li><code>conv = get_conversation_template("vicuna")</code> 获取指定模型（如 “vicuna”）的对话模板。</li><li><code>roles</code> 字典将 “human” 和 “gpt” 分别映射到对话模板中定义的角色。</li></ul></li><li><strong>示例</strong>:<ul><li>如果对话模板是 “vicuna”，则 <code>roles</code> 可能是 <code>{"human": "user", "gpt": "assistant"}</code>。<ul><li><code>conv = get_conversation_template("vicuna")</code> 得到的模板如下：<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Conversation(name=<span class="string">'vicuna_v1.1'</span>, system_template=<span class="string">'{system_message}'</span>, system_message=<span class="string">"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."</span>, roles=(<span class="string">'USER'</span>, <span class="string">'ASSISTANT'</span>), messages=[], offset=<span class="number">0</span>, sep_style=&lt;SeparatorStyle.ADD_COLON_TWO: <span class="number">2</span>&gt;, sep=<span class="string">' '</span>, sep2=<span class="string">'&lt;/s&gt;'</span>, stop_str=<span class="literal">None</span>, stop_token_ids=<span class="literal">None</span>)</span><br></pre></td></tr></tbody></table></figure></li><li><code>roles</code> 将 “human” 映射到 “USER”，将 “gpt” 映射到 “ASSISTANT”。<code>{'human': 'USER', 'gpt': 'ASSISTANT'}</code></li></ul></li></ul></li></ul><h4 id="2-prompt-模板"><a href="#2-prompt-模板" class="headerlink" title="2. prompt 模板"></a>2. prompt 模板</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Apply prompt templates</span></span><br><span class="line">conversations = []</span><br><span class="line"><span class="keyword">for</span> i, source <span class="keyword">in</span> <span class="built_in">enumerate</span>(sources):</span><br><span class="line">    <span class="keyword">if</span> roles[source[<span class="number">0</span>][<span class="string">"from"</span>]] != conv.roles[<span class="number">0</span>]:</span><br><span class="line">        <span class="comment"># Skip the first one if it is not from human</span></span><br><span class="line">        source = source[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">    conv.messages = []</span><br><span class="line">    <span class="keyword">for</span> j, sentence <span class="keyword">in</span> <span class="built_in">enumerate</span>(source):</span><br><span class="line">        role = roles[sentence[<span class="string">"from"</span>]]</span><br><span class="line">        <span class="keyword">assert</span> role == conv.roles[j % <span class="number">2</span>], <span class="string">f"<span class="subst">{i}</span>"</span></span><br><span class="line">        conv.append_message(role, sentence[<span class="string">"value"</span>])</span><br><span class="line">    conversations.append(conv.get_prompt())</span><br></pre></td></tr></tbody></table></figure><ul><li><p><strong>功能</strong>: 为源数据应用提示模板，构建对话。</p></li><li><p><strong>实现</strong>:</p><ul><li>遍历 <code>sources</code>（原始对话数据），将每个对话源转换为模板格式的对话。</li><li>如果对话的第一部分不是 “human” 角色发起，则跳过该部分。</li><li>为每个句子指定角色，并将其添加到对话模板中。</li><li>最终，每个处理后的对话被添加到 <code>conversations</code> 列表中。</li></ul></li><li><p><strong>示例</strong>:</p><ul><li>假如我们的 source 是 dummy input 中的第一条数据:<br><code>python source = [{'from': 'human', 'value': 'Who are you?'}, {'from': 'gpt', 'value': 'I am Vicuna, a language model trained by researchers from Large Model Systems Organization (LMSYS).'}, {'from': 'human', 'value': 'Have a nice day!'}, {'from': 'gpt', 'value': 'You too!'}]</code></li><li><code>conversations</code> 在 Vicuna template 下,我们会使用<code>SeparatorStyle.ADD_COLON_TWO</code>作为分隔符风格，构成的数据可能是 [“A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user’s questions. USER: Who are you? ASSISTANT: I am Vicuna, a language model trained by researchers from Large Model Systems Organization (LMSYS).USER: Have a nice day! ASSISTANT: You too!“]</li><li><details><summary> get_prompt的实现 </summary>`get_prompt` 方法的实现根据不同的 `SeparatorStyle` 有着不同的行为。下面是一个表格，详细介绍了各种风格的 `get_prompt` 方法，以及对应的英文示例：<table><thead><tr><th>分隔符风格 (<code>SeparatorStyle</code>)</th><th>描述</th><th>示例</th></tr></thead><tbody><tr><td><code>ADD_COLON_SINGLE</code></td><td>在每个消息后加冒号和分隔符。</td><td>USER: Hello there!\nASSISTANT: Hi, how can I help?\n</td></tr><tr><td><code>ADD_COLON_TWO</code></td><td>使用两种分隔符交替，通常在不同角色之间切换。</td><td>USER: What’s the weather?\nASSISTANT: It’s sunny today.\n\n</td></tr><tr><td><code>ADD_COLON_SPACE_SINGLE</code></td><td>消息后加冒号、空格和分隔符。</td><td>USER: Can you book a flight?\nASSISTANT: Sure, where to?\n</td></tr><tr><td><code>NO_COLON_SINGLE</code></td><td>消息直接跟在角色后，不加冒号，后接分隔符。</td><td>USERWhat are you doing?\nASSISTANTI’m here to assist you.\n</td></tr><tr><td><code>NO_COLON_TWO</code></td><td>无冒号，使用两种分隔符交替。</td><td>USERHow’s the project going?\nASSISTANTIt’s on track.\n\n</td></tr><tr><td><code>ADD_NEW_LINE_SINGLE</code></td><td>每条消息前换行，消息后加分隔符。</td><td>USER\nHow can I reset my password?\nASSISTANT\nYou can reset it via email.\n</td></tr><tr><td><code>RWKV</code></td><td>特殊格式，通常用于特定模型。</td><td>USER: What is AI?\n\nASSISTANT: AI stands for Artificial Intelligence.\n\n</td></tr><tr><td><code>LLAMA2</code></td><td>特殊标签格式，针对特定模型。</td><td>[INST] USER How does blockchain work?\nASSISTANT It is a distributed ledger.\n\n</td></tr><tr><td><code>CHATGLM</code></td><td>特定于 <code>CHATGLM</code> 模型的格式。</td><td>[Round 1]\nUSER: Tell me a joke.\nASSISTANT: Why did the chicken cross the road?\n</td></tr><tr><td><code>CHATML</code></td><td>类似 <code>CHATGLM</code>，但每条消息前后都有换行。</td><td>USER\nDo you like music?\n\nASSISTANT\nYes, I enjoy many genres.\n\n</td></tr><tr><td><code>CHATGLM3</code></td><td>适用于 <code>CHATGLM3</code> 模型的格式。</td><td>USER\nCan you play chess?\nASSISTANTYes, I can play.\n</td></tr><tr><td><code>CHATINTERN</code></td><td>适用于 <code>CHATINTERN</code> 模型的格式，使用特殊标记。</td><td><s>USER:Where is the nearest ATM?<s>\nASSISTANT:It’s next to the post office.\n</s></s></td></tr><tr><td><code>DOLLY</code></td><td>特定于 <code>DOLLY</code> 模型的格式。</td><td>USER:\nWhat is quantum computing?\nASSISTANT:\nIt involves computation using quantum-mechanical phenomena.\n\n</td></tr><tr><td><code>PHOENIX</code></td><td>适用于 <code>PHOENIX</code> 模型，消息被特殊标记包裹。</td><td>USER: <s>How to bake a cake?</s>\nASSISTANT: <s>You need flour, sugar, and eggs.</s>\n</td></tr><tr><td><code>ROBIN</code></td><td>类似 <code>ADD_NEW_LINE_SINGLE</code>，但角色后有换行。</td><td>USER:\nIs AI dangerous?\nASSISTANT:\nIt depends on how it’s used.\n</td></tr><tr><td><code>FALCON_CHAT</code></td><td>类似 <code>ADD_COLON_SINGLE</code>，但可适用于 <code>FALCON</code> 模型。</td><td>USER: What is the capital of France?\nASSISTANT: It’s Paris.\n</td></tr><tr><td><code>METAMATH</code></td><td>对话中使用特殊前缀和后缀，适用于 <code>METAMATH</code> 模型。</td><td>USER:\nWhat is 2+2?\n: It’s 4\n</td></tr><tr><td><code>DEEPSEEK_CHAT</code></td><td>适用于 <code>DEEPSEEK</code> 模型的特定格式。</td><td>USER: What’s your favorite color?\nASSISTANT: I like blue.\n\n</td></tr><tr><td><code>YUAN2</code></td><td>适用于 <code>YUAN2</code> 模型，特殊的分隔符应用。</td><td>How are you today?<n>I’m fine, thank you!<n></n></n></td></tr></tbody></table>每种风格都有其特定的格式，这在处理与不同模型或任务相关的对话数据时非常重要。通过 <code>get_prompt</code> 方法的不同实现，可以灵活地适应各种需求，使对话生成或处理更加准确和高效。</details></li></ul></li></ul><h4 id="3-对话的分词"><a href="#3-对话的分词" class="headerlink" title="3. 对话的分词"></a>3. 对话的分词</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Tokenize conversations</span></span><br><span class="line">input_ids = tokenizer(</span><br><span class="line">    conversations,</span><br><span class="line">    return_tensors=<span class="string">"pt"</span>,</span><br><span class="line">    padding=<span class="string">"max_length"</span>,</span><br><span class="line">    max_length=tokenizer.model_max_length,</span><br><span class="line">    truncation=<span class="literal">True</span>,</span><br><span class="line">).input_ids</span><br><span class="line">targets = input_ids.clone()</span><br></pre></td></tr></tbody></table></figure><ul><li><p><strong>功能</strong>: 文本对话首先被分词处理，转换成模型能够处理的数值序列。然后，这些序列被克隆以形成初始的训练目标。这样做的目的是为了在训练过程中提供一个基准，指导模型学习生成正确的输出。在后续步骤中，这些目标可能会根据特定的训练目标进行调整。</p></li><li><p><strong>实现</strong>：</p><ul><li><code>tokenizer</code> 函数接收文本列表（这里是 <code>conversations</code>），并返回一个包含数值化表示的 <code>input_ids</code>。<ul><li><code>return_tensors="pt"</code> 指定返回的数据类型为 PyTorch 张量。</li><li><code>padding="max_length"</code> 和 <code>max_length=tokenizer.model_max_length</code> 确保所有输入长度统一，不足的部分使用填充。</li><li><code>truncation=True</code> 表示如果输入过长，将其截断到最大长度。</li></ul></li><li>在训练期间，模型需要知道期望的输出以计算损失和进行反向传播。这些期望的输出被称为 “targets”。<code>targets = input_ids.clone()</code> 表示创建 <code>input_ids</code> 的一个副本作为初始的目标。<ul><li>之所以需要克隆 <code>input_ids</code>，是因为在许多语言模型训练任务中（特别是像自回归模型这样的生成任务），模型的目标输出往往与输入非常相似，但在某些细节上存在差异。</li><li>在后续步骤中，这个 <code>targets</code> 可能会根据特定的训练需求进一步修改或掩码（例如，在对话任务中，可能只对模型生成的回复部分计算损失，而不是整个对话）。</li></ul></li></ul></li></ul><h4 id="4-目标掩码"><a href="#4-目标掩码" class="headerlink" title="4. 目标掩码"></a>4. 目标掩码</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Mask targets. Only compute loss on the assistant outputs.</span></span><br><span class="line">sep = conv.sep + conv.roles[<span class="number">1</span>] + <span class="string">": "</span></span><br><span class="line"><span class="keyword">for</span> conversation, target <span class="keyword">in</span> <span class="built_in">zip</span>(conversations, targets):</span><br><span class="line">    total_len = <span class="built_in">int</span>(target.ne(tokenizer.pad_token_id).<span class="built_in">sum</span>())</span><br><span class="line"></span><br><span class="line">    turns = conversation.split(conv.sep2)</span><br><span class="line">    cur_len = <span class="number">1</span></span><br><span class="line">    target[:cur_len] = IGNORE_TOKEN_ID</span><br><span class="line">    <span class="keyword">for</span> i, turn <span class="keyword">in</span> <span class="built_in">enumerate</span>(turns):</span><br><span class="line">        <span class="keyword">if</span> turn == <span class="string">""</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        turn_len = <span class="built_in">len</span>(tokenizer(turn).input_ids)</span><br><span class="line"></span><br><span class="line">        parts = turn.split(sep)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(parts) != <span class="number">2</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        parts[<span class="number">0</span>] += sep</span><br><span class="line">        <span class="comment"># "-2" is hardcoded for the Llama tokenizer to make the offset correct.</span></span><br><span class="line">        instruction_len = <span class="built_in">len</span>(tokenizer(parts[<span class="number">0</span>]).input_ids) - <span class="number">2</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> i != <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> tokenizer.legacy:</span><br><span class="line">            <span class="comment"># The legacy and non-legacy modes handle special tokens differently</span></span><br><span class="line">            instruction_len -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Ignore the user instructions</span></span><br><span class="line">        target[cur_len : cur_len + instruction_len] = IGNORE_TOKEN_ID</span><br><span class="line">        cur_len += turn_len</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> i != <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> tokenizer.legacy:</span><br><span class="line">            <span class="comment"># The legacy and non-legacy modes handle special tokens differently</span></span><br><span class="line">            cur_len -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    target[cur_len:] = IGNORE_TOKEN_ID</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="literal">False</span>:  <span class="comment"># Inspect and check the correctness of masking</span></span><br><span class="line">        z = target.clone()</span><br><span class="line">        z = torch.where(z == IGNORE_TOKEN_ID, tokenizer.unk_token_id, z)</span><br><span class="line">        rank0_print(tokenizer.decode(z))</span><br><span class="line">        exit()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> cur_len &lt; tokenizer.model_max_length:</span><br><span class="line">        <span class="keyword">if</span> cur_len != total_len:</span><br><span class="line">            target[:] = IGNORE_TOKEN_ID</span><br><span class="line">            rank0_print(</span><br><span class="line">                <span class="string">f"WARNING: tokenization mismatch: <span class="subst">{cur_len}</span> vs. <span class="subst">{total_len}</span>."</span></span><br><span class="line">                <span class="string">f" #turn = <span class="subst">{<span class="built_in">len</span>(turns) - <span class="number">1</span>}</span>. (ignored)"</span></span><br><span class="line">            )</span><br></pre></td></tr></tbody></table></figure><ul><li><p><strong>功能</strong>: 对目标输出进行掩码处理，以便模型只对特定输出计算损失。目标是对生成的 targets（即模型的输出标签）进行掩码处理。这是为了确保在训练过程中只对助手（assistant）的输出计算损失，而不是整个对话。</p></li><li><p><strong>实现</strong>：</p><ul><li><code>sep = conv.sep + conv.roles[1] + ": "</code> 定义了用于识别助手回复的分隔符。在这个例子中，<code>sep</code> 可能是 “\n\nAssistant: “。</li><li>循环遍历每个处理后的对话 (<code>conversation</code>) 及其对应的目标 (<code>target</code>)。</li><li><code>total_len</code> 是当前目标序列中非填充（padding）部分的长度。</li><li><code>turns</code> 是将对话根据 <code>conv.sep2</code> 分隔成不同轮次的列表。</li></ul></li></ul><ol><li>对每个轮次进行处理</li></ol><ul><li>每个轮次（turn）包含用户和助手的消息。</li><li>使用 <code>tokenizer(turn)</code> 将每个轮次的文本转换为模型能理解的 ID 序列。</li><li>通过 <code>parts = turn.split(sep)</code> 分离用户和助手的消息。</li><li><code>instruction_len</code> 是用户消息部分的长度（在某些情况下需要调整，比如 <code>-2</code> 是为了适应特定的分词器）。</li></ul><ol start="2"><li>掩码目标</li></ol><ul><li><code>target[cur_len : cur_len + instruction_len] = IGNORE_TOKEN_ID</code> 将用户消息部分的目标 ID 替换为 <code>IGNORE_TOKEN_ID</code>，这意味着在计算损失时会忽略这部分。</li><li><code>cur_len</code> 用于跟踪当前处理到的位置。</li><li>每处理完一个轮次，更新 <code>cur_len</code>。</li></ul><ol start="3"><li>最终处理</li></ol><ul><li><code>target[cur_len:] = IGNORE_TOKEN_ID</code> 确保在最后一个轮次之后的所有内容都被忽略。</li><li>如果 <code>cur_len</code> 小于 <code>tokenizer.model_max_length</code>，但不等于 <code>total_len</code>，则表示有不一致性，此时会发出警告，并将整个目标序列设置为 <code>IGNORE_TOKEN_ID</code>。</li></ul><h4 id="5-返回处理后的数据"><a href="#5-返回处理后的数据" class="headerlink" title="5. 返回处理后的数据"></a>5. 返回处理后的数据</h4><ul><li><strong>功能</strong>: 返回预处理后的数据，包括输入 ID、目标标签和注意力掩码。</li><li><strong>实现</strong>:<ul><li>返回一个字典，包含 <code>input_ids</code>（模型输入）、<code>labels</code>（训练目标）和 <code>attention_mask</code>（指示哪些部分是有效输入的掩码）。</li></ul></li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>这个 <code>preprocess</code> 函数通过将原始文本数据转换为模型可以理解的格式，为训练准备数据。它涵盖了从文本处理到分词，再到目标掩码的整个预处理流程。这个过程对于任何基于对话的自然语言处理任务至关重要，特别是在需要模型专注于对特定部分的响应时。</p><h2 id="4-数据集类"><a href="#4-数据集类" class="headerlink" title="4. 数据集类"></a>4. 数据集类</h2><p>这些类继承自 PyTorch 的 <code>Dataset</code> 类，并且是为特定的数据处理任务定制的。这些类包含具体的方法来处理和准备数据，以便用于模型训练。这些类包括：</p><ul><li><code>SupervisedDataset</code>: 用于有监督学习的数据集。它处理原始数据，将其转换为适合模型训练的格式。</li><li><code>LazySupervisedDataset</code>: 类似于 <code>SupervisedDataset</code>，但使用懒加载方式处理数据。这意味着数据只在需要时才被加载和处理，这对于处理大型数据集特别有用。</li></ul><p>这些类通常包含 <code>__init__</code>, <code>__len__</code>, 和 <code>__getitem__</code> 方法，分别用于初始化数据集、获取数据集大小和检索特定索引的数据。这样的设计模式使得数据集可以轻松地与 PyTorch 的 DataLoader 配合使用，从而实现高效的数据加载和批处理。</p><h3 id="1-SupervisedDataset-类"><a href="#1-SupervisedDataset-类" class="headerlink" title="1. SupervisedDataset 类"></a>1. SupervisedDataset 类</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SupervisedDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="string">"""Dataset for supervised fine-tuning."""</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, raw_data, tokenizer: transformers.PreTrainedTokenizer</span>):</span><br><span class="line">        <span class="built_in">super</span>(SupervisedDataset, self).__init__()</span><br><span class="line"></span><br><span class="line">        rank0_print(<span class="string">"Formatting inputs..."</span>)</span><br><span class="line">        sources = [example[<span class="string">"conversations"</span>] <span class="keyword">for</span> example <span class="keyword">in</span> raw_data]</span><br><span class="line">        data_dict = preprocess(sources, tokenizer)</span><br><span class="line"></span><br><span class="line">        self.input_ids = data_dict[<span class="string">"input_ids"</span>]</span><br><span class="line">        self.labels = data_dict[<span class="string">"labels"</span>]</span><br><span class="line">        self.attention_mask = data_dict[<span class="string">"attention_mask"</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.input_ids)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, i</span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, torch.Tensor]:</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">dict</span>(</span><br><span class="line">            input_ids=self.input_ids[i],</span><br><span class="line">            labels=self.labels[i],</span><br><span class="line">            attention_mask=self.attention_mask[i],</span><br><span class="line">        )</span><br></pre></td></tr></tbody></table></figure><p><code>SupervisedDataset</code> 类是一个用于有监督学习的数据集类，特别是为了微调（fine-tuning）任务设计。这个类继承自 PyTorch 的 <code>Dataset</code> 类，并重写了其方法以适应特定的数据处理需求。下面是对这个类的详细介绍：<code>SupervisedDataset</code> 类提供了一种结构化和高效的方法来处理和加载用于有监督学习的对话数据。它遵循 PyTorch 数据集（<code>Dataset</code>）的标准结构，使得与 PyTorch 的数据加载器（<code>DataLoader</code>）等其他组件兼容，从而方便在训练循环中使用。通过预处理步骤，该类确保数据以适当的格式提供给模型，以便进行有效的训练。</p><ul><li><strong>类名</strong>: <code>SupervisedDataset</code></li><li><strong>继承</strong>: <code>Dataset</code>（来自 PyTorch）</li><li><strong>目的</strong>: 用于有监督的模型微调任务。</li></ul><h4 id="1-1-init-self-raw-data-tokenizer-transformers-PreTrainedTokenizer"><a href="#1-1-init-self-raw-data-tokenizer-transformers-PreTrainedTokenizer" class="headerlink" title="1.1 __init__(self, raw_data, tokenizer: transformers.PreTrainedTokenizer)"></a>1.1 <code>__init__(self, raw_data, tokenizer: transformers.PreTrainedTokenizer)</code></h4><ul><li><strong>调用时机</strong>：创建 <code>SupervisedDataset</code> 类的实例时。这通常发生在准备训练数据集的阶段，当你创建数据加载器（DataLoader）之前。</li><li><strong>功能</strong>：初始化数据集实例，处理原始对话数据，并将其转换为模型可以理解的格式。</li><li><strong>参数</strong>：<ul><li><code>raw_data</code>：包含对话数据的列表或类似结构。</li><li><code>tokenizer</code>：一个预训练的分词器实例，用于将文本转换为模型可以处理的格式。</li></ul></li><li><strong>返回值</strong>：无返回值，但此方法会设置 <code>input_ids</code>、<code>labels</code> 和 <code>attention_mask</code> 作为类的内部状态。</li><li><strong>实现细节</strong>:<ul><li>使用列表推导式从 <code>raw_data</code> 中提取每个样本的对话内容。</li><li>调用 <code>preprocess</code> 函数处理这些对话，将其转换为适合模型输入的格式。</li><li>从返回的 <code>data_dict</code> 中提取 <code>input_ids</code>（模型输入 ID）、<code>labels</code>（目标标签）和 <code>attention_mask</code>（注意力掩码）。</li></ul></li></ul><h4 id="1-2-len-self"><a href="#1-2-len-self" class="headerlink" title="1.2 __len__(self)"></a>1.2 <code>__len__(self)</code></h4><ul><li><strong>调用时机</strong>：当需要获取数据集大小时，例如在设置数据加载器时，或者在训练循环中迭代数据集时。</li><li><strong>功能</strong>：返回数据集中的样本数量。</li><li><strong>返回值</strong>：一个整数，表示数据集中的样本数量。</li><li><strong>实现</strong>: 直接返回 <code>input_ids</code> 的长度，即样本的数量。</li></ul><h4 id="1-3-getitem-self-i"><a href="#1-3-getitem-self-i" class="headerlink" title="1.3 __getitem__(self, i)"></a>1.3 <code>__getitem__(self, i)</code></h4><ul><li><strong>调用时机</strong>：在数据加载器请求数据集的特定样本时，这通常发生在训练或评估循环的每个迭代中。</li><li><strong>功能</strong>：获取指定索引 <code>i</code> 处的数据样本。</li><li><strong>参数</strong>：<ul><li><code>i</code>：所请求样本的索引。</li></ul></li><li><strong>返回值</strong>：一个字典，包含索引 <code>i</code> 处样本的 <code>input_ids</code>、<code>labels</code> 和 <code>attention_mask</code>。这些是 PyTorch 张量（<code>torch.Tensor</code>），适用于模型的训练或评估。</li></ul><p>在有监督学习的场景中，<code>SupervisedDataset</code> 类扮演着数据预处理和封装的角色，确保数据以正确的格式提供给模型。<code>__init__</code> 方法在数据集实例化时调用，负责数据的初始化和预处理。<code>__len__</code> 和 <code>__getitem__</code> 方法则在训练和评估过程中被频繁调用，分别用于获取数据集的大小和提取特定的数据样本。这些方法的设计和实现使得 <code>SupervisedDataset</code> 类可以无缝地与 PyTorch 的其他数据处理和训练工具集成。</p><h3 id="2-LazySupervisedDataset-类"><a href="#2-LazySupervisedDataset-类" class="headerlink" title="2. LazySupervisedDataset 类"></a>2. LazySupervisedDataset 类</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LazySupervisedDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="string">"""Dataset for supervised fine-tuning."""</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, raw_data, tokenizer: transformers.PreTrainedTokenizer</span>):</span><br><span class="line">        <span class="built_in">super</span>(LazySupervisedDataset, self).__init__()</span><br><span class="line">        self.tokenizer = tokenizer</span><br><span class="line"></span><br><span class="line">        rank0_print(<span class="string">"Formatting inputs...Skip in lazy mode"</span>)</span><br><span class="line">        self.tokenizer = tokenizer</span><br><span class="line">        self.raw_data = raw_data</span><br><span class="line">        self.cached_data_dict = {}</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.raw_data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, i</span>) -&gt; <span class="type">Dict</span>[<span class="built_in">str</span>, torch.Tensor]:</span><br><span class="line">        <span class="keyword">if</span> i <span class="keyword">in</span> self.cached_data_dict:</span><br><span class="line">            <span class="keyword">return</span> self.cached_data_dict[i]</span><br><span class="line"></span><br><span class="line">        ret = preprocess([self.raw_data[i][<span class="string">"conversations"</span>]], self.tokenizer)</span><br><span class="line">        ret = <span class="built_in">dict</span>(</span><br><span class="line">            input_ids=ret[<span class="string">"input_ids"</span>][<span class="number">0</span>],</span><br><span class="line">            labels=ret[<span class="string">"labels"</span>][<span class="number">0</span>],</span><br><span class="line">            attention_mask=ret[<span class="string">"attention_mask"</span>][<span class="number">0</span>],</span><br><span class="line">        )</span><br><span class="line">        self.cached_data_dict[i] = ret</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ret</span><br></pre></td></tr></tbody></table></figure><p><code>LazySupervisedDataset</code> 类是另一种数据集实现，用于有监督的模型微调。与 <code>SupervisedDataset</code> 相比，它采用了一种“懒加载”（lazy loading）的策略。以下是对该类的详细解释，以及它与非懒加载版本的比较。</p><h4 id="2-1-init-self-raw-data-tokenizer-transformers-PreTrainedTokenizer"><a href="#2-1-init-self-raw-data-tokenizer-transformers-PreTrainedTokenizer" class="headerlink" title="2.1 __init__(self, raw_data, tokenizer: transformers.PreTrainedTokenizer)"></a>2.1 <code>__init__(self, raw_data, tokenizer: transformers.PreTrainedTokenizer)</code></h4><ul><li><strong>作用</strong>：初始化 <code>LazySupervisedDataset</code> 实例。</li><li><strong>实现细节</strong>：<ul><li>将原始数据 (<code>raw_data</code>) 和分词器 (<code>tokenizer</code>) 保存为类的属性。</li><li>初始化一个空字典 <code>cached_data_dict</code>，用于缓存已处理的数据。</li></ul></li><li><strong>与 <code>SupervisedDataset</code> 的差异</strong>：<ul><li>在 <code>LazySupervisedDataset</code> 中，原始数据不是在初始化时立即处理，而是存储原始形式以便稍后处理。</li><li><code>cached_data_dict</code> 用于缓存按需处理的数据，以避免重复处理。</li></ul></li></ul><h4 id="2-2-len-self"><a href="#2-2-len-self" class="headerlink" title="2.2 __len__(self)"></a>2.2 <code>__len__(self)</code></h4><ul><li><p><strong>作用</strong>：返回数据集中的样本数量。</p></li><li><p><strong>实现</strong>：直接返回原始数据 (<code>raw_data</code>) 的长度。</p></li><li><p><strong>与 <code>SupervisedDataset</code> 的差异</strong>：</p><ul><li><p>在 <code>LazySupervisedDataset</code> 类中，<code>__len__</code> 方法确实返回的是数据集中样本的数量，但是这里的“样本数量”是指原始数据 (<code>raw_data</code>) 中的样本数量，而不是处理后的数据的数量。由于 <code>LazySupervisedDataset</code> 采用懒加载策略，数据在初始化时并未被处理，因此 <code>__len__</code> 方法基于原始数据计算长度是合理的。</p></li><li><p>这意味着即便数据尚未被转换为模型可用的格式，<code>__len__</code> 方法仍能准确反映数据集中待处理样本的数量。这与 <code>SupervisedDataset</code> 的主要区别在于后者在初始化时就对所有数据进行预处理，因此其 <code>__len__</code> 方法返回的是已处理数据的数量。而在 <code>LazySupervisedDataset</code> 中，数据处理是按需进行的，因此 <code>__len__</code> 返回的是原始数据中的样本数。</p></li></ul></li></ul><h4 id="getitem-self-i"><a href="#getitem-self-i" class="headerlink" title="__getitem__(self, i)"></a><code>__getitem__(self, i)</code></h4><ul><li><strong>作用</strong>：按需获取并处理指定索引 <code>i</code> 处的数据样本。</li><li><strong>实现细节</strong>：<ul><li>首先检查索引 <code>i</code> 是否在缓存 <code>cached_data_dict</code> 中。</li><li>如果是，则直接返回缓存的数据；如果不是，则处理原始数据中索引 <code>i</code> 处的样本，并将处理后的结果添加到缓存中。</li><li>返回一个包含 <code>input_ids</code>、<code>labels</code> 和 <code>attention_mask</code> 的字典。</li></ul></li><li><strong>与 <code>SupervisedDataset</code> 的差异</strong>：<ul><li><code>LazySupervisedDataset</code> 在 <code>__getitem__</code> 被调用时才处理数据，而 <code>SupervisedDataset</code> 在初始化时就处理所有数据。</li><li><code>LazySupervisedDataset</code> 使用缓存来避免重复处理同一样本，而 <code>SupervisedDataset</code> 不需要这种机制，因为所有数据在初始化时就已经被处理。</li></ul></li></ul><h4 id="懒加载-vs-非懒加载"><a href="#懒加载-vs-非懒加载" class="headerlink" title="懒加载 vs 非懒加载"></a>懒加载 vs 非懒加载</h4><ul><li><strong>懒加载（Lazy Loading）</strong>：<ul><li><strong>优点</strong>：减少内存占用，因为只有需要时才处理数据。对于大型数据集非常有用。</li><li><strong>缺点</strong>：可能增加训练时的数据加载时间，尤其是当缓存未命中时。</li></ul></li><li><strong>非懒加载（Eager Loading）</strong>：<ul><li><strong>优点</strong>：在训练开始前一次性处理所有数据，可以减少训练过程中的延迟。</li><li><strong>缺点</strong>：需要更多的初始内存来存储处理后的所有数据，对于非常大的数据集可能不实用。</li></ul></li></ul><h3 id="3-make-supervised-data-module-函数"><a href="#3-make-supervised-data-module-函数" class="headerlink" title="3. make_supervised_data_module 函数"></a>3. <code>make_supervised_data_module</code> 函数</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">make_supervised_data_module</span>(<span class="params"></span></span><br><span class="line"><span class="params">    tokenizer: transformers.PreTrainedTokenizer, data_args</span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="type">Dict</span>:</span><br><span class="line">    <span class="string">"""Make dataset and collator for supervised fine-tuning."""</span></span><br><span class="line">    dataset_cls = (</span><br><span class="line">        LazySupervisedDataset <span class="keyword">if</span> data_args.lazy_preprocess <span class="keyword">else</span> SupervisedDataset</span><br><span class="line">    )</span><br><span class="line">    rank0_print(<span class="string">"Loading data..."</span>)</span><br><span class="line"></span><br><span class="line">    train_json = json.load(<span class="built_in">open</span>(data_args.data_path, <span class="string">"r"</span>))</span><br><span class="line">    train_dataset = dataset_cls(train_json, tokenizer=tokenizer)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> data_args.eval_data_path:</span><br><span class="line">        eval_json = json.load(<span class="built_in">open</span>(data_args.eval_data_path, <span class="string">"r"</span>))</span><br><span class="line">        eval_dataset = dataset_cls(eval_json, tokenizer=tokenizer)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        eval_dataset = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">dict</span>(train_dataset=train_dataset, eval_dataset=eval_dataset)</span><br></pre></td></tr></tbody></table></figure><p>函数 <code>make_supervised_data_module</code> 的目的是为有监督的模型微调创建数据集和数据整理器（collator）。这个函数根据提供的参数构建适合训练和评估的数据集。下面是对这个函数的超级详细解释：</p><h3 id="函数签名"><a href="#函数签名" class="headerlink" title="函数签名"></a>函数签名</h3><ul><li><strong>函数名</strong>：<code>make_supervised_data_module</code></li><li><strong>参数</strong>：<ul><li><code>tokenizer</code>: <code>transformers.PreTrainedTokenizer</code> 的实例，用于文本的分词处理。</li><li><code>data_args</code>: 包含数据相关设置的对象，通常包括数据文件路径等信息。</li></ul></li><li><strong>返回值</strong>：一个字典，包含训练和评估数据集。</li></ul><h3 id="函数实现细节"><a href="#函数实现细节" class="headerlink" title="函数实现细节"></a>函数实现细节</h3><h4 id="1-选择数据集类"><a href="#1-选择数据集类" class="headerlink" title="1. 选择数据集类"></a>1. 选择数据集类</h4><ul><li>根据 <code>data_args.lazy_preprocess</code> 的值选择使用 <code>LazySupervisedDataset</code> 还是 <code>SupervisedDataset</code> 类。<ul><li>如果 <code>data_args.lazy_preprocess</code> 为 <code>True</code>，则使用 <code>LazySupervisedDataset</code> 实现懒加载。</li><li>否则，使用 <code>SupervisedDataset</code> 进行预加载。</li></ul></li><li>这一选择影响数据的加载方式，即数据是一次性全部加载并预处理，还是按需加载和处理。</li></ul><h4 id="2-加载训练数据"><a href="#2-加载训练数据" class="headerlink" title="2. 加载训练数据"></a>2. 加载训练数据</h4><ul><li>使用 <code>json.load(open(data_args.data_path, "r"))</code> 加载训练数据。<ul><li>这里假设训练数据以 JSON 格式存储，并且 <code>data_args.data_path</code> 包含了数据文件的路径。</li></ul></li><li>创建训练数据集 <code>train_dataset</code> 实例，传入加载的训练数据和分词器。</li></ul><h4 id="3-加载评估数据（如果提供）"><a href="#3-加载评估数据（如果提供）" class="headerlink" title="3. 加载评估数据（如果提供）"></a>3. 加载评估数据（如果提供）</h4><ul><li>检查是否提供了评估数据路径 <code>data_args.eval_data_path</code>。<ul><li>如果提供，同样使用 <code>json.load</code> 加载评估数据。</li><li>创建评估数据集 <code>eval_dataset</code> 实例。</li></ul></li><li>如果没有提供评估数据路径，将 <code>eval_dataset</code> 设置为 <code>None</code>。</li></ul><h4 id="4-返回结果"><a href="#4-返回结果" class="headerlink" title="4. 返回结果"></a>4. 返回结果</h4><ul><li>返回一个字典，包含两个键：<code>train_dataset</code> 和 <code>eval_dataset</code>。<ul><li><code>train_dataset</code> 对应训练数据集实例。</li><li><code>eval_dataset</code> 对应评估数据集实例（如果有的话）。</li></ul></li></ul><h2 id="5-训练流程"><a href="#5-训练流程" class="headerlink" title="5. 训练流程"></a>5. 训练流程</h2><p>了解了，我将为 <code>train()</code> 函数中每行代码提供更详细的解释：</p><h3 id="解析命令行参数"><a href="#解析命令行参数" class="headerlink" title="解析命令行参数"></a>解析命令行参数</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">parser = transformers.HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))</span><br></pre></td></tr></tbody></table></figure><ul><li>创建一个 <code>HfArgumentParser</code> 实例，这是一个帮助解析命令行参数的工具，特别用于处理 Hugging Face transformers 库中的参数。</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model_args, data_args, training_args = parser.parse_args_into_dataclasses()</span><br></pre></td></tr></tbody></table></figure><ul><li>解析命令行参数并将它们映射到三个数据类 (<code>ModelArguments</code>, <code>DataArguments</code>, <code>TrainingArguments</code>) 的实例中。</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">local_rank = training_args.local_rank</span><br></pre></td></tr></tbody></table></figure><ul><li><code>local_rank</code> 用于标识分布式训练中的进程编号。<code>training_args.local_rank</code> 获取这个编号。</li></ul><h3 id="设置模型配置"><a href="#设置模型配置" class="headerlink" title="设置模型配置"></a>设置模型配置</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">config = transformers.AutoConfig.from_pretrained(</span><br><span class="line">    model_args.model_name_or_path,</span><br><span class="line">    cache_dir=training_args.cache_dir,</span><br><span class="line">    trust_remote_code=model_args.trust_remote_code,</span><br><span class="line">)</span><br></pre></td></tr></tbody></table></figure><ul><li>从预训练模型的配置创建 <code>AutoConfig</code> 实例。它自动加载与特定模型相关的配置。</li><li><code>model_args.model_name_or_path</code>: 指定模型的名称或路径。</li><li><code>cache_dir=training_args.cache_dir</code>: 指定缓存目录。</li><li><code>trust_remote_code=model_args.trust_remote_code</code>: 指定是否信任从远程下载的代码。</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">orig_ctx_len = <span class="built_in">getattr</span>(config, <span class="string">"max_position_embeddings"</span>, <span class="literal">None</span>)</span><br></pre></td></tr></tbody></table></figure><ul><li>使用 <code>getattr</code> 函数从配置中获取 <code>max_position_embeddings</code> 属性，该属性指示模型的最大位置嵌入数（即模型能处理的最大序列长度）。如果不存在该属性，则返回 <code>None</code>。<code>getattr</code> 是一个 Python 内置函数，用于获取对象的属性值。如果属性不存在，返回第三个参数指定的默认值（此处为 <code>None</code>）。</li><li><code>orig_ctx_len</code> 存储模型配置中的 <code>max_position_embeddings</code> 属性值，即模型可以处理的最大位置嵌入数（通常与最大序列长度相关）。</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> orig_ctx_len <span class="keyword">and</span> training_args.model_max_length &gt; orig_ctx_len:</span><br><span class="line">    scaling_factor = <span class="built_in">float</span>(math.ceil(training_args.model_max_length / orig_ctx_len))</span><br><span class="line">    config.rope_scaling = {<span class="string">"type"</span>: <span class="string">"linear"</span>, <span class="string">"factor"</span>: scaling_factor}</span><br></pre></td></tr></tbody></table></figure><ul><li>如果提供的模型最大长度 (<code>training_args.model_max_length</code>) 超过了原始模型的最大长度 (<code>orig_ctx_len</code>)，则计算一个缩放因子以进行位置编码的调整。这通常用于处理超出预训练模型原始设计的序列长度。</li><li><code>rope_scaling</code> 用于调整相对位置编码。</li><li><code>scaling_factor</code> 和 RoPE 缩放<ul><li>如果模型的最大长度超过原始配置的最大长度，<code>scaling_factor</code> 被用来计算缩放因子。</li><li>这涉及到 Rotary Positional Embedding（RoPE）的概念，即在位置嵌入中使用的技术，可以随序列长度线性缩放。</li><li>缩放因子用于调整位置嵌入，使其适应更长的序列。</li></ul></li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">config.use_cache = <span class="literal">False</span></span><br></pre></td></tr></tbody></table></figure><ul><li>禁用模型在前向传播时缓存中间计算结果的功能，这有助于减少内存消耗。这个设置告诉模型在前向传播时不使用或保存缓存。</li></ul><h3 id="加载模型和分词器"><a href="#加载模型和分词器" class="headerlink" title="加载模型和分词器"></a>加载模型和分词器</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = transformers.AutoModelForCausalLM.from_pretrained(</span><br><span class="line">    model_args.model_name_or_path,</span><br><span class="line">    config=config,</span><br><span class="line">    cache_dir=training_args.cache_dir,</span><br><span class="line">    trust_remote_code=model_args.trust_remote_code,</span><br><span class="line">)</span><br></pre></td></tr></tbody></table></figure><ul><li>加载预训练的因果语言模型（Causal Language Model）。这类模型通常用于生成任务。</li><li><code>trust_remote_code</code>这个参数用于确定是否信任从远程（如 Hugging Face Hub）加载的自定义模型代码。</li><li><code>cache_dir=training_args.cache_dir</code><ul><li>指定下载和缓存预训练模型和分词器的目录。</li><li>如果指定，模型和分词器将从这个目录加载，如果不存在，将从远程下载并缓存到此目录。</li></ul></li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tokenizer = transformers.AutoTokenizer.from_pretrained(</span><br><span class="line">    model_args.model_name_or_path,</span><br><span class="line">    cache_dir=training_args.cache_dir,</span><br><span class="line">    model_max_length=training_args.model_max_length,</span><br><span class="line">    padding_side=model_args.padding_side,</span><br><span class="line">    use_fast=<span class="literal">False</span>,</span><br><span class="line">    trust_remote_code=model_args.trust_remote_code,</span><br><span class="line">)</span><br></pre></td></tr></tbody></table></figure><ul><li>加载与模型对应的分词器。</li><li><code>use_fast=False</code>: 表示不使用快速分词器，快速分词器通常是基于 Rust 的分词器，提供更高效的分词处理。</li><li><code>padding_side=model_args.padding_side</code>: 指定填充（padding）应该发生在序列的哪一侧。</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> tokenizer.pad_token != tokenizer.unk_token:</span><br><span class="line">    tokenizer.pad_token = tokenizer.unk_token</span><br></pre></td></tr></tbody></table></figure><ul><li>将分词器的填充令牌设置为未知令牌（<code>unk_token</code>），如果它们不一致的话。这是因为某些模型需要在填充位置使用特定的令牌。</li></ul><h3 id="加载数据"><a href="#加载数据" class="headerlink" title="加载数据"></a>加载数据</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_module = make_supervised_data_module(tokenizer=tokenizer, data_args=data_args)</span><br></pre></td></tr></tbody></table></figure><ul><li>调用 <code>make_supervised_data_module</code> 函数，为训练和评估准备数据集。这个函数会根据 <code>data_args</code> 中的设置，选择使用懒加载或预加载的方式处理数据。</li></ul><h3 id="初始化并启动训练器"><a href="#初始化并启动训练器" class="headerlink" title="初始化并启动训练器"></a>初始化并启动训练器</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">trainer = Trainer(</span><br><span class="line">    model=model, tokenizer=tokenizer, args=training_args, **data_module</span><br><span class="line">)</span><br></pre></td></tr></tbody></table></figure><ul><li>初始化 <code>Trainer</code> 对象，传入模型、分词器、训练参数以及通过 <code>make_supervised_data_module</code> 函数准备好的数据。</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> <span class="built_in">list</span>(pathlib.Path(training_args.output_dir).glob(<span class="string">"checkpoint-*"</span>)):</span><br><span class="line">    trainer.train(resume_from_checkpoint=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    trainer.train()</span><br></pre></td></tr></tbody></table></figure><ul><li>检查是否存在训练检查点，如果存在，则从检查点恢复训练；如果不存在，开始新的训练过程。</li></ul><h3 id="保存模型"><a href="#保存模型" class="headerlink" title="保存模型"></a>保存模型</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model.config.use_cache = <span class="literal">True</span></span><br><span class="line">trainer.save_state()</span><br></pre></td></tr></tbody></table></figure><ul><li>启用模型的缓存并保存训练器的状态。</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> trainer.is_deepspeed_enabled:</span><br><span class="line">    trainer.save_model()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    trainer_save_model_safe(trainer)</span><br></pre></td></tr></tbody></table></figure><ul><li>检查是否启用了 DeepSpeed。如果启用了，则使用 <code>trainer.save_model()</code> 保存模型。如果没有启用 DeepSpeed，则使用 <code>trainer_save_model_safe</code> 安全地保存模型，特别是在使用分布式训练时。</li></ul><h3 id="Trainer-类解释"><a href="#Trainer-类解释" class="headerlink" title="Trainer 类解释"></a>Trainer 类解释</h3><p><code>Trainer</code> 是 Hugging Face Transformers 库提供的一个类，用于封装模型的训练逻辑。以下是对 <code>Trainer</code> 类的功能的详细介绍：</p><ul><li><p><strong>模型训练与评估</strong>：<code>Trainer</code> 类负责设置和执行模型的训练和评估过程。它自动处理数据的批处理、梯度计算、优化器步骤和设备管理等任务。</p></li><li><p><strong>参数</strong>：在初始化时，<code>Trainer</code> 接受多种参数，包括模型（<code>model</code>）、分词器（<code>tokenizer</code>）、训练参数（如学习率、批大小等，通过 <code>training_args</code> 传入）和数据集。</p></li><li><p><strong>灵活性和高级功能</strong>：<code>Trainer</code> 支持多种训练设置，如多 GPU 训练、混合精度训练和 TPU 训练。它还支持自定义回调函数，用于在训练过程中执行特定操作。</p></li><li><p><strong>简化 API</strong>：<code>Trainer</code> 类提供了一个简化的 API，使得用户可以用几行代码配置和运行模型训练。它抽象了许多底层细节，使得用户可以专注于模型的构建和训练策略。</p></li><li><p><strong>检查点和恢复</strong>：<code>Trainer</code> 支持保存和加载检查点，这意味着训练过程可以在中断后从上次保存的状态恢复。</p></li></ul><p>总体来说，<code>Trainer</code> 类是一个功能强大且灵活的工具，为训练复杂的 Transformer 模型提供了便利和高效性。</p><h2 id="6-run-bash"><a href="#6-run-bash" class="headerlink" title="6. run bash"></a>6. run bash</h2><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">torchrun --nproc_per_node=8 --master_port=20001 fastchat/train/train.py \</span><br><span class="line">    --model_name_or_path ~/vicuna-7b-v1.5-16k  \</span><br><span class="line">    --data_path data/dummy_conversation.json \</span><br><span class="line">    --fp16 True \</span><br><span class="line">    --output_dir output_vicuna \</span><br><span class="line">    --num_train_epochs 3 \</span><br><span class="line">    --per_device_train_batch_size 8 \</span><br><span class="line">    --per_device_eval_batch_size 1 \</span><br><span class="line">    --gradient_accumulation_steps 1 \</span><br><span class="line">    --evaluation_strategy <span class="string">"no"</span> \</span><br><span class="line">    --save_strategy <span class="string">"steps"</span> \</span><br><span class="line">    --save_steps 1200 \</span><br><span class="line">    --save_total_limit 10 \</span><br><span class="line">    --learning_rate 2e-5 \</span><br><span class="line">    --weight_decay 0. \</span><br><span class="line">    --warmup_ratio 0.03 \</span><br><span class="line">    --lr_scheduler_type <span class="string">"cosine"</span> \</span><br><span class="line">    --logging_steps 1 \</span><br><span class="line">    --fsdp <span class="string">"full_shard auto_wrap"</span> \</span><br><span class="line">    --fsdp_transformer_layer_cls_to_wrap <span class="string">'LlamaDecoderLayer'</span> \</span><br><span class="line">    --model_max_length 2048 \</span><br><span class="line">    --gradient_checkpointing True \</span><br><span class="line">    --lazy_preprocess True</span><br></pre></td></tr></tbody></table></figure><h3 id="torchrun"><a href="#torchrun" class="headerlink" title="torchrun"></a>torchrun</h3><p><code>torchrun</code> 是 PyTorch 提供的一个命令行工具，用于启动分布式训练。它是 <code>torch.distributed.launch</code> 模块的一部分，旨在简化在多个进程上运行 PyTorch 程序的过程。以下是对 <code>torchrun</code> 中使用的参数的详细解释：</p><ol><li><p><code>--nproc_per_node=8</code></p><ul><li><code>--nproc_per_node</code> 指定每个节点（在这种情况下通常是一台机器）上要启动的进程数。这里设置为 8，意味着在当前节点上将启动 8 个训练进程。</li><li>作用：用于控制每个节点上的并行度。在多 GPU 系统中，这通常等于 GPU 的数量。</li></ul></li><li><p><code>--master_port=20001</code></p><ul><li><code>--master_port</code> 指定主节点用于通信的端口。这里设置为 20001。</li><li>作用：在分布式训练中，不同进程需要通过网络进行通信。这个参数指定了用于进程间通信的端口。</li></ul></li><li><p><code>fastchat/train/train.py</code></p><ul><li>这不是 <code>torchrun</code> 的参数，而是指定了要执行的 Python 脚本，即训练脚本的路径。</li></ul></li></ol><p>在分布式训练中，<code>torchrun</code> 负责在每个进程中正确地设置环境变量，如 <code>LOCAL_RANK</code>（当前进程在其节点上的排名）、<code>WORLD_SIZE</code>（总进程数）和 <code>RANK</code>（全局进程排名）。这些环境变量对于使用 PyTorch 分布式包（如 <code>torch.distributed</code>）进行有效通信至关重要。</p><h3 id="使用示例"><a href="#使用示例" class="headerlink" title="使用示例"></a>使用示例</h3><p>假设您有一台拥有 8 个 GPU 的机器，您想在所有 GPU 上并行运行训练。使用 <code>torchrun</code>，您的命令可能如下所示：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torchrun --nproc_per_node=8 --master_port=20001 fastchat/train/train.py --其他参数</span><br></pre></td></tr></tbody></table></figure><p>这个命令会在每个 GPU 上启动一个训练进程，每个进程运行 <code>train.py</code> 脚本，并且所有进程能够通过分布式通信有效协作。</p><p><code>torchrun</code> 是分布式训练的关键工具，它简化了在多个进程上启动 PyTorch 程序的流程，特别是在多 GPU 环境中。通过自动设置必要的环境变量，<code>torchrun</code> 使得实现和运行分布式训练变得更加容易和可靠。</p><h3 id="2-参数"><a href="#2-参数" class="headerlink" title="2. 参数"></a>2. 参数</h3><ol><li><p><code>--model_name_or_path</code></p><ul><li>可以是预训练模型的官方名称（如 “bert-base-uncased”）、自定义训练的模型路径或 Hugging Face Model Hub 上的模型。</li><li>作用：指定用于训练的模型。</li></ul></li><li><p><code>--data_path</code></p><ul><li>路径可以是本地文件系统上的路径。</li><li>作用：指定训练使用的数据文件。</li></ul></li><li><p><code>--fp16</code></p><ul><li>可取值为 True 或 False。</li><li>作用：启用或禁用混合精度训练，以提高训练速度和降低显存使用。</li></ul></li><li><p><code>--output_dir</code></p><ul><li>任何有效的文件路径。</li><li>作用：指定输出目录，用于保存训练过程中产生的文件。</li></ul></li><li><p><code>--num_train_epochs</code></p><ul><li>任何正整数。</li><li>作用：指定训练的轮次。</li></ul></li><li><p><code>--per_device_train_batch_size</code> 和 <code>--per_device_eval_batch_size</code></p><ul><li>任何正整数。</li><li>作用：分别指定每个设备上的训练和评估批次大小。</li></ul></li><li><p><code>--gradient_accumulation_steps</code></p><ul><li>任何正整数。</li><li>作用：指定梯度累积的步骤数，用于在有限的显存下增加有效的批次大小。</li></ul></li><li><p><code>--evaluation_strategy</code></p><ul><li>可取值包括 “no”、”steps”、”epoch”。</li><li>作用：指定评估的策略，如每个 epoch 或特定步数后进行评估，或不进行评估。</li></ul></li><li><p><code>--save_strategy</code></p><ul><li>可取值包括 “no”、”steps”、”epoch”。</li><li>作用：指定模型保存的策略。</li></ul></li><li><p><code>--save_steps</code> 和 <code>--save_total_limit</code></p><ul><li><code>--save_steps</code> 取任何正整数。</li><li><code>--save_total_limit</code> 取任何正整数或 None。</li><li>作用：分别指定保存模型的步数间隔和最大保存的检查点数量。</li></ul></li><li><p><code>--learning_rate</code></p><ul><li>任何正浮点数。</li><li>作用：指定优化器的学习率。</li></ul></li><li><p><code>--weight_decay</code></p><ul><li>任何非负浮点数。</li><li>作用：指定权重衰减，用于正则化。</li></ul></li><li><p><code>--warmup_ratio</code></p><ul><li>任何非负浮点数，通常在 0 到 1 之间。</li><li>作用：指定预热的比例，即学习率在初始阶段逐渐增加的过程。</li></ul></li><li><p><code>--lr_scheduler_type</code></p><ul><li>可取值如 “linear”、”cosine”、”cosine_with_restarts”、”polynomial” 等。</li><li>作用：指定学习率调度器的类型。</li></ul></li><li><p><code>--logging_steps</code></p><ul><li>任何正整数。</li><li>作用：指定记录日志的步数间隔。</li></ul></li><li><p><code>--fsdp</code></p><ul><li>可取值如 “full_shard”、”auto_wrap” 等，或它们的组合。</li><li>作用：指定使用全分片数据并行（Fully Sharded Data Parallel）的配置。</li></ul></li><li><p><code>--fsdp_transformer_layer_cls_to_wrap</code></p><ul><li>指定要在 FSDP 中包装的特定层的类名。</li><li>作用：针对大型模型的分布式训练进行优化。</li></ul></li><li><p><code>--model_max_length</code></p><ul><li>任何正整数。</li><li>作用：指定模型处理的最大序列长度。</li></ul></li><li><p><code>--gradient_checkpointing</code></p><ul><li>可取值为 True 或 False。</li><li>作用：启用或禁用梯度检查点，以减少显存使用。</li></ul></li><li><p><code>--lazy_preprocess</code></p><ul><li>可取值为 True 或 False。</li><li>作用：启用或禁用懒加载预处理，即按需加载和处理数据。</li></ul></li></ol><p>这些参数共同构成了一个复杂的训练配置，允许用户根据特定需求灵活调整模型训练过程。</p><h2 id="7-总结"><a href="#7-总结" class="headerlink" title="7. 总结"></a>7. 总结</h2><p>随着本文的结束，我们完成了对 FastChat 平台中 train.py 脚本的深入解析，这只是我们系列技术博客中的第一部分。在这一部分中，我们聚焦于 train.py 脚本的结构和功能，涵盖了从数据预处理到模型训练和保存等关键步骤。通过这次解析，读者不仅能够更好地理解 FastChat 平台的工作原理，还能获得如何有效利用这个工具进行大型语言模型训练的宝贵知识。</p><p>随着我们技术博客系列的不断展开，我们将继续深入探索 FastChat 的其他组件和功能。接下来的文章将进一步拓展我们的讨论范围，涉及到更多高级功能和实际应用场景。我们期望这些内容能够为对 AI 和机器学习感兴趣的读者提供更全面、深入的见解。</p><p>最后，我们鼓励读者持续关注我们的博客，以获取关于 FastChat 及其在大型语言模型训练领域应用的最新信息和分析。无论您是该领域的专家还是初学者，我们相信这个系列将为您提供价值和启发。敬请期待我们下一篇文章的发布，它将为您揭开 FastChat 更多令人兴奋的面纱。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;FastChat-训练脚本代码逐行解析-Train-py-【FastChat-系列第-1-篇】&quot;&gt;&lt;a href=&quot;#FastChat-训练脚本代码逐行解析-Train-py-【FastChat-系列第-1-篇】&quot; class=&quot;headerlink&quot; title</summary>
      
    
    
    
    <category term="NLP Insights" scheme="https://chenhuiyu.github.io/categories/NLP-Insights/"/>
    
    
    <category term="LLM" scheme="https://chenhuiyu.github.io/tags/LLM/"/>
    
    <category term="FastChat" scheme="https://chenhuiyu.github.io/tags/FastChat/"/>
    
    <category term="Train" scheme="https://chenhuiyu.github.io/tags/Train/"/>
    
  </entry>
  
  <entry>
    <title>FastChat Training Script Code Analysis - Train.py 【FastChat Series Part 1】</title>
    <link href="https://chenhuiyu.github.io/2024/02/27/NLP%20Insights/FastChat%20Training%20Script%20Code%20Analysis%20-%20Train.py%20%E3%80%90FastChat%20Series%20Part%201%E3%80%91/"/>
    <id>https://chenhuiyu.github.io/2024/02/27/NLP%20Insights/FastChat%20Training%20Script%20Code%20Analysis%20-%20Train.py%20%E3%80%90FastChat%20Series%20Part%201%E3%80%91/</id>
    <published>2024-02-26T17:43:18.000Z</published>
    <updated>2024-02-26T18:00:42.331Z</updated>
    
    <content type="html"><![CDATA[<h1 id="FastChat-Training-Script-Code-Analysis-Train-py-【FastChat-Series-Part-1】"><a href="#FastChat-Training-Script-Code-Analysis-Train-py-【FastChat-Series-Part-1】" class="headerlink" title="FastChat Training Script Code Analysis - Train.py 【FastChat Series Part 1】"></a>FastChat Training Script Code Analysis - Train.py 【FastChat Series Part 1】</h1><p>In this article, we delve into the train.py script of FastChat (<a href="https://github.com/lm-sys/FastChat">https://github.com/lm-sys/FastChat</a>) (<a href="https://github.com/lm-sys/FastChat/blob/main/fastchat/train/train.py">https://github.com/lm-sys/FastChat/blob/main/fastchat/train/train.py</a>), a key component for training and optimizing large language models (LLMs). FastChat is an advanced open-source platform focused on developing, deploying, and evaluating chatbots based on LLMs. The platform not only supports top-tier models like Vicuna and MT-Bench but also includes a distributed multi-model service system equipped with a Web UI and RESTful API compatible with OpenAI, enabling efficient training and evaluation of models.</p><p>We provide a detailed analysis of the train.py script’s source code. This script is a training script for natural language processing models based on the transformers library, covering critical steps such as data preprocessing, model training, and saving. Our goal is to offer a detailed explanation of each class and function in train.py, including their functionality and role in the overall training process.</p><h2 id="1-Importing-Modules"><a href="#1-Importing-Modules" class="headerlink" title="1. Importing Modules"></a>1. Importing Modules</h2><h3 id="1-Built-in-Modules"><a href="#1-Built-in-Modules" class="headerlink" title="1. Built-in Modules"></a>1. Built-in Modules</h3><p>These are standard library modules that come with Python and don’t require additional installation.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> dataclasses <span class="keyword">import</span> dataclass, field</span><br></pre></td></tr></tbody></table></figure><p>Imports Python’s <code>dataclasses</code> module for creating classes with default values.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br></pre></td></tr></tbody></table></figure><p>Imports the <code>json</code> module for handling JSON format data.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br></pre></td></tr></tbody></table></figure><p>Imports the <code>math</code> module for mathematical operations.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pathlib</span><br></pre></td></tr></tbody></table></figure><p>Imports the <code>pathlib</code> module for handling file paths.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Dict</span>, <span class="type">Optional</span>, <span class="type">Sequence</span></span><br></pre></td></tr></tbody></table></figure><p>Imports the <code>typing</code> module for type annotations.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></tbody></table></figure><h3 id="2-Dependency-Libraries"><a href="#2-Dependency-Libraries" class="headerlink" title="2. Dependency Libraries"></a>2. Dependency Libraries</h3><p>These are external libraries typically installed via a package manager like pip.<br>Imports the <code>numpy</code> library, commonly used for scientific computing.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></tbody></table></figure><p>Imports <code>PyTorch</code>, a popular deep learning framework.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br></pre></td></tr></tbody></table></figure><p>Imports <code>Dataset</code> from <code>torch</code> for creating custom datasets.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> transformers</span><br></pre></td></tr></tbody></table></figure><p>Imports the <code>transformers</code> library, a popular natural language processing library.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Trainer</span><br></pre></td></tr></tbody></table></figure><p>Imports <code>Trainer</code> from <code>transformers</code> for training models.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers.trainer_pt_utils <span class="keyword">import</span> LabelSmoother</span><br></pre></td></tr></tbody></table></figure><p>Imports <code>LabelSmoother</code> from <code>transformers</code> for label smoothing.</p><h3 id="3-Project-Specific-Functions"><a href="#3-Project-Specific-Functions" class="headerlink" title="3. Project-Specific Functions"></a>3. Project-Specific Functions</h3><p>These are functions or classes custom-implemented in the Fast Chat project.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fastchat.conversation <span class="keyword">import</span> SeparatorStyle</span><br></pre></td></tr></tbody></table></figure><p>Imports <code>SeparatorStyle</code> from the <code>fastchat</code> package for defining conversation separator styles. The <code>SeparatorStyle</code> class is an enumeration class created using Python’s <code>enum</code> module, defining a series of separator styles. Enumerations are a programming concept used to define a named set of constants, making code clearer and more maintainable.</p><p>In the <code>SeparatorStyle</code> class, each member represents a specific style of separator. These styles are often used in text processing, especially in scenarios where different sections or elements need to be distinguished. For instance, in handling dialog or textual data, different methods might be needed to differentiate between user input and machine responses.</p><p>Regarding the use of the <code>auto()</code> function:</p><ul><li><code>auto()</code> is a special function provided by Python’s <code>enum</code> module. It automatically assigns a unique value to each member in an enumeration class.</li><li>Without using <code>auto()</code>, you would need to manually assign a unique value to each enumeration member. <code>auto()</code> simplifies this process by letting Python handle the assignment of these values automatically.</li><li>The values assigned by <code>auto()</code> are usually integers, starting from 1 and increasing sequentially.</li></ul><p>In the case of the <code>SeparatorStyle</code> class, <code>auto()</code> is used to automatically assign a unique integer value to each type of separator style. For example, <code>ADD_COLON_SINGLE</code>, <code>ADD_COLON_TWO</code>, etc., will be given different integer values.</p><p>The names of each enumeration member (such as <code>ADD_COLON_SINGLE</code>, <code>NO_COLON_SINGLE</code>, etc.) typically describe the characteristics of that separator style. For instance, <code>ADD_COLON_SINGLE</code> might represent adding a colon as a separator after a certain element, whereas <code>NO_COLON_SINGLE</code> means no colon is added.</p><p>This approach makes referencing and handling these separator styles in the code more convenient and clear. For example, different separator styles can be chosen based on different scenarios or requirements without having to remember their specific values.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> fastchat.model.model_adapter <span class="keyword">import</span> get_conversation_template</span><br></pre></td></tr></tbody></table></figure><p>Imports <code>get_conversation_template</code> from the <code>fastchat</code> package for obtaining conversation templates. In this code segment, the call logic primarily involves obtaining the default conversation template for a specific model. The call chain is as follows:</p><ol><li><p><strong>Starting Call - <code>get_conversation_template(model_path: str)</code></strong></p><ul><li>This function is the starting point of the call chain. It accepts a parameter <code>model_path</code>, specifying the path of the model.</li><li>The purpose of this function is to obtain the default conversation template for the given model path.</li></ul></li><li><p><strong>Call <code>get_model_adapter(model_path: str)</code></strong></p><ul><li>The <code>get_conversation_template</code> function first calls <code>get_model_adapter</code>, passing in the model path.</li><li>The purpose of <code>get_model_adapter</code> is to find and return a suitable <code>BaseModelAdapter</code> object for the provided model path.</li><li>This function first tries to match the basename of <code>model_path</code>. If no match is found, it tries the full path.</li><li>If a suitable adapter is found, it is returned; otherwise, a <code>ValueError</code> is thrown.</li></ul></li><li><p><strong>Execute <code>BaseModelAdapter.get_default_conv_template(model_path: str)</code></strong></p><ul><li>Once the appropriate model adapter is obtained, <code>get_conversation_template</code> retrieves the default conversation template by calling the <code>get_default_conv_template</code> method of that adapter.</li><li>Note that this method is defined in the <code>BaseModelAdapter</code> class but might be overridden in subclasses.</li></ul></li><li><p><strong>Call <code>get_conv_template(name: str)</code></strong></p><ul><li>Inside the <code>get_default_conv_template</code> method, it calls the <code>get_conv_template</code> function, usually passing a predefined template name like <code>"one_shot"</code>.</li><li>The purpose of <code>get_conv_template</code> is to retrieve a specified name’s template from the global registry of conversation templates <code>conv_templates</code>.</li></ul></li><li><p><strong>Obtain and Return a <code>Conversation</code> Object</strong></p><ul><li>The <code>get_conv_template</code> function returns an instance of the <code>Conversation</code> class, usually copied from the <code>conv_templates</code> dictionary.</li><li>Finally, this <code>Conversation</code> instance is returned to the original call site of <code>get_conversation_template</code>.</li></ul></li></ol><p>Summarizing the call chain:</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">get_conversation_template(model_path)</span><br><span class="line">  -&gt; get_model_adapter(model_path)</span><br><span class="line">  -&gt; [BaseModelAdapter].get_default_conv_template(model_path)</span><br><span class="line">    -&gt; get_conv_template(name)</span><br><span class="line">      -&gt; Return Conversation Object</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><p>In this process, the code navigates through a series of function calls to find a suitable model adapter based on the provided model path and retrieve a specific conversation template from it. This design pattern allows flexibility in providing different conversation templates for different models, enhancing the reusability and extensibility of the code.</p><hr><h2 id="2-Configuration-Classes"><a href="#2-Configuration-Classes" class="headerlink" title="2. Configuration Classes"></a>2. Configuration Classes</h2><p>These classes are defined using Python’s <code>dataclass</code> decorator and are mainly used for storing configurations and parameters. These classes usually do not contain complex methods or logic but are used to define and store data structures. These classes include:</p><ul><li><code>ModelArguments</code>: Stores parameters related to the model, like model path, trust in remote code, etc.</li><li><code>DataArguments</code>: Stores parameters related to data, like data path, evaluation data path, and whether to use lazy preprocessing.</li><li><code>TrainingArguments</code>: Stores parameters related to training, like cache directory, optimizer type, model maximum length, etc. This class extends <code>transformers.TrainingArguments</code> and adds some custom parameters.</li></ul><p>These classes are mainly used to simplify and organize parameter management in the code, making parameter modification and access more convenient.</p><h3 id="1-ModelArguments-Class"><a href="#1-ModelArguments-Class" class="headerlink" title="1. ModelArguments Class"></a>1. ModelArguments Class</h3><h4 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ModelArguments</span>:</span><br><span class="line">    model_name_or_path: <span class="type">Optional</span>[<span class="built_in">str</span>] = field(default=<span class="string">"facebook/opt-125m"</span>)</span><br><span class="line">    trust_remote_code: <span class="built_in">bool</span> = field(</span><br><span class="line">        default=<span class="literal">False</span>,</span><br><span class="line">        metadata={</span><br><span class="line">            <span class="string">"help"</span>: <span class="string">"Whether or not to allow for custom models defined on the Hub in their own modeling files"</span></span><br><span class="line">        },</span><br><span class="line">    )</span><br><span class="line">    padding_side: <span class="built_in">str</span> = field(</span><br><span class="line">        default=<span class="string">"right"</span>, metadata={<span class="string">"help"</span>: <span class="string">"The padding side in tokenizer"</span>}</span><br><span class="line">    )</span><br></pre></td></tr></tbody></table></figure><h4 id="Explanation"><a href="#Explanation" class="headerlink" title="Explanation"></a>Explanation</h4><p><code>ModelArguments</code> is a data class (<code>dataclass</code>) used for storing model-related configuration parameters.<br><strong>Attributes:</strong></p><ol><li><code>model_name_or_path</code>: Specifies the name or path of the pretrained model.</li><li><code>trust_remote_code</code>: Whether to allow custom models that have their modeling files defined on the Hub.</li><li><code>padding_side</code>: Specifies the padding side in the tokenizer, typically right or left padding.</li></ol><details><summary> Introduction to `@dataclass` decorator, click to expand </summary>`@dataclass` is a decorator used to automate the generation of special methods like `__init__()`, `__repr__()`, `__eq__()` etc., thus simplifying the writing of data classes. This decorator is part of Python 3.7 and is in the `dataclasses` module.<p>When you use <code>@dataclass</code> before a class definition, Python automatically adds some special methods based on the fields defined in the class. This is very useful for creating classes that store a small amount of data but do not need complex methods.</p><p>Specifically, using <code>@dataclass</code>:</p><ol><li><p><strong>Automatically generates a constructor (<code>__init__</code> method)</strong>: Python creates an <code>__init__</code> method automatically based on the fields defined in the class, so you don’t need to manually write this method to initialize your class instances.</p></li><li><p><strong>Automatically generates a <code>__repr__</code> method</strong>: This makes printing the class instances provide a more readable string representation, usually including the class name and its fields and their values.</p></li><li><p><strong>Automatically generates an <code>__eq__</code> method</strong>: This allows you to use the <code>==</code> operator to compare two instances of the class, comparing the values of the instance fields.</p></li><li><p><strong>Support for type annotations</strong>: When defining fields, you can use type annotations, which not only help with clarity of code but can also be checked for type correctness using some tools.</p></li></ol><p>In the case of the <code>ModelArguments</code> class, the <code>@dataclass</code> decorator will generate the above-mentioned methods. This means you can easily create an instance of <code>ModelArguments</code>, and when printing or comparing these instances, you will get the expected behavior.</p><p>For example, when you create an instance of <code>ModelArguments</code>:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">args = ModelArguments()</span><br></pre></td></tr></tbody></table></figure><p>This will call the automatically generated <code>__init__</code> method, using the default values “facebook/opt-125m” for <code>model_name_or_path</code>, <code>False</code> for <code>trust_remote_code</code>, and “right” for <code>padding_side</code>.</p><p>When you print this instance:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(args)</span><br></pre></td></tr></tbody></table></figure><p>This will call the automatically generated <code>__repr__</code> method, showing a detailed view of the class instance, like <code>ModelArguments(model_name_or_path="facebook/opt-125m", trust_remote_code=False, padding_side="right")</code>.</p><p>Thus, the <code>@dataclass</code> decorator simplifies the process of creating classes, making the code more concise and maintainable.</p><p>Overall, the <code>@dataclass</code> decorator is a convenient tool provided by Python for quickly creating classes mainly used for storing data.</p></details><h3 id="2-DataArguments-Class"><a href="#2-DataArguments-Class" class="headerlink" title="2. DataArguments Class"></a>2. DataArguments Class</h3><h4 id="Code-1"><a href="#Code-1" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DataArguments</span>:</span><br><span class="line">    data_path: <span class="built_in">str</span> = field(</span><br><span class="line">        default=<span class="literal">None</span>, metadata={<span class="string">"help"</span>: <span class="string">"Path to the training data."</span>}</span><br><span class="line">    )</span><br><span class="line">    eval_data_path: <span class="built_in">str</span> = field(</span><br><span class="line">        default=<span class="literal">None</span>, metadata={<span class="string">"help"</span>: <span class="string">"Path to the evaluation data."</span>}</span><br><span class="line">    )</span><br><span class="line">    lazy_preprocess: <span class="built_in">bool</span> = <span class="literal">False</span></span><br></pre></td></tr></tbody></table></figure><h4 id="Explanation-1"><a href="#Explanation-1" class="headerlink" title="Explanation"></a>Explanation</h4><p><strong>DataArguments Class</strong></p><ul><li><code>DataArguments</code> is also a data class used for storing data-related configuration parameters.</li><li>Attributes:<ul><li><code>data_path</code>: Path to the training data.</li><li><code>eval_data_path</code>: Path to the evaluation data.</li><li><code>lazy_preprocess</code>: Whether to use lazy loading for data preprocessing, i.e., load and process data as needed.</li></ul></li></ul><h3 id="3-TrainingArguments-Class"><a href="#3-TrainingArguments-Class" class="headerlink" title="3. TrainingArguments Class"></a>3. TrainingArguments Class</h3><h4 id="Code-2"><a href="#Code-2" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@dataclass</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TrainingArguments</span>(transformers.TrainingArguments):</span><br><span class="line">    cache_dir: <span class="type">Optional</span>[<span class="built_in">str</span>] = field(default=<span class="literal">None</span>)</span><br><span class="line">    optim: <span class="built_in">str</span> = field(default=<span class="string">"adamw_torch"</span>)</span><br><span class="line">    model_max_length: <span class="built_in">int</span> = field(</span><br><span class="line">        default=<span class="number">512</span>,</span><br><span class="line">        metadata={</span><br><span class="line">            <span class="string">"help"</span>: <span class="string">"Maximum sequence length. Sequences will be right padded (and possibly truncated)."</span></span><br><span class="line">        },</span><br><span class="line">    )</span><br></pre></td></tr></tbody></table></figure><h4 id="Explanation-2"><a href="#Explanation-2" class="headerlink" title="Explanation"></a>Explanation</h4><p><code>TrainingArguments</code> class extends <code>transformers.TrainingArguments</code>.</p><ol><li><p><strong>TrainingArguments Class</strong></p><ul><li><code>TrainingArguments</code> is a data class that, by extending <code>transformers.TrainingArguments</code>, gains the capability to handle training parameters.</li><li>Attributes defined in <code>TrainingArguments</code>:<ul><li><code>cache_dir</code>: Specifies the directory path for caching the model and tokenizer.</li><li><code>optim</code>: Defines the type of optimizer to use, like <code>'adamw_torch'</code>.</li><li><code>model_max_length</code>: Specifies the maximum sequence length the model can handle.</li></ul></li></ul></li><li><p><strong>transformers.TrainingArguments Class</strong></p><ul><li><code>transformers.TrainingArguments</code> is a class in the transformers library that is used for configuring various parameters in the model training process.</li><li>This class contains a plethora of attributes for controlling the training process, such as:<ul><li><code>output_dir</code>: Specifies the directory to save the model and training results.</li><li><code>num_train_epochs</code>: Number of training epochs.</li><li><code>per_device_train_batch_size</code>: Batch size per device for training.</li><li><code>save_steps</code>: Steps interval for saving the model.</li><li><code>evaluation_strategy</code>: Strategy for evaluating the model, like at the end of each epoch.</li><li><code>learning_rate</code>: Learning rate.</li><li><code>warmup_steps</code>: Steps used for warmup in the learning rate schedule.</li></ul></li><li><code>transformers.TrainingArguments</code> also</li></ul></li></ol><p> contains many other parameters for fine-tuning the training process, including logging, model saving strategies, learning rate scheduling, and more.</p><p>By extending <code>transformers.TrainingArguments</code>, the <code>TrainingArguments</code> class not only inherits all these training parameter configurations but can also add some custom training parameters, like in this case <code>cache_dir</code>, <code>optim</code>, and <code>model_max_length</code>. This approach enhances code reusability and flexibility, allowing you to adjust and extend training configurations as per the specific requirements of your project.</p><h2 id="3-Functional-Utility-Functions"><a href="#3-Functional-Utility-Functions" class="headerlink" title="3. Functional Utility Functions"></a>3. Functional Utility Functions</h2><h3 id="1-rank0-print-args"><a href="#1-rank0-print-args" class="headerlink" title="1. rank0_print(*args)"></a>1. rank0_print(*args)</h3><h4 id="Code-3"><a href="#Code-3" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">local_rank = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">rank0_print</span>(<span class="params">*args</span>):</span><br><span class="line">    <span class="keyword">if</span> local_rank == <span class="number">0</span>:</span><br><span class="line">        <span class="built_in">print</span>(*args)</span><br></pre></td></tr></tbody></table></figure><h4 id="Explanation-3"><a href="#Explanation-3" class="headerlink" title="Explanation"></a>Explanation</h4><p>Defines a global variable local_rank for distributed training.<br>Defines a function rank0_print to print information only if local_rank is 0, used for controlling output in distributed training. This way, repetitive printing of the same information across multiple nodes is avoided, making the output clearer and more concise.</p><ul><li>Used to print information only on the main node (rank 0) in a distributed training environment.</li><li>Parameters: A variable number of arguments for printing.</li></ul><h3 id="2-trainer-save-model-safe-trainer-transformers-Trainer"><a href="#2-trainer-save-model-safe-trainer-transformers-Trainer" class="headerlink" title="2. trainer_save_model_safe(trainer: transformers.Trainer)"></a>2. <code>trainer_save_model_safe(trainer: transformers.Trainer)</code></h3><h4 id="Code-4"><a href="#Code-4" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">trainer_save_model_safe</span>(<span class="params">trainer: transformers.Trainer</span>):</span><br><span class="line">    <span class="keyword">from</span> torch.distributed.fsdp <span class="keyword">import</span> FullyShardedDataParallel <span class="keyword">as</span> FSDP</span><br><span class="line">    <span class="keyword">from</span> torch.distributed.fsdp <span class="keyword">import</span> StateDictType, FullStateDictConfig</span><br><span class="line"></span><br><span class="line">    save_policy = FullStateDictConfig(offload_to_cpu=<span class="literal">True</span>, rank0_only=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">with</span> FSDP.state_dict_type(</span><br><span class="line">        trainer.model, StateDictType.FULL_STATE_DICT, save_policy</span><br><span class="line">    ):</span><br><span class="line">        trainer.save_model()</span><br></pre></td></tr></tbody></table></figure><p>The function <code>trainer_save_model_safe(trainer: transformers.Trainer)</code> aims to safely save models trained with the PyTorch distributed framework. Let’s delve into the details of this function and its key components.</p><h4 id="Explanation-4"><a href="#Explanation-4" class="headerlink" title="Explanation"></a>Explanation</h4><ol><li><p>Parameters:</p><ul><li><code>trainer</code>: An instance of <code>transformers.Trainer</code>. This class is one of the core components of the Hugging Face Transformers library, used for training and evaluating models.</li></ul></li><li><p>Functionality:</p><ul><li>The main purpose of this function is to safely save models in a distributed training environment. It particularly considers the model saving strategy when using Fully Sharded Data Parallel (FSDP).</li></ul></li><li><p>FSDP</p><ul><li><strong>FullyShardedDataParallel (FSDP)</strong><ul><li>This is a component of PyTorch’s distributed training framework. FSDP helps reduce memory usage on each GPU by sharding model parameters across multiple GPUs, allowing the training of larger models.</li><li>In this context, FSDP is primarily used for handling and saving model states in distributed training.</li></ul></li><li><strong>StateDictType</strong><ul><li>This is an enumeration type that defines how to save the model’s state dictionary. In FSDP environments, saving and loading model states might require special handling.</li></ul></li><li><strong>FullStateDictConfig</strong><ul><li>This class configures parameters for saving the full state dictionary. It’s part of FSDP’s functionality and is used to control how the model state is saved.</li></ul></li></ul></li><li><p>Function Implementation</p><ul><li><strong>Setting Save Policy</strong><ul><li><code>save_policy = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)</code> creates a save policy. Here, two key parameters are specified:<ul><li><code>offload_to_cpu</code>: Offload model parameters to CPU before saving the state dictionary, which helps reduce GPU memory usage.</li><li><code>rank0_only</code>: Save the model only on rank 0 (usually the main node). In distributed training, this avoids saving the same model copy on every node, saving storage space.</li></ul></li></ul></li><li><strong>Saving the Model</strong><ul><li>Using the <code>with FSDP.state_dict_type(trainer.model, StateDictType.FULL_STATE_DICT, save_policy)</code> context manager, the type and policy for saving the model’s state dictionary are set.</li><li>Within this context, <code>trainer.save_model()</code> is called to save the model. Due to the <code>save_policy</code>, the model is saved securely following the specified configuration.</li></ul></li></ul></li></ol><p>The function <code>trainer_save_model_safe</code> encapsulates a safe model saving logic, particularly for scenarios involving PyTorch’s FSDP in distributed training. It ensures that only a complete model state is saved on one node and offloads model parameters to CPU before saving, optimizing memory usage and storage efficiency. This is crucial for training large models and managing large-scale distributed training environments.</p><h3 id="3-preprocess-sources-tokenizer-transformers-PreTrainedTokenizer-gt-Dict"><a href="#3-preprocess-sources-tokenizer-transformers-PreTrainedTokenizer-gt-Dict" class="headerlink" title="3.preprocess(sources,tokenizer: transformers.PreTrainedTokenizer) -> Dict"></a>3.<code>preprocess(sources,tokenizer: transformers.PreTrainedTokenizer) -&gt; Dict</code></h3><h4 id="Code-5"><a href="#Code-5" class="headerlink" title="Code"></a>Code</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess</span>(<span class="params"></span></span><br><span class="line"><span class="params">    sources,</span></span><br><span class="line"><span class="params">    tokenizer: transformers.PreTrainedTokenizer,</span></span><br><span class="line"><span class="params"></span>) -&gt; <span class="type">Dict</span>:</span><br><span class="line">    conv = get_conversation_template(<span class="string">"vicuna"</span>)</span><br><span class="line">    roles = {<span class="string">"human"</span>: conv.roles[<span class="number">0</span>], <span class="string">"gpt"</span>: conv.roles[<span class="number">1</span>]}</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Apply prompt templates</span></span><br><span class="line">    conversations = []</span><br><span class="line">    <span class="keyword">for</span> i, source <span class="keyword">in</span> <span class="built_in">enumerate</span>(sources):</span><br><span class="line">        <span class="keyword">if</span> roles[source[<span class="number">0</span>][<span class="string">"from"</span>]] != conv.roles[<span class="number">0</span>]:</span><br><span class="line">            <span class="comment"># Skip the first one if it is not from human</span></span><br><span class="line">            source = source[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">        conv.messages = []</span><br><span class="line">        <span class="keyword">for</span> j, sentence <span class="keyword">in</span> <span class="built_in">enumerate</span>(source):</span><br><span class="line">            role = roles[sentence[<span class="string">"from"</span>]]</span><br><span class="line">            <span class="keyword">assert</span> role == conv.roles[j % <span class="number">2</span>], <span class="string">f"<span class="subst">{i}</span>"</span></span><br><span class="line">            conv.append_message(role, sentence[<span class="string">"value"</span>])</span><br><span class="line">        conversations.append(conv.get_prompt())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Tokenize conversations</span></span><br><span class="line">    input_ids = tokenizer(</span><br><span class="line">        conversations,</span><br><span class="line">        return_tensors=<span class="string">"pt"</span>,</span><br><span class="line">        padding=<span class="string">"max_length"</span>,</span><br><span class="line">        max_length=tokenizer.model_max_length,</span><br><span class="line">        truncation=<span class="literal">True</span>,</span><br><span class="line">    ).input_ids</span><br><span class="line">    targets = input_ids.clone()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">assert</span> conv.sep_style == SeparatorStyle.ADD_COLON_TWO</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Mask targets. Only compute loss on the assistant outputs.</span></span><br><span class="line">    sep = conv.sep + conv.roles[<span class="number">1</span>] + <span class="string">": "</span></span><br><span class="line">    <span class="keyword">for</span> conversation, target <span class="keyword">in</span> <span class="built_in">zip</span>(conversations, targets):</span><br><span class="line">        total_len = <span class="built_in">int</span>(target.ne(tokenizer.pad_token_id).<span class="built_in">sum</span>())</span><br><span class="line"></span><br><span class="line">        turns = conversation.split(conv.sep2)</span><br><span class="line">        cur_len = <span class="number">1</span></span><br><span class="line">        target[:cur_len] = IGNORE_TOKEN_ID</span><br><span class="line">        <span class="keyword">for</span> i, turn <span class="keyword">in</span> <span class="built_in">enumerate</span>(turns):</span><br><span class="line">            <span class="keyword">if</span> turn == <span class="string">""</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            turn_len = <span class="built_in">len</span>(tokenizer(turn).input_ids)</span><br><span class="line"></span><br><span class="line">            parts = turn.split(sep)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(parts) != <span class="number">2</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            parts[<span class="number">0</span>] += sep</span><br><span class="line">            <span class="comment"># "-2" is hardcoded for the Llama tokenizer to make the offset correct.</span></span><br><span class="line">            instruction_len = <span class="built_in">len</span>(tokenizer(parts[<span class="number">0</span>]).input_ids) - <span class="number">2</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i != <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> tokenizer.legacy:</span><br><span class="line">                <span class="comment"># The legacy and non-legacy modes handle special tokens differently</span></span><br><span class="line">                instruction_len -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># Ignore the user instructions</span></span><br><span class="line">            target[cur_len : cur_len + instruction_len] = IGNORE_TOKEN_ID</span><br><span class="line">            cur_len += turn_len</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i != <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> tokenizer.legacy:</span><br><span class="line">                <span class="comment"># The legacy and non-legacy modes handle special tokens differently</span></span><br><span class="line">                cur_len -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        target[cur_len:] = IGNORE_TOKEN_ID</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="literal">False</span>:  <span class="comment"># Inspect and check the correctness of masking</span></span><br><span class="line">            z = target.clone()</span><br><span class="line">            z = torch.where(z == IGNORE_TOKEN_ID, tokenizer.unk_token_id, z)</span><br><span class="line">            rank0_print(tokenizer.decode(z))</span><br><span class="line">            exit()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> cur_len &lt; tokenizer.model_max_length:</span><br><span class="line">           </span><br><span class="line"></span><br><span class="line"> <span class="keyword">if</span> cur_len != total_len:</span><br><span class="line">                target[:] = IGNORE_TOKEN_ID</span><br><span class="line">                rank0_print(</span><br><span class="line">                    <span class="string">f"WARNING: tokenization mismatch: <span class="subst">{cur_len}</span> vs. <span class="subst">{total_len}</span>."</span></span><br><span class="line">                    <span class="string">f" #turn = <span class="subst">{<span class="built_in">len</span>(turns) - <span class="number">1</span>}</span>. (ignored)"</span></span><br><span class="line">                )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">dict</span>(</span><br><span class="line">        input_ids=input_ids,</span><br><span class="line">        labels=targets,</span><br><span class="line">        attention_mask=input_ids.ne(tokenizer.pad_token_id),</span><br><span class="line">    )</span><br></pre></td></tr></tbody></table></figure><p>The function <code>preprocess(sources, tokenizer: transformers.PreTrainedTokenizer) -&gt; Dict</code> is intended for preprocessing dialogue data to be suitable for training machine learning models. This function can be broken down into several main parts for a more detailed explanation:</p><h4 id="1-Obtaining-Conversation-Templates-and-Role-Definitions"><a href="#1-Obtaining-Conversation-Templates-and-Role-Definitions" class="headerlink" title="1. Obtaining Conversation Templates and Role Definitions"></a>1. Obtaining Conversation Templates and Role Definitions</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conv = get_conversation_template(<span class="string">"vicuna"</span>)</span><br><span class="line">roles = {<span class="string">"human"</span>: conv.roles[<span class="number">0</span>], <span class="string">"gpt"</span>: conv.roles[<span class="number">1</span>]}</span><br></pre></td></tr></tbody></table></figure><ul><li><strong>Functionality</strong>: Initializes conversation templates and defines the roles of dialogue participants.</li><li><strong>Implementation</strong>:<ul><li><code>conv = get_conversation_template("vicuna")</code> obtains the conversation template for a specified model (e.g., “vicuna”).</li><li>The <code>roles</code> dictionary maps “human” and “gpt” to the roles defined in the conversation template.</li></ul></li><li><strong>Example</strong>:<ul><li>If the conversation template is for “vicuna”, then <code>roles</code> might map “human” to “user” and “gpt” to “assistant”. For example, <code>{'human': 'USER', 'gpt': 'ASSISTANT'}</code>.</li></ul></li></ul><h4 id="2-Applying-Prompt-Templates"><a href="#2-Applying-Prompt-Templates" class="headerlink" title="2. Applying Prompt Templates"></a>2. Applying Prompt Templates</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Apply prompt templates</span></span><br><span class="line">conversations = []</span><br><span class="line"><span class="keyword">for</span> i, source <span class="keyword">in</span> <span class="built_in">enumerate</span>(sources):</span><br><span class="line">    <span class="keyword">if</span> roles[source[<span class="number">0</span>][<span class="string">"from"</span>]] != conv.roles[<span class="number">0</span>]:</span><br><span class="line">        <span class="comment"># Skip the first one if it is not from human</span></span><br><span class="line">        source = source[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">    conv.messages = []</span><br><span class="line">    <span class="keyword">for</span> j, sentence <span class="keyword">in</span> <span class="built_in">enumerate</span>(source):</span><br><span class="line">        role = roles[sentence[<span class="string">"from"</span>]]</span><br><span class="line">        <span class="keyword">assert</span> role == conv.roles[j % <span class="number">2</span>], <span class="string">f"<span class="subst">{i}</span>"</span></span><br><span class="line">        conv.append_message(role, sentence[<span class="string">"value"</span>])</span><br><span class="line">    conversations.append(conv.get_prompt())</span><br></pre></td></tr></tbody></table></figure><ul><li><strong>Functionality</strong>: Applies prompt templates to source data to construct dialogues.</li><li><strong>Implementation</strong>:<ul><li>Iterates through <code>sources</code> (original dialogue data), transforming each dialogue source into a conversation in template format.</li><li>If the first part of a dialogue is not initiated by the “human” role, it skips that part.</li><li>Assigns a role to each sentence and adds it to the conversation template.</li><li>Ultimately, each processed dialogue is added to the <code>conversations</code> list.</li></ul></li><li><strong>Example</strong>:<ul><li>Suppose we have a source which is the first item in dummy input: <code>python source = [{'from': 'human', 'value': 'Who are you?'}, {'from': 'gpt', 'value': 'I am Vicuna, a language model trained by researchers from Large Model Systems Organization (LMSYS).'}, {'from': 'human', 'value': 'Have a nice day!'}, {'from': 'gpt', 'value': 'You too!'}]</code></li><li><code>conversations</code> under the Vicuna template, using <code>SeparatorStyle.ADD_COLON_TWO</code> as the separator style, might look like [“A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user’s questions. USER: Who are you? ASSISTANT: I am Vicuna, a language model trained by researchers from Large Model Systems Organization (LMSYS).USER: Have a nice day! ASSISTANT: You too!“]</li><li><details><summary> Implementation of get_prompt </summary>The `get_prompt` method implementation varies depending on the `SeparatorStyle`. Below is a table detailing the `get_prompt` method for various styles, along with English examples:<table><thead><tr><th>Separator Style (<code>SeparatorStyle</code>)</th><th>Description</th><th>Example</th></tr></thead><tbody><tr><td><code>ADD_COLON_SINGLE</code></td><td>Adds a colon and separator after each message.</td><td>USER: Hello there!\nASSISTANT: Hi, how can I help?\n</td></tr><tr><td><code>ADD_COLON_TWO</code></td><td>Uses two alternating separators, usually between different roles.</td><td>USER: What’s the weather?\nASSISTANT: It’s sunny today.\n\n</td></tr><tr><td><code>ADD_COLON_SPACE_SINGLE</code></td><td>Adds a colon, space, and separator after each message.</td><td>USER: Can you book a flight?\nASSISTANT: Sure, where to?\n</td></tr><tr><td><code>NO_COLON_SINGLE</code></td><td>Messages directly follow roles without a colon, followed by a separator.</td><td>USERWhat are you doing?\nASSISTANTI’m here to assist you.\n</td></tr><tr><td><code>NO_COLON_TWO</code></td><td>No colons, with two alternating separators.</td><td>USERHow’s the project going?\nASSISTANTIt’s on track.\n\n</td></tr><tr><td><code>ADD_NEW_LINE_SINGLE</code></td><td>Each message is preceded by a newline, followed by a separator.</td><td>USER\nHow can I reset my password?\nASSISTANT\nYou can reset it via email.\n</td></tr><tr><td><code>RWKV</code></td><td>Special format, usually for specific models.</td><td>USER: What is AI?\n\nASSISTANT: AI stands for Artificial Intelligence.\n\n</td></tr><tr><td><code>LLAMA2</code></td><td>Special label format for specific models.</td><td>[INST] USER How does blockchain work?\nASSISTANT It is a distributed ledger.\n\n</td></tr><tr><td><code>CHATGLM</code></td><td>Specific format for <code>CHATGLM</code> model.</td><td>[Round 1]\nUSER: Tell me a joke.\nASSISTANT: Why did the chicken cross the road?\n</td></tr><tr><td><code>CHATML</code></td><td>Similar to <code>CHATGLM</code>, but with newlines before and after each message.</td><td>USER\nDo you like music?\n\nASSISTANT\nYes, I enjoy many genres.\n\n</td></tr><tr><td><code>CHATGLM3</code></td><td>Format for <code>CHATGLM3</code> model.</td><td>USER\nCan you play chess?\nASSISTANTYes, I can play.\n</td></tr><tr><td><code>CHATINTERN</code></td><td>Format for <code>CHATINTERN</code> model, using special markers.</td><td><s>USER:Where is the nearest ATM?<s>\nASSISTANT:It’s next to the post office.\n</s></s></td></tr><tr><td><code>DOLLY</code></td><td>Specific format for <code>DOLLY</code> model.</td><td>USER:\nWhat is quantum computing?\nASSISTANT:\nIt involves computation using quantum-mechanical phenomena.\n\n</td></tr><tr><td><code>PHOENIX</code></td><td>For <code>PHOENIX</code> model, messages are wrapped in special markers.</td><td>USER: <s>How to bake a cake?</s>\nASSISTANT: <s>You need flour, sugar, and eggs.</s>\n</td></tr><tr><td><code>ROBIN</code></td><td>Similar to <code>ADD_NEW_LINE_SINGLE</code>, but with a newline after roles.</td><td>USER:\nIs AI dangerous?\nASSISTANT:\nIt depends on how it’s used.\n</td></tr><tr><td></td><td></td><td></td></tr></tbody></table></details></li></ul></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;FastChat-Training-Script-Code-Analysis-Train-py-【FastChat-Series-Part-1】&quot;&gt;&lt;a href=&quot;#FastChat-Training-Script-Code-Analysis-Train-py-</summary>
      
    
    
    
    <category term="NLP Insights" scheme="https://chenhuiyu.github.io/categories/NLP-Insights/"/>
    
    
    <category term="LLM" scheme="https://chenhuiyu.github.io/tags/LLM/"/>
    
    <category term="FastChat" scheme="https://chenhuiyu.github.io/tags/FastChat/"/>
    
    <category term="Train" scheme="https://chenhuiyu.github.io/tags/Train/"/>
    
  </entry>
  
  <entry>
    <title>英语学习日记：De Facto</title>
    <link href="https://chenhuiyu.github.io/2024/02/26/Life%20Reflections/%E8%8B%B1%E8%AF%AD%E5%AD%A6%E4%B9%A0%E6%97%A5%E8%AE%B0%EF%BC%9ADe%20Facto/"/>
    <id>https://chenhuiyu.github.io/2024/02/26/Life%20Reflections/%E8%8B%B1%E8%AF%AD%E5%AD%A6%E4%B9%A0%E6%97%A5%E8%AE%B0%EF%BC%9ADe%20Facto/</id>
    <published>2024-02-26T06:29:00.000Z</published>
    <updated>2024-02-26T06:29:53.822Z</updated>
    
    <content type="html"><![CDATA[<p>大家好！🌐 今天我们要探讨一个在英语对话和写作中常见的短语：<strong>de facto</strong></p><h3 id="理解-“De-Facto”"><a href="#理解-“De-Facto”" class="headerlink" title="理解 “De Facto”"></a>理解 “De Facto”</h3><ol><li><p><strong>含义</strong>：’De facto’ 这个短语用来描述一些实际上存在的事物，即使它们没有被官方认可或法律确立。就像是在说“实际上”或“实践中”，而不是“理论上”或“官方上”。</p></li><li><p><strong>词源</strong>：这个短语有着非常有趣的历史。它源自拉丁语，其中 ‘de’ 意为 ‘来自’，’facto’ 意味着 ‘事实’。随着时间的推移，它被英语采纳，并保留了从拉丁语中原始的精髓。</p></li></ol><h3 id="例句"><a href="#例句" class="headerlink" title="例句"></a>例句</h3><ul><li>🌟 <em>In many organizations, there is a <strong>de facto</strong> leader who isn’t officially the boss but is respected and followed by the team.</em></li><li>🌟 <em>While English is the <strong>de facto</strong> language of international business, it’s not the official language in many countries where it’s widely spoken.</em></li><li>🌟 <em>The museum, though not formally recognized, acts as the <strong>de facto</strong> cultural center of the small town.</em></li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>‘De facto’ 这个术语是一种微妙但强大的方式，用来描述情况的现实，区别于其官方或法律地位。将这样的短语纳入你的语言库不仅丰富了你的词汇，还增强了你表达细微想法的能力。继续探索并拥抱语言的美妙吧！📚💬</p><p>记住，语言是一段旅程，不是终点。祝学习愉快，我们下次见！🚀🌟</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;大家好！🌐 今天我们要探讨一个在英语对话和写作中常见的短语：&lt;strong&gt;de facto&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&quot;理解-“De-Facto”&quot;&gt;&lt;a href=&quot;#理解-“De-Facto”&quot; class=&quot;headerlink&quot; title=&quot;理解</summary>
      
    
    
    
    <category term="Life Reflections" scheme="https://chenhuiyu.github.io/categories/Life-Reflections/"/>
    
    
    <category term="Language Learning" scheme="https://chenhuiyu.github.io/tags/Language-Learning/"/>
    
    <category term="English Vocabulary" scheme="https://chenhuiyu.github.io/tags/English-Vocabulary/"/>
    
  </entry>
  
  <entry>
    <title>De Facto: Unveiling the Power of an Intriguing Phrase</title>
    <link href="https://chenhuiyu.github.io/2024/02/26/Life%20Reflections/De%20Facto:%20Unveiling%20the%20Power%20of%20an%20Intriguing%20Phrase%20/"/>
    <id>https://chenhuiyu.github.io/2024/02/26/Life%20Reflections/De%20Facto:%20Unveiling%20the%20Power%20of%20an%20Intriguing%20Phrase%20/</id>
    <published>2024-02-26T06:25:00.000Z</published>
    <updated>2024-02-26T06:29:42.776Z</updated>
    
    <content type="html"><![CDATA[<p>Hello, language enthusiasts! 🌐 Today, we’re diving into a fascinating phrase that often pops up in English conversations and writings: <strong>de facto</strong>. Let’s explore its meaning, origins, and how to use it effectively in sentences. Whether you’re a language learner or a word nerd, you’ll find this exploration both enlightening and fun! ✨</p><h3 id="Understanding-“De-Facto”"><a href="#Understanding-“De-Facto”" class="headerlink" title="Understanding “De Facto”"></a>Understanding “De Facto”</h3><ol><li><p><strong>Meaning</strong>: The term ‘de facto’ is used to describe something that exists in reality, even if it’s not officially recognized or legally established. It’s like saying “in practice” or “in actuality,” as opposed to “in theory” or “officially.”</p></li><li><p><strong>Origins</strong>: This phrase has an interesting journey. It comes from Latin, where ‘de’ means ‘from’ and ‘facto’ means ‘fact.’ Over time, it’s been adopted into English, retaining its original essence from Latin.</p></li></ol><h3 id="Examples-in-Sentences"><a href="#Examples-in-Sentences" class="headerlink" title="Examples in Sentences"></a>Examples in Sentences</h3><ul><li>🌟 <em>In many organizations, there is a <strong>de facto</strong> leader who isn’t officially the boss but is respected and followed by the team.</em></li><li>🌟 <em>While English is the <strong>de facto</strong> language of international business, it’s not the official language in many countries where it’s widely spoken.</em></li><li>🌟 <em>The museum, though not formally recognized, acts as the <strong>de facto</strong> cultural center of the small town.</em></li></ul><h3 id="Wrapping-Up"><a href="#Wrapping-Up" class="headerlink" title="Wrapping Up"></a>Wrapping Up</h3><p>The term ‘de facto’ is a subtle but powerful way to describe the reality of a situation, distinguishing it from its official or legal status. Incorporating such phrases into your language arsenal not only enriches your vocabulary but also enhances your ability to express nuanced ideas. Keep exploring and embracing the beauty of language! 📚💬</p><p>Remember, language is a journey, not a destination. Happy learning, and see you in the next post! 🚀🌟</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Hello, language enthusiasts! 🌐 Today, we’re diving into a fascinating phrase that often pops up in English conversations and writings: &lt;</summary>
      
    
    
    
    <category term="Life Reflections" scheme="https://chenhuiyu.github.io/categories/Life-Reflections/"/>
    
    
    <category term="Language Learning" scheme="https://chenhuiyu.github.io/tags/Language-Learning/"/>
    
    <category term="English Vocabulary" scheme="https://chenhuiyu.github.io/tags/English-Vocabulary/"/>
    
  </entry>
  
  <entry>
    <title>英语学习日记：探索金融术语 &#39;Giro Date&#39;</title>
    <link href="https://chenhuiyu.github.io/2024/02/21/Life%20Reflections/%E8%8B%B1%E8%AF%AD%E5%AD%A6%E4%B9%A0%E6%97%A5%E8%AE%B0%EF%BC%9A%E6%8E%A2%E7%B4%A2%E9%87%91%E8%9E%8D%E6%9C%AF%E8%AF%AD%20&#39;Giro%20Date&#39;/"/>
    <id>https://chenhuiyu.github.io/2024/02/21/Life%20Reflections/%E8%8B%B1%E8%AF%AD%E5%AD%A6%E4%B9%A0%E6%97%A5%E8%AE%B0%EF%BC%9A%E6%8E%A2%E7%B4%A2%E9%87%91%E8%9E%8D%E6%9C%AF%E8%AF%AD%20&#39;Giro%20Date&#39;/</id>
    <published>2024-02-21T07:33:00.000Z</published>
    <updated>2024-02-21T07:34:04.140Z</updated>
    
    <content type="html"><![CDATA[<p>今天的英语学习之旅中，我遇到了一个有趣的金融术语：“Giro date”。这个探索过程不仅丰富了我的词汇，还加深了我对英语中金融概念的理解。让我和你分享一下我是如何分析这个术语及其在金融世界中的重要性。</p><h3 id="发现词源-🌍"><a href="#发现词源-🌍" class="headerlink" title="发现词源 🌍"></a>发现词源 🌍</h3><ul><li><strong>意大利语根源</strong>: 我的研究发现 ‘Giro’ 来自意大利语单词 “girare”，意味着转账或支付。</li><li><strong>金融语境</strong>: 在金融领域中，’giro’ 通常指的是通过银行或其他金融机构的转账。</li></ul><h3 id="学习其用法-💡"><a href="#学习其用法-💡" class="headerlink" title="学习其用法 💡"></a>学习其用法 💡</h3><ul><li><strong>定义</strong>: 在金融交易中，’Giro date’ 特指支付或结算日期。</li><li><strong>银行业重要性</strong>: 这是一个预定的日期，资金预期在此日期被支付或结算。</li></ul><h3 id="实际应用-📘"><a href="#实际应用-📘" class="headerlink" title="实际应用 📘"></a>实际应用 📘</h3><ul><li><strong>发票支付日期</strong>: “Please ensure that the giro date for the invoice is set to the 25th of this month.”（请确保发票的支付日期设为本月25日。）</li><li><strong>贷款还款日期</strong>: “The giro date for the loan repayment is automatically set for the 1st of each month.”（贷款还款的支付日期自动设定为每月的第一天。）</li></ul><p>这些例子帮助我巩固了对这个术语的理解，展示了它在银行业务、财务管理和账务处理中的用途。</p><h3 id="反思与前行-🌟"><a href="#反思与前行-🌟" class="headerlink" title="反思与前行 🌟"></a>反思与前行 🌟</h3><p>随我继续在英语学习的迷人世界中旅行，每一个术语都解锁了新的知识和理解！📚✨</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;今天的英语学习之旅中，我遇到了一个有趣的金融术语：“Giro date”。这个探索过程不仅丰富了我的词汇，还加深了我对英语中金融概念的理解。让我和你分享一下我是如何分析这个术语及其在金融世界中的重要性。&lt;/p&gt;
&lt;h3 id=&quot;发现词源-🌍&quot;&gt;&lt;a href=&quot;#发现词源</summary>
      
    
    
    
    <category term="Life Reflections" scheme="https://chenhuiyu.github.io/categories/Life-Reflections/"/>
    
    
    <category term="Language Learning" scheme="https://chenhuiyu.github.io/tags/Language-Learning/"/>
    
    <category term="English Vocabulary" scheme="https://chenhuiyu.github.io/tags/English-Vocabulary/"/>
    
  </entry>
  
  <entry>
    <title>English Learning Journey: Unraveling the Term &#39;Giro Date&#39;</title>
    <link href="https://chenhuiyu.github.io/2024/02/21/Life%20Reflections/English%20Learning%20Journey:%20Unraveling%20the%20Term%20&#39;Giro%20Date&#39;/"/>
    <id>https://chenhuiyu.github.io/2024/02/21/Life%20Reflections/English%20Learning%20Journey:%20Unraveling%20the%20Term%20&#39;Giro%20Date&#39;/</id>
    <published>2024-02-21T07:32:00.000Z</published>
    <updated>2024-02-21T10:47:41.097Z</updated>
    
    <content type="html"><![CDATA[<p>Today in my English learning journey, I encountered an intriguing financial term: “Giro date”. This exploration not only expanded my vocabulary but also deepened my understanding of financial concepts in English. Let me share with you how I dissected this term and its relevance in the financial world.</p><h3 id="Discovering-the-Origin-🌍"><a href="#Discovering-the-Origin-🌍" class="headerlink" title="Discovering the Origin 🌍"></a>Discovering the Origin 🌍</h3><ul><li><strong>Italian Roots</strong>: My research revealed that ‘Giro’ originates from the Italian word “girare,” meaning to transfer or pay. </li><li><strong>Financial Context</strong>: In the realm of finance, ‘giro’ typically refers to a bank or institutional transfer.</li></ul><h3 id="Learning-the-Usage-💡"><a href="#Learning-the-Usage-💡" class="headerlink" title="Learning the Usage 💡"></a>Learning the Usage 💡</h3><ul><li><strong>Definition</strong>: In financial transactions, ‘Giro date’ specifically denotes the payment or settlement date.</li><li><strong>Banking Significance</strong>: It’s a predetermined date when funds are expected to be paid or settled.</li></ul><h3 id="Applying-it-in-Context-📘"><a href="#Applying-it-in-Context-📘" class="headerlink" title="Applying it in Context 📘"></a>Applying it in Context 📘</h3><ul><li><strong>Invoice Payment Date</strong>: I practiced using the term in a sentence: “Please ensure that the giro date for the invoice is set to the 25th of this month.”</li><li><strong>Loan Repayment Date</strong>: Another example I came up with was, “The giro date for the loan repayment is automatically set for the 1st of each month.”</li></ul><p>These examples helped cement the term in my mind, illustrating its use in banking, financial management, and account processing.</p><h3 id="Reflections-and-Forward-Steps-🌟"><a href="#Reflections-and-Forward-Steps-🌟" class="headerlink" title="Reflections and Forward Steps 🌟"></a>Reflections and Forward Steps 🌟</h3><p>Understanding ‘Giro date’ was not just about adding a new word to my vocabulary; it was about comprehending a concept that plays a vital role in financial transactions. This learning experience has made me appreciate the nuances of financial English and motivated me to delve deeper into industry-specific terminology.</p><p>Join me as I continue my journey through the fascinating world of English language learning, where every term unlocks new knowledge and understanding! 📚✨</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Today in my English learning journey, I encountered an intriguing financial term: “Giro date”. This exploration not only expanded my voca</summary>
      
    
    
    
    <category term="Life Reflections" scheme="https://chenhuiyu.github.io/categories/Life-Reflections/"/>
    
    
    <category term="Language Learning" scheme="https://chenhuiyu.github.io/tags/Language-Learning/"/>
    
    <category term="English Vocabulary" scheme="https://chenhuiyu.github.io/tags/English-Vocabulary/"/>
    
  </entry>
  
  <entry>
    <title>How to Resolve SSH Key Issues with Multiple Git Services</title>
    <link href="https://chenhuiyu.github.io/2024/02/19/Debugging%20Diaries/How%20to%20Resolve%20SSH%20Key%20Issues%20with%20Multiple%20Git%20Services/"/>
    <id>https://chenhuiyu.github.io/2024/02/19/Debugging%20Diaries/How%20to%20Resolve%20SSH%20Key%20Issues%20with%20Multiple%20Git%20Services/</id>
    <published>2024-02-19T07:49:55.000Z</published>
    <updated>2024-02-27T02:34:40.172Z</updated>
    
    <content type="html"><![CDATA[<h1 id="How-to-Resolve-SSH-Key-Issues-with-Multiple-Git-Services-🗝️"><a href="#How-to-Resolve-SSH-Key-Issues-with-Multiple-Git-Services-🗝️" class="headerlink" title="How to Resolve SSH Key Issues with Multiple Git Services 🗝️"></a>How to Resolve SSH Key Issues with Multiple Git Services 🗝️</h1><p>When using Git with different Git services such as GitHub and GitLab, you may encounter SSH key issues. This article will guide you on how to set up and configure SSH keys so that you can work smoothly with multiple services simultaneously.</p><h2 id="1-Generate-SSH-Keys-🔑"><a href="#1-Generate-SSH-Keys-🔑" class="headerlink" title="1. Generate SSH Keys 🔑"></a>1. Generate SSH Keys 🔑</h2><p>First, generate a separate SSH key for each Git service.</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -b 4096 -C <span class="string">"your_email@example.com"</span></span><br></pre></td></tr></tbody></table></figure><p>When generating the keys, save each key with a different filename, for example, <code>id_rsa_github</code> and <code>id_rsa_gitlab</code>.</p><h2 id="2-Add-SSH-Keys-to-Git-Services-🌐"><a href="#2-Add-SSH-Keys-to-Git-Services-🌐" class="headerlink" title="2. Add SSH Keys to Git Services 🌐"></a>2. Add SSH Keys to Git Services 🌐</h2><p>Log in to your GitHub and GitLab accounts, then add the generated public keys (<code>.pub</code> files) to the SSH key sections of each respective account.</p><h2 id="3-Configure-SSH-⚙️"><a href="#3-Configure-SSH-⚙️" class="headerlink" title="3. Configure SSH ⚙️"></a>3. Configure SSH ⚙️</h2><p>Create or edit the <code>~/.ssh/config</code> file to configure different SSH keys for each service.</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GitHub</span></span><br><span class="line">Host github.com</span><br><span class="line">  HostName github.com</span><br><span class="line">  User git</span><br><span class="line">  IdentityFile ~/.ssh/id_rsa_github</span><br><span class="line"></span><br><span class="line"><span class="comment"># GitLab</span></span><br><span class="line">Host gitlab.com</span><br><span class="line">  HostName gitlab.com</span><br><span class="line">  User git</span><br><span class="line">  IdentityFile ~/.ssh/id_rsa_gitlab</span><br></pre></td></tr></tbody></table></figure><p>If your company uses a custom GitLab instance, add a separate configuration block for it.</p><h2 id="4-Test-SSH-Connections-🧪"><a href="#4-Test-SSH-Connections-🧪" class="headerlink" title="4. Test SSH Connections 🧪"></a>4. Test SSH Connections 🧪</h2><p>Test if you can successfully connect to each service via SSH.</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ssh -T git@github.com</span><br><span class="line">ssh -T git@gitlab.com</span><br></pre></td></tr></tbody></table></figure><h2 id="5-Handling-Common-Errors-❗"><a href="#5-Handling-Common-Errors-❗" class="headerlink" title="5. Handling Common Errors ❗"></a>5. Handling Common Errors ❗</h2><p>If you encounter errors such as “Permission denied (publickey)”, check the following:</p><ul><li>Ensure SSH keys are correctly added to the respective Git services.</li><li>Verify if the <code>~/.ssh/config</code> file is configured correctly.</li><li>Use the <code>ssh-add</code> command to ensure SSH keys are loaded into the SSH Agent.</li></ul><h2 id="6-Common-Issues-and-Solutions-💡"><a href="#6-Common-Issues-and-Solutions-💡" class="headerlink" title="6. Common Issues and Solutions 💡"></a>6. Common Issues and Solutions 💡</h2><ul><li><strong>Multiple GitLab Instances</strong>: If you use a custom GitLab instance along with GitLab.com, ensure they are separately configured in the SSH <code>config</code> file.</li><li><strong>Network Issues</strong>: Check if any network settings (like proxies, VPNs) might affect SSH connections.</li></ul><hr><p>By following the above steps, you should be able to resolve most SSH key-related issues, especially when dealing with multiple Git services. If you have further questions or specific scenarios, feel free to ask in the comments. 🚀✨</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;How-to-Resolve-SSH-Key-Issues-with-Multiple-Git-Services-🗝️&quot;&gt;&lt;a href=&quot;#How-to-Resolve-SSH-Key-Issues-with-Multiple-Git-Services-🗝️</summary>
      
    
    
    
    <category term="Debugging Diaries" scheme="https://chenhuiyu.github.io/categories/Debugging-Diaries/"/>
    
    
    <category term="IssueFix" scheme="https://chenhuiyu.github.io/tags/IssueFix/"/>
    
  </entry>
  
  <entry>
    <title>如何解决多个 Git 服务的 SSH 密钥问题</title>
    <link href="https://chenhuiyu.github.io/2024/02/19/Debugging%20Diaries/%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E5%A4%9A%E4%B8%AA%20Git%20%E6%9C%8D%E5%8A%A1%E7%9A%84%20SSH%20%E5%AF%86%E9%92%A5%E9%97%AE%E9%A2%98/"/>
    <id>https://chenhuiyu.github.io/2024/02/19/Debugging%20Diaries/%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E5%A4%9A%E4%B8%AA%20Git%20%E6%9C%8D%E5%8A%A1%E7%9A%84%20SSH%20%E5%AF%86%E9%92%A5%E9%97%AE%E9%A2%98/</id>
    <published>2024-02-19T07:47:55.000Z</published>
    <updated>2024-02-27T02:34:39.008Z</updated>
    
    <content type="html"><![CDATA[<h1 id="如何解决多个-Git-服务的-SSH-密钥问题-🗝️"><a href="#如何解决多个-Git-服务的-SSH-密钥问题-🗝️" class="headerlink" title="如何解决多个 Git 服务的 SSH 密钥问题 🗝️"></a>如何解决多个 Git 服务的 SSH 密钥问题 🗝️</h1><p>在使用 Git 和不同的 Git 服务（如 GitHub 和 GitLab）时，可能会遇到 SSH 密钥的问题。本文将指导你如何设置和配置 SSH 密钥，以便可以同时与多个服务顺利工作。</p><h2 id="1-生成-SSH-密钥-🔑"><a href="#1-生成-SSH-密钥-🔑" class="headerlink" title="1. 生成 SSH 密钥 🔑"></a>1. 生成 SSH 密钥 🔑</h2><p>首先，为每个 Git 服务生成一个独立的 SSH 密钥。</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa -b 4096 -C <span class="string">"your_email@example.com"</span></span><br></pre></td></tr></tbody></table></figure><p>在生成密钥时，将每个密钥保存为不同的文件名，例如 <code>id_rsa_github</code> 和 <code>id_rsa_gitlab</code>。</p><h2 id="2-将-SSH-密钥添加到-Git-服务-🌐"><a href="#2-将-SSH-密钥添加到-Git-服务-🌐" class="headerlink" title="2. 将 SSH 密钥添加到 Git 服务 🌐"></a>2. 将 SSH 密钥添加到 Git 服务 🌐</h2><p>登录到你的 GitHub 和 GitLab 账户，然后将生成的公钥（<code>.pub</code> 文件）添加到各自账户的 SSH 密钥部分。</p><h2 id="3-配置-SSH-⚙️"><a href="#3-配置-SSH-⚙️" class="headerlink" title="3. 配置 SSH ⚙️"></a>3. 配置 SSH ⚙️</h2><p>创建或编辑 <code>~/.ssh/config</code> 文件，为每个服务配置不同的 SSH 密钥。</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GitHub</span></span><br><span class="line">Host github.com</span><br><span class="line">  HostName github.com</span><br><span class="line">  User git</span><br><span class="line">  IdentityFile ~/.ssh/id_rsa_github</span><br><span class="line"></span><br><span class="line"><span class="comment"># GitLab</span></span><br><span class="line">Host gitlab.com</span><br><span class="line">  HostName gitlab.com</span><br><span class="line">  User git</span><br><span class="line">  IdentityFile ~/.ssh/id_rsa_gitlab</span><br></pre></td></tr></tbody></table></figure><p>如果你的公司使用自定义的 GitLab 实例，请为其添加一个单独的配置块。</p><h2 id="4-测试-SSH-连接-🧪"><a href="#4-测试-SSH-连接-🧪" class="headerlink" title="4. 测试 SSH 连接 🧪"></a>4. 测试 SSH 连接 🧪</h2><p>测试是否能成功通过 SSH 连接到每个服务。</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ssh -T git@github.com</span><br><span class="line">ssh -T git@gitlab.com</span><br></pre></td></tr></tbody></table></figure><h2 id="5-处理常见错误-❗"><a href="#5-处理常见错误-❗" class="headerlink" title="5. 处理常见错误 ❗"></a>5. 处理常见错误 ❗</h2><p>如果遇到错误，如 “Permission denied (publickey)”，请检查以下几点：</p><ul><li>确认 SSH 密钥是否已正确添加到相应的 Git 服务。</li><li>检查 <code>~/.ssh/config</code> 文件是否正确配置。</li><li>使用 <code>ssh-add</code> 命令确保 SSH 密钥已加载到 SSH Agent。</li></ul><h2 id="6-常见问题和解决方案-💡"><a href="#6-常见问题和解决方案-💡" class="headerlink" title="6. 常见问题和解决方案 💡"></a>6. 常见问题和解决方案 💡</h2><ul><li><strong>多个 GitLab 实例</strong>：如果你使用了公司的自定义 GitLab 实例和 GitLab.com，请确保 SSH <code>config</code> 文件中它们的配置是分开的。</li><li><strong>网络问题</strong>：检查是否有网络设置（如代理、VPN）可能影响 SSH 连接。</li></ul><hr><p>通过遵循以上步骤，你应该能够解决大部分与 SSH 密钥相关的问题，特别是在处理多个 Git 服务时。如果有进一步的问题或者特殊情况，欢迎在评论中提出。 🚀✨</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;如何解决多个-Git-服务的-SSH-密钥问题-🗝️&quot;&gt;&lt;a href=&quot;#如何解决多个-Git-服务的-SSH-密钥问题-🗝️&quot; class=&quot;headerlink&quot; title=&quot;如何解决多个 Git 服务的 SSH 密钥问题 🗝️&quot;&gt;&lt;/a&gt;如何解决多</summary>
      
    
    
    
    <category term="Debugging Diaries" scheme="https://chenhuiyu.github.io/categories/Debugging-Diaries/"/>
    
    
    <category term="IssueFix" scheme="https://chenhuiyu.github.io/tags/IssueFix/"/>
    
  </entry>
  
  <entry>
    <title>理解大型语言模型中Fine-tuning和Further Pretraining的区别</title>
    <link href="https://chenhuiyu.github.io/2024/02/19/NLP%20Insights/%E7%90%86%E8%A7%A3%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%ADFine-tuning%E5%92%8CFurther%20Pretraining%E7%9A%84%E5%8C%BA%E5%88%AB/"/>
    <id>https://chenhuiyu.github.io/2024/02/19/NLP%20Insights/%E7%90%86%E8%A7%A3%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%ADFine-tuning%E5%92%8CFurther%20Pretraining%E7%9A%84%E5%8C%BA%E5%88%AB/</id>
    <published>2024-02-19T07:10:29.000Z</published>
    <updated>2024-02-19T08:56:59.892Z</updated>
    
    <content type="html"><![CDATA[<h1 id="理解大型语言模型中-Fine-tuning-和-Further-Pretraining-的区别"><a href="#理解大型语言模型中-Fine-tuning-和-Further-Pretraining-的区别" class="headerlink" title="理解大型语言模型中 Fine-tuning 和 Further Pretraining 的区别"></a>理解大型语言模型中 Fine-tuning 和 Further Pretraining 的区别</h1><p>在自然语言处理（NLP）领域，大型语言模型，如 GPT 和 BERT 的出现，彻底改变了我们处理文本分类、情感分析和问答等任务的方式。在这些模型的应用中，Fine-tuning（微调）和 Further Pretraining（进一步预训练）是两种关键技术。虽然它们看起来相似，但实际上服务于 NLP 流程中的不同需求和场景。</p><h2 id="什么是-Fine-tuning？"><a href="#什么是-Fine-tuning？" class="headerlink" title="什么是 Fine-tuning？"></a>什么是 Fine-tuning？</h2><p>Fine-tuning 是指在特定任务的数据集上进一步训练（或“微调”）一个预训练好的模型的过程。这种方法在数据集相对较小但标注良好的情况下特别有效。</p><h3 id="示例场景：情感分析"><a href="#示例场景：情感分析" class="headerlink" title="示例场景：情感分析"></a>示例场景：情感分析</h3><p>假设你有一组电影评论数据，每条评论都标记了正面或负面情感。你想创建一个模型来预测评论的情感。</p><h4 id="Python-代码示例（使用-PyTorch-和-HuggingFace-的-Transformers）"><a href="#Python-代码示例（使用-PyTorch-和-HuggingFace-的-Transformers）" class="headerlink" title="Python 代码示例（使用 PyTorch 和 HuggingFace 的 Transformers）"></a>Python 代码示例（使用 PyTorch 和 HuggingFace 的 Transformers）</h4><p>This notebook demonstrates the fine-tuning of a BERT model on the IMDB dataset for sentiment analysis. For detailed code implementation, please refer to the following link:<a href="https://colab.research.google.com/drive/15naxP8pNMoCCBMgMSOv4ETDRGL46YR38?usp=sharing">link</a>.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> (</span><br><span class="line">    BertTokenizer,</span><br><span class="line">    BertForSequenceClassification,</span><br><span class="line">    Trainer,</span><br><span class="line">    TrainingArguments,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset, DatasetDict</span><br><span class="line"></span><br><span class="line"><span class="string">"""## 1.加载和准备IMDB数据集样本</span></span><br><span class="line"><span class="string">选取一部分数据用于Fine-tuning。</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载IMDB数据集</span></span><br><span class="line">dataset = load_dataset(<span class="string">'imdb'</span>, split=<span class="string">'train'</span>)</span><br><span class="line">small_dataset = dataset.shuffle(seed=<span class="number">42</span>).select(<span class="built_in">range</span>(<span class="number">10000</span>))  <span class="comment"># 选取前10000个样本</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化tokenizer</span></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">"bert-base-uncased"</span>)</span><br><span class="line"></span><br><span class="line">device = <span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 编码数据集</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">examples</span>):</span><br><span class="line">    <span class="keyword">return</span> tokenizer(examples[<span class="string">"text"</span>], padding=<span class="string">"max_length"</span>, truncation=<span class="literal">True</span>, max_length=<span class="number">512</span>)</span><br><span class="line"></span><br><span class="line">encoded_small_dataset = small_dataset.<span class="built_in">map</span>(encode, batched=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertModel</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">visualize_attention</span>(<span class="params">sentence, model, tokenizer</span>):</span><br><span class="line">    model.to(device)</span><br><span class="line">    <span class="comment"># 将模型设置为评估模式</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将输入文本转换为模型可以理解的形式</span></span><br><span class="line">    inputs = tokenizer(sentence, return_tensors=<span class="string">"pt"</span>).to(device) <span class="comment"># 确保输入也在相同设备</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用模型获取注意力权重</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        outputs = model(**inputs)</span><br><span class="line">    attentions = outputs.attentions</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 选择要可视化的层和头</span></span><br><span class="line">    layer = <span class="number">5</span></span><br><span class="line">    head = <span class="number">1</span></span><br><span class="line">    attention = attentions[layer][<span class="number">0</span>, head].cpu().numpy() <span class="comment"># 将注意力权重移回CPU进行可视化</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 设置可视化的tokens</span></span><br><span class="line">    tokens = tokenizer.convert_ids_to_tokens(inputs[<span class="string">"input_ids"</span>][<span class="number">0</span>].cpu()) <span class="comment"># 同样确保tokens在CPU上</span></span><br><span class="line">    <span class="comment"># 绘制注意力矩阵</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">10</span>, <span class="number">10</span>))</span><br><span class="line">    plt.matshow(attention, cmap=<span class="string">'viridis'</span>)</span><br><span class="line">    plt.xticks(<span class="built_in">range</span>(<span class="built_in">len</span>(tokens)), tokens, rotation=<span class="number">90</span>)</span><br><span class="line">    plt.yticks(<span class="built_in">range</span>(<span class="built_in">len</span>(tokens)), tokens)</span><br><span class="line">    plt.colorbar()</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="string">"""## 2. 可视化一个样本句子的注意力权重（未经Fine-tuning）</span></span><br><span class="line"><span class="string">选择数据集中的一个句子并展示其原始BERT模型的注意力权重。</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用未经Fine-tuning的模型</span></span><br><span class="line">model = BertModel.from_pretrained(<span class="string">"bert-base-uncased"</span>, output_attentions=<span class="literal">True</span>)</span><br><span class="line">sample_sentence = <span class="string">"I love this movie, it's fantastic!"</span></span><br><span class="line">visualize_attention(sample_sentence, model, tokenizer)</span><br><span class="line"></span><br><span class="line"><span class="string">"""## 3. Fine-tuning BERT模型</span></span><br><span class="line"><span class="string">在选取的IMDB样本上进行Fine-tuning。</span></span><br><span class="line"><span class="string">### 3.1 准备数据加载器</span></span><br><span class="line"><span class="string">为了训练模型，我们需要创建PyTorch的DataLoader。这将使我们能够在训练过程中有效地加载数据。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">### 3.2 设置Fine-tuning环境</span></span><br><span class="line"><span class="string">初始化模型、优化器以及损失函数。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">### 3.3 Fine-tuning模型</span></span><br><span class="line"><span class="string">执行Fine-tuning的训练循环。执行以上代码将在IMDB数据集的小样本上对BERT模型进行Fine-tuning。这可能需要一些时间，具体取决于您的硬件配置。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据集转换为PyTorch Tensor</span></span><br><span class="line">encoded_small_dataset.set_format(<span class="string">'torch'</span>, columns=[<span class="string">'input_ids'</span>, <span class="string">'attention_mask'</span>, <span class="string">'label'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建数据加载器</span></span><br><span class="line">train_loader = DataLoader(encoded_small_dataset, batch_size=<span class="number">8</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertConfig, BertForSequenceClassification</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载配置并设置输出注意力权重</span></span><br><span class="line">config = BertConfig.from_pretrained(<span class="string">"bert-base-uncased"</span>, output_attentions=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化用于序列分类的BERT模型</span></span><br><span class="line"><span class="comment"># 使用更新后的配置加载模型</span></span><br><span class="line">model = BertForSequenceClassification.from_pretrained(<span class="string">"bert-base-uncased"</span>, config=config)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置优化器</span></span><br><span class="line">optimizer = optim.AdamW(model.parameters(), lr=<span class="number">5e-5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用交叉熵损失函数</span></span><br><span class="line">criterion = torch.nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置训练的轮次</span></span><br><span class="line">epochs = <span class="number">8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    model.train()</span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> train_loader:</span><br><span class="line">        <span class="comment"># 将数据移至GPU</span></span><br><span class="line">        input_ids = batch[<span class="string">'input_ids'</span>].to(device)</span><br><span class="line">        attention_mask = batch[<span class="string">'attention_mask'</span>].to(device)</span><br><span class="line">        labels = batch[<span class="string">'label'</span>].to(device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 模型前向传播</span></span><br><span class="line">        outputs = model(input_ids, attention_mask=attention_mask)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算损失</span></span><br><span class="line">        loss = criterion(outputs.logits, labels)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 反向传播和优化</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        total_loss += loss.item()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"Epoch: <span class="subst">{epoch+<span class="number">1</span>}</span>, Loss: <span class="subst">{total_loss/<span class="built_in">len</span>(train_loader)}</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">"""### 4. 可视化同一句子的注意力权重（经过Fine-tuning）</span></span><br><span class="line"><span class="string">使用Fine-tuning后的模型再次可视化同一句子的注意力权重。您可以重用之前提供的visualize_attention函数：</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">visualize_attention(sample_sentence, model, tokenizer)</span><br></pre></td></tr></tbody></table></figure><p>在这个例子中，BERT 模型在电影评论数据集上进行了 fine-tuning，用于情感分析。</p><h2 id="什么是-Further-Pretraining？"><a href="#什么是-Further-Pretraining？" class="headerlink" title="什么是 Further Pretraining？"></a>什么是 Further Pretraining？</h2><p>Further Pretraining（也称为 Domain-adaptive Pretraining，领域适应性预训练）是在一个新的数据集上继续训练一个预训练模型的过程，这个新的数据集与特定的领域更相关，但不一定为特定任务标注。</p><h3 id="示例场景：法律文档分析"><a href="#示例场景：法律文档分析" class="headerlink" title="示例场景：法律文档分析"></a>示例场景：法律文档分析</h3><p>假设你正在处理法律文档，并希望利用一个在通用文本上训练的语言模型。</p><h4 id="Further-Pretraining-的代码示例"><a href="#Further-Pretraining-的代码示例" class="headerlink" title="Further Pretraining 的代码示例"></a>Further Pretraining 的代码示例</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertModel, BertTokenizer</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载预训练的BERT模型和分词器</span></span><br><span class="line">model = BertModel.from_pretrained(<span class="string">'bert-base-uncased'</span>)</span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">'bert-base-uncased'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备法律文档数据集</span></span><br><span class="line"><span class="comment"># 假设'legal_documents'是法律文档的文本列表</span></span><br><span class="line">encoded_input = tokenizer(legal_documents, padding=<span class="literal">True</span>, truncation=<span class="literal">True</span>, max_length=<span class="number">512</span>, return_tensors=<span class="string">'pt'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 继续预训练模型</span></span><br><span class="line"><span class="comment"># 这一步通常包括掩码语言建模或其他预训练目标</span></span><br><span class="line"><span class="comment"># 这里提供一个概念性示例</span></span><br><span class="line">model.train()</span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> encoded_input:</span><br><span class="line">    outputs = model(**batch)</span><br><span class="line">    <span class="comment"># ... 执行进一步训练步骤</span></span><br></pre></td></tr></tbody></table></figure><p>在这个例子中，BERT 模型在法律文档数据集上进行了进一步的预训练，使其在进行特定法律 NLP 任务的 fine-tuning 之前，更擅长理解法律术语和概念。</p><h2 id="关键区别"><a href="#关键区别" class="headerlink" title="关键区别"></a>关键区别</h2><ul><li><strong>目的</strong>：Fine-tuning 针对具有标签数据的特定任务进行模型调整，而 Further Pretraining 则是使模型更好地适应特定领域或语言风格。</li><li><strong>数据集</strong>：Fine-tuning 使用特定任务的标注数据集。Further Pretraining 使用更大的、特定领域的数据集，这些数据集可能不是为特定任务标注的。</li><li><strong>训练目标</strong>：Fine-tuning 涉及调整模型进行特定预测，而 Further Pretraining 侧重于在新领域中的通用语言理解</li></ul><p>。</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>Fine-tuning 和 Further Pretraining 都是 NLP 领域的强大技术。通过理解它们的区别和应用，我们可以更好地利用大型语言模型来解决各种领域中的多样化和复杂任务。无论你是在构建社交媒体帖子的情感分析模型，还是调整模型以理解法律文档，这些技术都为 NLP 领域的不断发展提供了稳健的解决方案。</p><hr><p><strong>注意</strong>：提供的代码示例是概念性的，需要适当的环境设置，包括必要的库和数据集，才能执行。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;理解大型语言模型中-Fine-tuning-和-Further-Pretraining-的区别&quot;&gt;&lt;a href=&quot;#理解大型语言模型中-Fine-tuning-和-Further-Pretraining-的区别&quot; class=&quot;headerlink&quot; title</summary>
      
    
    
    
    <category term="NLP Insights" scheme="https://chenhuiyu.github.io/categories/NLP-Insights/"/>
    
    
    <category term="LLM" scheme="https://chenhuiyu.github.io/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>Understanding the Differences Between Fine-tuning and Further Pretraining in Large Language Models</title>
    <link href="https://chenhuiyu.github.io/2024/02/19/NLP%20Insights/Understanding%20the%20Differences%20Between%20Fine-tuning%20and%20Further%20Pretraining%20in%20Large%20Language%20Models/"/>
    <id>https://chenhuiyu.github.io/2024/02/19/NLP%20Insights/Understanding%20the%20Differences%20Between%20Fine-tuning%20and%20Further%20Pretraining%20in%20Large%20Language%20Models/</id>
    <published>2024-02-19T07:06:29.000Z</published>
    <updated>2024-02-19T08:53:58.806Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Understanding-the-Differences-Between-Fine-tuning-and-Further-Pretraining-in-Large-Language-Models"><a href="#Understanding-the-Differences-Between-Fine-tuning-and-Further-Pretraining-in-Large-Language-Models" class="headerlink" title="Understanding the Differences Between Fine-tuning and Further Pretraining in Large Language Models"></a>Understanding the Differences Between Fine-tuning and Further Pretraining in Large Language Models</h1><p>In the world of Natural Language Processing (NLP), the advent of large language models like GPT and BERT has revolutionized how we approach tasks such as text classification, sentiment analysis, and question-answering. Two pivotal techniques in leveraging these models are Fine-tuning and Further Pretraining. While they may seem similar at a glance, they cater to different needs and scenarios in the NLP pipeline.</p><h2 id="What-is-Fine-tuning"><a href="#What-is-Fine-tuning" class="headerlink" title="What is Fine-tuning?"></a>What is Fine-tuning?</h2><p>Fine-tuning is a process where a pretrained model is further trained (or ‘fine-tuned’) on a specific task with a dataset corresponding to that task. This approach is particularly effective when the dataset is relatively small but well-labeled.</p><h3 id="Example-Scenario-Sentiment-Analysis"><a href="#Example-Scenario-Sentiment-Analysis" class="headerlink" title="Example Scenario: Sentiment Analysis"></a>Example Scenario: Sentiment Analysis</h3><p>Imagine you have a dataset of movie reviews, each labeled as positive or negative. You want to create a model that can predict the sentiment of a review.</p><h4 id="Code-Snippet-in-Python-using-PyTorch-and-HuggingFace’s-Transformers"><a href="#Code-Snippet-in-Python-using-PyTorch-and-HuggingFace’s-Transformers" class="headerlink" title="Code Snippet in Python (using PyTorch and HuggingFace’s Transformers)"></a>Code Snippet in Python (using PyTorch and HuggingFace’s Transformers)</h4><p>This notebook demonstrates the fine-tuning of a BERT model on the IMDB dataset for sentiment analysis. For detailed code implementation, please refer to the following link:<a href="https://colab.research.google.com/drive/15naxP8pNMoCCBMgMSOv4ETDRGL46YR38?usp=sharing">link</a>.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br></pre></td><td class="code"><pre><span class="line">ls -al ~/.ssh</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> (</span><br><span class="line">    BertTokenizer,</span><br><span class="line">    BertForSequenceClassification,</span><br><span class="line">    Trainer,</span><br><span class="line">    TrainingArguments,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset, DatasetDict</span><br><span class="line"></span><br><span class="line"><span class="string">"""## 1. Load and Prepare IMDB Dataset Samples</span></span><br><span class="line"><span class="string">Select a portion of the data for Fine-tuning.</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Load IMDB dataset</span></span><br><span class="line">dataset = load_dataset(<span class="string">'imdb'</span>, split=<span class="string">'train'</span>)</span><br><span class="line">small_dataset = dataset.shuffle(seed=<span class="number">42</span>).select(<span class="built_in">range</span>(<span class="number">10000</span>))  <span class="comment"># Selecting the first 10,000 samples</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize tokenizer</span></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">"bert-base-uncased"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Encode the dataset</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">examples</span>):</span><br><span class="line">    <span class="keyword">return</span> tokenizer(examples[<span class="string">"text"</span>], padding=<span class="string">"max_length"</span>, truncation=<span class="literal">True</span>, max_length=<span class="number">512</span>)</span><br><span class="line"></span><br><span class="line">encoded_small_dataset = small_dataset.<span class="built_in">map</span>(encode, batched=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertModel</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">visualize_attention</span>(<span class="params">sentence, model, tokenizer</span>):</span><br><span class="line">    <span class="comment"># Set the model to evaluation mode</span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Convert the input text into a format understandable by the model</span></span><br><span class="line">    inputs = tokenizer(sentence, return_tensors=<span class="string">"pt"</span>).to(device) <span class="comment"># Making sure inputs are on the same device</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Get attention weights using the model</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        outputs = model(**inputs)</span><br><span class="line">    attentions = outputs.attentions</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Choose the layer and head to visualize</span></span><br><span class="line">    layer = <span class="number">5</span></span><br><span class="line">    head = <span class="number">1</span></span><br><span class="line">    attention = attentions[layer][<span class="number">0</span>, head].cpu().numpy() <span class="comment"># Move attention weights back to CPU for visualization</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Set tokens for visualization</span></span><br><span class="line">    tokens = tokenizer.convert_ids_to_tokens(inputs[<span class="string">"input_ids"</span>][<span class="number">0</span>].cpu()) <span class="comment"># Also make sure tokens are on CPU</span></span><br><span class="line">    <span class="comment"># Plot attention matrix</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">10</span>, <span class="number">10</span>))</span><br><span class="line">    plt.matshow(attention, cmap=<span class="string">'viridis'</span>)</span><br><span class="line">    plt.xticks(<span class="built_in">range</span>(<span class="built_in">len</span>(tokens)), tokens, rotation=<span class="number">90</span>)</span><br><span class="line">    plt.yticks(<span class="built_in">range</span>(<span class="built_in">len</span>(tokens)), tokens)</span><br><span class="line">    plt.colorbar()</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="string">"""## 2. Visualize Attention Weights of a Sample Sentence (Before Fine-tuning)</span></span><br><span class="line"><span class="string">Select a sentence from the dataset and visualize the attention weights of the original BERT model.</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Use the model without Fine-tuning</span></span><br><span class="line">model = BertModel.from_pretrained(<span class="string">"bert-base-uncased"</span>, output_attentions=<span class="literal">True</span>)</span><br><span class="line">sample_sentence = <span class="string">"I love this movie, it's fantastic!"</span></span><br><span class="line">visualize_attention(sample_sentence, model, tokenizer)</span><br><span class="line"></span><br><span class="line"><span class="string">"""## 3. Fine-tuning the BERT Model</span></span><br><span class="line"><span class="string">Perform Fine-tuning on the selected IMDB samples.</span></span><br><span class="line"><span class="string">### 3.1 Prepare Data Loaders</span></span><br><span class="line"><span class="string">To train the model, we need to create PyTorch's DataLoader. This will allow us to efficiently load data during training.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">### 3.2 Set up Fine-tuning Environment</span></span><br><span class="line"><span class="string">Initialize the model, optimizer, and loss function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">### 3.3 Fine-tuning the Model</span></span><br><span class="line"><span class="string">Execute the training loop for Fine-tuning. Running the above code will Fine-tune the BERT model on a small sample of the IMDB dataset. This may take some time depending on your hardware configuration.</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line"><span class="comment"># Convert the dataset to PyTorch Tensor</span></span><br><span class="line">encoded_small_dataset.set_format(<span class="string">'torch'</span>, columns=[<span class="string">'input_ids'</span>, <span class="string">'attention_mask'</span>, <span class="string">'label'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create data loader</span></span><br><span class="line">train_loader = DataLoader(encoded_small_dataset, batch_size=<span class="number">8</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertConfig, BertForSequenceClassification</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load the configuration and set output attention weights</span></span><br><span class="line">config = BertConfig.from_pretrained(<span class="string">"bert-base-uncased"</span>, output_attentions=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize the BERT model for sequence classification</span></span><br><span class="line"><span class="comment"># Load the model with updated configuration</span></span><br><span class="line">model = BertForSequenceClassification.from_pretrained(<span class="string">"bert-base-uncased"</span>, config=config)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Set up the optimizer</span></span><br><span class="line">optimizer = optim.AdamW(model.parameters(), lr=<span class="number">5e-5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use the cross-entropy loss function</span></span><br><span class="line">criterion = torch.nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">device = <span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span></span><br><span class="line">model.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the number of epochs for training</span></span><br><span class="line">epochs = <span class="number">8</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    model.train()</span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> train_loader:</span><br><span class="line">        <span class="comment"># Move the data to GPU</span></span><br><span class="line">        input_ids = batch[<span class="string">'input_ids'</span>].to(device)</span><br><span class="line">        attention_mask = batch[<span class="string">'attention_mask'</span>].to(device)</span><br><span class="line">        labels = batch[<span class="string">'label'</span>].to(device)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Model forward pass</span></span><br><span class="line">        outputs = model(input_ids, attention_mask=attention_mask)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Compute the loss</span></span><br><span class="line">        loss = criterion(outputs.logits, labels)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Backpropagation and optimization</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        total_loss += loss.item()</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"Epoch: <span class="subst">{epoch+<span class="number">1</span>}</span>, Loss: <span class="subst">{total_loss/<span class="built_in">len</span>(train_loader)}</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">"""### 4. Visualize Attention Weights of the Same Sentence (After Fine-tuning)</span></span><br><span class="line"><span class="string">Visualize the attention weights of the same sentence using the model after Fine-tuning. You can reuse the visualize_attention function provided earlier:</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">visualize_attention(sample_sentence, model, tokenizer)</span><br></pre></td></tr></tbody></table></figure><p>In this example, the BERT model is fine-tuned on the movie reviews dataset for sentiment analysis.</p><h2 id="What-is-Further-Pretraining"><a href="#What-is-Further-Pretraining" class="headerlink" title="What is Further Pretraining?"></a>What is Further Pretraining?</h2><p>Further Pretraining, also known as Domain-adaptive Pretraining, is where a pretrained model is further trained on a new dataset that is more closely related to the specific domain of interest but not necessarily labeled for a specific task.</p><h3 id="Example-Scenario-Legal-Document-Analysis"><a href="#Example-Scenario-Legal-Document-Analysis" class="headerlink" title="Example Scenario: Legal Document Analysis"></a>Example Scenario: Legal Document Analysis</h3><p>Suppose you’re working on legal documents and wish to leverage a language model trained on general texts.</p><h4 id="Code-Snippet-for-Further-Pretraining"><a href="#Code-Snippet-for-Further-Pretraining" class="headerlink" title="Code Snippet for Further Pretraining"></a>Code Snippet for Further Pretraining</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertModel, BertTokenizer</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load a pre-trained BERT model and tokenizer</span></span><br><span class="line">model = BertModel.from_pretrained(<span class="string">'bert-base-uncased'</span>)</span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">'bert-base-uncased'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Prepare the legal documents dataset</span></span><br><span class="line"><span class="comment"># Assume 'legal_documents' is a list of text from legal documents</span></span><br><span class="line">encoded_input = tokenizer(legal_documents, padding=<span class="literal">True</span>, truncation=<span class="literal">True</span>, max_length=<span class="number">512</span>, return_tensors=<span class="string">'pt'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Further pretrain the model</span></span><br><span class="line"><span class="comment"># This step typically involves masked language modeling or other pretraining objectives</span></span><br><span class="line"><span class="comment"># Here we provide a conceptual example</span></span><br><span class="line">model.train()</span><br><span class="line"><span class="keyword">for</span> batch <span class="keyword">in</span> encoded_input:</span><br><span class="line">    outputs = model(**batch)</span><br><span class="line">    <span class="comment"># ... Perform further training steps</span></span><br></pre></td></tr></tbody></table></figure><p>In this case, the BERT model is further pretrained on a legal document dataset, making it more adept at understanding legal jargon and concepts before being fine-tuned on a specific legal NLP task.</p><h2 id="Key-Differences"><a href="#Key-Differences" class="headerlink" title="Key Differences"></a>Key Differences</h2><ul><li><strong>Purpose</strong>: Fine-tuning is tailored for a specific task with labeled data, while Further Pretraining is about adapting the model to a specific domain or style of language.</li><li><strong>Dataset</strong>: Fine-tuning uses task-specific, labeled datasets. Further Pretraining uses larger, domain-specific datasets, which may not be labeled for a specific task.</li><li><strong>Training Objective</strong>: Fine-tuning involves adjusting the model to make specific predictions, while Further Pretraining focuses on general language understanding in a new domain.</li></ul><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>Both Fine-tuning and Further Pretraining are powerful techniques in NLP. By understanding their differences and applications, we can better leverage large language models to solve diverse and complex tasks in various domains. Whether you’re building a sentiment analysis model for social media posts or adapting a model to understand legal documents, these techniques offer robust solutions in the ever-evolving field of NLP.</p><hr><p><strong>Note</strong>: The code examples provided are conceptual and require a suitable environment setup, including necessary libraries and datasets, for execution.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Understanding-the-Differences-Between-Fine-tuning-and-Further-Pretraining-in-Large-Language-Models&quot;&gt;&lt;a href=&quot;#Understanding-the-Diff</summary>
      
    
    
    
    <category term="NLP Insights" scheme="https://chenhuiyu.github.io/categories/NLP-Insights/"/>
    
    
    <category term="LLM" scheme="https://chenhuiyu.github.io/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>🤖 Create a Telegram Bot with Python and OpenAI in 10 Minutes! 🚀</title>
    <link href="https://chenhuiyu.github.io/2023/09/24/NLP%20Insights/Create%20a%20Telegram%20Bot%20with%20Python%20and%20OpenAI%20in%2010%20Minutes/"/>
    <id>https://chenhuiyu.github.io/2023/09/24/NLP%20Insights/Create%20a%20Telegram%20Bot%20with%20Python%20and%20OpenAI%20in%2010%20Minutes/</id>
    <published>2023-09-23T19:13:03.000Z</published>
    <updated>2023-09-23T19:13:36.487Z</updated>
    
    <content type="html"><![CDATA[<h1 id="🤖-Create-a-Telegram-Bot-with-Python-and-OpenAI-in-10-Minutes-🚀"><a href="#🤖-Create-a-Telegram-Bot-with-Python-and-OpenAI-in-10-Minutes-🚀" class="headerlink" title="🤖 Create a Telegram Bot with Python and OpenAI in 10 Minutes! 🚀"></a>🤖 Create a Telegram Bot with Python and OpenAI in 10 Minutes! 🚀</h1><p>In this fun tutorial, we’ll show you how to create a Telegram bot that can chat with users and generate witty responses. We’ll explain each step in detail, making it easy for you to get started!</p><h2 id="Step-1-Create-a-Telegram-Bot-🤖"><a href="#Step-1-Create-a-Telegram-Bot-🤖" class="headerlink" title="Step 1: Create a Telegram Bot 🤖"></a>Step 1: Create a Telegram Bot 🤖</h2><p>First, let’s create your very own Telegram bot. Here’s how:</p><ol><li>Open the Telegram app and search for “BotFather.”</li><li>In the BotFather chat, use the <code>/newbot</code> command to create a new bot. You’ll need to give your bot a name, like “PunshineBot.”</li><li>BotFather will generate a unique API token for you. Be sure to save this token; we’ll use it in the code later.</li></ol><h2 id="Step-2-Import-Necessary-Libraries-📚"><a href="#Step-2-Import-Necessary-Libraries-📚" class="headerlink" title="Step 2: Import Necessary Libraries 📚"></a>Step 2: Import Necessary Libraries 📚</h2><p>We’ll first need to import some Python libraries to create the Telegram bot and perform natural language processing with OpenAI.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> Final, Deque, <span class="type">Dict</span>, <span class="type">Union</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> deque</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> openai</span><br><span class="line"><span class="keyword">from</span> telegram <span class="keyword">import</span> Update</span><br><span class="line"><span class="keyword">from</span> telegram.ext <span class="keyword">import</span> (</span><br><span class="line">    Application,</span><br><span class="line">    CommandHandler,</span><br><span class="line">    ContextTypes,</span><br><span class="line">    MessageHandler,</span><br><span class="line">    filters,</span><br><span class="line">)</span><br></pre></td></tr></tbody></table></figure><p>Make sure you have the Telegram Bot API and OpenAI Python library installed.</p><h2 id="Step-3-Set-API-Keys-and-Telegram-Token-🔑"><a href="#Step-3-Set-API-Keys-and-Telegram-Token-🔑" class="headerlink" title="Step 3: Set API Keys and Telegram Token 🔑"></a>Step 3: Set API Keys and Telegram Token 🔑</h2><p>In this step, we need to set the OpenAI API key and Telegram bot token. Make sure you’ve signed up for OpenAI and obtained your API key. Then, replace the example API key and Telegram token in the following code with your own:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">openai.api_key = <span class="string">"Your OpenAI API Key"</span></span><br><span class="line">TOKEN: Final = <span class="string">"Your Telegram Bot Token"</span></span><br></pre></td></tr></tbody></table></figure><h2 id="Step-4-Create-Start-and-Help-Commands-🚀"><a href="#Step-4-Create-Start-and-Help-Commands-🚀" class="headerlink" title="Step 4: Create Start and Help Commands 🚀"></a>Step 4: Create Start and Help Commands 🚀</h2><p>Next, we’ll create two command handling functions for the start and help commands. These commands allow users to interact with the bot.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">start_command</span>(<span class="params">update: Update, context: ContextTypes.DEFAULT_TYPE</span>):</span><br><span class="line">    <span class="keyword">await</span> update.message.reply_text(<span class="string">"Hello, world! 😄"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">help_command</span>(<span class="params">update: Update, context: ContextTypes.DEFAULT_TYPE</span>):</span><br><span class="line">    <span class="keyword">await</span> update.message.reply_text(<span class="string">"I'm PunShine Bot, and I can help you find the funniest puns! 😂"</span>)</span><br></pre></td></tr></tbody></table></figure><p>The <code>start_command</code> function handles the start command, responding with “Hello, world! 😄” when users send the <code>/start</code> command. The <code>help_command</code> function handles the help command, providing assistance when users send the <code>/help</code> command.</p><h2 id="Step-5-Handle-User-Messages-📨"><a href="#Step-5-Handle-User-Messages-📨" class="headerlink" title="Step 5: Handle User Messages 📨"></a>Step 5: Handle User Messages 📨</h2><p>In this step, we define the <code>handle_response</code> function to process user messages and create the <code>message_handler</code>. The function adds user messages to the conversation history and uses the OpenAI API to generate responses.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">handle_response</span>(<span class="params">chat_id: <span class="type">Union</span>[<span class="built_in">int</span>, <span class="built_in">str</span>], text: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">    <span class="comment"># Get the chat history</span></span><br><span class="line">    chat_history = get_chat_history(chat_id)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Create a user message</span></span><br><span class="line">    user_message = {<span class="string">"role"</span>: <span class="string">"user"</span>, <span class="string">"content"</span>: text}</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Add the user message to the chat history</span></span><br><span class="line">    chat_history.append(user_message)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Build a message list, including a system message and user messages</span></span><br><span class="line">    messages = [</span><br><span class="line">        {</span><br><span class="line">            <span class="string">"role"</span>: <span class="string">"system"</span>,</span><br><span class="line">            <span class="string">"content"</span>: <span class="string">"You are PunshineBot, reply in English, keep it casual, use emojis, and keep responses short. 😄🚀"</span>,</span><br><span class="line">        }</span><br><span class="line">    ]</span><br><span class="line">    messages.extend(chat_history)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Use the OpenAI API to generate the bot's response</span></span><br><span class="line">    response = openai.ChatCompletion.create(</span><br><span class="line">        model=<span class="string">"gpt-3.5-turbo"</span>,</span><br><span class="line">        messages=messages,</span><br><span class="line">        temperature=<span class="number">1</span>,</span><br><span class="line">        max_tokens=<span class="number">256</span>,</span><br><span class="line">        top_p=<span class="number">1</span>,</span><br><span class="line">        frequency_penalty=<span class="number">0</span>,</span><br><span class="line">        presence_penalty=<span class="number">0</span>,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    bot_response = response[<span class="string">"choices"</span>][<span class="number">0</span>][<span class="string">"message"</span>][<span class="string">"content"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Add the bot's response to the conversation history</span></span><br><span class="line">    chat_history.append({<span class="string">"role"</span>: <span class="string">"assistant"</span>, <span class="string">"content"</span>: bot_response})</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> bot_response</span><br></pre></td></tr></tbody></table></figure><p>The <code>handle_response</code> function adds user messages to the conversation history and generates the bot’s response using the OpenAI API. The <code>message_handler</code> function processes messages sent by users, checking their type (group message or private chat) and content, and then calls the <code>handle_response</code> function to generate the bot’s response.</p><h2 id="Step-6-Run-the-Telegram-Bot-🚀"><a href="#Step-6-Run-the-Telegram-Bot-🚀" class="headerlink" title="Step 6: Run the Telegram Bot 🚀"></a>Step 6: Run the Telegram Bot 🚀</h2><p>Finally, in the main function, we create a Telegram bot application, add command handlers and message handlers, and start the Telegram bot to receive and process messages.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    app = Application.builder().token(TOKEN).build()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Add command handlers</span></span><br><span class="line">    app.add_handler(CommandHandler(<span class="string">"start"</span>, start_command))</span><br><span class="line">    app.add_handler(CommandHandler(<span class="string">"help"</span>, help_command))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Add message handler</span></span><br><span class="line">    app.add_handler(MessageHandler(filters.TEXT, callback=message_handler))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Start the Telegram bot</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"Bot started! 😎🤖"</span>)</span><br><span class="line">    app.run_polling(poll_interval=<span class="number">1</span>)</span><br></pre></td></tr></tbody></table></figure><p><code>app.run_polling(poll_interval=1)</code> starts the Telegram bot, polling for new messages every second. This keeps the bot running and triggers the respective handlers when it receives messages.</p><h2 id="Step-7-Host-Your-Telegram-Bot-Online-for-Free-24-x2F-7-🚀"><a href="#Step-7-Host-Your-Telegram-Bot-Online-for-Free-24-x2F-7-🚀" class="headerlink" title="Step 7: Host Your Telegram Bot Online for Free 24/7 🚀"></a>Step 7: Host Your Telegram Bot Online for Free 24/7 🚀</h2><p>In the previous tutorial, we showed you how to create a Telegram bot. However, to keep your bot online 24/7, you would need to leave your computer running, which can be inconvenient. In this blog post, we’ll show you how to host your bot for free on a cloud server so that it can run around the clock.</p><p>PythonAnywhere is a website that allows you to host Python code for free. It comes with some usage limitations, but it’s perfect for simple use cases. Here’s how to use PythonAnywhere to host your Telegram bot.</p><ol><li>Visit the <a href="https://www.pythonanywhere.com/">PythonAnywhere website</a> and create an account.</li></ol><h3 id="Step-7-1-Create-a-Python-Script"><a href="#Step-7-1-Create-a-Python-Script" class="headerlink" title="Step 7.1: Create a Python Script"></a>Step 7.1: Create a Python Script</h3><p>Once you’re logged into the PythonAnywhere dashboard, you’ll see an option called “Files.” Click on it and create a new file. Name it “telegram_bot.py.” In this file, paste the Telegram bot code you created earlier.</p><h3 id="Step-7-2-Install-Required-Packages"><a href="#Step-7-2-Install-Required-Packages" class="headerlink" title="Step 7.2: Install Required Packages"></a>Step 7.2: Install Required Packages</h3><p>Before running the Telegram bot, we need to install the necessary Python packages. PythonAnywhere provides a command-line interface where you can execute the following commands:</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install python-telegram-bot</span><br><span class="line">pip install openai</span><br></pre></td></tr></tbody></table></figure><p>This will install the Telegram bot library, allowing your code to communicate with the Telegram servers.</p><h3 id="Step-7-3-Run-the-Bot"><a href="#Step-7-3-Run-the-Bot" class="headerlink" title="Step 7.3: Run the Bot"></a>Step 7.3: Run the Bot</h3><p>Now, go back to the PythonAnywhere dashboard and find your “telegram_bot.py” file under “Files.” Click the “Run” button, and your bot will start running. The bot will stay online even if you close your computer.</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>You’ve now learned how to create a Python Telegram bot, integrate it with OpenAI to respond to user messages, and host it for free on a cloud server, ensuring it remains online 24/7. This way, you can interact with your bot without worrying about whether your computer is on. If you have other free hosting services or questions, feel free to share in the comments! Happy bot building! 🤖🚀😄</p><p>Additionally, if you want to learn more about hosting bots on Discord, you can watch this YouTube video: <a href="https://www.youtube.com/watch?v=2TI-tCVhe9k">How To Host Your Bot Online 24/7 For FREE With Python (Telegram, Discord, Etc)</a>. The video provides more detailed tutorials and examples.</p><p>If you have any questions or need further assistance, please don’t hesitate to leave a comment. Happy coding! 🚀🤖😄</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://www.youtube.com/watch?v=vZtm1wuA2yc">How To Create A Telegram Bot In Python For Beginners (2023 Tutorial)</a></li><li><a href="https://www.youtube.com/watch?v=2TI-tCVhe9k">How To Host Your Bot Online 24/7 For FREE With Python (Telegram, Discord, Etc)</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;🤖-Create-a-Telegram-Bot-with-Python-and-OpenAI-in-10-Minutes-🚀&quot;&gt;&lt;a href=&quot;#🤖-Create-a-Telegram-Bot-with-Python-and-OpenAI-in-10-Mi</summary>
      
    
    
    
    <category term="Code Chronicles" scheme="https://chenhuiyu.github.io/categories/Code-Chronicles/"/>
    
    
    <category term="Chatbot" scheme="https://chenhuiyu.github.io/tags/Chatbot/"/>
    
  </entry>
  
  <entry>
    <title>🤖 十分钟用 Python 和 OpenAI 创建 Telegram 机器人！ 🚀</title>
    <link href="https://chenhuiyu.github.io/2023/09/24/NLP%20Insights/%E5%8D%81%E5%88%86%E9%92%9F%E7%94%A8%20Python%20%E5%92%8C%20OpenAI%20%E5%88%9B%E5%BB%BA%20Telegram%20%E6%9C%BA%E5%99%A8%E4%BA%BA/"/>
    <id>https://chenhuiyu.github.io/2023/09/24/NLP%20Insights/%E5%8D%81%E5%88%86%E9%92%9F%E7%94%A8%20Python%20%E5%92%8C%20OpenAI%20%E5%88%9B%E5%BB%BA%20Telegram%20%E6%9C%BA%E5%99%A8%E4%BA%BA/</id>
    <published>2023-09-23T19:12:03.000Z</published>
    <updated>2023-09-23T19:13:00.772Z</updated>
    
    <content type="html"><![CDATA[<h1 id="🤖-十分钟用-Python-和-OpenAI-创建-Telegram-机器人！-🚀"><a href="#🤖-十分钟用-Python-和-OpenAI-创建-Telegram-机器人！-🚀" class="headerlink" title="🤖 十分钟用 Python 和 OpenAI 创建 Telegram 机器人！ 🚀"></a>🤖 十分钟用 Python 和 OpenAI 创建 Telegram 机器人！ 🚀</h1><p>在这个有趣的教程中，我们将向您展示如何创建一个具有的 Telegram 机器人，该机器人能够与用户聊天并生成幽默回复。我们将详细解释每一步，让您轻松入门！</p><h2 id="步骤-1：创建-Telegram-机器人-🤖"><a href="#步骤-1：创建-Telegram-机器人-🤖" class="headerlink" title="步骤 1：创建 Telegram 机器人 🤖"></a>步骤 1：创建 Telegram 机器人 🤖</h2><p>首先，让我们来创建您自己的 Telegram 机器人。这是如何做的：</p><ol><li>打开 Telegram 应用并搜索 “BotFather”。</li><li>在 BotFather 聊天中，使用 <code>/newbot</code> 命令创建一个新机器人。您需要为机器人取个名字，比如 “PunshineBot”。</li><li>BotFather 会为您生成一个独一无二的 API 令牌（Token）。一定要妥善保存这个令牌，稍后我们会在代码中用到它。</li></ol><img src="/2023/09/24/NLP%20Insights/%E5%8D%81%E5%88%86%E9%92%9F%E7%94%A8%20Python%20%E5%92%8C%20OpenAI%20%E5%88%9B%E5%BB%BA%20Telegram%20%E6%9C%BA%E5%99%A8%E4%BA%BA/step1.jpg" class=""><h2 id="步骤-2：导入必要的库-📚"><a href="#步骤-2：导入必要的库-📚" class="headerlink" title="步骤 2：导入必要的库 📚"></a>步骤 2：导入必要的库 📚</h2><p>我们首先需要导入一些 Python 库，以便创建 Telegram 机器人并与 OpenAI 进行自然语言处理。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> Final, Deque, <span class="type">Dict</span>, <span class="type">Union</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> deque</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> openai</span><br><span class="line"><span class="keyword">from</span> telegram <span class="keyword">import</span> Update</span><br><span class="line"><span class="keyword">from</span> telegram.ext <span class="keyword">import</span> (</span><br><span class="line">    Application,</span><br><span class="line">    CommandHandler,</span><br><span class="line">    ContextTypes,</span><br><span class="line">    MessageHandler,</span><br><span class="line">    filters,</span><br><span class="line">)</span><br></pre></td></tr></tbody></table></figure><p>请确保您已经安装了 Telegram Bot API 和 OpenAI Python 库。</p><h2 id="步骤-3：设置-API-密钥和-Telegram-令牌-🔑"><a href="#步骤-3：设置-API-密钥和-Telegram-令牌-🔑" class="headerlink" title="步骤 3：设置 API 密钥和 Telegram 令牌 🔑"></a>步骤 3：设置 API 密钥和 Telegram 令牌 🔑</h2><p>在这一步，我们需要设置 OpenAI API 密钥和 Telegram 机器人令牌。确保您已经注册了 OpenAI 并获取了 API 密钥。然后，将下面的示例代码中的 API 密钥和 Telegram 令牌替换为您自己的：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">openai.api_key = <span class="string">"您的 OpenAI API 密钥"</span></span><br><span class="line">TOKEN: Final = <span class="string">"您的 Telegram 机器人令牌"</span></span><br></pre></td></tr></tbody></table></figure><h2 id="步骤-4：创建启动和帮助命令-🚀"><a href="#步骤-4：创建启动和帮助命令-🚀" class="headerlink" title="步骤 4：创建启动和帮助命令 🚀"></a>步骤 4：创建启动和帮助命令 🚀</h2><p>接下来，我们将创建两个命令处理函数，用于处理启动和帮助命令。这些命令允许用户与机器人进行互动。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">start_command</span>(<span class="params">update: Update, context: ContextTypes.DEFAULT_TYPE</span>):</span><br><span class="line">    <span class="keyword">await</span> update.message.reply_text(<span class="string">"你好世界! 😄"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">help_command</span>(<span class="params">update: Update, context: ContextTypes.DEFAULT_TYPE</span>):</span><br><span class="line">    <span class="keyword">await</span> update.message.reply_text(<span class="string">"我是 PunShine Bot，我可以帮你找到最搞笑的双关语！ 😂"</span>)</span><br></pre></td></tr></tbody></table></figure><p><code>start_command</code> 函数用于处理启动命令，当用户发送 <code>/start</code> 命令时，机器人将回复 “你好世界! 😄”。<code>help_command</code> 函数用于处理帮助命令，当用户发送 <code>/help</code> 命令时，机器人将回复帮助信息。</p><h2 id="步骤-5：处理用户消息-📨"><a href="#步骤-5：处理用户消息-📨" class="headerlink" title="步骤 5：处理用户消息 📨"></a>步骤 5：处理用户消息 📨</h2><p>在这一步，我们定义了处理用户消息的 <code>handle_response</code> 函数，并创建了消息处理程序 <code>message_handler</code>。这个函数将用户的消息添加到对话历史记录中，并使用 OpenAI API 生成机器人的回复。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">handle_response</span>(<span class="params">chat_id: <span class="type">Union</span>[<span class="built_in">int</span>, <span class="built_in">str</span>], text: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">    <span class="comment"># 获取聊天历史记录</span></span><br><span class="line">    chat_history = get_chat_history(chat_id)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 创建用户消息</span></span><br><span class="line">    user_message = {<span class="string">"role"</span>: <span class="string">"user"</span>, <span class="string">"content"</span>: text}</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将用户消息添加到聊天历史记录</span></span><br><span class="line">    chat_history.append(user_message)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建消息列表，包括系统消息和用户消息</span></span><br><span class="line">    messages = [</span><br><span class="line">        {</span><br><span class="line">            <span class="string">"role"</span>: <span class="string">"system"</span>,</span><br><span class="line">            <span class="string">"content"</span>: <span class="string">"你是 PunshineBot，用中文回答，语气随意，别太正式，多用 emoji，回复要简短一点。 😄🚀"</span>,</span><br><span class="line">        }</span><br><span class="line">    ]</span><br><span class="line">    messages.extend(chat_history)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 使用 OpenAI API 生成机器人的回复</span></span><br><span class="line">    response = openai.ChatCompletion.create(</span><br><span class="line">        model=<span class="string">"gpt-3.5-turbo"</span>,</span><br><span class="line">        messages=messages,</span><br><span class="line">        temperature=<span class="number">1</span>,</span><br><span class="line">        max_tokens=<span class="number">256</span>,</span><br><span class="line">        top_p=<span class="number">1</span>,</span><br><span class="line">        frequency_penalty=<span class="number">0</span>,</span><br><span class="line">        presence_penalty=<span class="number">0</span>,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    bot_response = response[<span class="string">"choices"</span>][<span class="number">0</span>][<span class="string">"message"</span>][<span class="string">"content"</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将机器人的回复添加到对话历史记录</span></span><br><span class="line">    chat_history.append({<span class="string">"role"</span>: <span class="string">"assistant"</span>, <span class="string">"content"</span>: bot_response})</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> bot_response</span><br></pre></td></tr></tbody></table></figure><p><code>handle_response</code> 函数将用户消息添加到对话历史记录中，然后使用 OpenAI API 生成机器人的回复，并将机器人的回复添加到对话历史记录中，以保持上下文。</p><p><code>message_handler</code> 函数用于处理用户发送的消息。它检查消息的类型（群组消息或私聊消息）以及消息的内容，然后调用 <code>handle_response</code> 函数生成机器人的回复。</p><h2 id="步骤-6：运行-Telegram-机器人-🚀"><a href="#步骤-6：运行-Telegram-机器人-🚀" class="headerlink" title="步骤 6：运行 Telegram 机器人 🚀"></a>步骤 6：运行 Telegram 机器人 🚀</h2><p>最后，在主函数中，我们创建了一个 Telegram 机器人应用程序，并添加了命令处理程序和消息处理程序。然后，我们启动了 Telegram 机器人，使其可以接收和处理消息。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    app = Application.builder().token(TOKEN).build()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 添加命令处理程序</span></span><br><span class="line">    app.add_handler(CommandHandler(<span class="string">"start"</span>, start_command))</span><br><span class="line">    app.add_handler(CommandHandler(<span class="string">"help"</span>, help_command))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 添加消息处理程序</span></span><br><span class="line">    app.add_handler(MessageHandler(filters.TEXT, callback=message_handler))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 启动 Telegram 机器人</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"机器人已启动！ 😎🤖"</span>)</span><br><span class="line">    app.run_polling(poll_interval=<span class="number">1</span>)</span><br></pre></td></tr></tbody></table></figure><img src="/2023/09/24/NLP%20Insights/%E5%8D%81%E5%88%86%E9%92%9F%E7%94%A8%20Python%20%E5%92%8C%20OpenAI%20%E5%88%9B%E5%BB%BA%20Telegram%20%E6%9C%BA%E5%99%A8%E4%BA%BA/step6_zh.jpg" class=""><p><code>app.run_polling(poll_interval=1)</code> 启动了 Telegram 机器人，以每秒的间隔轮询新消息。这意味着机器人会一直运行，并在接收到消息时触发相应的处理程序。</p><h2 id="步骤-7：免费在线托管您的-Telegram-机器人-24-x2F-7-🚀"><a href="#步骤-7：免费在线托管您的-Telegram-机器人-24-x2F-7-🚀" class="headerlink" title="步骤 7：免费在线托管您的 Telegram 机器人 24/7 🚀"></a>步骤 7：免费在线托管您的 Telegram 机器人 24/7 🚀</h2><p>在之前的教程中，我们教您如何创建一个 Telegram 机器人。但是，要使您的机器人一直在线运行，您需要让您的计算机保持开启，这可能不太方便。在这篇博客中，我们将向您展示如何将您的机器人免费托管在云服务器上，以便您的机器人可以全天候在线运行。</p><p>PythonAnywhere 是一个允许您免费托管 Python 代码的网站，它具有一定的使用限制，但对于简单的用途来说，非常合适。在这里，我们将展示如何使用 PythonAnywhere 来托管您的 Telegram 机器人。</p><p>访问 <a href="https://www.pythonanywhere.com/">PythonAnywhere 网站</a> 并创建一个帐户。</p><img src="/2023/09/24/NLP%20Insights/%E5%8D%81%E5%88%86%E9%92%9F%E7%94%A8%20Python%20%E5%92%8C%20OpenAI%20%E5%88%9B%E5%BB%BA%20Telegram%20%E6%9C%BA%E5%99%A8%E4%BA%BA/step7.jpg" class=""><h3 id="7-1：创建一个-Python-脚本"><a href="#7-1：创建一个-Python-脚本" class="headerlink" title="7.1：创建一个 Python 脚本"></a>7.1：创建一个 Python 脚本</h3><p>一旦您登录到 PythonAnywhere 的仪表板，您将看到一个名为 “Files” 的选项。点击它，然后创建一个新文件，我们将命名它为 “telegram_bot.py”。在这个文件中，我们将粘贴之前创建的 Telegram 机器人代码。</p><h3 id="7-2：安装所需的包"><a href="#7-2：安装所需的包" class="headerlink" title="7.2：安装所需的包"></a>7.2：安装所需的包</h3><p>在运行 Telegram 机器人之前，我们需要安装所需的 Python 包。PythonAnywhere 提供了一个命令行界面，您可以在其中执行以下命令：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install python-telegram-bot</span><br><span class="line">pip install openai</span><br></pre></td></tr></tbody></table></figure><p>这将安装 Telegram 机器人库，使您的代码能够与 Telegram 服务器通信。</p><h3 id="7-3：运行机器人"><a href="#7-3：运行机器人" class="headerlink" title="7.3：运行机器人"></a>7.3：运行机器人</h3><p>现在，您可以返回到 PythonAnywhere 的仪表板，并在 “Files” 下找到您的 “telegram_bot.py” 文件。然后，点击 “Run” 按钮，您的机器人将开始运行。机器人将一直在线，即使您关闭了计算机也是如此。</p><h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>现在，您已经掌握了如何创建一个 Python Telegram 机器人，并与 OpenAI 集成，使其能够回应用户的消息并生成回复。通过这个简单的步骤，您可以将您的 Telegram 机器人免费托管在云服务器上，以便它可以全天候在线运行。这样，您就可以与您的机器人互动，而不必担心计算机是否开启。如果您有其他免费托管服务或其他问题，欢迎在评论中分享！祝您的机器人运行愉快！🤖🚀😄</p><p>另外，如果您想了解更多关于如何在 Discord 上托管机器人的信息，可以观看此 YouTube 视频：<a href="https://www.youtube.com/watch?v=2TI-tCVhe9k">How To Host Your Bot Online 24/7 For FREE With Python (Telegram, Discord, Etc)</a>。视频中有更详细的教程和示例。</p><p>如果您有任何问题或需要进一步的帮助，请随时留下评论。祝您编程愉快！ 🚀🤖😄</p><h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><ul><li><a href="https://www.youtube.com/watch?v=vZtm1wuA2yc">How To Create A Telegram Bot In Python For Beginners (2023 Tutorial)</a></li><li><a href="https://www.youtube.com/watch?v=2TI-tCVhe9k">How To Host Your Bot Online 24/7 For FREE With Python (Telegram, Discord, Etc)</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;🤖-十分钟用-Python-和-OpenAI-创建-Telegram-机器人！-🚀&quot;&gt;&lt;a href=&quot;#🤖-十分钟用-Python-和-OpenAI-创建-Telegram-机器人！-🚀&quot; class=&quot;headerlink&quot; title=&quot;🤖 十分钟</summary>
      
    
    
    
    <category term="Code Chronicles" scheme="https://chenhuiyu.github.io/categories/Code-Chronicles/"/>
    
    
    <category term="Chatbot" scheme="https://chenhuiyu.github.io/tags/Chatbot/"/>
    
  </entry>
  
  <entry>
    <title>Resolving Port Conflicts: Identifying and Terminating Processes</title>
    <link href="https://chenhuiyu.github.io/2023/09/13/Debugging%20Diaries/Resolving%20Port%20Conflicts:%20Identifying%20and%20Terminating%20Processes/"/>
    <id>https://chenhuiyu.github.io/2023/09/13/Debugging%20Diaries/Resolving%20Port%20Conflicts:%20Identifying%20and%20Terminating%20Processes/</id>
    <published>2023-09-13T04:10:55.000Z</published>
    <updated>2023-09-13T04:11:07.809Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Resolving-Port-Conflicts-Identifying-and-Terminating-Processes"><a href="#Resolving-Port-Conflicts-Identifying-and-Terminating-Processes" class="headerlink" title="Resolving Port Conflicts: Identifying and Terminating Processes"></a>Resolving Port Conflicts: Identifying and Terminating Processes</h1><h2 id="Table-of-Contents"><a href="#Table-of-Contents" class="headerlink" title="Table of Contents"></a>Table of Contents</h2><ul><li>Introduction 📝</li><li>Finding Processes on Windows 🕵️‍♂️</li><li>Finding Processes on Linux/macOS 🐧</li><li>Viewing Process Details 📊</li><li>Terminating Processes ⛔️</li></ul><h3 id="Introduction-📝"><a href="#Introduction-📝" class="headerlink" title="Introduction 📝"></a>Introduction 📝</h3><p>Sometimes, when you try to start an application or service, you may encounter an “Address already in use” error. This means that the specified port is already in use by another process. To resolve this issue, you need to identify which process is using that port and decide whether to terminate it or change the port configuration of your application.</p><h3 id="Finding-Processes-on-Windows-🕵️‍♂️"><a href="#Finding-Processes-on-Windows-🕵️‍♂️" class="headerlink" title="Finding Processes on Windows 🕵️‍♂️"></a>Finding Processes on Windows 🕵️‍♂️</h3><p>On Windows, you can use the Command Prompt to find the process that is using a specific port. Open the Command Prompt and run the following command:</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">netstat -ano | findstr :8080</span><br></pre></td></tr></tbody></table></figure><p>This command will list all processes using port 8080 and display their Process ID (PID).</p><h3 id="Finding-Processes-on-Linux-x2F-macOS-🐧"><a href="#Finding-Processes-on-Linux-x2F-macOS-🐧" class="headerlink" title="Finding Processes on Linux/macOS 🐧"></a>Finding Processes on Linux/macOS 🐧</h3><p>On Linux and macOS systems, you can use the terminal to find the process using a specific port. Open the terminal and run the following command:</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo lsof -i :8080</span><br></pre></td></tr></tbody></table></figure><p>This command will list all processes using port 8080 and display detailed information, including the PID and process name.</p><h3 id="Viewing-Process-Details-📊"><a href="#Viewing-Process-Details-📊" class="headerlink" title="Viewing Process Details 📊"></a>Viewing Process Details 📊</h3><p>Once you have the PID of the process, you can further view details about the process. On Linux and macOS, use the following command:</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ps -p &lt;PID&gt;</span><br></pre></td></tr></tbody></table></figure><p>On Windows, you can use the Task Manager or run the following command (replace <code>&lt;PID&gt;</code> with the correct PID):</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tasklist | findstr &lt;PID&gt;</span><br></pre></td></tr></tbody></table></figure><p>This will display detailed information about the process associated with the specified PID, including the process name and other relevant details.</p><h3 id="Terminating-Processes-⛔️"><a href="#Terminating-Processes-⛔️" class="headerlink" title="Terminating Processes ⛔️"></a>Terminating Processes ⛔️</h3><p>If you decide to terminate the process that is using a specific port, follow these steps:</p><p>On Windows, you can use the Task Manager or run the following command (replace <code>&lt;PID&gt;</code> with the correct PID):</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">taskkill /F /PID &lt;PID&gt;</span><br></pre></td></tr></tbody></table></figure><p>On Linux and macOS, you can run the following command (replace <code>&lt;PID&gt;</code> with the correct PID):</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo kill &lt;PID&gt;</span><br></pre></td></tr></tbody></table></figure><p>Please note that terminating a process may interrupt running applications or services, so use this operation carefully. Ensure that you know which process to terminate and avoid impacting critical system processes.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Resolving-Port-Conflicts-Identifying-and-Terminating-Processes&quot;&gt;&lt;a href=&quot;#Resolving-Port-Conflicts-Identifying-and-Terminating-Proce</summary>
      
    
    
    
    <category term="Debugging Diaries" scheme="https://chenhuiyu.github.io/categories/Debugging-Diaries/"/>
    
    
    <category term="IssueFix" scheme="https://chenhuiyu.github.io/tags/IssueFix/"/>
    
  </entry>
  
  <entry>
    <title>解决端口冲突：查找并终止进程</title>
    <link href="https://chenhuiyu.github.io/2023/09/13/Debugging%20Diaries/%E8%A7%A3%E5%86%B3%E7%AB%AF%E5%8F%A3%E5%86%B2%E7%AA%81%EF%BC%9A%E6%9F%A5%E6%89%BE%E5%B9%B6%E7%BB%88%E6%AD%A2%E8%BF%9B%E7%A8%8B/"/>
    <id>https://chenhuiyu.github.io/2023/09/13/Debugging%20Diaries/%E8%A7%A3%E5%86%B3%E7%AB%AF%E5%8F%A3%E5%86%B2%E7%AA%81%EF%BC%9A%E6%9F%A5%E6%89%BE%E5%B9%B6%E7%BB%88%E6%AD%A2%E8%BF%9B%E7%A8%8B/</id>
    <published>2023-09-13T04:10:55.000Z</published>
    <updated>2023-09-13T04:10:20.456Z</updated>
    
    <content type="html"><![CDATA[<h1 id="解决端口冲突：查找并终止进程"><a href="#解决端口冲突：查找并终止进程" class="headerlink" title="解决端口冲突：查找并终止进程"></a>解决端口冲突：查找并终止进程</h1><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li>简介</li><li>在Windows上查找进程 🕵️‍♂️</li><li>在Linux/macOS上查找进程 🐧</li><li>查看进程详细信息 📊</li><li>终止进程 ⛔️</li></ul><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>有时候，当你尝试启动一个应用程序或服务时，可能会遇到”Address already in use”（地址已经在使用）的错误，这意味着指定的端口已经被另一个进程占用。为了解决这个问题，你需要确定哪个进程正在使用该端口，并可以选择终止该进程或更改应用程序的端口配置。</p><h3 id="在Windows上查找进程-🕵️‍♂️"><a href="#在Windows上查找进程-🕵️‍♂️" class="headerlink" title="在Windows上查找进程 🕵️‍♂️"></a>在Windows上查找进程 🕵️‍♂️</h3><p>在Windows上，你可以使用命令提示符来查找正在使用特定端口的进程。打开命令提示符，并执行以下命令：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">netstat -ano | findstr :8080</span><br></pre></td></tr></tbody></table></figure><p>这个命令会列出所有正在使用端口8080的进程，并显示它们的进程ID（PID）。</p><h3 id="在Linux-x2F-macOS上查找进程-🐧"><a href="#在Linux-x2F-macOS上查找进程-🐧" class="headerlink" title="在Linux/macOS上查找进程 🐧"></a>在Linux/macOS上查找进程 🐧</h3><p>在Linux和macOS系统上，你可以使用终端来查找正在使用特定端口的进程。打开终端，并执行以下命令：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo lsof -i :8080</span><br></pre></td></tr></tbody></table></figure><p>这个命令会列出所有使用端口8080的进程，并显示它们的详细信息，包括PID和进程名。</p><h3 id="查看进程详细信息-📊"><a href="#查看进程详细信息-📊" class="headerlink" title="查看进程详细信息 📊"></a>查看进程详细信息 📊</h3><p>一旦你获得了进程的PID，你可以进一步查看有关进程的详细信息。在Linux和macOS上，使用以下命令：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ps -p &lt;PID&gt;</span><br></pre></td></tr></tbody></table></figure><p>在Windows上，你可以使用任务管理器或执行以下命令：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tasklist | findstr &lt;PID&gt;</span><br></pre></td></tr></tbody></table></figure><p>这将显示与特定PID相关的进程的详细信息，包括进程名称和其他详细信息。</p><h3 id="终止进程-⛔️"><a href="#终止进程-⛔️" class="headerlink" title="终止进程 ⛔️"></a>终止进程 ⛔️</h3><p>如果你确定要终止正在使用特定端口的进程，可以执行以下步骤：</p><p>在Windows上，你可以使用任务管理器或执行以下命令（使用正确的PID替换<pid>）：</pid></p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">taskkill /F /PID &lt;PID&gt;</span><br></pre></td></tr></tbody></table></figure><p>在Linux和macOS上，你可以执行以下命令（使用正确的PID替换<pid>）：</pid></p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo kill &lt;PID&gt;</span><br></pre></td></tr></tbody></table></figure><p>请注意，终止进程可能会导致正在运行的应用程序或服务中断，因此请谨慎使用此操作。确保你知道终止哪个进程，并确保不会影响到重要的系统进程。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;解决端口冲突：查找并终止进程&quot;&gt;&lt;a href=&quot;#解决端口冲突：查找并终止进程&quot; class=&quot;headerlink&quot; title=&quot;解决端口冲突：查找并终止进程&quot;&gt;&lt;/a&gt;解决端口冲突：查找并终止进程&lt;/h1&gt;&lt;h2 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot;</summary>
      
    
    
    
    <category term="Debugging Diaries" scheme="https://chenhuiyu.github.io/categories/Debugging-Diaries/"/>
    
    
    <category term="IssueFix" scheme="https://chenhuiyu.github.io/tags/IssueFix/"/>
    
  </entry>
  
  <entry>
    <title>Towards Open-World Recommendation with KnowledgeAugmentation from Large Language Models</title>
    <link href="https://chenhuiyu.github.io/2023/08/29/NLP%20Insights/Towards%20Open-World%20Recommendation%20with%20KnowledgeAugmentation%20from%20Large%20Language%20Models/"/>
    <id>https://chenhuiyu.github.io/2023/08/29/NLP%20Insights/Towards%20Open-World%20Recommendation%20with%20KnowledgeAugmentation%20from%20Large%20Language%20Models/</id>
    <published>2023-08-29T06:53:29.000Z</published>
    <updated>2023-08-29T06:54:03.225Z</updated>
    
    
    
    
    <category term="NLP Insights" scheme="https://chenhuiyu.github.io/categories/NLP-Insights/"/>
    
    
    <category term="LLM" scheme="https://chenhuiyu.github.io/tags/LLM/"/>
    
    <category term="Recommendation" scheme="https://chenhuiyu.github.io/tags/Recommendation/"/>
    
  </entry>
  
  <entry>
    <title>CTRL:Connect Tabular and Language Model for CTR Prediction</title>
    <link href="https://chenhuiyu.github.io/2023/08/29/NLP%20Insights/Connect%20Tabular%20and%20Language%20Model%20for%20CTR%20Prediction/"/>
    <id>https://chenhuiyu.github.io/2023/08/29/NLP%20Insights/Connect%20Tabular%20and%20Language%20Model%20for%20CTR%20Prediction/</id>
    <published>2023-08-29T03:15:29.000Z</published>
    <updated>2023-08-30T06:35:33.292Z</updated>
    
    
    
    
    <category term="NLP Insights" scheme="https://chenhuiyu.github.io/categories/NLP-Insights/"/>
    
    
    <category term="LLM" scheme="https://chenhuiyu.github.io/tags/LLM/"/>
    
    <category term="Recommendation" scheme="https://chenhuiyu.github.io/tags/Recommendation/"/>
    
  </entry>
  
  <entry>
    <title>无断开烦恼！远程服务器后台运行程序的3种方法：`nohup`、`tmux`和`screen`</title>
    <link href="https://chenhuiyu.github.io/2023/08/23/Code%20Chronicles/Python-Uninterrupted%20Remote%20Program%20Execution:%203%20Methods/"/>
    <id>https://chenhuiyu.github.io/2023/08/23/Code%20Chronicles/Python-Uninterrupted%20Remote%20Program%20Execution:%203%20Methods/</id>
    <published>2023-08-23T06:46:03.000Z</published>
    <updated>2023-08-23T06:46:56.120Z</updated>
    
    <content type="html"><![CDATA[<h1 id="无断开烦恼！远程服务器后台运行程序的3种方法：nohup、tmux和screen"><a href="#无断开烦恼！远程服务器后台运行程序的3种方法：nohup、tmux和screen" class="headerlink" title="无断开烦恼！远程服务器后台运行程序的3种方法：nohup、tmux和screen"></a>无断开烦恼！远程服务器后台运行程序的3种方法：<code>nohup</code>、<code>tmux</code>和<code>screen</code></h1><h1 id="Uninterrupted-Remote-Program-Execution-3-Methods"><a href="#Uninterrupted-Remote-Program-Execution-3-Methods" class="headerlink" title="Uninterrupted Remote Program Execution: 3 Methods"></a>Uninterrupted Remote Program Execution: 3 Methods</h1><p>在数据分析或机器学习项目中，经常需要在远程服务器上运行耗时长、计算密集型的任务。通过SSH连接到远程服务器是常见的操作方式。但是，如何确保在断开SSH连接之后，远程服务器上的程序能够继续运行呢？本文详细介绍了三种方法：<code>nohup</code>、<code>tmux</code>和<code>screen</code>。</p><h2 id="使用nohup命令"><a href="#使用nohup命令" class="headerlink" title="使用nohup命令"></a>使用<code>nohup</code>命令</h2><h3 id="开始"><a href="#开始" class="headerlink" title="开始"></a>开始</h3><ol><li><p><strong>连接到远程机器</strong>：在本地终端中执行以下命令。</p> <figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh username@remote-server-address</span><br></pre></td></tr></tbody></table></figure></li><li><p><strong>启动后台程序</strong>：在远程机器上执行。</p> <figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">nohup</span> your-command-to-run-the-program &amp;</span><br></pre></td></tr></tbody></table></figure></li></ol><p>例如：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">nohup</span> python train_model.py &amp;</span><br></pre></td></tr></tbody></table></figure><p>此方法会将程序的输出重定向到一个名为<code>nohup.out</code>的文件中。</p><h3 id="结束"><a href="#结束" class="headerlink" title="结束"></a>结束</h3><p>在远程机器上执行以下命令。</p><ol><li><p>**查找程序的进程ID (PID)**：</p> <figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ps aux | grep your-command-to-run-the-program</span><br></pre></td></tr></tbody></table></figure></li><li><p><strong>结束进程</strong>：</p> <figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">kill</span> -9 PID</span><br></pre></td></tr></tbody></table></figure></li><li><p><strong>输出含义</strong>:<br>当你执行如下命令：</p></li></ol><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">nohup</span> python train_model.py &amp;</span><br></pre></td></tr></tbody></table></figure><p>你可能会看到这样的输出：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[1] 337082</span><br><span class="line"><span class="built_in">nohup</span>: ignoring input and appending output to <span class="string">'nohup.out'</span></span><br></pre></td></tr></tbody></table></figure><p>这里，</p><ul><li><strong><a href="%E8%BF%99%E6%98%AF%E8%AF%A5%E5%90%8E%E5%8F%B0%E4%BD%9C%E4%B8%9A%E7%9A%84%E4%BD%9C%E4%B8%9A%E7%BC%96%E5%8F%B7%E3%80%82%E5%A6%82%E6%9E%9C%E4%BD%A0%E5%9C%A8%E5%90%8E%E5%8F%B0%E8%BF%90%E8%A1%8C%E5%A4%9A%E4%B8%AA%E4%BD%9C%E4%B8%9A%EF%BC%8C%E6%AF%8F%E4%B8%AA%E4%BD%9C%E4%B8%9A%E9%83%BD%E4%BC%9A%E6%9C%89%E4%B8%80%E4%B8%AA%E5%94%AF%E4%B8%80%E7%9A%84%E4%BD%9C%E4%B8%9A%E7%BC%96%E5%8F%B7%E3%80%82">1</a> 337082</strong> 表示该任务现在在后台运行，PID（进程ID）为337082。<ul><li></li><li>337082: 这是该后台作业对应的进程ID (PID)。你可以使用这个PID来监视或结束该进程。</li></ul></li><li><strong>nohup: ignoring input and appending output to ‘nohup.out’</strong> 表示该进程现在将忽略任何输入，并将输出追加到 <code>nohup.out</code> 文件。<ul><li>这是 <code>nohup</code> 命令的标准消息，含义如下：<ul><li><code>ignoring input</code>: 当你使用 <code>nohup</code> 命令，它将不会接收任何从终端输入的数据。这意味着，一旦你使用 <code>nohup</code> 启动了一个程序，你不能再向它提供任何交互式输入（除非程序有其他的输入方法）。</li><li><code>appending output to 'nohup.out'</code>: 默认情况下，<code>nohup</code> 会将程序的输出重定向到一个名为 <code>nohup.out</code> 的文件中。所以，如果你的程序在执行过程中产生了任何输出（如打印语句），这些输出都会被写入到 <code>nohup.out</code> 文件中。如果该文件之前不存在，<code>nohup</code> 会自动创建它；如果文件已存在，<code>nohup</code> 会将新的输出追加到文件的末尾。</li><li>如果你想查看程序的输出，你可以使用 <code>cat</code> 或 <code>tail</code> 命令来查看 <code>nohup.out</code> 文件的内容。例如，使用 <code>tail -f nohup.out</code> 可以实时查看该文件的末尾内容，这对于监视程序的运行状态很有用。</li></ul></li></ul></li></ul><h2 id="使用tmux"><a href="#使用tmux" class="headerlink" title="使用tmux"></a>使用<code>tmux</code></h2><h3 id="开始-1"><a href="#开始-1" class="headerlink" title="开始"></a>开始</h3><ol><li>**安装<code>tmux</code>**：在远程机器上执行以下命令。</li></ol><ul><li>Ubuntu/Debian  <figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install tmux <span class="comment"># Ubuntu/Debian</span></span><br></pre></td></tr></tbody></table></figure></li><li>MacOS  <figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install tmux</span><br></pre></td></tr></tbody></table></figure></li></ul><ol start="2"><li><p><strong>连接到远程机器</strong>：在本地终端中执行。</p> <figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh username@remote-server-address</span><br></pre></td></tr></tbody></table></figure></li><li><p><strong>开始新的tmux会话</strong>：在远程机器上执行。</p> <figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tmux</span><br></pre></td></tr></tbody></table></figure></li></ol><p>然后在tmux会话内运行您的程序。</p><ol start="4"><li><strong>断开会话</strong>：在远程机器上按 <code>Ctrl+b d</code>。</li></ol><h3 id="结束-1"><a href="#结束-1" class="headerlink" title="结束"></a>结束</h3><p>在远程机器上执行以下命令。</p><ol><li><p><strong>重新连接到tmux会话</strong>：</p> <figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tmux attach</span><br></pre></td></tr></tbody></table></figure></li><li><p><strong>结束程序和会话</strong>：使用<code>Ctrl+c</code>结束程序，然后按<code>Ctrl+b</code>再按<code>x</code>结束tmux会话。</p></li></ol><h2 id="使用screen"><a href="#使用screen" class="headerlink" title="使用screen"></a>使用<code>screen</code></h2><h3 id="开始-2"><a href="#开始-2" class="headerlink" title="开始"></a>开始</h3><ol><li>**安装<code>screen</code>**：在本地终端中执行。</li></ol><ul><li>Ubuntu/Debian  <figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install screen <span class="comment"># Ubuntu/Debian</span></span><br></pre></td></tr></tbody></table></figure></li><li>MacOS  <figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install screen</span><br></pre></td></tr></tbody></table></figure></li></ul><ol><li><p><strong>连接到远程机器</strong>：在本地终端中执行。</p> <figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh username@remote-server-address</span><br></pre></td></tr></tbody></table></figure></li><li><p><strong>开始新的screen会话</strong>：在远程机器上执行。</p> <figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">screen</span><br></pre></td></tr></tbody></table></figure></li></ol><p>然后在screen会话内运行您的程序。</p><ol start="4"><li><strong>断开会话</strong>：在远程机器上按 <code>Ctrl+a d</code>。</li></ol><h3 id="结束-2"><a href="#结束-2" class="headerlink" title="结束"></a>结束</h3><p>在远程机器上执行以下命令。</p><ol><li><p><strong>重新连接到screen会话</strong>：</p> <figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">screen -r</span><br></pre></td></tr></tbody></table></figure></li><li><p><strong>结束程序和会话</strong>：使用<code>Ctrl+c</code>结束程序，然后按<code>Ctrl+a</code>再按<code>k</code>结束screen会话。</p></li></ol><h2 id="特殊情况：macOS用户"><a href="#特殊情况：macOS用户" class="headerlink" title="特殊情况：macOS用户"></a>特殊情况：macOS用户</h2><p>macOS用户可以使用Homebrew来安装<code>tmux</code>或<code>screen</code>。</p><ul><li><p>安装<code>tmux</code>：</p>  <figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install tmux</span><br></pre></td></tr></tbody></table></figure></li><li><p>安装<code>screen</code>：</p>  <figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install screen</span><br></pre></td></tr></tbody></table></figure></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;无断开烦恼！远程服务器后台运行程序的3种方法：nohup、tmux和screen&quot;&gt;&lt;a href=&quot;#无断开烦恼！远程服务器后台运行程序的3种方法：nohup、tmux和screen&quot; class=&quot;headerlink&quot; title=&quot;无断开烦恼！远程服务器后</summary>
      
    
    
    
    <category term="Code Chronicles" scheme="https://chenhuiyu.github.io/categories/Code-Chronicles/"/>
    
    
    <category term="Python Basic" scheme="https://chenhuiyu.github.io/tags/Python-Basic/"/>
    
  </entry>
  
  <entry>
    <title>超越Python的边界：`subprocess` 助你一键执行外部命令</title>
    <link href="https://chenhuiyu.github.io/2023/08/22/Code%20Chronicles/Python-subprocess/"/>
    <id>https://chenhuiyu.github.io/2023/08/22/Code%20Chronicles/Python-subprocess/</id>
    <published>2023-08-22T11:06:03.000Z</published>
    <updated>2023-08-23T06:33:28.159Z</updated>
    
    <content type="html"><![CDATA[<h1 id="超越Python的边界：subprocess-助你一键执行外部命令"><a href="#超越Python的边界：subprocess-助你一键执行外部命令" class="headerlink" title="超越Python的边界：subprocess 助你一键执行外部命令"></a>超越Python的边界：<code>subprocess</code> 助你一键执行外部命令</h1><p>在日常开发中，有时候我们希望能够从 Python 脚本中执行系统命令或者其他程序。Python 提供了 <code>subprocess</code> 模块，使得这一操作变得既简单又安全。</p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p><code>subprocess</code> 模块是 Python 标准库的一部分，它提供了一种简单统一的方法来执行外部命令，与进程交互，读取它的输出，并获取它的返回码。无论你是在自动化某个系统任务，还是简单地想要从另一个程序中获取数据，<code>subprocess</code> 都能助你一臂之力。</p><h2 id="功能与用途"><a href="#功能与用途" class="headerlink" title="功能与用途"></a>功能与用途</h2><ol><li><p><strong>执行外部命令</strong>：你可以轻易地从 Python 脚本中运行任何外部命令，就像在命令行中输入命令一样。这种能力使得你能够在你的 Python 程序中调用并集成其他命令行工具，扩展你的应用的功能。</p></li><li><p><strong>捕获命令的输出</strong>：如果你想获取命令的输出并在 Python 脚本中处理，<code>subprocess</code> 也能满足你。你可以将命令的输出作为字符串捕获，然后进一步分析和处理，这在需要对命令输出进行解析或者提取时非常有用。</p></li><li><p><strong>错误处理</strong>：通过捕获返回码，你可以知道命令是否成功执行，或者是否发生了错误。这使得你能够根据命令的执行结果采取不同的操作，从而提高程序的健壮性。</p></li><li><p><strong>与进程交互</strong>：<code>subprocess</code> 不仅可以启动和停止进程，还可以与它们进行双向通信。这使得你能够在运行的外部进程中发送输入，并从其输出中获取数据，从而实现更高级的交互和控制。</p></li></ol><h2 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h2><h3 id="基本的命令执行"><a href="#基本的命令执行" class="headerlink" title="基本的命令执行"></a>基本的命令执行</h3><p>执行命令最简单的方法是使用 <code>subprocess.run()</code> 函数，它接受一个命令及其参数的列表，并返回一个 <code>CompletedProcess</code> 对象，其中包含了命令执行的结果。这使得你能够轻松地在你的脚本中运行外部命令，例如：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> subprocess</span><br><span class="line"></span><br><span class="line">result = subprocess.run([<span class="string">'ls'</span>, <span class="string">'-l'</span>])</span><br></pre></td></tr></tbody></table></figure><h3 id="捕获命令输出"><a href="#捕获命令输出" class="headerlink" title="捕获命令输出"></a>捕获命令输出</h3><p>想要捕获命令的输出到 Python 脚本中，可以设置 <code>capture_output=True</code> 参数。此外，通过设置 <code>text=True</code> 参数，你可以以文本形式获取命令的输出：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">result = subprocess.run([<span class="string">'ls'</span>, <span class="string">'-l'</span>], capture_output=<span class="literal">True</span>, text=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(result.stdout)</span><br></pre></td></tr></tbody></table></figure><h3 id="使用-shell"><a href="#使用-shell" class="headerlink" title="使用 shell"></a>使用 shell</h3><p>当你需要执行包含 shell 功能（例如管道或通配符）的命令时，可以设置 <code>shell=True</code> 参数，并将命令作为字符串传递给 <code>subprocess.run()</code>。这样，你可以执行更复杂的命令，如以下示例所示：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">result = subprocess.run(<span class="string">'ls -l | grep "my_file"'</span>, shell=<span class="literal">True</span>, capture_output=<span class="literal">True</span>, text=<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(result.stdout)</span><br></pre></td></tr></tbody></table></figure><p><strong>警告</strong>：虽然 <code>shell=True</code> 参数很有用，但使用时必须小心，因为它可能会让你的代码暴露于命令注入攻击。确保永远不要执行包含不受信任的输入的命令。</p><h3 id="错误处理"><a href="#错误处理" class="headerlink" title="错误处理"></a>错误处理</h3><p>命令如果返回一个非零的退出码，可以通过 <code>check=True</code> 参数来抛出异常。这使得你能够捕获命令执行过程中的错误，从而进行适当的处理：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    result = subprocess.run([<span class="string">'ls'</span>, <span class="string">'non_existent_file'</span>], check=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">except</span> subprocess.CalledProcessError:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"命令执行出错!"</span>)</span><br></pre></td></tr></tbody></table></figure><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p><code>subprocess</code> 模块是 Python 中执行外部命令的强大工具。通过它，开发者可以轻松地与操作系统及其他应用程序交互，扩展程序的功能。然而，在使用 <code>subprocess</code> 时需要注意命令的安全性，以避免潜在的安全风险。通过合理地利用 <code>subprocess</code> 模块，你可以更好地管理外部命令的执行，并将其融入到你的 Python 应用中，提升开发效率。希望本文能够帮助你更深入地理解和使用 <code>subprocess</code> 模块的种种功能。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;超越Python的边界：subprocess-助你一键执行外部命令&quot;&gt;&lt;a href=&quot;#超越Python的边界：subprocess-助你一键执行外部命令&quot; class=&quot;headerlink&quot; title=&quot;超越Python的边界：subprocess 助你一</summary>
      
    
    
    
    <category term="Code Chronicles" scheme="https://chenhuiyu.github.io/categories/Code-Chronicles/"/>
    
    
    <category term="Python Basic" scheme="https://chenhuiyu.github.io/tags/Python-Basic/"/>
    
  </entry>
  
  <entry>
    <title>Conver Pytorch Model to ONNX Format</title>
    <link href="https://chenhuiyu.github.io/2023/08/21/NLP%20Insights/Conver%20Pytorch%20Model%20to%20ONNX%20Format/"/>
    <id>https://chenhuiyu.github.io/2023/08/21/NLP%20Insights/Conver%20Pytorch%20Model%20to%20ONNX%20Format/</id>
    <published>2023-08-21T06:34:18.000Z</published>
    <updated>2023-08-21T06:36:38.141Z</updated>
    
    <content type="html"><![CDATA[<h1 id="使用-PyTorch-和-ONNX-检查模型一致性"><a href="#使用-PyTorch-和-ONNX-检查模型一致性" class="headerlink" title="使用 PyTorch 和 ONNX 检查模型一致性"></a>使用 PyTorch 和 ONNX 检查模型一致性</h1><p>在机器学习和深度学习的开发过程中，模型的互操作性变得越来越重要。ONNX (Open Neural Network Exchange) 是一种开放格式，用于表示机器学习和深度学习模型。它允许开发者在各种深度学习框架之间轻松地共享模型，从而提高了模型的可移植性和互操作性。</p><p>本教程将指导您完成以下步骤：</p><ol><li>将 PyTorch 模型转换为 ONNX 格式。</li><li>验证转换后的 ONNX 模型与原始 PyTorch 模型的输出是否一致。</li></ol><h2 id="1-导入必要的库"><a href="#1-导入必要的库" class="headerlink" title="1. 导入必要的库"></a>1. 导入必要的库</h2><p>首先，我们导入为模型转换和验证所需的所有库。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> onnx</span><br><span class="line"><span class="keyword">import</span> onnxruntime</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></tbody></table></figure><h2 id="2-定义模型转换函数"><a href="#2-定义模型转换函数" class="headerlink" title="2. 定义模型转换函数"></a>2. 定义模型转换函数</h2><p>为了将 PyTorch 模型转换为 ONNX 格式，我们定义了一个名为 <code>convert_onnx</code> 的函数。此函数使用 PyTorch 的内置函数 <code>torch.onnx.export</code> 将模型转换为 ONNX 格式。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">convert_onnx</span>(<span class="params">model, dummy_input, onnx_path</span>):</span><br><span class="line">    input_names = [<span class="string">'modelInput'</span>]</span><br><span class="line">    output_names = [<span class="string">"modelOutput"</span>]</span><br><span class="line">    torch.onnx.export(model=model,</span><br><span class="line">                      args=dummy_input,</span><br><span class="line">                      f=onnx_path,</span><br><span class="line">                      opset_version=<span class="number">10</span>,</span><br><span class="line">                      input_names=input_names,</span><br><span class="line">                      output_names=output_names)</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><p>此函数接收三个参数：PyTorch 模型、模拟输入数据以及要保存 ONNX 模型的路径。<code>torch.onnx.export</code> 函数需要模型、输入和保存路径作为参数，以及其他一些可选参数来指定输入和输出的名称。</p><h2 id="3-定义一致性检查函数"><a href="#3-定义一致性检查函数" class="headerlink" title="3. 定义一致性检查函数"></a>3. 定义一致性检查函数</h2><p>一旦我们有了 ONNX 格式的模型，就可以使用 <code>check_consistency</code> 函数来验证 PyTorch 模型和 ONNX 模型的输出是否一致。这是确保转换过程没有引入任何差异的关键步骤。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">check_consistency</span>(<span class="params">pytorch_model, onnx_model_path, input_tensor, tolerance=<span class="number">1e-6</span></span>):</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        pytorch_output_dict = pytorch_model(input_tensor)</span><br><span class="line">        pytorch_output = pytorch_output_dict[<span class="string">'y_pred'</span>].cpu().numpy()</span><br><span class="line"></span><br><span class="line">    ort_session = onnxruntime.InferenceSession(onnx_model_path)</span><br><span class="line">    ort_inputs = {ort_session.get_inputs()[<span class="number">0</span>].name: input_tensor.cpu().numpy()}</span><br><span class="line">    ort_output = ort_session.run(<span class="literal">None</span>, ort_inputs)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    difference = np.<span class="built_in">abs</span>(pytorch_output - ort_output)</span><br><span class="line">    consistent = np.<span class="built_in">all</span>(difference &lt;= tolerance)</span><br><span class="line">    <span class="keyword">return</span> consistent</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><p>此函数首先使用 PyTorch 模型计算输出，然后使用 ONNX 运行时计算 ONNX 模型的输出。最后，它比较两个输出，检查它们之间的差异是否在预定义的容忍范围内。</p><h2 id="4-示例调用"><a href="#4-示例调用" class="headerlink" title="4. 示例调用"></a>4. 示例调用</h2><p>为了确保上述函数的正确性，我们提供了一个简单的示例，展示了如何使用上述函数来转换模型并检查一致性。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载 PyTorch 模型 (此处只是一个示例，需要根据实际情况进行修改)</span></span><br><span class="line">model = YOUR_PYTORCH_MODEL</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转换为 ONNX 格式</span></span><br><span class="line">dummy_input = YOUR_INPUT_TENSOR</span><br><span class="line">onnx_path = <span class="string">"path_to_save_onnx_model.onnx"</span></span><br><span class="line">convert_onnx(model, dummy_input, onnx_path)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查一致性</span></span><br><span class="line">is_consistent = check_consistency(model, onnx_path, dummy_input)</span><br><span class="line"><span class="keyword">if</span> is_consistent:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"The outputs of the PyTorch model and the ONNX model are consistent!"</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"There is a discrepancy between the outputs of the PyTorch model and the ONNX model."</span>)</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><p>在实际应用中，确保根据您的实际模型和数据替换 <code>YOUR_PYTORCH_MODEL</code> 和 <code>YOUR_INPUT_TENSOR</code>。</p><hr><p>以上就是关于如何使用 PyTorch 和 ONNX 来检查模型一致性的教程。希望这篇文章对你有所帮助，如果有任何问题，欢迎在下方留言。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;使用-PyTorch-和-ONNX-检查模型一致性&quot;&gt;&lt;a href=&quot;#使用-PyTorch-和-ONNX-检查模型一致性&quot; class=&quot;headerlink&quot; title=&quot;使用 PyTorch 和 ONNX 检查模型一致性&quot;&gt;&lt;/a&gt;使用 PyTorch </summary>
      
    
    
    
    <category term="NLP Insights" scheme="https://chenhuiyu.github.io/categories/NLP-Insights/"/>
    
    
    <category term="Onnx" scheme="https://chenhuiyu.github.io/tags/Onnx/"/>
    
    <category term="Deployment" scheme="https://chenhuiyu.github.io/tags/Deployment/"/>
    
  </entry>
  
  <entry>
    <title>Training Llama 2 Model on Single GPU with int8 Quantization and LoRA</title>
    <link href="https://chenhuiyu.github.io/2023/08/02/NLP%20Insights/LLAMA2/"/>
    <id>https://chenhuiyu.github.io/2023/08/02/NLP%20Insights/LLAMA2/</id>
    <published>2023-08-02T07:38:29.000Z</published>
    <updated>2023-08-02T08:04:32.168Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Training-Llama-2-Model-on-Single-GPU-with-int8-Quantization-and-LoRA"><a href="#Training-Llama-2-Model-on-Single-GPU-with-int8-Quantization-and-LoRA" class="headerlink" title="Training Llama 2 Model on Single GPU with int8 Quantization and LoRA"></a>Training Llama 2 Model on Single GPU with int8 Quantization and LoRA</h1><h1 id="Llama-2"><a href="#Llama-2" class="headerlink" title="Llama 2"></a>Llama 2</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p><em>Llama 2</em> 是一个包含预训练和微调的生成式文本模型的集合，其规模从 70 亿到 700 亿个参数不等。Llama2模型是由Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert等人在<a href="https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/">Llama 2: Open Foundation and Fine-Tuned Chat Models</a>中提出的。</p><p>该论文的摘要如下：</p><p>在这项工作中，我们开发并发布了Llama 2，这是一组从70亿到700亿参数的预训练和微调的大型语言模型（LLMs）。我们的微调LLMs，称为Llama 2-Chat，针对对话用例进行了优化。我们的模型在我们测试的大多数基准上胜过开源聊天模型，并且基于我们对有用性和安全性的人类评估，可能是闭源模型的合适替代品。我们提供了关于微调和改进Llama 2-Chat安全性的方法的详细描述，以便社区能够在我们的工作基础上构建，并有助于LLMs的负责任发展。</p><p><a href="https://huggingface.co/models?search=llama2">在此处查看所有Llama2模型</a></p><h3 id="提示："><a href="#提示：" class="headerlink" title="提示："></a>提示：</h3><ul><li>通过填写<a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/">此表格</a>可以获得Llama2模型的权重</li><li>该架构与第一个Llama非常相似，增加了Groupe Query Attention（GQA）<a href="https://arxiv.org/pdf/2305.13245.pdf">此论文</a>之后</li><li>将<code>config.pretraining_tp</code>设置为不同于1的值将激活线性层的更准确但更慢的计算，这应更好地匹配原始logits。</li><li>原始模型使用<code>pad_id = -1</code>，这意味着没有填充令牌。我们不能使用相同的逻辑，请确保使用<code>tokenizer.add_special_tokens({"pad_token":"&lt;pad&gt;"})</code>添加填充令牌，并相应地调整令牌嵌入大小。您还应设置<code>model.config.pad_token_id</code>。模型的embed_tokens层用<code>self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.config.padding_idx)</code>初始化，确保编码填充令牌将输出零，因此在初始化时传递它是推荐的。</li><li>填写表格并获得模型检查点的访问权限后，您应该能够使用已转换的检查点。否则，如果您正在转换自己的模型，请随时使用转换脚本。可以使用以下（示例）命令调用脚本：<figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python src/transformers/models/llama/convert_llama_weights_to_hf.py \</span><br><span class="line">    --input_dir /path/to/downloaded/llama/weights --model_size 7B --output_dir /output/path</span><br></pre></td></tr></tbody></table></figure></li></ul><h2 id="模型详情"><a href="#模型详情" class="headerlink" title="模型详情"></a>模型详情</h2><p>注意：使用该模型受 Meta 许可证的约束。为了下载模型权重和分词器，请访问网站并在请求访问之前接受许可证。</p><p>Meta 开发并公开发布了 Llama 2 系列大型语言模型（LLMs），这是一系列规模从 70 亿到 700 亿参数的预训练和微调的生成式文本模型。我们的微调 LLMs，称为 Llama-2-Chat，经过优化用于对话应用场景。Llama-2-Chat 模型在我们测试的大多数基准测试中优于开源聊天模型，并在我们的人工评估中在有用性和安全性方面与一些流行的闭源模型（如ChatGPT和PaLM）持平。</p><h2 id="模型开发者"><a href="#模型开发者" class="headerlink" title="模型开发者"></a>模型开发者</h2><p>Model Developers Meta</p><h2 id="不同版本"><a href="#不同版本" class="headerlink" title="不同版本"></a>不同版本</h2><p>Llama 2 有不同规模的参数版本，包括 7B、13B 和 70B，以及预训练和微调的变体。</p><h2 id="输入输出"><a href="#输入输出" class="headerlink" title="输入输出"></a>输入输出</h2><p>输入模型仅支持文本输入。</p><p>输出模型仅生成文本。</p><h2 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h2><p>Llama 2 是一种自回归语言模型，采用了优化的 Transformer 架构。微调版本使用有监督的微调（SFT）和基于人类反馈的强化学习（RLHF）来与人类对 helpfulness 和 safety 的偏好保持一致。</p><h2 id="训练数据"><a href="#训练数据" class="headerlink" title="训练数据"></a>训练数据</h2><table><thead><tr><th>模型名称</th><th>训练数据</th><th>参数规模</th><th>内容长度</th><th>GQA</th><th>Tokens</th><th>LR</th></tr></thead><tbody><tr><td>Llama 2</td><td>一种新的公开可用的在线数据混合</td><td>7B</td><td>4k</td><td>✗</td><td>2.0T</td><td>3.0 x 10-4</td></tr><tr><td>Llama 2</td><td>一种新的公开可用的在线数据混合</td><td>13B</td><td>4k</td><td>✗</td><td>2.0T</td><td>3.0 x 10-4</td></tr><tr><td>Llama 2</td><td>一种新的公开可用的在线数据混合</td><td>70B</td><td>4k</td><td>✔</td><td>2.0T</td><td>1.5 x 10-4</td></tr></tbody></table><p>注：Token counts 仅指预训练数据。所有模型都使用全局 batch-size 为 4M tokens 进行训练。规模更大的模型（70B）使用 Grouped-Query Attention（GQA）来提高推理可伸缩性。</p><h2 id="模型训练日期"><a href="#模型训练日期" class="headerlink" title="模型训练日期"></a>模型训练日期</h2><p>Llama 2 在2023年1月至2023年7月之间进行训练。</p><h2 id="状态"><a href="#状态" class="headerlink" title="状态"></a>状态</h2><p>这是一个在离线数据集上训练的静态模型。随着我们根据社区反馈改进模型的安全性，将发布微调版本的未来版本。</p><h2 id="许可证"><a href="#许可证" class="headerlink" title="许可证"></a>许可证</h2><p>定制的商业许可证可在以下网址获取：<a href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/">https://ai.meta.com/resources/models-and-libraries/llama-downloads/</a></p><h2 id="研究论文"><a href="#研究论文" class="headerlink" title="研究论文"></a>研究论文</h2><p>《Llama-2: Open Foundation and Fine-tuned Chat Models》</p><h2 id="使用目的"><a href="#使用目的" class="headerlink" title="使用目的"></a>使用目的</h2><h3 id="预期用途"><a href="#预期用途" class="headerlink" title="预期用途"></a>预期用途</h3><p>Llama 2 旨在用于英语商业和研究用途。微调模型适用于类似助理的聊天应用，而预训练模型可适应多种自然语言生成任务。</p><p>要获得聊天版本的预期功能和性能，需要遵循特定的格式，包括 INST 和 &lt;<sys>&gt; 标签、BOS 和 EOS tokens，以及它们之间的空格和换行符（我们建议对输入调用 strip() 方法，以避免双空格）。有关详情，请参阅我们在 GitHub 上的参考代码：chat_completion。</sys></p><h3 id="不在范围内的用途"><a href="#不在范围内的用途" class="headerlink" title="不在范围内的用途"></a>不在范围内的用途</h3><ul><li>用于违反适用法律法规（包括贸易合规法）的任何方式。</li><li>用于除英语以外的其他语言。</li><li>用于 Llama 2 可接受使用政策和许可协议所禁止的任何其他方式。</li></ul><h2 id="硬件和软件"><a href="#硬件和软件" class="headerlink" title="硬件和软件"></a>硬件和软件</h2><h3 id="训练因素"><a href="#训练因素" class="headerlink" title="训练因素"></a>训练因素</h3><p>我们使用自定义训练库、Meta 的 Research Super Cluster 以及生产集群进行预训练。微调、标注和评估也是在第三方云计算上执行的。</p><h3 id="碳足迹"><a href="#碳足迹" class="headerlink" title="碳足迹"></a>碳足迹</h3><p>预训练过程中使用了累计 330 万 GPU 小时的计算，使用的硬件类型为 A100-80GB（TDP 为 350-400W）。预计总排放量为 539 tCO2eq，其中 100% 由 Meta 的可持续性计划抵消。</p><table><thead><tr><th>模型</th><th>时间（GPU 小时）</th><th>功耗（瓦）</th><th>排放碳量（tCO2eq）</th></tr></thead><tbody><tr><td>Llama 2 7B</td><td>184,320</td><td>400</td><td>31.22</td></tr><tr><td>Llama 2 13B</td><td>368,640</td><td>400</td><td>62.44</td></tr><tr><td>Llama 2 70B</td><td>1,720,320</td><td>400</td><td>291.42</td></tr><tr><td>总计</td><td>3,311,616</td><td></td><td>539.00</td></tr></tbody></table><p>预训练期间的二氧化碳排放量。时间：每个模型训练所需的总 GPU 时间。功耗：用于所使用的 GPU 设备的每个 GPU 的峰值功率容量，调整后的</p><p>功耗使用效率。100% 的排放直接由 Meta 的可持续性计划抵消，因为我们正在公开发布这些模型，预训练成本不需要由他人承担。</p><h2 id="训练数据-1"><a href="#训练数据-1" class="headerlink" title="训练数据"></a>训练数据</h2><h3 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h3><p>Llama 2 在来自公开来源的数据中预训练了 2 万亿个 tokens。微调数据包括公开可用的指导数据集，以及一百万个新的人工标注示例。预训练和微调数据集均不包含 Meta 用户数据。</p><h3 id="数据新鲜度"><a href="#数据新鲜度" class="headerlink" title="数据新鲜度"></a>数据新鲜度</h3><p>预训练数据截止日期为 2022 年 9 月，但一些微调数据更近，最多至 2023 年 7 月。</p><h2 id="评估结果"><a href="#评估结果" class="headerlink" title="评估结果"></a>评估结果</h2><p>在此部分，我们报告了 Llama 1 和 Llama 2 模型在标准学术基准测试上的结果。对于所有评估，我们使用我们的内部评估库。</p><table><thead><tr><th>模型</th><th>规模</th><th>Code</th><th>常识推理</th><th>世界知识</th><th>阅读理解</th><th>数学</th><th>MMLU</th><th>BBH</th><th>AGI Eval</th></tr></thead><tbody><tr><td>Llama 1</td><td>7B</td><td>14.1</td><td>60.8</td><td>46.2</td><td>58.5</td><td>6.95</td><td>35.1</td><td>30.3</td><td>23.9</td></tr><tr><td>Llama 1</td><td>13B</td><td>18.9</td><td>66.1</td><td>52.6</td><td>62.3</td><td>10.9</td><td>46.9</td><td>37.0</td><td>33.9</td></tr><tr><td>Llama 1</td><td>33B</td><td>26.0</td><td>70.0</td><td>58.4</td><td>67.6</td><td>21.4</td><td>57.8</td><td>39.8</td><td>41.7</td></tr><tr><td>Llama 1</td><td>65B</td><td>30.7</td><td>70.7</td><td>60.5</td><td>68.6</td><td>30.8</td><td>63.4</td><td>43.5</td><td>47.6</td></tr><tr><td>Llama 2</td><td>7B</td><td>16.8</td><td>63.9</td><td>48.9</td><td>61.3</td><td>14.6</td><td>45.3</td><td>32.6</td><td>29.3</td></tr><tr><td>Llama 2</td><td>13B</td><td>24.5</td><td>66.9</td><td>55.4</td><td>65.8</td><td>28.7</td><td>54.8</td><td>39.4</td><td>39.1</td></tr><tr><td>Llama 2</td><td>70B</td><td>37.5</td><td>71.9</td><td>63.6</td><td>69.4</td><td>35.2</td><td>68.9</td><td>51.2</td><td>54.2</td></tr></tbody></table><p>模型在 grouped academic benchmarks 上的整体表现。Code：我们报告模型在 HumanEval 和 MBPP 上的平均 pass@1 分数。常识推理：我们报告 PIQA、SIQA、HellaSwag、WinoGrande、ARC easy 和 challenge、OpenBookQA 和 CommonsenseQA 的平均分数。我们对 CommonSenseQA 进行了 7-shot 结果评估，对其他所有基准测试进行了 0-shot 结果评估。世界知识：我们在 NaturalQuestions 和 TriviaQA 上进行 5-shot 性能评估并报告平均分数。阅读理解：对于阅读理解，我们报告 SQuAD、QuAC 和 BoolQ 的 0-shot 平均分数。数学：我们报告 GSM8K（8-shot）和 MATH（4-shot）基准测试的平均分数。</p><h3 id="TruthfulQA-和-Toxigen"><a href="#TruthfulQA-和-Toxigen" class="headerlink" title="TruthfulQA 和 Toxigen"></a>TruthfulQA 和 Toxigen</h3><table><thead><tr><th>模型</th><th>规模</th><th>TruthfulQA</th><th>Toxigen</th></tr></thead><tbody><tr><td>Llama 1</td><td>7B</td><td>27.42</td><td>23.00</td></tr><tr><td>Llama 1</td><td>13B</td><td>41.74</td><td>23.08</td></tr><tr><td>Llama 1</td><td>33B</td><td>44.19</td><td>22.57</td></tr><tr><td>Llama 1</td><td>65B</td><td>48.71</td><td>21.77</td></tr><tr><td>Llama 2</td><td>7B</td><td>33.29</td><td>21.25</td></tr><tr><td>Llama 2</td><td>13B</td><td>41.86</td><td>26.10</td></tr><tr><td>Llama 2</td><td>70B</td><td>50.18</td><td>24.60</td></tr></tbody></table><p>预训练 LLMs 在自动安全基准测试上的评估结果。对于 TruthfulQA，我们呈现同时具有真实性和信息量的生成百分比（百分比越高越好）。对于 ToxiGen，我们呈现有害生成的百分比（百分比越小越好）。</p><h3 id="TruthfulQA-和-Toxigen（微调版本-LLMs）"><a href="#TruthfulQA-和-Toxigen（微调版本-LLMs）" class="headerlink" title="TruthfulQA 和 Toxigen（微调版本 LLMs）"></a>TruthfulQA 和 Toxigen（微调版本 LLMs）</h3><table><thead><tr><th>模型</th><th>规模</th><th>TruthfulQA</th><th>Toxigen</th></tr></thead><tbody><tr><td>Llama-2-Chat</td><td>7B</td><td>57.04</td><td>0.00</td></tr><tr><td>Llama-2-Chat</td><td>13B</td><td>62.18</td><td>0.00</td></tr><tr><td>Llama-2-Chat</td><td>70B</td><td>64.14</td><td>0.01</td></tr></tbody></table><p>不同安全数据集上微调 LLMs 的评估结果。度量标准定义同上。</p><h2 id="道德考虑和局限性"><a href="#道德考虑和局限性" class="headerlink" title="道德考虑和局限性"></a>道德考虑和局限性</h2><p>Llama 2 是一项具有风险的新技术。迄今为止的测试仅涵盖了英语，并且无法覆盖所有场景。因此，与所有 LLMs 一样，Llama 2 的潜在输出无法事先预测，并且在某些情况下可能会产生不准确、带偏见或其他不可取的响应。因此，在部署任何 Llama 2 应用程序之前，开发人员应根据其特定的模型应用进行安全测试和调整。</p><p>请参阅“负责任使用指南”，网址为：<a href="https://ai.meta.com/llama/responsible-use-guide/">https://ai.meta.com/llama/responsible-use-guide/</a></p><h2 id="报告问题"><a href="#报告问题" class="headerlink" title="报告问题"></a>报告问题</h2><p>请通过以下方式之一报告任何软件“bug”或模型的其他问题：</p><ul><li>报告模型问题：[github.com/facebookresearch/llama](<a href="https://github/">https://github</a></li></ul><p>.com/facebookresearch/llama)</p><ul><li>报告模型生成的有问题内容：<a href="https://developers.facebook.com/llama_output_feedback">developers.facebook.com/llama_output_feedback</a></li><li>报告 bug 和安全问题：<a href="https://facebook.com/whitehat/info">facebook.com/whitehat/info</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Training-Llama-2-Model-on-Single-GPU-with-int8-Quantization-and-LoRA&quot;&gt;&lt;a href=&quot;#Training-Llama-2-Model-on-Single-GPU-with-int8-Quant</summary>
      
    
    
    
    <category term="NLP Insights" scheme="https://chenhuiyu.github.io/categories/NLP-Insights/"/>
    
    
    <category term="LLM" scheme="https://chenhuiyu.github.io/tags/LLM/"/>
    
  </entry>
  
</feed>
