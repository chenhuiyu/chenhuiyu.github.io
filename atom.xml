<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>黑头呆鱼进化之旅</title>
  
  <subtitle>只身打码过草原</subtitle>
  <link href="https://chenhuiyu.github.io/atom.xml" rel="self"/>
  
  <link href="https://chenhuiyu.github.io/"/>
  <updated>2025-03-06T09:49:52.873Z</updated>
  <id>https://chenhuiyu.github.io/</id>
  
  <author>
    <name>Huiyu Chen</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Decoder-only与Encoder-only模型Padding策略的差异</title>
    <link href="https://chenhuiyu.github.io/2025/03/06/NLP%20Insights/Decoder%E6%A8%A1%E5%9E%8B%E5%92%8CEncoder%E6%A8%A1%E5%9E%8B%E5%9C%A8Padding%E4%B8%AD%E7%9A%84%E4%B8%8D%E5%90%8C/"/>
    <id>https://chenhuiyu.github.io/2025/03/06/NLP%20Insights/Decoder%E6%A8%A1%E5%9E%8B%E5%92%8CEncoder%E6%A8%A1%E5%9E%8B%E5%9C%A8Padding%E4%B8%AD%E7%9A%84%E4%B8%8D%E5%90%8C/</id>
    <published>2025-03-06T09:43:10.000Z</published>
    <updated>2025-03-06T09:49:52.873Z</updated>
    
    <content type="html"><![CDATA[<h2 id="📌-Padding-的含义"><a href="#📌-Padding-的含义" class="headerlink" title="📌 Padding 的含义"></a>📌 <strong>Padding 的含义</strong></h2><p>在大模型 (<strong>LLM</strong>) 中，<strong>padding</strong> 是用于将不同长度的序列调整为同一长度的方法，以便于批量 (<strong>batch</strong>) 处理。</p><p>例如：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">句子1: "I love NLP"</span><br><span class="line">句子2: "Padding is useful in LLM training"</span><br></pre></td></tr></tbody></table></figure><p>使用 <code>&lt;pad&gt;</code> token 进行对齐：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">"I love NLP &lt;pad&gt; &lt;pad&gt; &lt;pad&gt;"</span><br><span class="line">"Padding is useful in LLM training"</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="📌-Padding-位置的选择：Left-vs-Right"><a href="#📌-Padding-位置的选择：Left-vs-Right" class="headerlink" title="📌 Padding 位置的选择：Left vs Right"></a>📌 <strong>Padding 位置的选择：Left vs Right</strong></h2><p>Padding 有两种常见方式：</p><ul><li><p><strong>Right padding</strong>（右填充）：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">"I love NLP &lt;pad&gt; &lt;pad&gt;"</span><br></pre></td></tr></tbody></table></figure></li><li><p><strong>Left padding</strong>（左填充）：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">"&lt;pad&gt; &lt;pad&gt; I love NLP"</span><br></pre></td></tr></tbody></table></figure></li></ul><p>通常：</p><ul><li><strong>Decoder-only 模型</strong>（如 GPT, Llama）：采用 <strong>Left padding</strong></li><li><strong>Encoder-only 模型</strong>（如 BERT）：采用 <strong>Right padding</strong></li></ul><p>具体而言，Transformer 模型通常分为三类结构：</p><div class="table-container"><table><thead><tr><th>模型类型</th><th>代表模型</th><th>特征</th><th>常见用途</th></tr></thead><tbody><tr><td><strong>Encoder-only</strong></td><td><strong>BERT</strong>、RoBERTa、ALBERT、ELECTRA</td><td>双向注意力（Bidirectional Attention）</td><td>自然语言理解（NLU），如文本分类、序列标注</td></tr><tr><td><strong>Decoder-only</strong></td><td>GPT、GPT-2、GPT-3、GPT-4、LLaMA、Mistral</td><td>单向自回归注意力（Causal Attention）</td><td>文本生成、聊天、写作</td></tr><tr><td><strong>Encoder-Decoder</strong></td><td>Transformer原始论文中的模型、T5、BART、mT5、PEGASUS</td><td>Encoder为双向注意力，Decoder为单向自回归注意力</td><td>机器翻译、摘要生成、对话</td></tr></tbody></table></div><hr><h2 id="📌-为什么-Encoder-only-模型（如BERT）采用-Right-padding？"><a href="#📌-为什么-Encoder-only-模型（如BERT）采用-Right-padding？" class="headerlink" title="📌 为什么 Encoder-only 模型（如BERT）采用 Right padding？"></a>📌 为什么 Encoder-only 模型（如BERT）采用 Right padding？</h2><ul><li><strong>Encoder-only 模型</strong>（如 BERT）的核心目标是获得<strong>每个 token 的嵌入表示</strong>（Embedding representation）。</li><li>此类模型为<strong>双向注意力（Bidirectional Attention）</strong>，每个 token 可同时关注上下文，因此<strong>位置的轻微变化不会对结果造成严重干扰</strong>。</li><li>此外，encoder-only 模型中通常有特殊 token（如 <code>[CLS]</code>），位置相对稳定，用于句子分类或表示，因此采用 <strong>right padding</strong> 更自然，也更合理。</li></ul><p>示例说明：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[CLS] Hello I love NLP [SEP] &lt;pad&gt; &lt;pad&gt;</span><br></pre></td></tr></tbody></table></figure><ul><li>右填充后，<code>[CLS]</code> 和 <code>[SEP]</code> token 位置稳定，且便于模型专注于前面的有效信息。</li></ul><hr><h2 id="📌-为什么-Decoder-only-LLM-采用-Left-padding？"><a href="#📌-为什么-Decoder-only-LLM-采用-Left-padding？" class="headerlink" title="📌 为什么 Decoder-only LLM 采用 Left padding？"></a>📌 为什么 Decoder-only LLM 采用 Left padding？</h2><p>以 GPT 为代表的 <strong>Decoder-only 模型</strong> 是自回归（<strong>Autoregressive</strong>）模型，每个词的生成仅依赖于当前及之前的词，未来词不可见。因此：</p><ul><li><strong>位置编码的稳定性</strong>：<br>左填充确保真实 token 的相对位置稳定，模型生成新 token 时位置编码始终稳定于序列末尾。<ul><li>当采用<strong>绝对位置编码</strong>（Absolute Positional Encoding）时，每个 token（包括 <code>&lt;pad&gt;</code>）都有对应的位置编号。</li><li>对于左填充的 padding tokens，虽然它们占据了位置编号（如 1、2），但模型通过<strong>掩码机制</strong>忽略其对注意力和输出结果的影响。<br>示例：</li></ul></li></ul><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">位置编码: [ 1      2      3      4      5      6 ]</span><br><span class="line">Token:   [ &lt;pad&gt;, &lt;pad&gt;, Hello,  I,   love,  NLP ]</span><br><span class="line">掩码:     [  0,     0,     1,     1,     1,    1 ]</span><br></pre></td></tr></tbody></table></figure><ul><li>模型只关注掩码为 1 的有效 token，而忽略掩码为 0 的 padding tokens。</li><li><strong>注意力掩码（Attention Mask）</strong>：<br>左侧的 <code>&lt;pad&gt;</code> 会被<strong>注意力掩码（attention mask）忽略</strong>，从而避免 padding token 干扰有效 token 的位置编码和注意力计算。</li></ul><p>示例说明：</p><div class="table-container"><table><thead><tr><th>Token</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th></tr></thead><tbody><tr><td>Left</td><td><code>&lt;pad&gt;</code></td><td><code>&lt;pad&gt;</code></td><td>Hello</td><td>I</td><td>love</td><td>NLP</td></tr><tr><td>Right</td><td>Hello</td><td>I</td><td>love</td><td>NLP</td><td><code>&lt;pad&gt;</code></td><td><code>&lt;pad&gt;</code></td></tr></tbody></table></div><ul><li><strong>Left padding</strong> 下，最后有效 token 始终在同一位置（6）。</li><li><strong>Right padding</strong> 下，token 的位置随序列长度变化，影响位置编码的稳定性。</li></ul><hr><h2 id="📌-Padding-在训练与推理阶段的差异"><a href="#📌-Padding-在训练与推理阶段的差异" class="headerlink" title="📌 Padding 在训练与推理阶段的差异"></a>📌 <strong>Padding 在训练与推理阶段的差异</strong></h2><div class="table-container"><table><thead><tr><th>阶段 (Phase)</th><th>Padding 策略</th><th>原因</th></tr></thead><tbody><tr><td><strong>训练 (Training)</strong></td><td>批量处理时，Decoder-only 常用左填充；Encoder-only 模型则常用右填充</td><td>批量处理，加快计算效率</td></tr><tr><td><strong>推理 (Inference)</strong></td><td>通常单条序列，无需 padding；若需要批量推理，仍采用左填充</td><td>稳定位置编码</td></tr></tbody></table></div><hr><h2 id="📌-总结与关键要点（TL-DR）"><a href="#📌-总结与关键要点（TL-DR）" class="headerlink" title="📌 总结与关键要点（TL;DR）"></a>📌 <strong>总结与关键要点（TL;DR）</strong></h2><ul><li><strong>Padding</strong> 用于序列长度标准化。</li><li><strong>Decoder-only LLMs (GPT, Llama)</strong> 通常采用<strong>左填充（Left padding）</strong>，目的是<strong>稳定位置编码并避免未来信息泄漏</strong>；左侧 padding 会被掩码忽略，不干扰模型预测。</li><li><strong>Encoder-only 模型（如BERT系列）</strong>通常采用<strong>右填充（Right padding）</strong>，因为模型为双向注意力，且特殊token（如<code>[CLS]</code>）位置需要保持稳定。</li><li>位置编码中虽然 padding token 占位，但会被<strong>注意力掩码</strong>有效屏蔽，不影响模型的最终输出。</li></ul><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;📌-Padding-的含义&quot;&gt;&lt;a href=&quot;#📌-Padding-的含义&quot; class=&quot;headerlink&quot; title=&quot;📌 Padding 的含义&quot;&gt;&lt;/a&gt;📌 &lt;strong&gt;Padding 的含义&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;在大模型 </summary>
      
    
    
    
    <category term="NLP Insights" scheme="https://chenhuiyu.github.io/categories/NLP-Insights/"/>
    
    
    <category term="LLM" scheme="https://chenhuiyu.github.io/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>推理 LLM 的可视化指南：探索推理时计算技术与 DeepSeek-R1</title>
    <link href="https://chenhuiyu.github.io/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/"/>
    <id>https://chenhuiyu.github.io/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/</id>
    <published>2025-02-11T03:50:29.000Z</published>
    <updated>2025-02-11T13:20:11.564Z</updated>
    
    <content type="html"><![CDATA[<h1 id="推理-LLM-的可视化指南：探索推理时计算技术与-DeepSeek-R1"><a href="#推理-LLM-的可视化指南：探索推理时计算技术与-DeepSeek-R1" class="headerlink" title="推理 LLM 的可视化指南：探索推理时计算技术与 DeepSeek-R1"></a>推理 LLM 的可视化指南：探索推理时计算技术与 DeepSeek-R1</h1><p><strong>原文地址</strong>：<a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-reasoning-llms">A Visual Guide to Reasoning LLMs</a></p><p>📅 作者：Maarten Grootendorst</p><p>📆 日期：2025 年 2 月 3 日</p><hr><h2 id="📌-引言"><a href="#📌-引言" class="headerlink" title="📌 引言"></a>📌 引言</h2><p>DeepSeek-R1、OpenAI o3-mini 和 Google Gemini 2.0 Flash Thinking 是如何通过“推理”框架将 <strong>LLM（大型语言模型, Large Language Models）</strong> 扩展到新高度的典型示例。</p><p>它们标志着从 <strong>扩展训练时计算（train-time compute）</strong> 到 <strong>扩展推理时计算（test-time compute）</strong> 的范式转变。</p><p>在本篇文章中，我们提供了 <strong>超过 40 张定制可视化图表</strong>，带你深入探索：</p><ul><li><strong>推理 LLM（Reasoning LLMs）</strong> 领域</li><li><strong>推理时计算（Test-Time Compute）</strong> 机制</li><li><strong>DeepSeek-R1</strong> 的核心思想</li></ul><p>我们将逐步介绍相关概念，帮助你建立对这一新范式的直觉理解。<br><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/i24pmg2.png" class=""></p><hr><h2 id="📖-什么是推理-LLM？"><a href="#📖-什么是推理-LLM？" class="headerlink" title="📖 什么是推理 LLM？"></a>📖 什么是推理 LLM？</h2><p>与普通 <strong>LLM（Large Language Models，大型语言模型）</strong> 相比，<strong>推理 LLM</strong> 在回答问题之前，往往会将问题 <strong>分解为更小的步骤</strong>（通常称为 <strong>推理步骤（Reasoning Steps）</strong> 或 <strong>思考过程（Thought Process）</strong>）。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_143007.png" class=""><h3 id="🧠-“推理步骤”-或-“思考过程”-是什么？"><a href="#🧠-“推理步骤”-或-“思考过程”-是什么？" class="headerlink" title="🧠 “推理步骤” 或 “思考过程” 是什么？"></a>🧠 “推理步骤” 或 “思考过程” 是什么？</h3><p>尽管我们可以哲学化地探讨 LLM 是否真的能够像人类一样思考，但这些推理步骤实际上是将推理过程 分解为更小、更结构化的推断。<strong>推理 LLM 采用的是结构化推理方式</strong>，即：</p><ul><li><strong>普通 LLM</strong>：直接输出答案</li><li><strong>推理 LLM</strong>：通过系统性推理生成答案</li></ul><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_143054.png" class=""><p>换句话说，推理 LLM 不是<strong>学习“回答什么”</strong>，而是<strong>学习“如何回答”</strong>！</p><p>要理解推理 LLM 的构建原理，我们首先需要探讨 <strong>训练时计算（Train-Time Compute）</strong> 和 <strong>推理时计算（Test-Time Compute）</strong> 之间的差异。</p><hr><h2 id="🔍-什么是训练时计算（Train-time-Compute）？"><a href="#🔍-什么是训练时计算（Train-time-Compute）？" class="headerlink" title="🔍 什么是训练时计算（Train-time Compute）？"></a>🔍 什么是训练时计算（Train-time Compute）？</h2><p>直到 2024 年年中，为了在 <strong>预训练（Pretraining）</strong> 期间提高 LLM 的性能，研究人员通常会扩大以下规模：</p><ul><li><strong>模型参数数量（# of Parameters）</strong></li><li><strong>数据集规模（# of Tokens）</strong></li><li><strong>计算量（# of FLOPs, Floating Point Operations）</strong></li></ul><p>这些合称为 <strong>训练时计算（Train-time Compute）</strong>，即 <strong>“AI 的化石燃料”</strong>，指的是：</p><blockquote><p><strong>预训练预算越大，最终得到的模型就越好。</strong></p><p>训练时计算（Train-Time Compute）包括<strong>训练（training）</strong>所需的计算，以及<strong>微调（fine-tuning）</strong>所需的计算。长期以来，一直是提高 LLM 性能的主要关注点。</p></blockquote><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_143927.png" class=""><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_144121.png" class=""><h3 id="🔢-规模定律（Scaling-Laws）"><a href="#🔢-规模定律（Scaling-Laws）" class="headerlink" title="🔢 规模定律（Scaling Laws）"></a>🔢 规模定律（Scaling Laws）</h3><p>在 <strong>LLM（大型语言模型）</strong> 研究领域，<strong>模型规模（Scale）</strong> 与 <strong>模型性能（Performance）</strong> 之间的关系被称为 <strong>规模定律（Scaling Laws）</strong>。这些定律通常用于描述 <strong>计算资源、数据规模和模型参数</strong> 如何影响模型的整体表现。</p><p>这些关系通常以 <strong>对数-对数（log-log）</strong> 方式呈现，并且在图表上通常显示为一条 <strong>近似直线</strong>，以突出计算量的巨大增长。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_144846.png" class=""><p>这张图片展示了<strong>不同坐标尺度（线性 vs. 对数）对计算资源（Compute）和模型性能（Performance）之间关系的影响</strong>，强调了大模型增长的幂律关系（Power Law）。</p><ul><li><p><strong>左图（普通线性尺度 - Normal Scale）</strong></p><ul><li>横轴（X 轴）：计算资源（Compute），<strong>线性刻度</strong>。</li><li>纵轴（Y 轴）：性能（Performance），<strong>线性刻度</strong>。</li><li>曲线显示<strong>递减收益（Diminishing Returns）</strong>，即：<strong>随着计算资源的增加，性能增长趋缓</strong>，但仍然在上升。</li></ul></li><li><p><strong>右图（对数-对数尺度 - Log-log Scale）</strong></p><ul><li>横轴（X 轴）：计算资源（Compute），<strong>对数刻度</strong>。</li><li>纵轴（Y 轴）：性能（Performance），<strong>对数刻度</strong>。</li><li>在对数-对数尺度下，原本弯曲的曲线变成<strong>一条直线</strong>，说明计算资源和性能之间呈<strong>幂律关系（Power Law Relationship）</strong>。</li></ul></li></ul><p>这些定律通常遵循 <strong>幂律（Power Laws）</strong>，即：</p><blockquote><p><strong>某个变量（如计算量）增加，会导致另一个变量（如性能）按一定比例变化。</strong></p></blockquote><p>最著名的 <strong>规模定律</strong> 包括：</p><ul><li><strong>Kaplan 规模定律</strong>（Kaplan Scaling Law）：当计算资源一定时，<strong>增加模型的参数规模比增加数据规模更有效</strong>。表明模型性能与参数量、计算量和训练数据（Tokens）之间存在幂律关系，即 更多参数、更多计算资源能提升性能（GPT-3 论文提出）。</li><li><strong>Chinchilla 规模定律</strong>（Chinchilla Scaling Law）：模型的大小和数据规模同样重要，二者需 <strong>同步扩展</strong> 才能实现最佳性能（DeepMind 提出）。</li></ul><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_145639.png" class=""><p>这张图展示了<strong>大规模 AI 训练中的 Scaling Laws（缩放定律）</strong>，表明<strong>计算资源（Compute）、数据集规模（Dataset Size）和参数量（Parameters）</strong>对模型性能的影响。关键信息如下：</p><hr><p><strong>1. 纵轴（Y轴）：测试损失（Test Loss）</strong></p><ul><li><strong>目标是降低测试损失（Test Loss）</strong>，即提高模型的泛化性能。</li><li><strong>损失（L）越小，模型性能越好</strong>。</li></ul><p><strong>2. 横轴（X轴）：三种关键变量</strong></p><ul><li><p><strong>左图（Compute，计算资源）</strong>：</p><ul><li>X 轴是计算资源（PF-days, 非 embedding）。</li><li>计算资源越多，测试损失降低（性能提升）。</li><li>公式：  <script type="math/tex; mode=display">L = \left( \frac{C_{\text{min}}}{2.3 \times 10^8} \right)^{-0.050}</script></li><li><strong>体现计算资源的幂律关系</strong>：计算资源增加，损失减少，但收益递减（指数 -0.050）。</li></ul></li><li><p><strong>中图（Dataset Size，数据集规模）</strong>：</p><ul><li>X 轴是训练数据的 Token 数量。</li><li>数据规模越大，测试损失降低（性能提升）。</li><li>公式：<script type="math/tex; mode=display">L = \left( \frac{D}{5.4 \times 10^{13}} \right)^{-0.095}</script></li><li><strong>数据规模对损失的影响较大</strong>（指数 -0.095）。</li></ul></li><li><p><strong>右图（Parameters，参数量）</strong>：</p><ul><li>X 轴是模型参数量（非 embedding）。</li><li>参数数量越大，测试损失降低（性能提升）。</li><li>公式：<script type="math/tex; mode=display">L = \left( \frac{N}{8.8 \times 10^{13}} \right)^{-0.076}</script></li><li><strong>参数对损失的影响介于计算资源和数据规模之间</strong>（指数 -0.076）。</li></ul></li></ul><p>这些研究表明，<strong>模型规模、数据规模和计算资源必须协同扩展，才能最大化模型的性能</strong>。</p><ul><li><strong>计算资源增加 → 训练更强大模型</strong></li><li><strong>更多 Tokens → 更好泛化能力</strong></li><li><strong>参数增加 → 但需要与数据匹配，否则过拟合</strong></li></ul><p>Kaplan 规模定律认为，在 <strong>固定计算资源</strong> 的情况下，<strong>优先增加模型参数</strong> 通常比增加数据规模更有效。而 Chinchilla 规模定律则指出，<strong>模型参数和数据规模都应同步增长</strong>，以获得更优的模型性能。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_145814.png" class=""><p>然而，在 <strong>2024 年</strong>，研究人员发现，尽管计算资源、数据规模和模型参数 <strong>持续增长</strong>，但性能提升的 <strong>边际收益（Marginal Return）</strong> 却在 <strong>逐渐降低</strong>。</p><p>这引发了一个重要的问题：</p><p>❓ <strong>“我们是否已经遇到了 LLM 发展的瓶颈？”</strong></p><hr><h2 id="🚀-什么是推理时计算（Test-time-Compute）？"><a href="#🚀-什么是推理时计算（Test-time-Compute）？" class="headerlink" title="🚀 什么是推理时计算（Test-time Compute）？"></a>🚀 什么是推理时计算（Test-time Compute）？</h2><p>由于 <strong>训练时计算的成本极其昂贵</strong>，研究人员开始关注 <strong>推理时计算（Test-time Compute）</strong>，即：</p><blockquote><p><strong>让 LLM 在推理时“思考更长时间”</strong>，而非单纯依赖更大的模型和数据集。</p></blockquote><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_145856.png" class=""><p>对于<strong>非推理模型</strong>，它们通常 <strong>直接输出答案</strong>：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Q: 8 + 5 = ?</span><br><span class="line">A: 13</span><br></pre></td></tr></tbody></table></figure><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_145936.png" class=""><p>而<strong>推理模型</strong>则会 <strong>使用更多 token 进行推理</strong>，形成系统化的“思考”过程：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Q: 8 + 5 = ?</span><br><span class="line">A: 8 + 5 可拆解为 8 + 2 + 3 = 10 + 3 = 13</span><br></pre></td></tr></tbody></table></figure><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_150033.png" class=""><p>LLM 需要消耗计算资源（如显存计算）来生成答案。然而，如果所有计算资源都用于直接生成答案，那将会是低效的！</p><p>相反，通过提前生成包含额外信息、关系和新思考的更多 token，模型可以在推理过程中分配更多计算资源以生成最终答案。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_151252.png" class=""><p>这张图片展示了 <strong>大语言模型（LLM）</strong> 在计算过程中如何分配 <strong>token</strong>（标记）来优化推理能力和最终的回答质量。核心思想是：<strong>如果计算资源（如 GPU/VRAM 计算量）全部用于直接生成答案，而没有用于思考，那么效率会受到影响</strong>。相反，增加 <strong>思考过程</strong>（即生成更多的中间 token），可以提高模型的 <strong>推理能力</strong>，从而提升 <strong>最终的回答质量</strong>。</p><p><strong>1. Token 的使用与计算量</strong></p><ul><li><strong>LLM 生成答案是按 token 逐步输出的</strong>，每个 token 都会占用计算资源。</li><li><p><strong>分配更多的 token 进行思考</strong>，意味着模型可以在得出最终答案之前有更多的推理步骤，从而提高正确率。</p><p><strong>2. 三种不同的计算方式</strong></p></li><li><p><strong>场景 1（1 个 token：最少计算）</strong></p><ul><li>直接输出 <strong>“5”</strong> 作为答案。</li><li><strong>计算量最少</strong>，速度最快。</li><li><strong>如果问题较复杂，可能会出错</strong>，因为模型没有足够的计算时间来思考。</li></ul></li><li><p><strong>场景 2（6 个 token：中等计算）</strong></p><ul><li>模型生成一个简短的推理过程：<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Adding 3 and 2 gives 5</span><br></pre></td></tr></tbody></table></figure></li><li><strong>比第一种方法多了一些计算量</strong>，但仍然较为简洁。</li><li>这种方式适用于<strong>简单的数学运算或逻辑推理</strong>，但在更复杂的情况下仍可能出现错误。</li></ul></li><li><p><strong>场景 3（15 个 token：完整推理）</strong></p><ul><li>模型先进行详细的逐步推理：<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">3 + 1 = 4 , 4 + 1 = 5</span><br></pre></td></tr></tbody></table></figure>然后，模型再明确地总结：<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">the total is 5</span><br></pre></td></tr></tbody></table></figure></li><li><strong>推理过程更详细，占用的计算量最大</strong>。</li><li><strong>适用于需要多步推理的任务，如数学题、逻辑推理题等</strong>。</li></ul></li></ul><h3 id="🔢-规模定律（Scaling-Laws）-1"><a href="#🔢-规模定律（Scaling-Laws）-1" class="headerlink" title="🔢 规模定律（Scaling Laws）"></a>🔢 规模定律（Scaling Laws）</h3><p>相比于训练时计算，推理时计算的规模定律仍然较为新颖。值得注意的是，有两项研究揭示了推理时计算规模与训练时计算规模的关系。</p><p>首先，OpenAI 发表的一篇文章表明，推理时计算可能遵循与训练时计算相同的扩展趋势。<br><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_151722.png" class=""></p><blockquote><p><strong>来自“学习如何推理的 LLM”一文的注释图</strong>：红色虚线显示了 OpenAI 提出的新范式可能是推理时计算。<br>这张图展示了 <strong>训练时间计算（train-time compute）和测试时间计算（test-time compute）</strong> 对模型 <strong>pass@1 准确率（accuracy）</strong> 的影响，具体来说，它强调了 <strong>测试时间计算可能比训练时间计算更有利于扩展模型性能</strong>。</p><ol><li><strong>左图：训练时间计算 vs. 准确率</strong><ul><li><strong>X 轴（横轴）：训练时间计算（log scale，指数刻度）</strong>。</li><li><strong>Y 轴（纵轴）：pass@1 准确率</strong>（即模型在一次尝试中得到正确答案的概率）。</li><li><strong>黑色点</strong> 代表不同计算量下的模型表现，粉色虚线展示了大致的趋势。</li><li>可以看到，随着 <strong>训练计算量的增加，准确率逐渐提高</strong>，但增长趋势相对平稳。</li></ul></li></ol></blockquote><ol><li><strong>右图：测试时间计算 vs. 准确率</strong><ul><li><strong>X 轴（横轴）：测试时间计算（log scale）</strong>。</li><li><strong>Y 轴（纵轴）：pass@1 准确率</strong>。</li><li>同样，黑色点代表不同计算量下的模型表现，粉色虚线展示了大致的趋势。</li><li>这里可以看到，随着 <strong>测试时计算量增加，模型的准确率增长更显著，甚至超过了训练计算量的效果</strong>。<br>因此，他们认为，推理时计算的扩展可能代表着新的研究范式。</li></ul></li></ol><p>其次，一篇名为《Scaling Scaling Laws with Board Games》的论文研究了 AlphaZero 在不同计算量下玩 Hex 游戏的表现。<br><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_152050.png" class=""></p><blockquote><p><strong>来自“Scaling Scaling Laws with Board Games”一文的注释图</strong>：该图展示了他们如何构建不同规模的训练时计算和推理时计算。- <strong>AlphaZero</strong> 是 <strong>DeepMind</strong> 开发的一个 <strong>强化学习（Reinforcement Learning, RL）</strong> 训练的 AI。</p><ul><li>该算法通过 <strong>自我对弈（self-play）</strong> 训练，无需人为规则输入，即可掌握<strong>围棋、国际象棋、将棋等游戏</strong>。</li><li>它结合了 <strong>神经网络预测</strong> 和 <strong>蒙特卡洛树搜索（MCTS, Monte Carlo Tree Search）</strong> 来进行决策。</li></ul></blockquote><p>这张图片展示了 <strong>AlphaZero 算法</strong> 在<strong>训练阶段（train-time compute）和测试阶段（test-time compute）</strong>计算资源的不同应用。主要强调了：</p><ul><li><strong>训练时</strong>：依赖于<strong>更多参数和更长的训练时间</strong>来优化模型。</li><li><strong>测试时</strong>：依靠 <strong>更深入的树搜索（tree search）</strong> 来提升决策能力。</li></ul><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_152430.png" class=""><blockquote><p>来自“Scaling Scaling Laws with Board Games”一文的注释图：该图展示了训练时计算与推理时计算之间的关系。<br>研究结果表明，训练时计算和推理时计算紧密相关。每条虚线表示达到特定 ELO 分数所需的最小计算量。<br><strong>1. 坐标轴含义</strong></p><ul><li><strong>X 轴（横轴）：训练时计算量（Train-time Compute，FLOP-seconds）</strong></li><li><strong>Y 轴（纵轴）：推理时计算量（Test-time Compute，FLOP-seconds）</strong></li><li><strong>对数刻度（log scale）：计算量的增长呈指数级，而不是线性增长。</strong></li></ul></blockquote><p><strong>2. 关键数据趋势</strong></p><ul><li>不同颜色的曲线分别表示<strong>不同的 ELO 分数水平</strong>（-1500、-1250、-1000、-750、-500、-250）。</li><li><strong>虚线和实线</strong>：<ul><li><strong>虚线</strong> 表示某个 ELO 分数下的最优计算边界。</li><li><strong>实线</strong> 代表实际数据趋势。</li></ul></li></ul><ol><li><p><strong>训练计算和推理计算可以互相替代</strong></p><ul><li><strong>如果推理计算量增加（左上区域）</strong>，那么所需的训练计算量减少。</li><li><strong>如果训练计算量增加（右下区域）</strong>，那么所需的推理计算量减少。</li><li><strong>两者呈现负相关关系</strong>。</li></ul></li><li><p><strong>低训练计算 vs. 高推理计算</strong></p><ul><li>在 <strong>训练计算较少</strong> 的情况下（如左侧的红色圈），模型仍然可以达到相同的 ELO 水平，但需要 <strong>在推理时增加计算量</strong>（如更深的搜索树、更长的思考路径）。</li></ul></li><li><p><strong>高训练计算 vs. 低推理计算</strong></p><ul><li>在 <strong>训练计算充足</strong> 的情况下（如右侧的红色圈），模型可以<strong>减少推理计算需求</strong>，即 <strong>即使使用较少的搜索深度，仍然能获得较高的性能</strong>。</li></ul></li><li><p><strong>公式解释</strong></p><ul><li>公式：<script type="math/tex; mode=display">\log_{10}(\text{test compute}) = -1.2 \cdot \log_{10}(\text{train compute}) + 0.004 \cdot \text{elo} + 29</script></li><li>这说明：<ul><li><strong>训练计算（train compute）增加时，推理计算（test compute）减少（系数 -1.2）</strong>。</li><li><strong>更高的 ELO（更强的 AI）需要额外的计算（系数 0.004）</strong>。</li></ul></li></ul></li></ol><p>随着推理时计算扩展类似于训练时计算，研究范式正朝着“推理”模型利用更多推理时计算的方向发展。通过这种范式转变，这些“推理”模型不再单纯关注训练时计算（预训练和微调），而是平衡训练与推理。<br><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_152824.png" class=""></p><p>推理时计算甚至可以随长度扩展：<br><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_153922.png" class=""><br>这是我们在 DeepSeek-R1 研究中也将探讨的内容！</p><h3 id="📌-推理时计算的类别（Categories-of-Test-time-Compute）"><a href="#📌-推理时计算的类别（Categories-of-Test-time-Compute）" class="headerlink" title="📌 推理时计算的类别（Categories of Test-time Compute）"></a>📌 推理时计算的类别（Categories of Test-time Compute）</h3><p>推理模型（如 <strong>DeepSeek-R1</strong> 和 <strong>OpenAI o1</strong>）的成功表明，在推理过程中，除了简单地“思考更长时间”之外，还有更多的优化技术。</p><p>在本文中，我们将探讨 <strong>推理时计算（Test-time Compute）</strong> 的多种实现方式，包括：</p><ul><li><strong>链式思维（Chain-of-Thought）</strong></li><li><strong>答案修订（Revising Answers）</strong></li><li><strong>回溯推理（Backtracking）</strong></li><li><strong>多样性采样（Sampling）</strong></li><li><strong>其他方法</strong></li></ul><p>总体而言，推理时计算可归纳为以下 <strong>两大类别</strong>：</p><ol><li><p><strong>基于验证器的搜索（Search against Verifiers）</strong>  </p><ul><li>通过 <strong>采样多个答案</strong> 并 <strong>选择最佳答案</strong> 来优化推理。</li></ul></li><li><p><strong>修改提议分布（Modifying Proposal Distribution）</strong>  </p><ul><li>通过训练 <strong>“思考”过程</strong> 来提高推理能力。Proposal Distribution（提议分布，指在模型生成答案时，对不同可能答案的概率分布进行调整）</li></ul></li></ol><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_154456.png" class=""><p>从本质上讲：</p><ul><li><strong>基于验证器的搜索</strong> 更关注 <strong>输出质量</strong>（Output-focused）。</li><li><strong>修改提议分布</strong> 关注 <strong>输入结构</strong>（Input-focused）。</li></ul><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_154547.png" class=""><h3 id="🔍-两种主要验证器类型"><a href="#🔍-两种主要验证器类型" class="headerlink" title="🔍 两种主要验证器类型"></a>🔍 两种主要验证器类型</h3><p>为了更好地筛选和评估推理答案，我们引入了两种 <strong>验证器（Verifiers）</strong>：</p><ol><li><p><strong>结果奖励模型（Outcome Reward Models, ORM）</strong>  </p><ul><li>仅对最终答案进行评分，而不考虑推理过程。</li></ul></li><li><p><strong>过程奖励模型（Process Reward Models, PRM）</strong>  </p><ul><li>既评估最终答案，也对推理过程进行评分。</li></ul></li></ol><p>在接下来的部分，我们将详细探讨 <strong>如何将 ORM 和 PRM 应用于不同的验证方法</strong>！</p><p>顾名思义，<strong>结果奖励模型（Outcome Reward Model, ORM）</strong> 仅评估最终的答案质量，而不关注答案背后的推理过程：<br><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_160242.png" class=""></p><ul><li>ORM 只看最终输出，而不关心模型是如何得出这个答案的。</li></ul><p>相比之下，<strong>过程奖励模型（Process Reward Model, PRM）</strong> 则会评估推理过程本身：<br><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_160258.png" class=""></p><ul><li>PRM 既评估答案的正确性，也关注推理路径的合理性。</li></ul><h3 id="🧐-PRM-如何评估推理过程？"><a href="#🧐-PRM-如何评估推理过程？" class="headerlink" title="🧐 PRM 如何评估推理过程？"></a>🧐 PRM 如何评估推理过程？</h3><p>为了更清楚地说明推理步骤的重要性，让我们来看一个示例：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">问题：某个方程的解是多少？</span><br><span class="line"></span><br><span class="line">推理步骤 1：首先展开方程，得到 x = 3。</span><br><span class="line">推理步骤 2：错误地将 x = 3 改写为 x = 5。</span><br><span class="line">推理步骤 3：最终输出 x = 5。</span><br></pre></td></tr></tbody></table></figure><p>在上述示例中，虽然最终答案（x = 5）是错误的，但 ORM 仅评估最终输出，不会关注中间的错误推理。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_160332.png" class=""><p>或者在这个例子中，PRM 会发现 <strong>推理步骤 2 是错误的</strong>，并对此步骤给予低分，从而避免错误答案的出现。</p><hr><h3 id="🔍-ORM-vs-PRM-在推理中的应用"><a href="#🔍-ORM-vs-PRM-在推理中的应用" class="headerlink" title="🔍 ORM vs. PRM 在推理中的应用"></a>🔍 ORM vs. PRM 在推理中的应用</h3><p>现在你已经掌握了 <strong>结果奖励模型（ORM）</strong> 和 <strong>过程奖励模型（PRM）</strong> 之间的区别，我们接下来探讨如何将它们应用于各种 <strong>验证技术（Verification Techniques）</strong>。</p><h2 id="📌-基于验证器的搜索（Search-against-Verifiers）"><a href="#📌-基于验证器的搜索（Search-against-Verifiers）" class="headerlink" title="📌 基于验证器的搜索（Search against Verifiers）"></a>📌 基于验证器的搜索（Search against Verifiers）</h2><p>推理时计算的第一大类别是 <strong>基于验证器的搜索</strong>，它通常包含两个步骤：</p><ol><li><strong>生成多个推理过程和答案样本</strong></li><li><strong>使用验证器（奖励模型）对生成的输出进行评分</strong><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_160918.png" class=""></li></ol><h3 id="🤖-验证器的作用"><a href="#🤖-验证器的作用" class="headerlink" title="🤖 验证器的作用"></a>🤖 验证器的作用</h3><p>验证器通常是一个大型语言模型（LLM），经过微调以评估结果（ORM）或过程（PRM）。 使用验证器的一个主要优势是，无需重新训练或微调用于回答问题的大型语言模型（LLM），仅通过评分机制选择最佳答案。</p><hr><h3 id="✅-多数投票法（Majority-Voting）"><a href="#✅-多数投票法（Majority-Voting）" class="headerlink" title="✅ 多数投票法（Majority Voting）"></a>✅ 多数投票法（Majority Voting）</h3><p>最简单的方法是 <strong>不使用奖励模型或验证器</strong>，而是执行 <strong>多数投票（Majority Voting）</strong>。</p><p>📌 <strong>方法：</strong> 让 LLM 生成多个答案，选择出现次数最多的答案作为最终答案。</p><p>📌 <strong>示例：</strong></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Q: 15 × 3 = ?</span><br><span class="line">A1: 45</span><br><span class="line">A2: 42</span><br><span class="line">A3: 45</span><br><span class="line">最终答案: 45（因其出现频率最高）</span><br></pre></td></tr></tbody></table></figure><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_161249.png" class=""><p>这种方法也称为 <strong>自一致性（Self-Consistency）</strong>，强调 <strong>生成多个答案和推理步骤</strong> 的重要性。</p><hr><h3 id="🔢-Best-of-N-采样法（Best-of-N-Samples）"><a href="#🔢-Best-of-N-采样法（Best-of-N-Samples）" class="headerlink" title="🔢 Best-of-N 采样法（Best-of-N Samples）"></a>🔢 Best-of-N 采样法（Best-of-N Samples）</h3><p>Best-of-N 采样是第一个涉及验证器（Verifier）的方法，它的基本思想是生成 N 个样本答案，然后使用 奖励模型（Reward Model, RM） 对这些答案进行评分，并选择得分最高的答案。</p><p>📌 <strong>步骤：</strong></p><ol><li><strong>生成多个答案</strong>（使用较高或者不同的温度参数生成 N 个样本）。<img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_161825.png" class=""></li><li><strong>结果奖励模型（ORM, Outcome Reward Model）</strong>，每个答案都会通过 ORM 进行评分。选取得分最高的答案作为最终输出。<img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_161833.png" class="">📌 <strong>示例：</strong></li></ol><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Q: 8 + 5 = ?</span><br><span class="line">A1: 12 (得分 0.2)</span><br><span class="line">A2: 13 (得分 0.9)</span><br><span class="line">A3: 14 (得分 0.4)</span><br><span class="line">最终选择: A2（因其得分最高）</span><br></pre></td></tr></tbody></table></figure><p>📌 <strong>进一步优化：</strong></p><ul><li>若使用 <strong>PRM</strong>，则不只评估答案，还评估整个推理过程。与仅评估最终答案的结果奖励模型（ORM）不同，过程奖励模型（PRM）会评估推理过程的质量。PRM 关注推理的每个步骤，确保推理过程合理、连贯，并最终选择总评分最高的候选答案。<img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_161922.png" class=""></li><li><strong>加权 Best-of-N 采样（Weighted Best-of-N samples）</strong>:结合 ORM 和 PRM 两种验证方式，我们可以对所有候选答案进行加权评分，并选择总权重最高的答案。这种方法称为 加权 Best-of-N 采样（Weighted Best-of-N samples）：。<img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_162139.png" class=""></li></ul><hr><h3 id="🚀-使用过程奖励模型（PRM）的束搜索（Beam-Search）"><a href="#🚀-使用过程奖励模型（PRM）的束搜索（Beam-Search）" class="headerlink" title="🚀 使用过程奖励模型（PRM）的束搜索（Beam Search）"></a>🚀 使用过程奖励模型（PRM）的束搜索（Beam Search）</h3><p>在生成答案及其中间推理步骤的过程中，我们可以使用 <strong>束搜索（Beam Search）</strong> 进一步优化推理路径。</p><p>📌 <strong>束搜索的核心思想：</strong></p><ul><li>在推理过程中，生成多个可能的推理路径（称为“束”）。</li><li>使用 <strong>过程奖励模型（PRM, Process Reward Model）</strong> 对每条路径进行评分。</li><li>类似于 <strong>Tree of Thought</strong> 方法，始终保留得分最高的 <strong>前 3 条推理路径</strong>，并在推理过程中持续跟踪这些路径。</li><li>如果某条路径的得分较低（PRM 评分低），则提前停止该推理路径，以避免不必要的计算开销。</li></ul><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_162508.png" class=""><p>📌 <strong>优化后的答案筛选方式：</strong><br>最终，生成的所有答案将使用 <strong>Best-of-N 采样</strong> 方法进行加权评分，确保选出最佳推理路径的最终答案。</p><p>🚀 <strong>优势：</strong></p><ul><li>避免计算资源浪费，快速淘汰低质量推理路径。</li><li>结合 PRM，可以确保模型的推理过程更连贯、更符合逻辑。</li><li>通过 Best-of-N 方法进一步优化答案质量，使最终答案更加可靠。</li></ul><hr><h3 id="🎲-蒙特卡洛树搜索（Monte-Carlo-Tree-Search-MCTS）"><a href="#🎲-蒙特卡洛树搜索（Monte-Carlo-Tree-Search-MCTS）" class="headerlink" title="🎲 蒙特卡洛树搜索（Monte Carlo Tree Search, MCTS）"></a>🎲 蒙特卡洛树搜索（Monte Carlo Tree Search, MCTS）</h3><p>蒙特卡洛树搜索（Monte Carlo Tree Search, <strong>MCTS</strong>）是一种常用于决策树搜索的算法，在 LLM 的推理优化中也可以采用该方法。MCTS 通过四个步骤来优化推理路径：<br>📌 <strong>主要步骤：</strong></p><ol><li><strong>选择（Selection）：</strong> 根据预定义的公式，从当前搜索树中选择一个叶节点 进行扩展。</li><li><strong>扩展（Expand）：</strong> 在所选叶节点的基础上 创建新的子节点，以探索更多可能的推理路径。</li><li><strong>模拟（Rollouts）：</strong> 通过随机生成新的推理路径，持续扩展节点，直到达到终点（即得到最终答案）。</li><li><strong>回溯（Backpropagation）：</strong> 根据最终输出结果 更新父节点的评分，从而优化未来的搜索决策。</li></ol><p>在大语言模型（LLM）的推理过程中，我们通常希望找到最佳的推理路径，使其最终生成的答案最优。但在这个过程中，需要在 <strong>探索（Exploration）</strong> 和 <strong>利用（Exploitation）</strong> 之间取得平衡：</p><ul><li><strong>利用（Exploitation）</strong>：选择当前看起来最优的路径，以利用已知的高质量推理步骤。</li><li><strong>探索（Exploration）</strong>：选择访问次数较少的路径，以发现可能更优的推理步骤。</li></ul><h4 id="选择分数（Selection-Score）"><a href="#选择分数（Selection-Score）" class="headerlink" title="选择分数（Selection Score）"></a>选择分数（Selection Score）</h4><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_163149.png" class=""><p>在选择推理路径时，我们使用 <strong>选择分数（Selection Score）</strong> 计算每个推理步骤（即树的节点）的优先级，公式如下：</p><script type="math/tex; mode=display">\text{Selection Score} = \frac{\text{Total Node Reward}}{\text{Number of Node Visits}} + C \times \sqrt{\frac{\text{Number of Parent Node Visits}}{\text{Number of Node Visits}}}</script><p>其中：</p><ul><li><p><strong>第一项</strong>：<script type="math/tex">\frac{\text{Total Node Reward}}{\text{Number of Node Visits}}</script>（利用项，Exploitation Term）</p><ul><li><strong>Total Node Reward</strong>：该节点累计获得的奖励值（表示其历史表现）。</li><li><strong>Number of Node Visits</strong>：该节点被访问的次数。</li><li>这项计算的是该节点的 <strong>平均奖励值</strong>，高奖励的节点会被优先选择。</li></ul></li><li><p><strong>第二项</strong>：<script type="math/tex">C \times \sqrt{\frac{\text{Number of Parent Node Visits}}{\text{Number of Node Visits}}}</script>（探索项，Exploration Term）</p><ul><li><strong># of Parent Node Visits</strong>：父节点被访问的次数。</li><li><strong># of Node Visits</strong>：当前节点被访问的次数。</li><li><strong>C</strong>：一个超参数，控制探索与利用的平衡。</li><li>这项鼓励探索访问次数较少的节点，以防止过早陷入局部最优解。</li></ul></li></ul><p>总结：</p><ul><li><strong>第一项（Exploitation Term）</strong> 让算法倾向于选择 <strong>历史表现较好的路径</strong>。</li><li><strong>第二项（Exploration Term）</strong> 让算法倾向于 <strong>探索访问较少的路径</strong>，避免陷入局部最优。</li><li><strong>参数 C</strong> 控制这两者的平衡。</li></ul><h4 id="2-选择（Selection）与扩展（Expand）"><a href="#2-选择（Selection）与扩展（Expand）" class="headerlink" title="2. 选择（Selection）与扩展（Expand）"></a><strong>2. 选择（Selection）与扩展（Expand）</strong></h4><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_164346.png" class=""><p>这一阶段，我们使用 <strong>选择分数</strong> 来决定哪条推理路径值得继续扩展：</p><p><strong>（1）选择（Selection）</strong></p><ul><li><strong>输入：问题（Question）</strong></li><li><strong>LLM 生成多个推理步骤（Reasoning Steps）</strong><ul><li>例如，在图片中，LLM 生成了 3 个推理步骤：<ul><li><strong>Thought 1</strong>（评分 0.4）</li><li><strong>Thought 2</strong>（评分 0.2）</li><li><strong>Thought 3</strong>（评分 0.1）</li></ul></li></ul></li><li><strong>使用选择分数（Selection Score）选择最优路径</strong>（随机初始化）<ul><li>在示例中，评分最高的 <strong>Thought 1（0.4）</strong> 被选中。</li></ul></li></ul><p><strong>（2）扩展（Expand）</strong></p><ul><li><strong>在选中的推理路径上，生成新的推理步骤</strong></li><li>这些新推理步骤的初始值设为 0，表示它们还没有经过评估。</li></ul><p>这个过程类似于 <strong>MCTS 的拓展（Expansion）阶段</strong>，即：</p><ol><li>选择当前最优路径（使用 <strong>选择分数</strong>）。</li><li>在该路径下，扩展新的推理步骤（未评分的子节点）。</li></ol><h4 id="3-Rollouts（模拟）与-Backpropagation（反向传播）"><a href="#3-Rollouts（模拟）与-Backpropagation（反向传播）" class="headerlink" title="3. Rollouts（模拟）与 Backpropagation（反向传播）"></a><strong>3. Rollouts（模拟）与 Backpropagation（反向传播）</strong></h4><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_165052.png" class=""><p>一旦扩展了推理步骤，我们需要继续探索，并利用 <strong>模拟（Rollouts）</strong> 和 <strong>反向传播（Backpropagation）</strong> 来优化整个搜索过程。</p><h3 id="（3）Rollouts（模拟）"><a href="#（3）Rollouts（模拟）" class="headerlink" title="（3）Rollouts（模拟）"></a><strong>（3）Rollouts（模拟）</strong></h3><ul><li>选定路径后，我们继续展开推理步骤，直到 <strong>生成最终答案</strong>。</li><li>这个过程类似于 <strong>在 MCTS 中随机模拟游戏到结束</strong>：<ul><li>我们从当前节点出发，进行一系列推理，直到模型生成最终的答案。</li><li>在图片中，我们沿着 Thought 1（0.4） 继续展开推理步骤。</li><li>这些推理步骤最终会 <strong>生成多个答案</strong>（图片中紫色框）。</li></ul></li></ul><h3 id="（4）Backpropagation（反向传播）"><a href="#（4）Backpropagation（反向传播）" class="headerlink" title="（4）Backpropagation（反向传播）"></a><strong>（4）Backpropagation（反向传播）</strong></h3><ul><li>通过对 <strong>最终答案</strong> 进行评分，我们可以更新前面所有参与推理的节点分数：<ul><li><strong>PRM（Process Reward Model）</strong>：对推理步骤本身进行评分，衡量其合理性。</li><li><strong>ORM（Output Reward Model）</strong>：对最终答案进行评分，衡量其正确性。</li><li>这些评分 <strong>向上传播</strong>，更新 <strong>所有经过的节点</strong> 的奖励值。</li></ul></li><li>例如：<ul><li>在图片中，最终答案的评分导致 <strong>Thought 1</strong> 的评分从 0.4 提高到 <strong>0.8</strong>。</li><li>进一步向上传播，使得 <strong>父节点的选择分数也随之更新</strong>。</li></ul></li></ul><p>这个过程保证了：</p><ul><li><strong>较好的推理路径会逐渐获得更高的分数</strong>，提高被选中的概率。</li><li><strong>较差的推理路径会被逐渐淘汰</strong>，避免浪费计算资源。</li></ul><hr><h2 id="📌-修改提议分布（Modifying-Proposal-Distribution）"><a href="#📌-修改提议分布（Modifying-Proposal-Distribution）" class="headerlink" title="📌 修改提议分布（Modifying Proposal Distribution）"></a>📌 修改提议分布（Modifying Proposal Distribution）</h2><p><strong>修改提议分布（Modifying Proposal Distribution）</strong></p><p>在大语言模型（LLM）的推理过程中，我们可以通过修改提议分布（Modifying Proposal Distribution）来优化模型的推理能力。这种方法的核心思想是：</p><ul><li><strong>不再单纯依赖模型搜索正确推理步骤</strong>（基于输出的优化），</li><li><strong>而是让模型主动生成更优的推理步骤</strong>（基于输入的优化）。</li></ul><p>换句话说，我们不是在输出结果后进行检验，而是直接修改模型在推理过程中如何选择 token，让它更倾向于选择能够引导推理的 token，而不是立即输出最终答案。修改了用于采样补全（completions）、思维（thoughts）或标记（tokens）的概率分布。这种方法可以让模型生成的答案更加准确、可解释，并且在面对复杂问题时更具有鲁棒性（robustness）。</p><p><strong>1. 直接选择最高概率 Token（Greedy 选择）</strong></p><p>在默认情况下，LLM 生成多个可能的 token 作为输出候选项，并根据其概率进行排序，最终选择最高概率的 token 进行输出。这种方法称为<strong>贪心选择（Greedy Selection）</strong>。</p><p>你可以想象，我们有一个问题（question）和一个用于采样 token 的概率分布（distribution）。常见的策略是选择得分最高的 token。<br><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_170357.png" class=""></p><ul><li>例如，给定问题 <code>What is 3 + 2?</code>，LLM 可能会生成如下候选 token：<ul><li><code>5</code>（最高概率）</li><li><code>3</code></li><li><code>Adding</code></li><li><code>4</code></li><li><code>If</code></li></ul></li><li>在贪心策略下，模型会直接选择 <code>5</code> 作为最终答案，而不会进行推理。</li></ul><p>这种方法虽然快速，但存在如下问题：</p><ul><li><strong>缺乏推理能力</strong>：模型可能直接输出错误答案，因为它没有进行推理。</li><li><strong>可解释性差</strong>：对于复杂问题，用户无法理解模型是如何得出答案的。</li></ul><p><strong>2. 通过推理（Reasoning Before Answering）提高答案质量</strong></p><p>然而，请注意上图中有一些<strong>标记（tokens</strong>被标红。这些token更有可能引导模型进入一个合理的推理过程。虽然选择贪心（greedy）策略下得分最高的 token 不一定是错误的，但选择那些能引导模型进入推理过程的 token，通常会得到更好的答案。<br>让 LLM <strong>先进行推理，再给出答案</strong>，即：</p><ul><li>选择推理 token（如 <code>Adding</code>）</li><li>逐步生成推理过程，如：<ul><li><code>Adding → 3 and 2 gives → 5</code></li><li><code>If → 3 + 1 = 4, 4 + 1 = 5 → 5</code></li><li><code>The total is → 5</code></li></ul></li><li>通过推理链条逐步推导出 <code>5</code>，相比直接选择 <code>5</code>，这种方法更加可解释，并且能在复杂问题上表现更好。</li></ul><p><strong>3. 通过修改提议分布（Re-Ranking Token Probabilities）引导推理过程</strong><br><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_171002.png" class=""><br>当我们<strong>修改提议分布（proposal distribution，即 token 的概率分布）</strong>时，实际上是在<strong>重新排序（re-rank）</strong>这个分布，使得“推理相关”的 token 被选中的概率更高。<br>在这种方法下，我们调整 LLM 的提议分布，使其更倾向于选择推理 token，而非直接选择答案：</p><ul><li>默认情况下，<code>5</code> 具有最高概率，而 <code>Adding</code>、<code>If</code> 等推理 token 的概率较低。</li><li>通过修改提议分布，我们提高 <code>Adding</code>、<code>If</code> 的概率，使模型倾向于进行推理。</li></ul><p><strong>4. 如何实现修改提议分布？</strong></p><p>主要有两种方式：</p><ol><li><strong>通过 Prompt Engineering</strong><ul><li>修改 Prompt，引导模型生成推理步骤。</li><li>例如：<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Q: What is 3 + 2?</span><br><span class="line">A: Let's think step by step.</span><br></pre></td></tr></tbody></table></figure></li></ul></li><li><strong>训练模型更倾向于推理</strong><ul><li>在微调过程中，提供更多具有推理链的训练数据，让模型习惯生成推理 token。</li></ul></li></ol><p><strong>总结</strong></p><ul><li><strong>贪心选择（Greedy Selection）</strong>：快速，但缺乏推理，可解释性差。</li><li><strong>推理后回答（Reasoning Before Answering）</strong>：提高答案质量和可解释性。</li><li><strong>修改提议分布（Modifying Proposal Distribution）</strong>：调整 token 选择的概率，使模型更倾向于选择推理 token，提高整体答案的合理性。</li></ul><p>这种方法在<strong>数学计算、逻辑推理、法律推理等任务</strong>上尤为重要，使得 LLM <strong>不仅能“答对”，还能“说明白”</strong>。</p><h3 id="Prompting"><a href="#Prompting" class="headerlink" title="Prompting"></a><strong>Prompting</strong></h3><p>随着我们使用 <strong>prompt engineering</strong>（提示工程）来改进输出，我们会通过更新提示（prompt）来尝试提升模型的表现。这个过程也可能推动模型去展示先前我们看到的一些<strong>reasoning</strong>（推理）过程。</p><p><strong>1. 改变 Proposal Distribution</strong></p><p>在更改 <strong>proposal distribution</strong>时，我们可以给模型提供示例（也叫做 <strong>in-context learning</strong>），让它在生成答案时模仿类似的推理风格。下面的图就展示了一个示例的情形：</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_172130.png" class=""><p>&gt;<br>&gt;</p><blockquote><ul><li><strong>图示内容</strong>：左侧是一个简单的问题 “What is 3 + 2?”，模型内部用 “Thoughts” 表示隐藏的思考过程，比如：<ol><li>First, 3 and 1 gives 4.</li><li>Then, 4 and 1 gives 5.</li><li>I believe the answer is 5.</li></ol></li><li><strong>Answer</strong>（答案）：5</li><li>右侧用红色、蓝色等不同颜色的条形或方块表示推理过程的不同部分，示意有一部分属于隐藏的推理过程（红色），以及输出结果或若干中间步骤（蓝色）。</li></ul></blockquote><p>通过类似的示例，模型在推理时就可能模仿类似的格式来进行<strong>reasoning</strong>并给出最终答案。</p><p><strong>2. “Let’s think step-by-step” 的影响</strong></p><p>我们也可以通过在提示中直接使用 “Let’s think step-by-step” 来简化上述流程。这会改变模型的 <strong>proposal distribution</strong>，让 <strong>LLM</strong>（大型语言模型）倾向于在回答之前分步骤思考。如下图所示：</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_172232.png" class=""><p>&gt;<br>&gt;</p><blockquote><ul><li><strong>图示内容</strong>：这里将提示换成 “Let’s think step-by-step”，问题仍然是 “What is 3 + 2?”。</li><li>模型产生更显式的推理过程（用红色块示意），再输出正确答案 5。</li><li>整个思路类似图1，但更加突出“分步骤思考”对最终答案生成的影响。</li></ul></blockquote><p>然而，这并不意味着模型本身已经内化了这种推理能力——它<strong>并没有从根本上学会</strong>去“反思”或“修正”错误。如果模型一开始的推理过程是错误的，那么在这种静态且线性的流程中，它往往会一直延续这个错误，而不是对自身推理进行修正。</p><hr><h3 id="STaR（Self-Taught-Reasoner）"><a href="#STaR（Self-Taught-Reasoner）" class="headerlink" title="STaR（Self-Taught Reasoner）"></a><strong>STaR（Self-Taught Reasoner）</strong></h3><p>除了通过 <strong>prompting</strong>（提示）让模型临时展示推理步骤，我们还可以让模型在训练中因为“产生正确推理步骤”而得到奖励，从而让它真正“学会”推理。这通常需要在<strong>大量带有推理过程的数据</strong>上进行训练，并结合 <strong>reinforcement learning</strong>（强化学习）来奖励特定的行为。</p><p>一个颇受争议（“much-debated”）的技术就是 <strong>STaR</strong>，即 <strong>Self-Taught Reasoner</strong>。它是让 <strong>LLM</strong> 生成自己的推理数据，再把这些数据用于对模型进行<strong>精调</strong>（<em>fine-tuning</em>）的过程。</p><p><strong>1. STaR 的流程概述</strong></p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_172455.png" class=""><ul><li>这幅图概括了 STaR 的工作原理：<ol><li><strong>Generate reasoning + answer</strong>：模型先针对输入问题生成一段 <strong>reasoning</strong>（推理）和一个 <strong>answer</strong>（答案）；\<br>2a. 如果答案正确（Correct answer），则将 <strong>Question, Reasoning, Answer</strong> 作为训练样本添加到三元组数据集中（3a）；\<br>3b. 利用这些三元组数据进行 <strong>supervised fine-tuning</strong>（监督微调），让模型学会在类似情形下产出正确推理与答案。</li></ol></li></ul><p>如果模型给出了错误答案，则会触发另一条路径：</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_172608.png" class=""><ul><li>当 (2b) 模型答案错误时，我们提供正确答案作为 <strong>hint</strong>（提示），并让模型去思考“为什么这个答案是正确的”；</li><li>也就是 <strong>Generate reasoning only</strong> (why this answer is correct?)；</li><li>得到的这段新的推理依旧会被加入到三元组数据中，然后再进行 <strong>supervised fine-tuning</strong>。</li></ul><p>这里的关键要点是，我们可以通过这种方法<strong>显式</strong>地训练模型“应该如何进行推理”，而不仅仅是让它临时地模仿推理过程。我们要对模型的推理方式进行<strong>监督</strong>（<em>supervised fine-tuning</em>），从而把我们想要的推理模式“灌输”给模型。</p><p><strong>2. 自动生成合成训练样本</strong></p><p>STaR 的整个流程非常有趣，因为它会<strong>自动生成合成训练样本</strong>（<em>synthetic training examples</em>）。这些样本不仅包含问题和答案，还包含一系列推理步骤，能够帮助模型更好地学习如何“思考”。在其他研究中（例如 <strong>DeepSeek R-1</strong>），我们可以利用这些合成样本来<strong>蒸馏</strong>（<em>distill</em>，意为“提炼和保留关键信息”）推理过程到其它模型上。也就是说，一个掌握了推理能力的模型可以帮助另一个模型更快地学会类似的推理。</p><hr><p><strong>重点：</strong></p><ul><li><strong>Prompting</strong>（提示）能够影响模型的输出风格和思维过程，比如使用 “Let’s think step-by-step” 让模型显式给出推理步骤，但并不保证模型自动纠正错误。</li><li><strong>STaR</strong>（<strong>Self-Taught Reasoner</strong>）等方法则通过<strong>生成推理数据、监督微调和奖励机制</strong>，帮助模型真正学会按照指定的推理方式去思考和回答问题。</li><li>无论是哪一种方法，都可以视为对 <strong>proposal distribution</strong> 的调节：要么是提示时临时<strong>nudge</strong>（引导），要么是从训练根源上进行调教，让模型内化这种推理过程。</li><li>利用 <strong>in-context learning</strong> 提供示例，能够让模型模仿推理风格。</li><li>用 <strong>reinforcement learning</strong> 或<strong>监督微调</strong>（<strong>supervised fine-tuning</strong>）可以使模型逐渐掌握我们期望的推理模式。</li><li><strong>STaR</strong> 方法会自动收集“正确推理”数据并进行训练，使得模型在后续回答中更可能产生正确且符合要求的推理步骤。</li></ul><hr><h2 id="DeepSeek-R1"><a href="#DeepSeek-R1" class="headerlink" title="DeepSeek-R1"></a>DeepSeek-R1</h2><hr><h3 id="1-简介"><a href="#1-简介" class="headerlink" title="1. 简介"></a>1. 简介</h3><p><strong>DeepSeek-R1</strong> 是一个在推理（reasoning）模型领域的重大版本，其权重已经开源。它直接与 OpenAI 的 <strong>o1</strong> 推理模型展开竞争，并在这一领域产生了重大影响。</p><p>DeepSeek 项目在将推理功能优雅地整合进其基础模型（<strong>DeepSeek-V3-Base</strong>）方面成就卓著，采用了多种技术来完成这一目标。</p><p>有趣的是，该项目在训练过程中并未依赖额外的验证器（verifier），而且并不是单纯地依靠监督微调（supervised fine-tuning）来提炼推理行为。相反，<strong>强化学习（Reinforcement Learning, RL）</strong> 在其中扮演了重要角色。</p><p>以下我们将一起探究他们是如何在模型中训练出推理行为的！</p><hr><h3 id="2-DeepSeek-R1-Zero：推理的关键探索"><a href="#2-DeepSeek-R1-Zero：推理的关键探索" class="headerlink" title="2. DeepSeek-R1 Zero：推理的关键探索"></a>2. DeepSeek-R1 Zero：推理的关键探索</h3><p>在通往 <strong>DeepSeek-R1</strong> 的道路上，有一个名为 <strong>DeepSeek-R1 Zero</strong> 的实验性模型为这次突破打下了基础。它从 <strong>DeepSeek-V3-Base</strong> 出发，完全不使用大规模监督微调来加入推理数据，而是只依靠 <strong>强化学习</strong> 来获得推理能力。</p><h4 id="训练过程与系统提示（Prompt）"><a href="#训练过程与系统提示（Prompt）" class="headerlink" title="训练过程与系统提示（Prompt）"></a>训练过程与系统提示（Prompt）</h4><p>在此过程中，他们首先准备了一个非常直接的提示（prompt），其形式类似于系统提示（system prompt），用来作为推理管线的一部分。下文即展示了相关提示。请注意，其中明确指出了推理过程要写在 <code>&lt;think&gt;</code> 标签内、答案要写在 <code>&lt;answer&gt;</code> 标签内，但没有进一步规定推理过程应如何具体呈现或组织。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_183150.png" class=""><p>在上图中，可以看到一个简化版的对话示例（System prompt 与 User prompt）以及模型如何将<strong>推理</strong>（reasoning）放在 <code>&lt;think&gt;</code> 标签内、将<strong>答案</strong>（answer）放在 <code>&lt;answer&gt;</code> 标签内。该图突出展示了在提示（prompt）中对模型的约束：</p><ul><li><em>“The assistant first thinks about the reasoning process in the mind and then provides the user with the answer.”</em></li><li>要求使用 <code>&lt;think&gt;</code> 进行推理，使用 <code>&lt;answer&gt;</code> 进行回答。</li></ul><p>这里并未提供关于“推理过程”格式的其他例子或模板——完全由模型自己在训练中摸索出要如何输出“Chain-of-Thought”式的推理文字。</p><h4 id="强化学习奖励"><a href="#强化学习奖励" class="headerlink" title="强化学习奖励"></a>强化学习奖励</h4><p>在训练中，采用了两个基于规则（rule-based）的奖励机制：</p><ol><li><strong>准确性奖励（Accuracy rewards）</strong>\<br>通过测试给出的答案是否正确来进行奖励。若模型输出的答案正确，就会增加奖励。</li><li><strong>格式奖励（Format rewards）</strong>\<br>奖励模型对 <code>&lt;think&gt;</code> 和 <code>&lt;answer&gt;</code> 标签的正确使用。</li></ol><p>他们所使用的强化学习算法名为 <strong>Group Relative Policy Optimization（GRPO）</strong>。此算法的直观想法在于：使所有导致正确或错误答案的决策更易或更难再次出现。这些决策可能包括模型生成的某些标记（token）序列，也可能包括推理步骤本身（即思考过程）。下文给出了这一训练阶段的示意图。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_183402.png" class=""><p>在图中，重点展示了在 RL（强化学习）过程中所使用的两类奖励：</p><ul><li>“is <code>&lt;think&gt;</code> used?” —— 为使用 <code>&lt;think&gt;</code> 标签而打分。</li><li>“is <code>&lt;answer&gt;</code> used?” —— 为使用 <code>&lt;answer&gt;</code> 标签而打分。</li></ul><p>除此之外，还有对答案<strong>正确性</strong>的奖励（accuracy reward）。图中箭头所示的循环代表了在训练中不断迭代更新模型，使之越来越倾向于正确的推理方式并合乎格式要求。</p><h4 id="自发推理行为的出现"><a href="#自发推理行为的出现" class="headerlink" title="自发推理行为的出现"></a>自发推理行为的出现</h4><p>值得一提的是，研究人员并没有向模型提供任何示例来告诉它 <code>&lt;think&gt;</code> 标签中的内容应该如何书写或展开。他们仅仅告诉模型：</p><blockquote><p>“It should use <code>&lt;think&gt;</code> tags, and nothing more!”</p></blockquote><p>通过对“Chain-of-Thought”相关行为进行<strong>间接奖励</strong>（即只要推理正确、使用正确格式，就鼓励输出更完整的推理内容），模型在训练中自发地学会了越写越长的推理过程，也更易产生正确答案。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_183551.png" class=""><p>上图呈现了模型在训练过程中输出的推理长度随训练步数增加而逐渐变长的趋势。纵轴是每个响应的平均长度，横轴是训练步数。可以看到，曲线整体是向上攀升的，这表明模型不断倾向于输出更长、更详细的思考内容（Chain-of-Thought），并因此获得更高奖励。这种做法将大部分计算消耗从训练阶段（train-time compute）转移到了推理阶段（test-time compute），也就是在推理时才生成更长的思考过程。</p><p>根据研究，他们发现通过这种训练策略，模型能够自行发现最优的 Chain-of-Thought 风格的思考方式，并展现出高级的推理能力，例如：<strong>自我反思（self-reflection）</strong> 和 <strong>自我验证（self-verification）</strong>。</p><p>不过，DeepSeek-R1 Zero 的模型输出仍存在一些问题，比如可读性欠佳，且有时会混用多种语言。为了在产品化或发布级别进一步完善，研究人员提出了另一个选项，也就是在正式版本中使用的 <strong>DeepSeek R1</strong>。</p><hr><h3 id="3-深入了解-DeepSeek-R1"><a href="#3-深入了解-DeepSeek-R1" class="headerlink" title="3. 深入了解 DeepSeek-R1"></a>3. 深入了解 DeepSeek-R1</h3><p>要构建 <strong>DeepSeek-R1</strong>，作者共进行了以下五个关键步骤：</p><ol><li><strong>冷启动（Cold Start）</strong></li><li><strong>以推理为导向的强化学习（Reasoning-oriented Reinforcement Learning）</strong></li><li><strong>拒绝采样（Rejection Sampling）</strong></li><li><strong>监督微调（Supervised Fine-Tuning）</strong></li><li><strong>在所有场景下进行强化学习（Reinforcement Learning for all Scenarios）</strong></li></ol><p>接下来我们依次展开说明。</p><hr><h4 id="第一步：冷启动"><a href="#第一步：冷启动" class="headerlink" title="第一步：冷启动"></a>第一步：冷启动</h4><p>在第一步中，研究人员先使用了一个约 5000 个tokens的高质量推理数据集对 <strong>DeepSeek-V3-Base</strong> 进行微调，以避免产生可读性不佳的<strong>冷启动问题（cold start problem）</strong>。这个微调步骤可以让模型的输出更加可读，不至于在一开始就产生混乱的推理文本。下文展示了这一过程的示意图。</p><hr><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_184058.png" class=""><p>在图中可以看到：</p><ul><li>“DeepSeek-V3-Base” 通过<strong>监督微调（Supervised Fine-Tuning）</strong>的方式，引入了约 5000 条高质量推理样本。</li><li>这些样本包含了<strong>Reasoning</strong>（推理）和<strong>Answer</strong>（答案）两种部分。</li><li>该步骤目的是“防止冷启动”，即让模型在一开始就掌握基础的可读性推理。</li></ul><hr><h4 id="第二步：推理导向的强化学习"><a href="#第二步：推理导向的强化学习" class="headerlink" title="第二步：推理导向的强化学习"></a>第二步：推理导向的强化学习</h4><p>在得到一个初步微调后的模型后（上一步的成果），作者使用与 <strong>DeepSeek-V3-Zero</strong> 类似的强化学习流程对模型进行训练，但额外加入了<strong>目标语言一致性</strong>的奖励，以确保模型在推理和回答时不会混用多种语言。</p><p>除了之前提到的准确性（accuracy reward）和格式（format reward）等，还增加了<strong>语言奖励（language reward）</strong>来保证生成的语言风格或语言类型保持一致，不至于出现“中英文混杂”或“风格不稳”的现象。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_184229.png" class=""><ul><li><strong>Format reward</strong>：依旧关注 <code>&lt;think&gt;</code> 和 <code>&lt;answer&gt;</code> 的使用。</li><li><strong>Accuracy reward</strong>：检查答案是否正确，以及是否能通过相应的“单元测试”。</li><li><strong>Language reward</strong>：检查语言是否一致、通顺以及是否符合目标语言要求。</li></ul><p>这些奖励综合起来，通过强化学习（RL）循环使模型的推理和答案在可读性、准确度和语言风格方面逐渐优化。</p><hr><h4 id="第三步：拒绝采样"><a href="#第三步：拒绝采样" class="headerlink" title="第三步：拒绝采样"></a>第三步：拒绝采样</h4><p>在这一阶段，作者用<strong>第 2 步</strong>强化学习后得到的模型，来大规模生成<strong>合成推理数据</strong>，并配合 <strong>DeepSeek-V3-Base</strong> 模型来进行“评估”和“规则过滤”，最终产生约 60 万条高质量的推理样本可用于后续监督微调。同时，他们还另外生成了约 20 万条<strong>非推理样本</strong>，包含了写作、简单问答、自我认知、翻译等多种任务数据。下文总结了这一过程。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_184407.png" class=""><ul><li>左边展示了<strong>DeepSeek-V3-2</strong> 如何采样到大量<strong>Reasoning</strong>（推理）和<strong>Answer</strong>（答案），再利用基于规则的筛选和 <strong>DeepSeek-V3-Base</strong> 的判断（判断生成的内容质量），保留质量更好的推理数据（约 600,000 条）。</li><li>右边展示了<strong>非推理</strong>（non-reasoning）数据采样流程，来自 DeepSeek-V3-Base 所使用的一部分数据，总共约 200,000 条，这些数据主要涉及写作、事实性问答（factual QA）、自我认知、翻译等方面。</li></ul><p>由此，研究人员得到规模约 80 万条的“混合”数据，其中既有推理样本，也有非推理样本。</p><hr><h4 id="第四步：监督微调"><a href="#第四步：监督微调" class="headerlink" title="第四步：监督微调"></a>第四步：监督微调</h4><p>在得到上述 80 万条数据后，研究人员再次对 <strong>DeepSeek-V3-Base</strong> 进行监督微调，具体过程如下图所示。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_184526.png" class=""><ul><li>在图中，我们看到“DeepSeek-V3-Base”被用于执行<strong>监督微调（Supervised Fine-Tuning）</strong>，使用的正是前文所提到的 800,000 条<strong>高质量推理与非推理样本</strong>。</li><li>这一阶段使得模型在更大规模的数据基础上，学习到更广泛、更多样的推理形式和任务形式。</li></ul><hr><h4 id="第五步：在所有场景下的强化学习"><a href="#第五步：在所有场景下的强化学习" class="headerlink" title="第五步：在所有场景下的强化学习"></a>第五步：在所有场景下的强化学习</h4><p>在监督微调完成后，研究人员继续采用类似 <strong>DeepSeek-R1-Zero</strong> 的方法进行 <strong>RL（强化学习）</strong> 训练。但是，为了让模型更符合人类偏好，他们在这个阶段引入了更多的 <strong>“有益与无害”（helpfulness and harmlessness）</strong> 奖励信号，用来约束模型的回答。</p><p>同时，模型也被要求<strong>对推理过程进行总结（summarize）</strong>，以防止在最终输出时显示出过长、难以阅读的推理文本。这一步骤解决了前述提到的可读性问题。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_184907.png" class=""><ol><li><strong>Format reward（格式奖励）</strong>  <ul><li>是否正确使用 <code>&lt;think&gt;</code> 标签书写推理内容  </li><li>是否正确使用 <code>&lt;answer&gt;</code> 标签输出答案  </li></ul></li><li><strong>Accuracy reward（准确性奖励）</strong>  <ul><li>测试输出是否能编译（“does it compile?”）  </li><li>是否能通过单元测试（“does it pass unit tests?”）  </li></ul></li><li><strong>Preference rewards（偏好奖励）</strong>  <ul><li>关注 <strong>Helpfulness（有益）</strong>、<strong>Harmlessness（无害）</strong>、<strong>Human preference（人类偏好）</strong> 等  </li><li>由 RM（Reward Model） 模块来评估这些偏好指标  </li></ul></li></ol><p>图中可以看到，<strong>Reasoning</strong>（推理）阶段和 <strong>Answer</strong>（答案）阶段需要分别用 <code>&lt;think&gt;</code> 和 <code>&lt;answer&gt;</code> 标签进行明确区分。同时，为了输出更为精简、可读的内容，模型也可能产生一个 <strong>Summary</strong>（总结）片段。强化学习的迭代过程会同时考虑多种奖励信号，从而不断更新模型并得到最终版本的 <strong>DeepSeek-R1</strong>。</p><p>上图中，“RM” 即 Reward Model，用于对偏好进行打分（如对话是否友善、是否符合伦理要求等），再把结果反馈给模型。</p><p>“<strong>And that’s it!</strong>”这意味着 <strong>DeepSeek-R1</strong> 实际上是 <strong>DeepSeek-V3-Base</strong> 经过监督微调（Supervised Fine-Tuning）和强化学习（RL）进一步优化而成。大量的工作都用于保证<strong>高质量数据</strong>的生成与使用，进而训练出这样一个具备强大推理能力的模型。</p><hr><h2 id="将推理知识从-DeepSeek-R1-蒸馏到其他模型"><a href="#将推理知识从-DeepSeek-R1-蒸馏到其他模型" class="headerlink" title="将推理知识从 DeepSeek-R1 蒸馏到其他模型"></a>将推理知识从 DeepSeek-R1 蒸馏到其他模型</h2><p><strong>DeepSeek-R1</strong> 拥有 <strong>6710 亿（671B）</strong> 参数。这一规模的模型在普通消费级硬件上运行存在较大难度。出于实用性考虑，作者们研究了如何将 <strong>DeepSeek-R1</strong> 的推理能力“蒸馏（distill）”到更小的模型（如 <strong>Qwen-32B</strong>）上，以便能在消费级硬件上部署和使用。</p><h3 id="蒸馏过程：Teacher-Student-框架"><a href="#蒸馏过程：Teacher-Student-框架" class="headerlink" title="蒸馏过程：Teacher-Student 框架"></a>蒸馏过程：Teacher-Student 框架</h3><p>在蒸馏过程中，<strong>DeepSeek-R1</strong> 作为教师模型（Teacher），而规模更小的模型（如 Qwen-32B）作为学生模型（Student）。二者面对相同的提示（prompt）时，分别会输出一组<strong>词元概率分布（token probability distribution）</strong>。训练时，学生模型会尽量学习并接近教师模型的输出分布。</p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_185245.png" class=""><ul><li>教师（DeepSeek-R1）给出自己的“proposal distribution”。例如在回答“What is 3 + 2?”时，教师模型可能倾向输出“Adding”“If”“5”“3”“4”等标记，并赋予各自不同的概率。  </li><li>学生（Qwen-32B）则会在训练中不断更新自己的概率分布，使之更接近教师的分布。</li></ul><blockquote><p><strong>额外解释</strong>：  </p><ol><li><strong>概率分布（proposal distribution）</strong>：语言模型在生成下一个词元（token）时，会输出对所有可能词元的概率估计。  </li><li><strong>蒸馏（distillation）</strong>：通过比较教师和学生的分布差异，学生会逐步调整自身参数，使其输出更接近教师模型的风格和推理倾向。  </li></ol></blockquote><p>训练所使用的数据，正是之前提到的那 <strong>80 万条高质量样本</strong>——其中包含约 60 万推理样本和 20 万非推理样本。下图展示了这一数据流向： </p><img src="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1/20250211_185323.png" class=""><ul><li>左侧的 <strong>Reasoning</strong>（推理）和 <strong>Answer</strong>（答案）数据，合计 80 万条。  </li><li>由 <strong>DeepSeek-R1</strong>（Teacher）生成或评估，得到对应的概率分布。  </li><li>学生模型 <strong>Qwen-32B</strong> 则根据教师的分布进行学习，最终得到一个蒸馏版本 <strong>DeepSeek-R1-Distill-Qwen-32B</strong>。</li></ul><p><img src="https://user-images.githubusercontent.com/your-image-url.png" alt="使用 80 万条高质量样本蒸馏的流程（图10）"></p><blockquote><p><strong>额外解释</strong>：  </p><ul><li>学生模型不仅仅学习了那 80 万条样本本身的输入-输出模式，也学习到 <strong>DeepSeek-R1</strong> 在面对这些数据时所“倾向”采用的推理策略和概率分布，从而在更小模型上复现类似的推理能力。  </li><li>“Distilled” 模型往往会在推理质量与计算资源之间找到更好的平衡：虽然可能在性能上略逊色于老师模型，但依然能在大多数常见任务上达到令人满意的结果，并且所需资源更低。</li></ul></blockquote><hr><h2 id="其他未成功的尝试"><a href="#其他未成功的尝试" class="headerlink" title="其他未成功的尝试"></a>其他未成功的尝试</h2><p>在研究过程中，DeepSeek 团队也曾尝试过 <strong>Process Reward Models（PRMs）</strong> 和 <strong>Monte Carlo Tree Search（MCTS）</strong> 等方法来注入推理能力，但结果并不理想：</p><ol><li><p><strong>使用 MCTS</strong>  </p><ul><li>面临的主要问题是搜索空间过于庞大，只能对节点展开进行严格限制。这样一来，效果就大打折扣。  </li><li>此外，精细化训练 Reward Model 也相当困难。</li></ul></li><li><p><strong>使用 PRMs 进行 Best-of-N 策略</strong>  </p><ul><li>如果不断重训练 Reward Model 以防止模型出现“投机取巧”（reward hacking）行为，会带来高昂的计算开销。</li></ul></li></ol><p>这些结果并不意味着这些技术无效，而是说明它们在当前大规模语言模型上的实践还有诸多限制与难点。<strong>DeepSeek-R1</strong> 之所以取得成功，更多依赖于<strong>强化学习 + 监督微调</strong>的组合，以及对大规模高质量数据的挖掘与利用。</p><hr><h2 id="总结与展望"><a href="#总结与展望" class="headerlink" title="总结与展望"></a>总结与展望</h2><p>至此，我们已经大致回顾了 <strong>DeepSeek-R1</strong> 的推理训练之旅。希望以上内容能够让你更好地理解：  </p><ul><li><strong>Test-time compute（推理时计算）</strong> 可以通过模型输出更长、更精细的思考过程（Chain-of-Thought）来取得更佳效果。  </li><li>大规模“<strong>先监督微调，再强化学习</strong>”的训练流程，以及<strong>蒸馏</strong>到更小模型的技术路线，也展现了在硬件资源和推理性能间取得平衡的方法。  </li></ul><p>如前所述，<strong>DeepSeek-R1</strong> 引入了多种奖励机制，尤其是针对格式和人类偏好的奖励，来保证回答既正确又易读。“总结推理过程”（Summary）的做法也在很大程度上改善了纯文本Chain-of-Thought过长而导致的可读性问题。</p><hr><h2 id="更多资源"><a href="#更多资源" class="headerlink" title="更多资源"></a>更多资源</h2><p>如果你对 <strong>Large Language Models（LLMs）</strong> 中的推理话题感兴趣，以下资源值得参考：</p><ol><li><a href="https://newsletter.languagemodels.co/p/the-illustrated-deepseek-r1"><strong>The Illustrated DeepSeek-R1</strong></a>  <ul><li>Jay Alammar 制作的高质量可视化指南，详细介绍了 DeepSeek-R1 模型背后的原理与实现细节。  </li></ul></li><li><a href="https://huggingface.co/spaces/HuggingFaceH4/blogpost-scaling-test-time-compute"><strong>Hugging Face 的一篇博文</strong></a>  <ul><li>重点讨论了在推理阶段如何对计算量进行扩展，并给出了有趣的实验。  </li></ul></li><li><a href="https://www.youtube.com/watch?v=6PEJ96k1kiw"><strong>视频 “Speculations on Test-Time Scaling”</strong></a>  <ul><li>深入探讨了在推理阶段进行各种计算扩展的常用技术细节。</li></ul></li></ol><p>此外，作者在文中也提到了一本关于大型语言模型的著作，内含更多可视化和实验结果，是想进一步研究推理 LLMs 的朋友可以深入阅读的好资料。</p><ul><li><strong>Official Website of the Book</strong>: <a href="https://www.llm-book.com/">llm-book.com</a>  </li><li><strong>Amazon 购买链接</strong>: <a href="https://www.amazon.com/Hands-Large-Language-Models-Understanding/dp/1098150961">Hands-On Large Language Models: Understanding, Building, and Optimizing LLMs</a>  </li><li><strong>GitHub 代码仓库</strong>: <a href="https://github.com/handsOnLLM/Hands-On-Large-Language-Models">handsOnLLM/Hands-On-Large-Language-Models</a></li></ul><hr><h3 id="致谢"><a href="#致谢" class="headerlink" title="致谢"></a>致谢</h3><p>感谢你阅读本篇关于 <strong>DeepSeek-R1</strong> 的介绍文档。通过对所有图片与文字内容的依次解读，以及对每个环节所涉及的关键技术进行了更多解释，我们希望让你对 <strong>DeepSeek-R1</strong> 的训练流程、蒸馏方法和未成功的尝试都有更加全面的了解。</p><p>在未来，随着硬件性能的提升与更成熟的训练技术出现，<strong>深度推理</strong>与<strong>模型蒸馏</strong>必将在更多实际应用场景中发挥巨大作用。让我们拭目以待！</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;推理-LLM-的可视化指南：探索推理时计算技术与-DeepSeek-R1&quot;&gt;&lt;a href=&quot;#推理-LLM-的可视化指南：探索推理时计算技术与-DeepSeek-R1&quot; class=&quot;headerlink&quot; title=&quot;推理 LLM 的可视化指南：探索推理时计</summary>
      
    
    
    
    <category term="NLP Insights" scheme="https://chenhuiyu.github.io/categories/NLP-Insights/"/>
    
    
    <category term="LLM" scheme="https://chenhuiyu.github.io/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>MoE模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色</title>
    <link href="https://chenhuiyu.github.io/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2/"/>
    <id>https://chenhuiyu.github.io/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2/</id>
    <published>2025-02-11T03:50:29.000Z</published>
    <updated>2025-02-24T10:24:12.817Z</updated>
    
    <content type="html"><![CDATA[<h1 id="MoE-模型的的可视化指南：揭秘-MoE-在大型语言模型中的角色"><a href="#MoE-模型的的可视化指南：揭秘-MoE-在大型语言模型中的角色" class="headerlink" title="MoE 模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色"></a>MoE 模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色</h1><p><strong>原文地址</strong>：<a href="https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts">A Visual Guide to Mixture of Experts (MoE)</a></p><p>📅 作者：Maarten Grootendorst</p><p>📆 日期：2024 年 10 月 7 日</p><hr><h1 id="探索语言模型：混合专家模型（MoE）可视化指南"><a href="#探索语言模型：混合专家模型（MoE）可视化指南" class="headerlink" title="探索语言模型：混合专家模型（MoE）可视化指南"></a>探索语言模型：混合专家模型（MoE）可视化指南</h1><h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul><li><a href="#moe-模型的的可视化指南揭秘-moe-在大型语言模型中的角色">MoE 模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色</a></li><li><a href="#探索语言模型混合专家模型moe可视化指南">探索语言模型：混合专家模型（MoE）可视化指南</a><ul><li><a href="#目录">目录</a></li><li><a href="#什么是混合专家moe模型">什么是混合专家（MoE）模型？</a></li><li><a href="#experts">Experts</a><ul><li><a href="#dense-layers">Dense Layers</a></li><li><a href="#sparse-layers">Sparse Layers</a></li><li><a href="#what-does-an-expert-learn">What does an Expert Learn?</a></li><li><a href="#专家的架构architecture-of-experts">专家的架构（Architecture of Experts）</a></li></ul></li></ul></li></ul><p>当我们查看最新发布的大型语言模型（<strong>LLMs</strong>，Large Language Models）时，常常会在标题中看到 “<strong>MoE</strong>”。这个 “MoE” 代表什么？为什么这么多 LLM 都在使用它？</p><p>在这份可视化指南中，我们会通过 50 多个可视化图示，逐步探索这一关键组件：<strong>Mixture of Experts (MoE)</strong>。</p><p><strong>图示内容</strong>：在这张图中，可以看到一个典型 <strong>MoE</strong> 结构的两个主要组成部分：<strong>Experts</strong>（专家）和 <strong>Router</strong>（路由器或门控网络）。图中显示了一个 <strong>Router</strong>，以及下方并列的多个 <strong>Experts</strong>，表明在 <strong>LLM</strong> 架构中，MoE 会将输入根据需要路由到合适的专家。<br><strong>图 1 详细说明</strong>：</p><ol><li><strong>Router</strong>：决定将输入（例如 token）发送给哪一个或哪几个专家。</li><li><strong>Experts</strong>：若干个不同的子模型（通常是 <strong>FFNN</strong> 结构），每个专家可能在不同方面具有专长。</li><li><strong>工作流程</strong>：输入先通过 <strong>Router</strong>，再被分配到不同的专家进行处理，最后汇总结果。</li></ol><h2 id="什么是混合专家（MoE）模型？"><a href="#什么是混合专家（MoE）模型？" class="headerlink" title="什么是混合专家（MoE）模型？"></a>什么是混合专家（MoE）模型？</h2><p><strong>Mixture of Experts (MoE)</strong> 是一种技术，它使用许多不同的子模型（或“<strong>experts</strong>”）来提升大型语言模型的质量。</p><p>在 MoE 中，有两个主要组件：</p><ol><li><strong>Experts</strong><ul><li>每个 <strong>FFNN</strong> 层都不再是一个单独的网络，而是有一组“专家”可供选择。</li><li>这些“专家”通常也是 <strong>FFNN</strong>（Feedforward Neural Network）结构。</li></ul></li><li><strong>Router</strong> 或 <strong>gate network</strong><ul><li>负责决定哪些 <strong>tokens</strong> 被发送到哪些专家。</li></ul></li></ol><p>在一个带有 MoE 的 <strong>LLM</strong> 的每一层，我们都能看到（在某种程度上）有所专门化的专家：</p><p><strong>图示内容</strong>：展示了在 <strong>LLM</strong> 的每一层都可以拥有多个 <strong>Experts</strong>。它强调了这些专家在不同的上下文中能够处理不同的输入 token。<br><strong>图2详细说明</strong>：  </p><ol><li><strong>层结构</strong>：图中用不同的层级（Layer 1、Layer 2、Layer 3……）表示多层模型。  </li><li><strong>Experts</strong>：在每一层，都有若干个专家（Expert 1、Expert 2、Expert 3、Expert 4），这些专家并行存在。  </li><li><strong>目标</strong>：强调专家在特定上下文或特定输入时更具备“专业性”，从而被选中来处理该输入。  </li></ol><p>尽管 MoE 并不会在特定领域（如心理学或生物学）上专门训练专家，但它们仍可能在词法或句法级别上形成一定的偏向：</p><ul><li><strong>MoE 专家可能学习到不同的语言特征</strong><ul><li><strong>Expert 1</strong> 处理<strong>标点符号</strong>（Punctuation）：如 <code>, . : &amp; - ?</code> 等。</li><li><strong>Expert 2</strong> 处理<strong>动词</strong>（Verbs）：如 <code>said, read, miss</code> 等。</li><li><strong>Expert 3</strong> 处理<strong>连接词</strong>（Conjunctions）：如 <code>the, and, if, not</code> 等。</li><li><strong>Expert 4</strong> 处理<strong>视觉描述词</strong>（Visual Descriptions）：如 <code>dark, outer, yellow</code> 等。</li></ul></li></ul><p>更具体地说，他们的专长是在特定上下文中处理特定的标记（tokens）。</p><hr><p><strong>Router (gate network)</strong> 选择最适合给定输入的专家或专家组合：</p><p><strong>图示内容</strong>：展示了 <strong>Router</strong> 如何在每一层根据输入选择合适的专家。图中高亮了被选中的专家，以及输入 token 的流动过程。<br><strong>图3详细说明</strong>：  </p><ol><li><strong>输入</strong>：图顶部的 Input 代表模型接收到的 token 或向量表示。  </li><li><strong>Router</strong>：位于网络结构中，起到决策作用。  </li><li><strong>专家选择</strong>：被选中的专家会接收输入，其余专家则不被激活。  </li><li><strong>输出</strong>：来自被激活专家的结果被汇总或继续流向下游层。  </li></ol><p>需要注意的是，每个专家并不是整个 LLM，而是 <strong>LLM</strong> 架构中的一个子模型部分。</p><hr><h2 id="Experts"><a href="#Experts" class="headerlink" title="Experts"></a>Experts</h2><p>为了理解专家（<strong>Experts</strong>）是什么以及它们如何工作，我们先来看看 MoE 希望替代的东西：<strong>dense layers</strong>。</p><h3 id="Dense-Layers"><a href="#Dense-Layers" class="headerlink" title="Dense Layers"></a>Dense Layers</h3><p>所有的 <strong>Mixture of Experts (MoE)</strong> 都基于 LLM 中一个相对基础的功能：<strong>Feedforward Neural Network (FFNN)</strong>。</p><p>回忆一下，一个标准的 <strong>decoder-only Transformer</strong> 架构中，<strong>FFNN</strong> 通常是在 <strong>layer normalization</strong> 之后应用的：</p><p><strong>图示内容</strong>：展示了一个典型的 <strong>decoder</strong> 结构，每个 <strong>decoder block</strong> 包含 <strong>Masked Self-Attention</strong> 和 <strong>FFNN</strong>（中间会有 <strong>Layer Norm</strong>）。  </p><ol><li><strong>Position Embedding</strong>：在输入 token 之前或同时加入位置编码信息。  </li><li><strong>Decoder Block</strong>：包含 <strong>Masked Self-Attention</strong>、<strong>Layer Norm</strong> 和 <strong>FFNN</strong>。  </li><li><strong>FFNN</strong>：在图中用紫色方块表示，是该层对输入进一步变换以捕捉更复杂关系的关键组件。  </li></ol><p><strong>FFNN</strong> 可以利用注意力机制产生的上下文信息，对其进行进一步的转换，以捕捉数据中更复杂的关系。</p><p>不过，为了学习这些复杂关系，<strong>FFNN</strong> 的规模会随之增长，通常会在输入上进行扩张（例如，中间层维度会变大）：<br><br><strong>图示内容</strong>：展示了一个 <strong>FFNN</strong> 的结构，输入先被映射到更高维度，然后再被映射回输出维度。  </p><ol><li><strong>输入维度</strong>：图中显示有 512 个输入单元。  </li><li><strong>隐藏层</strong>：通常会有 4 倍或更多的扩张（图中示例为 4 倍扩张到 2048 维）。  </li><li><strong>输出维度</strong>：再映射回 512 维的输出。  </li></ol><h3 id="Sparse-Layers"><a href="#Sparse-Layers" class="headerlink" title="Sparse Layers"></a>Sparse Layers</h3><p>在传统的 Transformer 中，<strong>FFNN</strong> 称为 <strong>dense model</strong>，因为它的所有参数（权重和偏置）都会被激活。也就是说，模型的全部参数都参与计算输出。</p><p>如果我们仔细观察 <strong>dense model</strong>，可以看到输入会激活所有的参数：<br><br><strong>图示内容</strong>：展示了一个“密集”模型，输入层的每个神经元都与隐藏层所有神经元相连，隐藏层所有神经元又与输出层神经元相连。<br><strong>图6详细说明</strong>：  </p><ol><li><strong>全连接</strong>：图中所有节点都连接到下一层的所有节点，表示无稀疏性。  </li><li><strong>所有参数被激活</strong>：没有任何“闲置”或“未激活”的参数。  </li></ol><p>与之对比，<strong>sparse models</strong>（稀疏模型）只激活一部分总参数，这与 <strong>Mixture of Experts</strong> 密切相关。</p><p>为了说明这一点，我们可以把 <strong>dense model</strong> 切分成多个部分（即专家，<strong>experts</strong>），重新训练它，并且在推理（inference）时只激活其中一部分：</p><p><strong>图示内容</strong>：将原本的密集模型分割成多个专家（Expert 1、Expert 2、Expert 3、Expert 4）。在推理阶段，只选择一部分专家进行激活。  </p><ol><li><strong>模型切分</strong>：原有的大网络被拆分成多个较小的“专家”。  </li><li><strong>稀疏激活</strong>：并不是所有专家都被激活，只有部分专家在某些输入下被激活。  </li><li><strong>好处</strong>：通过稀疏激活，可以在不显著增加计算成本的情况下，拥有更多的潜在参数容量。  </li></ol><p>其核心思想是：在训练期间，每个专家学习不同的信息；在推理时，只用到与当前任务最相关的那些专家。</p><p>当我们提出一个问题时，就会选择最适合该任务的专家：</p><p><strong>图示内容</strong>：展示了一个示例：当输入是 “What is 1 + 1?” 这样的数字相关问题时，路由器只激活与数字相关的专家。  </p><ol><li><strong>输入</strong>：一个表示算术问题的句子或 token。  </li><li><strong>专家选择</strong>：只激活 “Numbers” 领域的专家。  </li><li><strong>输出</strong>：专家给出结果 “2”。  </li></ol><h3 id="What-does-an-Expert-Learn"><a href="#What-does-an-Expert-Learn" class="headerlink" title="What does an Expert Learn?"></a>What does an Expert Learn?</h3><p>正如前面所提到的，专家（<strong>Experts</strong>）往往学习到比整个领域更细致的知识。有人会觉得称它们为“专家”可能会带来误解，但这是因为每个专家往往只专注于某些特定类型的输入特征或上下文。<br></p><p><strong>图示内容</strong>：展示了一个表格或对照，说明在某些情况下，不同的专家可能学习到不同的特征（比如标点符号、动词、数字等）。  </p><ol><li><strong>示例化专家</strong>：Punctuation、Conjunctions、Verbs、Numbers 等。  </li><li><strong>分层位置</strong>：不同专家可能出现在模型的不同层。  </li><li><strong>分配</strong>：某些 token 会路由到某些专家，以获得更有效的处理。  </li></ol><p>在 <strong>decoder</strong> 模型中，专家之间可能没有那么明显的领域分工。然而，这并不意味着所有专家都完全相同。<br>在 <strong>Mixtral 8x7B</strong> 这篇论文中，有一个很好的示例：每个 token 会被标记为其首选专家，这些专家并不一定对应直观的语义领域，但在统计上表现出某些倾向。</p><p>这张可视化示例还展示了，experts（专家）更倾向于关注句法（syntax），而不是特定的领域（domain）。因此，虽然 decoder experts（解码器专家）似乎并没有明确的“专业领域（specialism）”，但它们似乎会在某些特定类型的 tokens（标记）上被持续地使用。</p><p>在[图1]中，展示了一段关于 MoELayer 的示例代码或可视化结果，色块区分了不同部分，强调了<strong>专家（experts）与路由器（router）</strong>之间的关系。通过色块可以看出：</p><ul><li>experts 列表（在代码中用 nn.ModuleList 表示）包含了多个子网络（即多个 FFNN，Feed-Forward Neural Network，前馈神经网络）。</li><li>gate（门控网络，也称 router）负责选择哪些专家会被激活。</li><li>整体上可以看到，这些专家通常关注到输入句子的句法层面，而非特定主题或领域。</li></ul><h3 id="专家的架构（Architecture-of-Experts）"><a href="#专家的架构（Architecture-of-Experts）" class="headerlink" title="专家的架构（Architecture of Experts）"></a>专家的架构（Architecture of Experts）</h3>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;MoE-模型的的可视化指南：揭秘-MoE-在大型语言模型中的角色&quot;&gt;&lt;a href=&quot;#MoE-模型的的可视化指南：揭秘-MoE-在大型语言模型中的角色&quot; class=&quot;headerlink&quot; title=&quot;MoE 模型的的可视化指南：揭秘 MoE 在大型语言模型</summary>
      
    
    
    
    <category term="NLP Insights" scheme="https://chenhuiyu.github.io/categories/NLP-Insights/"/>
    
    
    <category term="LLM" scheme="https://chenhuiyu.github.io/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>追逐与倒影</title>
    <link href="https://chenhuiyu.github.io/2024/12/11/Life%20Reflections/%E8%BF%BD%E9%80%90%E4%B8%8E%E5%80%92%E5%BD%B1/"/>
    <id>https://chenhuiyu.github.io/2024/12/11/Life%20Reflections/%E8%BF%BD%E9%80%90%E4%B8%8E%E5%80%92%E5%BD%B1/</id>
    <published>2024-12-10T18:29:06.000Z</published>
    <updated>2024-12-10T17:50:34.018Z</updated>
    
    <content type="html"><![CDATA[<h3 id="追逐与倒影"><a href="#追逐与倒影" class="headerlink" title="追逐与倒影"></a>追逐与倒影</h3><p>在清晨的第一缕光洒下之前，世间一切尚未显形。光与影的边界模糊，彷佛可以交叠，又彷佛注定分离。人们常说，朝阳是希望的象征，可它升起时，必将抛下一地影子。光和影之间，究竟是追逐还是相伴？这样的思考让我想起一则古老的寓言：一匹马在沙漠中追逐远方的绿洲，却不知道那不过是海市蜃楼，它每前进一步，绿洲也随之远去。</p><p>有时我们追寻的目标，如同沙漠中的绿洲一般，它并非虚无，但也不完全真实。它是一种存在于心中的映像，一个无法企及的彼岸。无论我们怎样靠近，那份距离似乎总是恒定，甚至在我们伸手触碰的一刹那，它便如烟雾般消散。是目标变了，还是我们的执念让它愈加模糊？</p><p>镜中的倒影也是如此。当你站在镜前凝视自己时，你看见的那个“你”，究竟是谁？是一个忠实的再现，还是一场温柔的欺骗？镜中的倒影总会回应你的动作，可是你永远无法拥抱它，甚至连碰触都无法做到。这种触不可及的关系，既令人惋惜，又教人思索。倘若生命中许多事物都像这面镜子，是否意味着我们注定只能遥望，却无法真正拥有？</p><p>“人类最大的悲剧在于，他们注定要追求那些不可得之物。”起初，我对这句话嗤之以鼻。世界这么大，怎么可能所有的追求都是徒劳？然而，当经历了一些无法言说的感受后，我逐渐明白，这种“不可得”并非指绝对的失败，而是一种与目标间无法消弭的间隙。它可能是时间的错位，也可能是空间的疏离，甚至可能只是心境的不同。</p><p>也许，正是这种无法彻底握住的感觉，赋予了追逐的过程以意义。倘若一切触手可得，生命是否会因少了些许遗憾而失去色彩？光因为有影子才得以分明，爱因为不可得才显得深刻。我们在遗憾中体会到希望，在距离中发现自我，这是否是一种隐秘的平衡？</p><p>夜幕降临时，清晨的影子会被黑暗吞没，但它并未真正消失，只是换了一种形态，成为心底一束无法熄灭的微光。这光指引着我们，虽不可捉摸，却又真实存在。或许，我们无需执着于是否能得到，而是学会在追逐的过程中，接受光与影交织的美。</p><p>世间的一切，皆是倒影。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;追逐与倒影&quot;&gt;&lt;a href=&quot;#追逐与倒影&quot; class=&quot;headerlink&quot; title=&quot;追逐与倒影&quot;&gt;&lt;/a&gt;追逐与倒影&lt;/h3&gt;&lt;p&gt;在清晨的第一缕光洒下之前，世间一切尚未显形。光与影的边界模糊，彷佛可以交叠，又彷佛注定分离。人们常说，朝阳是希望的象</summary>
      
    
    
    
    <category term="Life Reflections" scheme="https://chenhuiyu.github.io/categories/Life-Reflections/"/>
    
    
    <category term="杂谈" scheme="https://chenhuiyu.github.io/tags/%E6%9D%82%E8%B0%88/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://chenhuiyu.github.io/2024/12/06/NLP%20Insights/Evaluation%20of%20Generation-Based%20Large%20Language%20Models%20(LLMs):%20Opportunities%20and%20Challenges%20from%20Generation%20to%20Judgment/"/>
    <id>https://chenhuiyu.github.io/2024/12/06/NLP%20Insights/Evaluation%20of%20Generation-Based%20Large%20Language%20Models%20(LLMs):%20Opportunities%20and%20Challenges%20from%20Generation%20to%20Judgment/</id>
    <published>2024-12-06T07:50:03.238Z</published>
    <updated>2024-12-06T09:59:18.340Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Evaluation-of-Generation-Based-Large-Language-Models-LLMs-Opportunities-and-Challenges-from-Generation-to-Judgment"><a href="#Evaluation-of-Generation-Based-Large-Language-Models-LLMs-Opportunities-and-Challenges-from-Generation-to-Judgment" class="headerlink" title="Evaluation of Generation-Based Large Language Models (LLMs): Opportunities and Challenges from Generation to Judgment"></a>Evaluation of Generation-Based Large Language Models (LLMs): Opportunities and Challenges from Generation to Judgment</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>Evaluation tasks in artificial intelligence (AI) and natural language processing (NLP) have long been challenging. Traditional evaluation methods, such as those based on matching or embeddings, are limited in assessing complex attributes. The recent development of large language models (LLMs) has given rise to the “LLM-as-a-Judge” paradigm, which utilizes LLMs for scoring, ranking, or selection tasks. This paper provides a comprehensive review of LLM evaluation methodologies, including their definitions, classification frameworks, benchmarks, and future research directions.</p><hr><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h2><h3 id="1-1-Background"><a href="#1-1-Background" class="headerlink" title="1.1 Background"></a>1.1 Background</h3><p>Evaluation is one of the core issues in machine learning and NLP. Traditional evaluation methods such as BLEU and ROUGE often rely on text overlap and lack applicability in complex scenarios. With the development of deep learning and LLMs (e.g., GPT-4), researchers have proposed the “LLM-as-a-Judge” paradigm to address the limitations of traditional evaluation methods.</p><h3 id="1-2-Research-Questions"><a href="#1-2-Research-Questions" class="headerlink" title="1.2 Research Questions"></a>1.2 Research Questions</h3><p>This paper aims to explore the following questions:</p><ul><li><strong>What do LLMs evaluate?</strong></li><li><strong>How is evaluation conducted?</strong></li><li><strong>Where are LLMs applied for evaluation?</strong></li></ul><hr><h2 id="2-Preliminary-Knowledge"><a href="#2-Preliminary-Knowledge" class="headerlink" title="2. Preliminary Knowledge"></a>2. Preliminary Knowledge</h2><h3 id="2-1-Input-Formats"><a href="#2-1-Input-Formats" class="headerlink" title="2.1 Input Formats"></a>2.1 Input Formats</h3><p>Evaluation inputs can be categorized as follows:</p><ul><li><strong>Point-Wise</strong>: Evaluation of a single sample.</li><li><strong>Pair/List-Wise</strong>: Comparative evaluation of multiple samples.</li></ul><h3 id="2-2-Output-Formats"><a href="#2-2-Output-Formats" class="headerlink" title="2.2 Output Formats"></a>2.2 Output Formats</h3><p>Evaluation outputs include:</p><ul><li><strong>Scores</strong>: Quantitative scoring of samples.</li><li><strong>Ranking</strong>: Ordering based on merit.</li><li><strong>Selection</strong>: Choosing the best option among candidates.</li></ul><hr><h2 id="3-Evaluation-Attributes"><a href="#3-Evaluation-Attributes" class="headerlink" title="3. Evaluation Attributes"></a>3. Evaluation Attributes</h2><h3 id="3-1-Helpfulness"><a href="#3-1-Helpfulness" class="headerlink" title="3.1 Helpfulness"></a>3.1 Helpfulness</h3><p>LLMs evaluate the helpfulness of responses by guiding user tasks and generating feedback, which is crucial in AI alignment.</p><h3 id="3-2-Harmlessness"><a href="#3-2-Harmlessness" class="headerlink" title="3.2 Harmlessness"></a>3.2 Harmlessness</h3><p>Evaluating the harmlessness of text is key to generating safe content. LLMs assist in data labeling or directly assess potential harmful content.</p><h3 id="3-3-Reliability"><a href="#3-3-Reliability" class="headerlink" title="3.3 Reliability"></a>3.3 Reliability</h3><p>LLMs detect factual accuracy and consistency, e.g., generating supporting evidence or conducting conversation-level reliability evaluations.</p><h3 id="3-4-Relevance"><a href="#3-4-Relevance" class="headerlink" title="3.4 Relevance"></a>3.4 Relevance</h3><p>LLMs assess the relevance of generated or retrieved content, applicable in scenarios like conversations and retrieval-augmented generation (RAG).</p><h3 id="3-5-Feasibility"><a href="#3-5-Feasibility" class="headerlink" title="3.5 Feasibility"></a>3.5 Feasibility</h3><p>In complex tasks, LLMs judge the feasibility of candidate steps or actions to optimize decision paths.</p><h3 id="3-6-Overall-Quality"><a href="#3-6-Overall-Quality" class="headerlink" title="3.6 Overall Quality"></a>3.6 Overall Quality</h3><p>By scoring across multiple dimensions, LLMs provide an overall evaluation, suitable for comprehensive comparisons in generation tasks.</p><hr><h3 id="4-Methodology"><a href="#4-Methodology" class="headerlink" title="4. Methodology"></a>4. Methodology</h3><h4 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h4><p>The methodology section focuses on optimizing the capabilities of LLMs as evaluators (LLM-as-a-Judge) through two approaches: fine-tuning and prompt engineering.</p><ol><li><strong>Fine-Tuning Techniques</strong>: Enhancing LLM judgment capabilities using supervised fine-tuning (SFT) and preference learning with labeled or synthetic feedback.</li><li><strong>Prompt Engineering</strong>: Designing effective prompt strategies, such as operation swapping, rule enhancement, and multi-agent collaboration, to improve inference and evaluation accuracy and reliability.</li></ol><hr><h4 id="4-1-Fine-Tuning-Techniques"><a href="#4-1-Fine-Tuning-Techniques" class="headerlink" title="4.1 Fine-Tuning Techniques"></a>4.1 Fine-Tuning Techniques</h4><h5 id="Data-Sources"><a href="#Data-Sources" class="headerlink" title="Data Sources"></a>Data Sources</h5><h6 id="1-Human-Labeled-Data"><a href="#1-Human-Labeled-Data" class="headerlink" title="1. Human-Labeled Data"></a>1. <strong>Human-Labeled Data</strong></h6><p>Human-labeled data provides high-quality training samples that help LLMs learn human preferences. Key studies and innovations include:</p><ol><li><p><strong>PandaLM</strong> [Wang et al., 2024h]:</p><ul><li>Collected a diverse dataset with 300,000 samples for instruction-generation tasks.</li><li>Enhanced generalization by integrating data sources like open-domain QA and dialogue generation.</li><li>Introduced standardized annotation workflows for consistency and emphasized multilingual support.</li></ul></li><li><p><strong>AspectInstruct</strong> [Liu et al., 2024a]:</p><ul><li>Introduced a dataset tailored for multi-dimensional evaluation, covering 65 tasks and 27 evaluation dimensions.</li><li>Designed a unique task segmentation mechanism for contextual understanding and dimension prioritization.</li></ul></li></ol><h6 id="2-Synthetic-Data"><a href="#2-Synthetic-Data" class="headerlink" title="2. Synthetic Data"></a>2. <strong>Synthetic Data</strong></h6><p>Synthetic data generated by LLMs reduces dependency on human labeling and expands data coverage. Key studies and innovations include:</p><ol><li><p><strong>JudgeLM</strong> [Zhu et al., 2023]:</p><ul><li>Generated a dataset with 100,000 samples, covering various instruction-generation scenarios.</li><li>Introduced task-seeding methods to ensure diversity and specificity.</li></ul></li><li><p><strong>Meta-Rewarding</strong> [Wu et al., 2024]:</p><ul><li>Proposed “meta-rewarding,” using LLM self-evaluation signals to enhance training effectiveness.</li></ul></li></ol><h5 id="Fine-Tuning-Methods"><a href="#Fine-Tuning-Methods" class="headerlink" title="Fine-Tuning Methods"></a>Fine-Tuning Methods</h5><h6 id="1-Supervised-Fine-Tuning-SFT"><a href="#1-Supervised-Fine-Tuning-SFT" class="headerlink" title="1. Supervised Fine-Tuning (SFT)"></a>1. <strong>Supervised Fine-Tuning (SFT)</strong></h6><p>SFT trains LLMs using human-labeled or synthetic data to learn evaluation criteria. Key studies include:</p><ol><li><p><strong>FLAMe</strong> [Vu et al., 2024]:</p><ul><li>Leveraged a multi-task learning framework with 5 million samples for multi-task SFT.</li><li>Unified evaluation standards across diverse tasks.</li></ul></li><li><p><strong>JSFT</strong> [Lee et al., 2024]:</p><ul><li>Combined SFT with preference learning to optimize performance on diverse evaluation tasks.</li></ul></li></ol><h6 id="2-Preference-Learning"><a href="#2-Preference-Learning" class="headerlink" title="2. Preference Learning"></a>2. <strong>Preference Learning</strong></h6><p>Preference learning optimizes LLM comparison and ranking capabilities for complex evaluations. Key studies include:</p><ol><li><p><strong>HALU-J</strong> [Wang et al., 2024a]:</p><ul><li>Employed directed preference optimization (DPO) with multi-evidence selection mechanisms.</li></ul></li><li><p><strong>Self-Taught Evaluators</strong> [Wang et al., 2024f]:</p><ul><li>Used self-generated suboptimal responses as negative samples for dynamic improvement.</li></ul></li></ol><hr><h3 id="5-Applications"><a href="#5-Applications" class="headerlink" title="5. Applications"></a>5. Applications</h3><h4 id="Overview-1"><a href="#Overview-1" class="headerlink" title="Overview"></a>Overview</h4><p>The applications of LLM-as-a-Judge have expanded from generation evaluation to alignment, retrieval, and reasoning. This section systematically introduces these applications, their specific tasks, and representative studies.</p><hr><h4 id="5-1-Evaluation"><a href="#5-1-Evaluation" class="headerlink" title="5.1 Evaluation"></a>5.1 Evaluation</h4><h5 id="Overview-2"><a href="#Overview-2" class="headerlink" title="Overview"></a>Overview</h5><p>LLM-as-a-Judge was initially applied to evaluation tasks like dialogue generation and summarization. Key studies include:</p><ol><li><p><strong>MD-Judge</strong> [Li et al., 2024f]:</p><ul><li>Evaluated safety-related Q&amp;A frameworks, focusing on harmfulness and ethical risks.</li></ul></li><li><p><strong>Chan Framework</strong> [Chan et al., 2023]:</p><ul><li>Introduced a multi-agent debate framework for improved evaluation quality.</li></ul></li><li><p><strong>ICE</strong> [Jain et al., 2023b]:</p><ul><li>Used few-shot examples for interactive multi-dimensional evaluation.</li></ul></li></ol><hr><h3 id="7-Challenges-and-Future-Directions"><a href="#7-Challenges-and-Future-Directions" class="headerlink" title="7. Challenges and Future Directions"></a>7. Challenges and Future Directions</h3><h4 id="Overview-3"><a href="#Overview-3" class="headerlink" title="Overview"></a>Overview</h4><p>Despite its powerful capabilities, LLM-as-a-Judge faces challenges such as evaluation bias, adaptability to dynamic tasks, and the potential of human-AI collaborative evaluation. This section explores these challenges and outlines future research directions.</p><h5 id="7-1-Bias-and-Vulnerabilities"><a href="#7-1-Bias-and-Vulnerabilities" class="headerlink" title="7.1 Bias and Vulnerabilities"></a>7.1 Bias and Vulnerabilities</h5><ol><li><strong>OffsetBias</strong> [Park et al., 2024]:<ul><li>Proposed a de-biasing framework to mitigate positional and content biases.</li></ul></li></ol><h5 id="7-2-Dynamic-and-Complex-Evaluations"><a href="#7-2-Dynamic-and-Complex-Evaluations" class="headerlink" title="7.2 Dynamic and Complex Evaluations"></a>7.2 Dynamic and Complex Evaluations</h5><ol><li><strong>Tree of Thought (ToT)</strong> [Yao et al., 2023a]:<ul><li>Enhanced multi-step reasoning with dynamic state evaluation mechanisms.</li></ul></li></ol><h5 id="7-3-Self-Evaluation-and-Human-AI-Collaboration"><a href="#7-3-Self-Evaluation-and-Human-AI-Collaboration" class="headerlink" title="7.3 Self-Evaluation and Human-AI Collaboration"></a>7.3 Self-Evaluation and Human-AI Collaboration</h5><ol><li><p><strong>Self-Taught Evaluators</strong> [Wang et al., 2024f]:</p><ul><li>Highlighted the potential for models to improve through self-learning mechanisms.</li></ul></li><li><p><strong>Meta-Rewarding</strong> [Wu et al., 2024]:</p><ul><li>Demonstrated the advantages of integrating self-evaluation signals into optimization.</li></ul></li></ol><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Evaluation-of-Generation-Based-Large-Language-Models-LLMs-Opportunities-and-Challenges-from-Generation-to-Judgment&quot;&gt;&lt;a href=&quot;#Evalua</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>基于生成的大语言模型（LLM）评估：从生成到判断的机遇与挑战</title>
    <link href="https://chenhuiyu.github.io/2024/12/06/NLP%20Insights/%E5%9F%BA%E4%BA%8E%E7%94%9F%E6%88%90%E7%9A%84%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%88LLM%EF%BC%89%E8%AF%84%E4%BC%B0%EF%BC%9A%E4%BB%8E%E7%94%9F%E6%88%90%E5%88%B0%E5%88%A4%E6%96%AD%E7%9A%84%E6%9C%BA%E9%81%87%E4%B8%8E%E6%8C%91%E6%88%98/"/>
    <id>https://chenhuiyu.github.io/2024/12/06/NLP%20Insights/%E5%9F%BA%E4%BA%8E%E7%94%9F%E6%88%90%E7%9A%84%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%88LLM%EF%BC%89%E8%AF%84%E4%BC%B0%EF%BC%9A%E4%BB%8E%E7%94%9F%E6%88%90%E5%88%B0%E5%88%A4%E6%96%AD%E7%9A%84%E6%9C%BA%E9%81%87%E4%B8%8E%E6%8C%91%E6%88%98/</id>
    <published>2024-12-06T06:34:18.000Z</published>
    <updated>2024-12-06T08:08:36.085Z</updated>
    
    <content type="html"><![CDATA[<hr><h1 id="基于生成的大语言模型（LLM）评估：从生成到判断的机遇与挑战"><a href="#基于生成的大语言模型（LLM）评估：从生成到判断的机遇与挑战" class="headerlink" title="基于生成的大语言模型（LLM）评估：从生成到判断的机遇与挑战"></a>基于生成的大语言模型（LLM）评估：从生成到判断的机遇与挑战</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>人工智能（AI）与自然语言处理（NLP）领域中的评估任务长期面临挑战。传统的评估方法（如基于匹配或嵌入的技术）在判断复杂属性时效果有限。近期大语言模型（LLM）的发展催生了“LLM-as-a-Judge”范式，利用LLM对任务进行评分、排序或选择。本论文对LLM评估方法进行了全面综述，包括其定义、分类框架、评估基准，以及未来的研究方向。</p><hr><h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><h3 id="1-1-背景"><a href="#1-1-背景" class="headerlink" title="1.1 背景"></a>1.1 背景</h3><p>评估是机器学习和NLP的核心问题之一，传统评估方法如BLEU和ROUGE通常基于文本重叠，缺乏对复杂场景的适用性。随着深度学习和LLM的发展（如GPT-4），研究者提出了“LLM-as-a-Judge”模式，以解决传统评估的局限。</p><h3 id="1-2-研究问题"><a href="#1-2-研究问题" class="headerlink" title="1.2 研究问题"></a>1.2 研究问题</h3><p>本论文旨在探讨以下问题：</p><ul><li><strong>评估内容：LLM评估什么？</strong></li><li><strong>评估方法：如何进行评估？</strong></li><li><strong>应用场景：LLM在哪里评估？</strong></li></ul><hr><h2 id="2-预备知识"><a href="#2-预备知识" class="headerlink" title="2. 预备知识"></a>2. 预备知识</h2><h3 id="2-1-输入格式"><a href="#2-1-输入格式" class="headerlink" title="2.1 输入格式"></a>2.1 输入格式</h3><p>评估输入可分为：</p><ul><li><strong>点对点（Point-Wise）</strong>：单个样本评估。</li><li><strong>对/列表评估（Pair/List-Wise）</strong>：多个样本的比较评估。</li></ul><h3 id="2-2-输出格式"><a href="#2-2-输出格式" class="headerlink" title="2.2 输出格式"></a>2.2 输出格式</h3><p>评估输出包括：</p><ul><li><strong>评分（Score）</strong>：对样本进行量化评分。</li><li><strong>排序（Ranking）</strong>：根据优劣排序。</li><li><strong>选择（Selection）</strong>：从多个候选中选取最佳方案。</li></ul><hr><h2 id="3-评估属性"><a href="#3-评估属性" class="headerlink" title="3. 评估属性"></a>3. 评估属性</h2><h3 id="3-1-有用性（Helpfulness）"><a href="#3-1-有用性（Helpfulness）" class="headerlink" title="3.1 有用性（Helpfulness）"></a>3.1 有用性（Helpfulness）</h3><p>LLM通过指导用户任务和生成反馈，对响应的有用性进行评估。这在AI对齐（Alignment）中尤为重要。</p><h3 id="3-2-无害性（Harmlessness）"><a href="#3-2-无害性（Harmlessness）" class="headerlink" title="3.2 无害性（Harmlessness）"></a>3.2 无害性（Harmlessness）</h3><p>评估文本的无害性是生成安全内容的关键。LLM可辅助数据标注或直接评估潜在的有害内容。</p><h3 id="3-3-可靠性（Reliability）"><a href="#3-3-可靠性（Reliability）" class="headerlink" title="3.3 可靠性（Reliability）"></a>3.3 可靠性（Reliability）</h3><p>LLM可检测事实性和一致性。例如，通过生成辅助证据或进行对话级别的可靠性评估。</p><h3 id="3-4-相关性（Relevance）"><a href="#3-4-相关性（Relevance）" class="headerlink" title="3.4 相关性（Relevance）"></a>3.4 相关性（Relevance）</h3><p>LLM可评估生成或检索内容的相关性，适用于会话、检索增强生成（RAG）等场景。</p><h3 id="3-5-可行性（Feasibility）"><a href="#3-5-可行性（Feasibility）" class="headerlink" title="3.5 可行性（Feasibility）"></a>3.5 可行性（Feasibility）</h3><p>在复杂任务中，LLM可对候选步骤或行动进行可行性判断，从而优化决策路径。</p><h3 id="3-6-总体质量（Overall-Quality）"><a href="#3-6-总体质量（Overall-Quality）" class="headerlink" title="3.6 总体质量（Overall Quality）"></a>3.6 总体质量（Overall Quality）</h3><p>LLM通过多维度评分生成整体评价，适用于生成任务的综合比较。</p><hr><h3 id="4-方法论"><a href="#4-方法论" class="headerlink" title="4. 方法论"></a>4. 方法论</h3><h4 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h4><p>方法论部分主要探讨如何优化LLM作为评估者（LLM-as-a-Judge）的能力，从调优和提示技术两个方面进行阐述：</p><ol><li><strong>调优技术</strong>：通过监督微调（SFT）和偏好学习等方法，利用人工标注数据或合成反馈来增强LLM的判断能力。</li><li><strong>提示技术</strong>：设计高效的提示策略（如操作交换、规则增强、多代理协作等）以提升LLM在推理和评估过程中的准确性和可靠性。</li></ol><hr><h4 id="4-1-调优技术"><a href="#4-1-调优技术" class="headerlink" title="4.1 调优技术"></a>4.1 调优技术</h4><h5 id="数据来源"><a href="#数据来源" class="headerlink" title="数据来源"></a>数据来源</h5><h6 id="1-人工标注数据"><a href="#1-人工标注数据" class="headerlink" title="1. 人工标注数据"></a>1. <strong>人工标注数据</strong></h6><p>人工标注数据提供了高质量的训练样本，帮助LLM学习人类偏好。以下是核心研究及其创新点：</p><ol><li><p><strong>PandaLM</strong>【Wang et al., 2024h】：</p><ul><li>PandaLM项目收集了多样化的人工标注数据集，涵盖指令生成任务的300,000个样本。</li><li>作者通过整合多种数据源（如开放领域问答和对话生成）来增强模型的泛化能力。</li><li>该研究的关键创新在于引入了标准化的标注流程，以确保数据质量与一致性。</li><li>此外，PandaLM强调多语言支持，通过跨文化的数据标注提高模型的适用性。</li><li>最终，PandaLM被证明在多个评估任务上表现优异，其输出与人工评估高度相关。</li></ul></li><li><p><strong>AspectInstruct</strong>【Liu et al., 2024a】：</p><ul><li>该研究首次提出了一个针对多维度评估的指令调优数据集，涵盖65个任务和27个评估维度。</li><li>数据集中包含对话生成、摘要和数据到文本转换等复杂任务的多方面评分。</li><li>作者设计了独特的任务分割机制，使模型能够根据上下文理解并优先评估特定维度。</li><li>研究的亮点在于数据集的多样性和全面性，为多任务评估提供了新的基准。</li><li>最终，该数据集显著提升了LLM在不同评估场景中的多维度理解和评估能力。</li></ul></li></ol><h6 id="2-合成数据"><a href="#2-合成数据" class="headerlink" title="2. 合成数据"></a>2. <strong>合成数据</strong></h6><p>合成数据通过LLM生成训练样本，减少了对人工标注的依赖，同时扩展了数据覆盖范围。以下是核心研究及其创新点：</p><ol><li><p><strong>JudgeLM</strong>【Zhu et al., 2023】：</p><ul><li>研究者利用GPT-4生成包含任务种子、生成答案及相关评估的高质量数据集。</li><li>数据集中包含10万个样本，覆盖了指令生成任务的多种场景。</li><li>核心创新点在于引入了生成任务种子的方法，确保生成数据的多样性和针对性。</li><li>作者还设计了一种基于偏好学习的优化方法，以提高LLM对细粒度任务的判断能力。</li><li>研究表明，经过这种优化后的JudgeLM在多个基准测试中超越了传统方法。</li></ul></li><li><p><strong>Meta-Rewarding</strong>【Wu et al., 2024】：</p><ul><li>提出了一种新颖的“元奖励”（Meta-Rewarding）方法，通过LLM自我评估生成的判断信号增强训练效果。</li><li>该方法要求模型在生成答案后对自己的输出进行评分，从而生成偏好数据。</li><li>创新点在于采用策略模型作为评估者，显著提高了数据生成效率和质量。</li><li>此外，该研究通过逐步改进的偏好数据训练LLM，提高了其评估任务的鲁棒性。</li><li>最终，Meta-Rewarding展示了LLM自我增强能力的潜力，成为偏好学习领域的重要进展。</li></ul></li></ol><h5 id="调优方法"><a href="#调优方法" class="headerlink" title="调优方法"></a>调优方法</h5><h6 id="1-监督微调（SFT）"><a href="#1-监督微调（SFT）" class="headerlink" title="1. 监督微调（SFT）"></a>1. <strong>监督微调（SFT）</strong></h6><p>监督微调通过使用人工标注或合成数据，让LLM从示例中学习判断标准。以下是核心研究及其创新点：</p><ol><li><p><strong>FLAMe</strong>【Vu et al., 2024】：</p><ul><li>该研究提出了Foundational Large Autorater Models (FLAMe)，利用超过500万个样本进行大规模多任务监督微调。</li><li>FLAMe在多任务数据中引入了统一的评价标准，提高了模型在多样化任务中的评估能力。</li><li>创新点在于采用多任务学习框架，将多个评估维度集成到一个模型中。</li><li>作者还设计了任务分层训练策略，使模型能够逐步掌握复杂的评估任务。</li><li>实验结果表明，FLAMe在多个生成任务上的表现优于传统评估指标。</li></ul></li><li><p><strong>JSFT</strong>【Lee et al., 2024】：</p><ul><li>提出了Judge-augmented Supervised Fine-Tuning（JSFT）方法，通过扩展偏好学习数据增强微调效果。</li><li>数据集中包含点对点和对比评估任务，以全面覆盖多种评估场景。</li><li>创新点在于引入了多阶段训练策略，结合监督学习和偏好学习优化模型性能。</li><li>此外，研究者设计了简化提示机制，显著提高了模型处理复杂输入的能力。</li><li>JSFT的实验结果显示，其生成的评估结果在多个基准上超过了现有方法。</li></ul></li></ol><h6 id="2-偏好学习"><a href="#2-偏好学习" class="headerlink" title="2. 偏好学习"></a>2. <strong>偏好学习</strong></h6><p>偏好学习通过优化LLM的比较和排序能力，适用于复杂评估任务。以下是核心研究及其创新点：</p><ol><li><p><strong>HALU-J</strong>【Wang et al., 2024a】：</p><ul><li>提出了一种基于批评的偏好学习方法，专注于选择相关证据并生成详细批评。</li><li>创新点在于设计了多证据选择机制，提高了LLM的可靠性评估能力。</li><li>该方法通过Directed Preference Optimization（DPO）进行优化，使模型能够更准确地判断任务间的优劣。</li><li>HALU-J还结合了上下文推理，扩展了偏好学习的应用场景。</li><li>实验表明，HALU-J显著提升了复杂任务的评估准确性，尤其是在事实性和逻辑性判断上。</li></ul></li><li><p><strong>Self-Taught Evaluators</strong>【Wang et al., 2024f】：</p><ul><li>该研究提出了一种自学习的评估者方法，利用被扰乱的指令生成低质量数据作为偏好学习的负样本。</li><li>自学习方法通过自动生成的次优响应，提供了丰富的训练数据。</li><li>创新点在于通过动态调整偏好信号，提升了模型的适应性和通用性。</li><li>作者还设计了基于多轮交互的学习策略，使模型能够在动态环境中自我优化。</li><li>实验结果显示，Self-Taught Evaluators在多个开放式生成任务中表现优异。</li></ul></li></ol><h3 id="4-2-提示技术"><a href="#4-2-提示技术" class="headerlink" title="4.2 提示技术"></a>4.2 提示技术</h3><h4 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h4><p>提示技术（Prompting）通过设计高效的提示策略和推理流程优化LLM的评估能力。这部分探讨如何在推理阶段利用提示技术提升判断精度，减少偏差，并增强模型的评估鲁棒性。主要方法包括操作交换、规则增强、多代理协作、演示、多轮交互以及比较加速。</p><hr><h4 id="4-2-1-操作交换（Swapping-Operation）"><a href="#4-2-1-操作交换（Swapping-Operation）" class="headerlink" title="4.2.1 操作交换（Swapping Operation）"></a>4.2.1 操作交换（Swapping Operation）</h4><h5 id="概述-2"><a href="#概述-2" class="headerlink" title="概述"></a>概述</h5><p>操作交换技术通过更改候选项顺序减少评估的偏置性，确保LLM对输入顺序不敏感，从而提高评估的公平性和可靠性。</p><h6 id="1-MT-Bench【Zheng-et-al-2023】："><a href="#1-MT-Bench【Zheng-et-al-2023】：" class="headerlink" title="1. MT-Bench【Zheng et al., 2023】："></a>1. <strong>MT-Bench</strong>【Zheng et al., 2023】：</h6><ul><li>本研究首次系统性地提出操作交换技术，通过多轮评估减少LLM的顺序敏感性。</li><li>创新点在于引入“对称性检查”机制：将候选项顺序互换，若评分结果一致，则标记为稳定，否则标记为不稳定。</li><li>作者发现操作交换能够有效减少由于位置偏差导致的错误判断。</li><li>该技术应用于多任务评估中，尤其是在复杂生成任务的排序中表现突出。</li><li>MT-Bench为后续的LLM评估技术提供了一个重要的公平性基准。</li></ul><h6 id="2-Starling【Zhu-et-al-2024a】："><a href="#2-Starling【Zhu-et-al-2024a】：" class="headerlink" title="2. Starling【Zhu et al., 2024a】："></a>2. <strong>Starling</strong>【Zhu et al., 2024a】：</h6><ul><li>提出一种类似链式推理（Chain-of-Thought, CoT）的提示技术，通过全面评估所有候选项的两两关系，再总结为最终排序。</li><li>创新点在于强制模型生成所有可能的对比结果，确保评估全面且无偏。</li><li>作者还设计了一种交叉验证机制，进一步提高评估稳定性。</li><li>实验显示，这种方法显著减少了位置偏差带来的误差，特别是在排序任务中表现优异。</li><li>Starling验证了链式思维结合操作交换技术的潜力，尤其在复杂对比任务中的效果显著。</li></ul><hr><h4 id="4-2-2-规则增强（Rule-Augmentation）"><a href="#4-2-2-规则增强（Rule-Augmentation）" class="headerlink" title="4.2.2 规则增强（Rule Augmentation）"></a>4.2.2 规则增强（Rule Augmentation）</h4><h5 id="概述-3"><a href="#概述-3" class="headerlink" title="概述"></a>概述</h5><p>规则增强技术通过在提示中嵌入明确的原则、标准或参考内容，使模型能够更加系统地评估任务，从而提升评估的准确性和一致性。</p><h6 id="1-Constitutional-AI【Bai-et-al-2022】："><a href="#1-Constitutional-AI【Bai-et-al-2022】：" class="headerlink" title="1. Constitutional AI【Bai et al., 2022】："></a>1. <strong>Constitutional AI</strong>【Bai et al., 2022】：</h6><ul><li>本研究引入了“原则驱动”的规则增强方法，利用帮助性、无害性和诚实性等标准指导模型评估。</li><li>创新点在于为每个评估维度定义详细的评分标准，并通过原则约束生成内容。</li><li>作者采用多层提示设计，使LLM能够逐步推理并给出最终评估。</li><li>实验表明，这种方法显著提升了模型在复杂场景中的判断一致性。</li><li>Constitutional AI成为后续研究的重要基石，为基于规则的评估技术奠定了基础。</li></ul><h6 id="2-OAIF【Guo-et-al-2024】："><a href="#2-OAIF【Guo-et-al-2024】：" class="headerlink" title="2. OAIF【Guo et al., 2024】："></a>2. <strong>OAIF</strong>【Guo et al., 2024】：</h6><ul><li>提出了在线AI反馈（Online AI Feedback, OAIF）框架，通过实时原则指导提升模型评估的灵活性。</li><li>核心创新点在于动态调整评估规则，使模型能够适应多变的任务需求。</li><li>OAIF引入了细粒度的多维评分策略，为每个候选项生成独立的评估报告。</li><li>作者验证了这种方法在实时决策中的潜力，尤其在对话和生成任务中表现突出。</li><li>OAIF展现了规则增强的实时适应能力，为实时评估任务提供了新方向。</li></ul><hr><h4 id="4-2-3-多代理协作（Multi-agent-Collaboration）"><a href="#4-2-3-多代理协作（Multi-agent-Collaboration）" class="headerlink" title="4.2.3 多代理协作（Multi-agent Collaboration）"></a>4.2.3 多代理协作（Multi-agent Collaboration）</h4><h5 id="概述-4"><a href="#概述-4" class="headerlink" title="概述"></a>概述</h5><p>多代理协作通过组合多个LLM的评估结果，减少单一模型的偏差，提高评估的准确性和鲁棒性。这种方法强调模型之间的角色分工和合作。</p><h6 id="1-Peer-Rank-PR-【Li-et-al-2023】："><a href="#1-Peer-Rank-PR-【Li-et-al-2023】：" class="headerlink" title="1. Peer Rank (PR)【Li et al., 2023】："></a>1. <strong>Peer Rank (PR)</strong>【Li et al., 2023】：</h6><ul><li>提出了同行排名算法，整合多个LLM的对比偏好生成最终排序。</li><li>创新点在于设计了“加权投票”机制，根据模型之间的评分一致性调整权重。</li><li>该研究还探讨了代理间的协作效率和鲁棒性，提出了优化协作路径的方法。</li><li>PR的实验结果显示，其生成的评估结果在排序准确性上优于传统单模型方法。</li><li>该研究为多模型协作技术奠定了理论基础，是后续研究的重要参考。</li></ul><h6 id="2-Cascaded-Selective-Evaluation【Jung-et-al-2024】："><a href="#2-Cascaded-Selective-Evaluation【Jung-et-al-2024】：" class="headerlink" title="2. Cascaded Selective Evaluation【Jung et al., 2024】："></a>2. <strong>Cascaded Selective Evaluation</strong>【Jung et al., 2024】：</h6><ul><li>设计了级联选择评估框架，首先由较弱的模型进行初步评估，仅在需要时调用更强大的模型。</li><li>创新点在于通过分级策略优化计算成本，同时确保评估结果的高质量。</li><li>作者提出了一种交叉验证机制，结合多个代理的结果生成最终判断。</li><li>研究表明，这种级联策略在复杂任务中表现出显著的资源效率提升。</li><li>Cascaded Selective Evaluation展示了多代理协作在资源有限情况下的潜力。</li></ul><hr><h4 id="4-2-4-演示（Demonstration）"><a href="#4-2-4-演示（Demonstration）" class="headerlink" title="4.2.4 演示（Demonstration）"></a>4.2.4 演示（Demonstration）</h4><h5 id="概述-5"><a href="#概述-5" class="headerlink" title="概述"></a>概述</h5><p>演示技术利用具体的示例作为提示，帮助LLM学习评估标准。这种方法通过少量高质量样例显著提高模型的评估能力。</p><h6 id="1-ALLURE【Hasanbeig-et-al-2023】："><a href="#1-ALLURE【Hasanbeig-et-al-2023】：" class="headerlink" title="1. ALLURE【Hasanbeig et al., 2023】："></a>1. <strong>ALLURE</strong>【Hasanbeig et al., 2023】：</h6><ul><li>提出了迭代演示技术，通过在提示中加入显著偏差的示例提高模型的鲁棒性。</li><li>创新点在于采用动态演示方法，逐步更新提示以适应不同的评估任务。</li><li>研究表明，这种方法在低资源场景中表现出色，尤其是在新任务的适应性上有显著提升。</li><li>作者还探讨了如何选择代表性样例以最大化演示效果。</li><li>ALLURE验证了高质量演示样例在提升评估能力方面的重要性。</li></ul><h6 id="2-ICE【Jain-et-al-2023b】："><a href="#2-ICE【Jain-et-al-2023b】：" class="headerlink" title="2. ICE【Jain et al., 2023b】："></a>2. <strong>ICE</strong>【Jain et al., 2023b】：</h6><ul><li>提出了交互式多维评估框架，通过少量上下文示例指导LLM评估。</li><li>创新点在于将评估任务分解为多个独立维度，每个维度都有针对性的示例支持。</li><li>研究表明，ICE框架显著减少了模型在多维任务中的评估偏差。</li><li>实验结果显示，其生成的评估结果在与人工评价的一致性上达到高水平。</li><li>ICE为多维度评估任务的提示设计提供了新思路。</li></ul><hr><h4 id="4-2-5-多轮交互（Multi-turn-Interaction）"><a href="#4-2-5-多轮交互（Multi-turn-Interaction）" class="headerlink" title="4.2.5 多轮交互（Multi-turn Interaction）"></a>4.2.5 多轮交互（Multi-turn Interaction）</h4><h5 id="概述-6"><a href="#概述-6" class="headerlink" title="概述"></a>概述</h5><p>多轮交互通过动态调整提示和上下文信息，为LLM提供更全面的评估依据，适用于需要多步推理的复杂任务。</p><h6 id="1-KIEval【Yu-et-al-2024】："><a href="#1-KIEval【Yu-et-al-2024】：" class="headerlink" title="1. KIEval【Yu et al., 2024】："></a>1. <strong>KIEval</strong>【Yu et al., 2024】：</h6><ul><li>提出了知识交互式评估框架，通过动态问答生成丰富的上下文信息。</li><li>创新点在于引入了“交互者”角色，模拟用户和模型之间的动态交互。</li><li>作者设计了一种鲁棒性检测机制，避免因上下文污染导致的错误评估。</li><li>研究表明，KIEval在复杂任务中的表现优于传统静态评估方法。</li><li>此框架适用于多维度评估，特别是在需要动态调整上下文的场景中。</li></ul><h6 id="2-Auto-Arena【Zhao-et-al-2024c】："><a href="#2-Auto-Arena【Zhao-et-al-2024c】：" class="headerlink" title="2. Auto-Arena【Zhao et al., 2024c】："></a>2. <strong>Auto-Arena</strong>【Zhao et al., 2024c】：</h6><ul><li>设计了一种多轮辩论框架，允许多个模型围绕特定任务进行交互讨论。</li><li>创新点在于结合多轮问答和动态评分机制，从不同角度对候选答案进行评估。</li><li>研究表明，这种方法能够揭示候选答案间的深层次差异。</li><li>作者还探讨了如何通过动态调整辩论内容提高评估效率。</li><li>Auto-Arena展示了多轮交互在复杂评估任务中的潜力。</li></ul><hr><h4 id="4-2-6-比较加速（Comparison-Acceleration）"><a href="#4-2-6-比较加速（Comparison-Acceleration）" class="headerlink" title="4.2.6 比较加速（Comparison Acceleration）"></a>4.2.6 比较加速（Comparison Acceleration）</h4><h5 id="概述-7"><a href="#概述-7" class="headerlink" title="概述"></a>概述</h5><p>比较加速技术通过优化比较流程，减少多候选排序任务的计算成本，提高评估效率。</p><h6 id="1-Ranked-Pairing【Zhai-et-al-2024】："><a href="#1-Ranked-Pairing【Zhai-et-al-2024】：" class="headerlink" title="1. Ranked Pairing【Zhai et al., 2024】："></a>1. <strong>Ranked Pairing</strong>【Zhai et al., 2024】：</h6><ul><li>提出了一种基于基线比较的排序方法，通过对所有候选项与基线进行比较确定优劣。</li><li>创新点在于避免传统两两比较的高计算开销，显著提高了评估效率。</li><li>作者还设计了一种自适应比较策略，进一步优化排序性能。</li><li>研究表明，Ranked Pairing在大规模排序任务中表现出极高的效率。</li><li>此方法特别适用于需要快速生成排序结果的场景。</li></ul><h6 id="2-Tournament-based-Comparison【Lee-et-al-2024】："><a href="#2-Tournament-based-Comparison【Lee-et-al-2024】：" class="headerlink" title="2. Tournament-based Comparison【Lee et al., 2024】："></a>2. <strong>Tournament-based Comparison</strong>【Lee et al., 2024】：</h6><ul><li>采用锦标赛式的比较方法，构建树状结构逐层筛选最佳</li></ul><p>候选。</p><ul><li>创新点在于结合拒绝采样和多轮比较，减少了低质量候选的影响。</li><li>作者探讨了不同树结构设计对评估效率和准确性的影响。</li><li>实验结果显示，该方法在多候选任务中显著提高了计算效率。</li><li>Tournament-based Comparison展示了基于结构化比较的潜在优势。</li></ul><hr><h3 id="5-应用场景"><a href="#5-应用场景" class="headerlink" title="5. 应用场景"></a>5. 应用场景</h3><h4 id="概述-8"><a href="#概述-8" class="headerlink" title="概述"></a>概述</h4><p>LLM-as-a-Judge的应用场景已从最初的生成任务评估扩展到多个领域，包括评估、对齐（Alignment）、检索和推理（Reasoning）。这一部分系统性地介绍这些应用场景，讨论每种应用的具体任务和代表性研究。</p><hr><h4 id="5-1-评估"><a href="#5-1-评估" class="headerlink" title="5.1 评估"></a>5.1 评估</h4><h5 id="概述-9"><a href="#概述-9" class="headerlink" title="概述"></a>概述</h5><p>LLM-as-a-Judge最初的核心应用是评估任务，包括开放式生成任务（如对话生成、摘要生成）、推理任务，以及其他新兴任务。通过LLM评估，能够更精准地捕捉复杂生成任务中的质量、相关性及逻辑性等维度。</p><h6 id="1-MD-Judge【Li-et-al-2024f】："><a href="#1-MD-Judge【Li-et-al-2024f】：" class="headerlink" title="1. MD-Judge【Li et al., 2024f】："></a>1. <strong>MD-Judge</strong>【Li et al., 2024f】：</h6><ul><li>提出了专门针对安全性相关问答的评估框架，用于检测LLM在生成敏感内容时的可靠性。</li><li>创新点在于设计了多维度的安全性评估标准，包括潜在伤害性、道德风险以及语言误导性。</li><li>作者通过对比多个LLM的评估能力，验证了MD-Judge框架的鲁棒性。</li><li>此框架在评估复杂场景（如恶意问题）的生成效果方面表现突出。</li><li>MD-Judge为生成模型的安全性评估提供了一个新的基准。</li></ul><h6 id="2-Chan框架【Chan-et-al-2023】："><a href="#2-Chan框架【Chan-et-al-2023】：" class="headerlink" title="2. Chan框架【Chan et al., 2023】："></a>2. <strong>Chan框架</strong>【Chan et al., 2023】：</h6><ul><li>提出了一个多代理辩论框架，通过让多个LLM角色分别生成答案并彼此评估，提升生成任务的评估质量。</li><li>创新点在于设计了角色分工机制，不同模型在辩论中扮演不同的立场，从多角度评估候选答案。</li><li>研究表明，该框架能够显著提升评估结果的细粒度和多样性。</li><li>作者还探讨了模型间的交互如何影响评估的一致性和公平性。</li><li>Chan框架在开放式文本生成任务中的应用表明，模型之间的协作能够显著改进评估质量。</li></ul><h6 id="3-ICE【Jain-et-al-2023b】："><a href="#3-ICE【Jain-et-al-2023b】：" class="headerlink" title="3. ICE【Jain et al., 2023b】："></a>3. <strong>ICE</strong>【Jain et al., 2023b】：</h6><ul><li>提出了交互式多维评估框架，通过少量上下文示例指导LLM评估。</li><li>创新点在于将评估任务分解为多个独立维度，每个维度都有针对性的示例支持。</li><li>研究表明，ICE框架显著减少了模型在多维任务中的评估偏差。</li><li>实验结果显示，其生成的评估结果在与人工评价的一致性上达到高水平。</li><li>ICE为多维度评估任务的提示设计提供了新思路。</li></ul><hr><h4 id="5-2-对齐（Alignment）"><a href="#5-2-对齐（Alignment）" class="headerlink" title="5.2 对齐（Alignment）"></a>5.2 对齐（Alignment）</h4><h5 id="概述-10"><a href="#概述-10" class="headerlink" title="概述"></a>概述</h5><p>对齐任务的目标是通过训练或微调使LLM的生成内容更符合人类的价值观和偏好。LLM-as-a-Judge被广泛用于生成对齐数据和评估对齐效果。</p><h6 id="1-Constitutional-AI【Bai-et-al-2022】：-1"><a href="#1-Constitutional-AI【Bai-et-al-2022】：-1" class="headerlink" title="1. Constitutional AI【Bai et al., 2022】："></a>1. <strong>Constitutional AI</strong>【Bai et al., 2022】：</h6><ul><li>提出了基于原则对齐的框架，通过定义帮助性、无害性和诚实性等原则，优化生成模型的输出。</li><li>创新点在于将原则融入奖励建模过程，利用LLM生成的偏好信号构建对齐数据集。</li><li>作者通过多轮实验验证了这种基于规则的对齐方法对生成质量的显著提升。</li><li>此框架适用于各种生成任务，尤其在减少有害输出方面效果显著。</li><li>Constitutional AI的成功展示了基于规则的对齐方法的潜力。</li></ul><h6 id="2-DIRECT-RLAIF【Lee-et-al-2023】："><a href="#2-DIRECT-RLAIF【Lee-et-al-2023】：" class="headerlink" title="2. DIRECT-RLAIF【Lee et al., 2023】："></a>2. <strong>DIRECT-RLAIF</strong>【Lee et al., 2023】：</h6><ul><li>提出了一种直接强化学习对齐反馈（DIRECT-RLAIF）方法，通过较大的LLM生成偏好信号指导较小模型。</li><li>核心创新点在于利用较强的LLM模型作为动态评估者，避免传统奖励模型中存在的“奖励陈旧性”问题。</li><li>作者验证了这种方法在对齐生成任务中的有效性，特别是在开放式对话中的显著改进。</li><li>DIRECT-RLAIF为更高效的对齐方法提供了理论基础。</li><li>研究结果表明，这种方法可以在较少人工干预的情况下生成符合人类偏好的内容。</li></ul><h6 id="3-OAIF【Guo-et-al-2024】："><a href="#3-OAIF【Guo-et-al-2024】：" class="headerlink" title="3. OAIF【Guo et al., 2024】："></a>3. <strong>OAIF</strong>【Guo et al., 2024】：</h6><ul><li>提出了在线AI反馈（Online AI Feedback, OAIF）框架，通过实时原则指导提升模型评估的灵活性。</li><li>核心创新点在于动态调整评估规则，使模型能够适应多变的任务需求。</li><li>OAIF引入了细粒度的多维评分策略，为每个候选项生成独立的评估报告。</li><li>作者验证了这种方法在实时决策中的潜力，尤其在对话和生成任务中表现突出。</li><li>OAIF展现了规则增强的实时适应能力，为实时评估任务提供了新方向。</li></ul><hr><h4 id="5-3-检索（Retrieval）"><a href="#5-3-检索（Retrieval）" class="headerlink" title="5.3 检索（Retrieval）"></a>5.3 检索（Retrieval）</h4><h5 id="概述-11"><a href="#概述-11" class="headerlink" title="概述"></a>概述</h5><p>在检索场景中，LLM-as-a-Judge主要用于提升文档排序的精度和检索增强生成（RAG）的效果。通过更高效的排序算法，LLM能够在传统检索和复杂生成任务中提供更高质量的相关性评估。</p><h6 id="1-Ranked-Pairing【Zhai-et-al-2024】：-1"><a href="#1-Ranked-Pairing【Zhai-et-al-2024】：-1" class="headerlink" title="1. Ranked Pairing【Zhai et al., 2024】："></a>1. <strong>Ranked Pairing</strong>【Zhai et al., 2024】：</h6><ul><li>提出了一种基于基线比较的排序方法，通过对所有候选项与基线进行比较确定优劣。</li><li>创新点在于避免传统两两比较的高计算开销，显著提高了评估效率。</li><li>作者还设计了一种自适应比较策略，进一步优化排序性能。</li><li>研究表明，Ranked Pairing在大规模排序任务中表现出极高的效率。</li><li>此方法特别适用于需要快速生成排序结果的场景。</li></ul><h6 id="2-LLM-Eval【Lin-and-Chen-2023a】："><a href="#2-LLM-Eval【Lin-and-Chen-2023a】：" class="headerlink" title="2. LLM-Eval【Lin and Chen, 2023a】："></a>2. <strong>LLM-Eval</strong>【Lin and Chen, 2023a】：</h6><ul><li>提出了在对话生成中的相关性评估框架，利用LLM替代人工标注。</li><li>创新点在于设计了结合上下文和生成内容的提示技术，确保评估更加精确。</li><li>作者通过对比实验验证了LLM在会话相关性评估中的潜力，结果与人工标注高度一致。</li><li>此框架显著减少了评估成本，同时提升了效率。</li><li>LLM-Eval在对话生成任务中的应用表明，模型在生成评估中的角色日益重要。</li></ul><h6 id="3-ToT-Tree-of-Thought-【Yao-et-al-2023a】："><a href="#3-ToT-Tree-of-Thought-【Yao-et-al-2023a】：" class="headerlink" title="3. ToT (Tree of Thought)【Yao et al., 2023a】："></a>3. <strong>ToT (Tree of Thought)</strong>【Yao et al., 2023a】：</h6><ul><li>提出了通过树状结构增强推理能力的方法，并结合LLM进行评估。</li><li>创新点在于引入了状态评估模块，通过逐步筛选最优推理路径提升检索和生成任务的精度。</li><li>研究表明，ToT框架显著提升了复杂任务的解决能力，尤其在多步推理和决策中表现优异。</li><li>作者还提出了评估路径的动态调整机制，使LLM能够更灵活地应对多样化任务。</li><li>ToT验证了结构化评估框架在复杂任务中的有效性。</li></ul><hr><h4 id="5-4-推理（Reasoning）"><a href="#5-4-推理（Reasoning）" class="headerlink" title="5.4 推理（Reasoning）"></a>5.4 推理（Reasoning）</h4><h5 id="概述-12"><a href="#概述-12" class="headerlink" title="概述"></a>概述</h5><p>推理任务的核心是评估LLM的中间推理过程和最终答案的正确性。LLM-as-a-Judge在数学推理、时间推理和复杂逻辑推理任务中展示了显著的评估能力。</p><h6 id="1-HALU-J【Wang-et-al-2024a】："><a href="#1-HALU-J【Wang-et-al-2024a】：" class="headerlink" title="1. HALU-J【Wang et al., 2024a】："></a>1. <strong>HALU-J</strong>【Wang et al., 2024a】：</h6><ul><li>提出了一种基于批评的偏好学习方法，专注于选择相关证据并生成详细批评。</li><li>创新点在于设计了多证据选择机制，提高了LLM的可靠性评估能力。</li><li>该方法通过Directed Preference Optimization（DPO）进行优化，使模型能够更准确地判断任务间的优劣。</li><li>HALU-J还结合了上下文推理，扩展了偏好学习的应用场景。</li><li>实验表明，HALU-J显著提升了复杂任务的评估准确性，尤其是在事实性和逻辑性判断上。</li></ul><h6 id="2-KIEval【Yu-et-al-2024】："><a href="#2-KIEval【Yu-et-al-2024】：" class="headerlink" title="2. KIEval【Yu et al., 2024】："></a>2. <strong>KIEval</strong>【Yu et al., 2024】：</h6><ul><li>提出了知识交互式评估框架，通过动态问答生成丰富的上下文信息。</li><li>创新点在于引入了“交互者”角色，模拟用户和模型之间的动态交互。</li><li>作者设计了一种鲁棒性检测机制，避免因上下文污染导致的错误评估。</li><li>研究表明，KIEval在复杂任务中的表现优于传统静态评估方法。</li><li>此框架适用于多维度评估，特别是在需要动态调整上下文的场景中。</li></ul><hr><h3 id="6-评估基准"><a href="#6-评估基准" class="headerlink" title="6. 评估基准"></a>6. 评估基准</h3><h4 id="概述-13"><a href="#概述-13" class="headerlink" title="概述"></a>概述</h4><p>评估基准是验证LLM-as-a-Judge能力的重要工具。本节整理并介绍当前用于不同评估维度的基准，包括有用性、无害性、可靠性等方面的具体框架和其核心思想。这些基准覆盖了从对话生成到复杂任务推理的广泛应用场景，为后续研究提供了关键数据支持。</p><hr><h5 id="6-1-综合评估基准"><a href="#6-1-综合评估基准" class="headerlink" title="6.1 综合评估基准"></a>6.1 综合评估基准</h5><h6 id="1-SORRY-Bench【Xie-et-al-2024a】："><a href="#1-SORRY-Bench【Xie-et-al-2024a】：" class="headerlink" title="1. SORRY-Bench【Xie et al., 2024a】："></a>1. <strong>SORRY-Bench</strong>【Xie et al., 2024a】：</h6><ul><li>设计了一个专注于安全性和无害性评估的综合基准，重点测试LLM对潜在有害内容的拒绝能力。</li><li>创新点在于提供了一个多模型对比框架，包括开源和专有LLM的表现分析。</li><li>基准数据集涵盖多种潜在危险场景，如政治敏感内容和虚假信息生成。</li><li>作者还引入了动态拒绝率作为衡量指标，展示了不同模型在拒绝任务中的细粒度表现。</li><li>实验表明，小型LLM经过微调后可以在安全性评估中达到与大型模型相当的水平。</li></ul><h6 id="2-HalluJudge【Luo-et-al-2024】："><a href="#2-HalluJudge【Luo-et-al-2024】：" class="headerlink" title="2. HalluJudge【Luo et al., 2024】："></a>2. <strong>HalluJudge</strong>【Luo et al., 2024】：</h6><ul><li>提出了一个专门用于对话级事实性评估的基准，涵盖大规模对话数据集。</li><li>核心创新在于设计了一种细粒度的事实性评分机制，通过引入上下文验证生成内容的准确性。</li><li>数据集中包括多种类型的事实性错误，如数据遗漏、模糊表述和直接虚假信息。</li><li>HalluJudge还整合了自动化和人工评估方法，提高了基准的覆盖面和可靠性。</li><li>实验结果表明，HalluJudge能够显著提高LLM在对话场景中的事实性检测能力。</li></ul><hr><h5 id="6-2-专用领域评估基准"><a href="#6-2-专用领域评估基准" class="headerlink" title="6.2 专用领域评估基准"></a>6.2 专用领域评估基准</h5><h6 id="1-FaithScore【Jing-et-al-2024】："><a href="#1-FaithScore【Jing-et-al-2024】：" class="headerlink" title="1. FaithScore【Jing et al., 2024】："></a>1. <strong>FaithScore</strong>【Jing et al., 2024】：</h6><ul><li>FaithScore是第一个跨模态的可靠性评估框架，适用于文本和图像生成任务。</li><li>创新点在于设计了多模态评估方法，结合语言和视觉信号来验证生成内容的真实性。</li><li>数据集覆盖了从事实描述到跨模态推理的多个任务，测试了模型的全局一致性和细节准确性。</li><li>FaithScore还引入了多阶段评分机制，逐步分解任务以提高评估的精细化程度。</li><li>实验显示，FaithScore在多模态生成任务中的评估结果与人工评分高度一致。</li></ul><h6 id="2-GEMBA【Kocmi-and-Federmann-2023】："><a href="#2-GEMBA【Kocmi-and-Federmann-2023】：" class="headerlink" title="2. GEMBA【Kocmi and Federmann, 2023】："></a>2. <strong>GEMBA</strong>【Kocmi and Federmann, 2023】：</h6><ul><li>GEMBA基准专注于机器翻译和文本摘要任务的整体质量评估。</li><li>核心创新点在于结合BLEU等传统指标和LLM生成的综合评分，提供更全面的评估结果。</li><li>数据集中包含多种语言和领域的真实文本，覆盖多样化的任务需求。</li><li>作者设计了一种动态反馈机制，允许LLM在评估过程中进行自适应调整。</li><li>GEMBA基准的引入显著推动了机器翻译和摘要任务中LLM-as-a-Judge的应用。</li></ul><h6 id="3-Just-Eval【Lin-et-al-2023】："><a href="#3-Just-Eval【Lin-et-al-2023】：" class="headerlink" title="3. Just-Eval【Lin et al., 2023】："></a>3. <strong>Just-Eval</strong>【Lin et al., 2023】：</h6><ul><li>提出了一个基于生成内容有用性和无害性的综合基准，适用于广泛的开放式任务。</li><li>创新点在于为不同任务设计了定制化的评估标准，并结合多维评分系统生成最终评价。</li><li>数据集中涵盖了对话、问答和复杂推理等任务，验证了基准的通用性。</li><li>作者还分析了模型在不同任务和领域上的表现，提供了详细的对比结果。</li><li>Just-Eval的应用表明，评估框架需要结合任务特点进行优化，才能最大化评估的准确性。</li></ul><hr><h4 id="6-3-动态评估基准"><a href="#6-3-动态评估基准" class="headerlink" title="6.3 动态评估基准"></a>6.3 动态评估基准</h4><h6 id="1-RevisEval【Zhang-et-al-2024e】："><a href="#1-RevisEval【Zhang-et-al-2024e】：" class="headerlink" title="1. RevisEval【Zhang et al., 2024e】："></a>1. <strong>RevisEval</strong>【Zhang et al., 2024e】：</h6><ul><li>RevisEval通过引入动态自我修正机制，让LLM在生成评估之前对输出进行多次调整。</li><li>核心创新在于结合LLM的自我纠错能力，将最终输出用于多维度评估。</li><li>数据集中覆盖了对话生成、摘要和复杂推理任务，验证了基准的动态适应能力。</li><li>RevisEval引入了多轮反馈机制，允许模型在评估过程中迭代改进。</li><li>实验结果表明，动态评估能够显著提升复杂任务中评估的精确性和稳定性。</li></ul><h6 id="2-Meta-ranking【Liu-et-al-2024c】："><a href="#2-Meta-ranking【Liu-et-al-2024c】：" class="headerlink" title="2. Meta-ranking【Liu et al., 2024c】："></a>2. <strong>Meta-ranking</strong>【Liu et al., 2024c】：</h6><ul><li>Meta-ranking框架通过弱模型生成初步排序，再由强模型进行最终评估。</li><li>创新点在于使用多阶段的排名方法，提高评估效率并降低计算开销。</li><li>数据集中包含了多种任务类型，并通过实验验证了Meta-ranking的通用性。</li><li>该框架特别适用于大规模排序任务，显著减少了评估时间。</li><li>Meta-ranking展示了弱模型和强模型协作评估的潜力，是多模型评估的新方向。</li></ul><hr><h3 id="7-挑战与未来方向"><a href="#7-挑战与未来方向" class="headerlink" title="7. 挑战与未来方向"></a>7. 挑战与未来方向</h3><h4 id="概述-14"><a href="#概述-14" class="headerlink" title="概述"></a>概述</h4><p>尽管LLM-as-a-Judge在评估任务中展现了强大能力，但依然面临着多方面的挑战。主要问题包括评估偏差与脆弱性、动态与复杂任务中的适应性，以及人机协同评估的潜力。本节探讨这些挑战并提出未来的研究方向。</p><hr><h5 id="7-1-偏差与脆弱性"><a href="#7-1-偏差与脆弱性" class="headerlink" title="7.1 偏差与脆弱性"></a>7.1 偏差与脆弱性</h5><h6 id="1-OffsetBias【Park-et-al-2024】："><a href="#1-OffsetBias【Park-et-al-2024】：" class="headerlink" title="1. OffsetBias【Park et al., 2024】："></a>1. <strong>OffsetBias</strong>【Park et al., 2024】：</h6><ul><li>OffsetBias通过设计一个去偏优化框架，减少LLM在评估任务中的位置偏差和内容偏见。</li><li>创新点在于使用合成数据生成“坏”样本，通过训练模型识别并修正偏差。</li><li>作者提出了一种多维度的去偏学习机制，确保评估在不同场景下的一致性。</li><li>研究表明，OffsetBias能够显著降低模型在生成任务中的不公平表现。</li><li>此方法为减少LLM评估中的偏差问题提供了重要方向。</li></ul><h6 id="2-SORRY-Bench【Xie-et-al-2024a】："><a href="#2-SORRY-Bench【Xie-et-al-2024a】：" class="headerlink" title="2. SORRY-Bench【Xie et al., 2024a】："></a>2. <strong>SORRY-Bench</strong>【Xie et al., 2024a】：</h6><ul><li>进一步研究了模型在拒绝有害内容时可能出现的误拒绝问题。</li><li>创新点在于结合动态评分机制和拒绝数据集，分析模型在多种任务中的拒绝倾向。</li><li>作者指出，小型模型在特定场景中可能比大型模型更高效。</li><li>实验结果表明，SORRY-Bench能够帮助识别并减轻评估偏差。</li><li>此基准成为探讨评估脆弱性的一个重要工具。</li></ul><hr><h5 id="7-2-动态与复杂评估"><a href="#7-2-动态与复杂评估" class="headerlink" title="7.2 动态与复杂评估"></a>7.2 动态与复杂评估</h5><h6 id="1-Tree-of-Thought-ToT-【Yao-et-al-2023a】："><a href="#1-Tree-of-Thought-ToT-【Yao-et-al-2023a】：" class="headerlink" title="1. Tree of Thought (ToT)【Yao et al., 2023a】："></a>1. <strong>Tree of Thought (ToT)</strong>【Yao et al., 2023a】：</h6><ul><li>ToT通过树状结构优化复杂任务的多步推理和评估。</li><li>创新点在于结合动态状态评估机制，使评估更加适应复杂多变的任务需求。</li><li>数据集中覆盖了需要多步推理的复杂任务，如问答和决策优化。</li><li>实验表明，ToT框架显著提升了复杂任务的解决能力和评估准确性。</li><li>该研究为动态评估提供了新的理论和实践支持。</li></ul><h6 id="2-RAIN【Li-et-al-2024】："><a href="#2-RAIN【Li-et-al-2024】：" class="headerlink" title="2. RAIN【Li et al., 2024】："></a>2. <strong>RAIN</strong>【Li et al., 2024】：</h6><ul><li>RAIN提出了可回溯的自回归推理机制，让LLM能够在评估过程中动态修正错误。</li><li>创新点在于结合自我评估和多轮推理机制，确保最终输出的高质量。</li><li>作者还设计了一种动态调整机制，使模型能够适应不同任务的变化。</li><li>实验显示，RAIN在复杂任务中的评估能力优于传统静态方法。</li><li>此框架展示了动态评估在复杂场景中的潜力。</li></ul><hr><h5 id="7-3-自我评估与人机协同"><a href="#7-3-自我评估与人机协同" class="headerlink" title="7.3 自我评估与人机协同"></a>7.3 自我评估与人机协同</h5><h6 id="1-Self-Taught-Evaluators【Wang-et-al-2024f】："><a href="#1-Self-Taught-Evaluators【Wang-et-al-2024f】：" class="headerlink" title="1. Self-Taught Evaluators【Wang et al., 2024f】："></a>1. <strong>Self-Taught Evaluators</strong>【Wang et al., 2024f】：</h6><ul><li>提出了一种自我学习框架，模型通过生成低质量数据对自身进行动态优化。</li><li>创新点在于引入了一种动态评估机制，让模型能够逐步提升自身评估能力。</li><li>数据集中包括了多种类型的任务，为自我评估提供了广泛支持。</li><li>Self-Taught Evaluators展示了模型在无需人工干预情况下的自我提升能力。</li><li>此框架为自动化评估任务提供了新思路。</li></ul><h6 id="2-Meta-Rewarding【Wu-et-al-2024】："><a href="#2-Meta-Rewarding【Wu-et-al-2024】：" class="headerlink" title="2. Meta-Rewarding【Wu et al., 2024】："></a>2. <strong>Meta-Rewarding</strong>【Wu et al., 2024】：</h6><ul><li>Meta-Rewarding通过将LLM的自评估信号作为偏好数据，用于进一步优化模型。</li><li>创新点在于结合策略模型自我反馈，增强模型的自适应能力。</li><li>作者还探讨了如何动态调整评估策略以提高鲁棒性。</li><li>实验表明，Meta-Rewarding能够显著提升复杂任务中的评估效果。</li><li>该研究展示了人机协同评估的潜在优势。</li></ul><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;hr&gt;
&lt;h1 id=&quot;基于生成的大语言模型（LLM）评估：从生成到判断的机遇与挑战&quot;&gt;&lt;a href=&quot;#基于生成的大语言模型（LLM）评估：从生成到判断的机遇与挑战&quot; class=&quot;headerlink&quot; title=&quot;基于生成的大语言模型（LLM）评估：从生成到判断的机遇</summary>
      
    
    
    
    <category term="NLP Insights" scheme="https://chenhuiyu.github.io/categories/NLP-Insights/"/>
    
    
    <category term="Onnx" scheme="https://chenhuiyu.github.io/tags/Onnx/"/>
    
    <category term="Deployment" scheme="https://chenhuiyu.github.io/tags/Deployment/"/>
    
  </entry>
  
  <entry>
    <title>Reflections on Identity and Subjectivity</title>
    <link href="https://chenhuiyu.github.io/2024/12/03/Life%20Reflections/Reflections%20on%20Identity%20and%20Subjectivity/"/>
    <id>https://chenhuiyu.github.io/2024/12/03/Life%20Reflections/Reflections%20on%20Identity%20and%20Subjectivity/</id>
    <published>2024-12-03T06:11:06.000Z</published>
    <updated>2024-12-03T06:12:31.833Z</updated>
    
    <content type="html"><![CDATA[<h1 id="PR-Application-Rejected-Reflections-on-Identity-and-Subjectivity"><a href="#PR-Application-Rejected-Reflections-on-Identity-and-Subjectivity" class="headerlink" title="PR Application Rejected: Reflections on Identity and Subjectivity"></a>PR Application Rejected: Reflections on Identity and Subjectivity</h1><p>When I received the news of my PR application being rejected, after a brief moment of shock, what arose within me was not merely frustration but a peculiar sense of “existential dilemma.” On the surface, it seemed like just an administrative outcome, yet it profoundly mirrored the multiple tensions between the structure of contemporary global mobility and the construction of subjectivity.</p><ul><li>Amid the tension between globalization and national sovereignty, is it even possible to affirm an individual’s identity?</li><li>Does the rejection of a PR application symbolically exclude an individual from a collective sense of belonging?</li></ul><hr><h2 id="PR-Application-From-the-Fantasy-of-Rights-to-the-Maze-of-Identity"><a href="#PR-Application-From-the-Fantasy-of-Rights-to-the-Maze-of-Identity" class="headerlink" title="PR Application: From the Fantasy of Rights to the Maze of Identity"></a>PR Application: From the Fantasy of Rights to the Maze of Identity</h2><p>Within the theoretical framework of Anthony Giddens’ <em>Modernity and Self-Identity</em>, applying for PR is not merely a pursuit of residency rights but a symbolic quest for identity stability and future possibilities. However, in the context of globalization, this pursuit often falls into what Derrida describes as the structure of <em>différance</em>: the realization of rights is perpetually deferred, and the confirmation of identity remains suspended.</p><p>In this context, rejection is tantamount to a form of <strong>symbolic violence</strong>. It not only disrupts my plans for the future but also shatters the illusion of subjectivity I held within this domain.</p><hr><h2 id="Subjectivity-vs-Institutional-Discipline"><a href="#Subjectivity-vs-Institutional-Discipline" class="headerlink" title="Subjectivity vs. Institutional Discipline"></a>Subjectivity vs. Institutional Discipline</h2><p>Bourdieu’s field theory reveals the distribution of power in social practices, and the practice of PR applications is a concrete field where power disciplines individuals. Rejection is not merely an administrative outcome but an invisible disciplining of the subject, hinting at the imbalance of power between individuals and institutions in the era of platform capitalism.</p><p>Through Foucault’s lens of discipline, this process not only constrains individuals’ <strong>physical mobility</strong> but also profoundly affects the <strong>emotional and mental freedom</strong> of individuals.</p><hr><h2 id="From-Loss-to-Reflection"><a href="#From-Loss-to-Reflection" class="headerlink" title="From Loss to Reflection"></a>From Loss to Reflection</h2><p>In a sense, rejection is not an end but an opportunity for <strong>reconstruction</strong>. Bauman’s concept of <em>liquid modernity</em> might help me interpret this failure: in a constantly fluid world, fixed identities and stable senses of belonging are scarce resources. Perhaps I need to redefine my position amid this loss and find my own meaning in the fragments of grand narratives.</p><p>As Žižek puts it: <strong>“True freedom is not about getting what you want but about confronting the trauma of reality.”</strong> The failure of my PR application may not be the end of identity but a challenge to how I reconstruct subjectivity in the face of uncertainty.</p><p>Thus, this is not an ending but a dialectical transformation: in the moment of shattered stability, perhaps lies the beginning of transcending grand narratives and rediscovering the meaning of one’s existence.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;PR-Application-Rejected-Reflections-on-Identity-and-Subjectivity&quot;&gt;&lt;a href=&quot;#PR-Application-Rejected-Reflections-on-Identity-and-Subj</summary>
      
    
    
    
    <category term="Life Reflections" scheme="https://chenhuiyu.github.io/categories/Life-Reflections/"/>
    
    
    <category term="Living in Singapore" scheme="https://chenhuiyu.github.io/tags/Living-in-Singapore/"/>
    
  </entry>
  
  <entry>
    <title>身份与主体性的反思</title>
    <link href="https://chenhuiyu.github.io/2024/12/03/Life%20Reflections/%E8%BA%AB%E4%BB%BD%E4%B8%8E%E4%B8%BB%E4%BD%93%E6%80%A7%E7%9A%84%E5%8F%8D%E6%80%9D/"/>
    <id>https://chenhuiyu.github.io/2024/12/03/Life%20Reflections/%E8%BA%AB%E4%BB%BD%E4%B8%8E%E4%B8%BB%E4%BD%93%E6%80%A7%E7%9A%84%E5%8F%8D%E6%80%9D/</id>
    <published>2024-12-03T06:11:06.000Z</published>
    <updated>2024-12-03T06:11:18.797Z</updated>
    
    <content type="html"><![CDATA[<h1 id="永居申请被拒：身份与主体性的反思"><a href="#永居申请被拒：身份与主体性的反思" class="headerlink" title="永居申请被拒：身份与主体性的反思"></a>永居申请被拒：身份与主体性的反思</h1><p>当我接到永居申请被拒的消息时，短暂的愣神之后，内心涌动的却并非单纯的挫败，而是一种奇异的“生存论困境”感。表面上，这似乎只是一次行政结果的体现，但其背后却深刻折射了当代全球流动性结构与主体性建构之间的多重张力。</p><ul><li>在全球化与国家主权的张力下，个体身份的确认究竟是否可能？</li><li>当永居申请被拒时，是否意味着个体被象征性地排除在某种集体意义之外？</li></ul><hr><h2 id="永居申请：从权利幻想到身份迷宫"><a href="#永居申请：从权利幻想到身份迷宫" class="headerlink" title="永居申请：从权利幻想到身份迷宫"></a>永居申请：从权利幻想到身份迷宫</h2><p>在吉登斯的“现代性与自我认同”理论框架下，永居申请不仅是一种居留权的争取，更是一种对身份稳定性与未来可能性的符号化追求。然而，在全球化语境下，这种追求往往陷入德里达所描述的“延异”结构：权利的实现总是被推迟，身份的确认总是悬置。</p><p>在此情境中，申请被拒的结果无异于一种<strong>符号暴力</strong>。它不仅断裂了我对未来的规划，也撕裂了我在这一场域中的主体性幻象。</p><hr><h2 id="主体性与制度规训的对抗"><a href="#主体性与制度规训的对抗" class="headerlink" title="主体性与制度规训的对抗"></a>主体性与制度规训的对抗</h2><p>布尔迪厄的场域理论揭示了权力在社会实践中的分布方式，而永居申请这一制度实践正是权力规训个体的具体化场域。拒绝不仅是一种行政结果，更是一种对主体的隐形规训，暗示了平台资本主义时代个体与制度之间的权力失衡。</p><p>福柯的规训视角让我们看到，这一过程不仅限制了个体的<strong>物理流动性</strong>，也深刻影响了<strong>情感与精神的自由流动</strong>。</p><hr><h2 id="从失落到反思"><a href="#从失落到反思" class="headerlink" title="从失落到反思"></a>从失落到反思</h2><p>从某种意义上说，被拒并非一种终结，而是一种<strong>重构的契机</strong>。鲍曼提出的“液态现代性”或许能帮助我理解这次失败：在一个不断流动的世界中，固定的身份和稳定的归属感本就是稀缺资源。或许，我需要在失落中重新定义自己的位置，从宏大叙事的破碎中找到属于自己的意义。</p><p>正如齐泽克所言：<strong>“真正的自由不是得到你想要的，而是面对现实的创伤。”</strong> 永居申请的失败也许不是身份的终结，而是对我如何在不确定性中重新构建主体性的终极挑战。</p><p>因此，这并非终结，而是一次辩证的转化：在稳定性破碎的瞬间，或许恰是我们超越宏大叙事、重新发现自我存在意义的开端。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;永居申请被拒：身份与主体性的反思&quot;&gt;&lt;a href=&quot;#永居申请被拒：身份与主体性的反思&quot; class=&quot;headerlink&quot; title=&quot;永居申请被拒：身份与主体性的反思&quot;&gt;&lt;/a&gt;永居申请被拒：身份与主体性的反思&lt;/h1&gt;&lt;p&gt;当我接到永居申请被拒的消息时</summary>
      
    
    
    
    <category term="Life Reflections" scheme="https://chenhuiyu.github.io/categories/Life-Reflections/"/>
    
    
    <category term="坡岛生活指北" scheme="https://chenhuiyu.github.io/tags/%E5%9D%A1%E5%B2%9B%E7%94%9F%E6%B4%BB%E6%8C%87%E5%8C%97/"/>
    
  </entry>
  
  <entry>
    <title>【Leetcode Python题解】「1346. Check If N and Its Double Exist」</title>
    <link href="https://chenhuiyu.github.io/2024/12/02/Code%20Chronicles/Leetcode%20Python%E9%A2%98%E8%A7%A3%E3%80%91%E3%80%8C1346.%20Check%20If%20N%20and%20Its%20Double%20Exist%E3%80%8D/"/>
    <id>https://chenhuiyu.github.io/2024/12/02/Code%20Chronicles/Leetcode%20Python%E9%A2%98%E8%A7%A3%E3%80%91%E3%80%8C1346.%20Check%20If%20N%20and%20Its%20Double%20Exist%E3%80%8D/</id>
    <published>2024-12-01T16:00:00.000Z</published>
    <updated>2024-12-02T02:29:45.500Z</updated>
    
    <content type="html"><![CDATA[<h1 id="【Leetcode-Python题解】「1346-Check-If-N-and-Its-Double-Exist」"><a href="#【Leetcode-Python题解】「1346-Check-If-N-and-Its-Double-Exist」" class="headerlink" title="【Leetcode Python题解】「1346. Check If N and Its Double Exist」"></a>【Leetcode Python题解】「1346. Check If N and Its Double Exist」</h1><h2 id="题目：1346-Check-If-N-and-Its-Double-Exist"><a href="#题目：1346-Check-If-N-and-Its-Double-Exist" class="headerlink" title="题目：1346. Check If N and Its Double Exist"></a>题目：<a href="https://leetcode.com/problems/check-if-n-and-its-double-exist/description/?envType=daily-question&amp;envId=2024-12-01">1346. Check If N and Its Double Exist</a></h2><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><p>给定一个整数数组 <code>arr</code>，检查是否存在两个不同的索引 <code>i</code> 和 <code>j</code>，满足：</p><ul><li><code>i != j</code></li><li><code>0 &lt;= i, j &lt; arr.length</code></li><li><code>arr[i] == 2 * arr[j]</code></li></ul><h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><p><strong>示例 1:</strong><br></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">输入：arr = [10,2,5,3]</span><br><span class="line">输出：true</span><br><span class="line">解释：对于 i = 0 和 j = 2，arr[i] = 10 等于 2 * 5 = 2 * arr[j]</span><br></pre></td></tr></tbody></table></figure><p></p><p><strong>示例 2:</strong><br></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">输入：arr = [3,1,7,11]</span><br><span class="line">输出：false</span><br><span class="line">解释：不存在满足条件的 i 和 j。</span><br></pre></td></tr></tbody></table></figure><p></p><h3 id="约束条件"><a href="#约束条件" class="headerlink" title="约束条件"></a>约束条件</h3><ul><li><code>2 &lt;= arr.length &lt;= 500</code></li><li><code>-10³ &lt;= arr[i] &lt;= 10³</code></li></ul><h2 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h2><p>这道题可以用多种方法解决，我们来分析两种主要的解法：暴力解法和哈希表解法。</p><h3 id="1-暴力解法"><a href="#1-暴力解法" class="headerlink" title="1. 暴力解法"></a>1. 暴力解法</h3><p>最直观的解法是使用两层循环，遍历所有可能的数对。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">checkIfExist</span>(<span class="params">arr</span>):</span><br><span class="line">    n = <span class="built_in">len</span>(arr)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            <span class="keyword">if</span> i != j <span class="keyword">and</span> arr[i] == <span class="number">2</span> * arr[j]:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></tbody></table></figure><p><strong>复杂度分析：</strong></p><ul><li>时间复杂度：O(n²)，其中 n 是数组长度</li><li>空间复杂度：O(1)，只使用了常数额外空间</li></ul><h3 id="2-哈希表解法"><a href="#2-哈希表解法" class="headerlink" title="2. 哈希表解法"></a>2. 哈希表解法</h3><p>使用哈希表可以显著优化时间复杂度。我们只需要一次遍历数组，同时用哈希表记录已经遇到的数字。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">checkIfExist</span>(<span class="params">arr</span>):</span><br><span class="line">    seen = <span class="built_in">set</span>()</span><br><span class="line">    <span class="keyword">for</span> num <span class="keyword">in</span> arr:</span><br><span class="line">        <span class="keyword">if</span> num * <span class="number">2</span> <span class="keyword">in</span> seen <span class="keyword">or</span> (num % <span class="number">2</span> == <span class="number">0</span> <span class="keyword">and</span> num // <span class="number">2</span> <span class="keyword">in</span> seen):</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        seen.add(num)</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></tbody></table></figure><p><strong>复杂度分析：</strong></p><ul><li>时间复杂度：O(n)，只需要遍历一次数组</li><li>空间复杂度：O(n)，需要额外的哈希表空间</li></ul><h3 id="代码优化案例"><a href="#代码优化案例" class="headerlink" title="代码优化案例"></a>代码优化案例</h3><p>让我们看一个初始版本的代码，以及如何优化它：</p><p><strong>原始版本：</strong><br></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">checkIfExist</span>(<span class="params">self, arr: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        hashmap = {}</span><br><span class="line">        <span class="keyword">for</span> i, item <span class="keyword">in</span> <span class="built_in">enumerate</span>(arr):</span><br><span class="line">            hashmap[i] = item</span><br><span class="line">            <span class="keyword">if</span> item * <span class="number">2</span> <span class="keyword">in</span> hashmap.values():</span><br><span class="line">                j = <span class="built_in">next</span>(k <span class="keyword">for</span> k, v <span class="keyword">in</span> hashmap.items() <span class="keyword">if</span> v == item * <span class="number">2</span>)</span><br><span class="line">                <span class="keyword">if</span> i != j:</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">            <span class="keyword">if</span> item//<span class="number">2</span> <span class="keyword">in</span> hashmap.values() <span class="keyword">and</span> item%<span class="number">2</span>==<span class="number">0</span>:</span><br><span class="line">                j = <span class="built_in">next</span>(k <span class="keyword">for</span> k, v <span class="keyword">in</span> hashmap.items() <span class="keyword">if</span> v == item//<span class="number">2</span>)</span><br><span class="line">                <span class="keyword">if</span> i != j:</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></tbody></table></figure><p></p><p><strong>优化版本：</strong><br></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">checkIfExist</span>(<span class="params">self, arr: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        seen = <span class="built_in">set</span>()</span><br><span class="line">        <span class="keyword">for</span> num <span class="keyword">in</span> arr:</span><br><span class="line">            <span class="keyword">if</span> num * <span class="number">2</span> <span class="keyword">in</span> seen <span class="keyword">or</span> (num % <span class="number">2</span> == <span class="number">0</span> <span class="keyword">and</span> num // <span class="number">2</span> <span class="keyword">in</span> seen):</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">            seen.add(num)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></tbody></table></figure><p></p><h3 id="优化要点"><a href="#优化要点" class="headerlink" title="优化要点"></a>优化要点</h3><ol><li><p><strong>数据结构选择</strong></p><ul><li>使用集合(set)替代字典(dict)</li><li>不需要存储索引信息，只关注值的存在性</li></ul></li><li><p><strong>代码简化</strong></p><ul><li>合并重复的检查逻辑</li><li>移除不必要的变量和计算</li><li>使用更简洁的条件判断</li></ul></li><li><p><strong>性能提升</strong></p><ul><li>避免使用 <code>hashmap.values()</code> 遍历</li><li>使用集合的 O(1) 查找特性</li><li>减少重复计算</li></ul></li></ol><h2 id="关键注意点"><a href="#关键注意点" class="headerlink" title="关键注意点"></a>关键注意点</h2><ol><li><p><strong>边界情况处理</strong></p><ul><li>考虑数组中有 0 的情况（0 的两倍仍然是 0）</li><li>注意负数的处理</li><li>确保不使用同一个索引（i != j）</li></ul></li><li><p><strong>数值检查</strong></p><ul><li>需要同时检查一个数的两倍和一半</li><li>检查一半时要确保数字是偶数</li></ul></li><li><p><strong>性能优化</strong></p><ul><li>使用恰当的数据结构（集合）</li><li>避免不必要的计算和遍历</li></ul></li></ol><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这道题展示了如何通过选择适当的数据结构和优化代码逻辑来提升算法的性能。从初始的暴力解法到使用哈希表，再到代码的优化，每一步都带来了显著的改进。最终的解决方案不仅运行效率高，而且代码简洁易懂。</p><p>关键是要理解：</p><ol><li>暴力解法虽然直观，但效率低下</li><li>哈希表提供了最优的时空权衡</li><li>代码优化不仅是为了效率，也是为了可读性和可维护性</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;【Leetcode-Python题解】「1346-Check-If-N-and-Its-Double-Exist」&quot;&gt;&lt;a href=&quot;#【Leetcode-Python题解】「1346-Check-If-N-and-Its-Double-Exist」&quot; clas</summary>
      
    
    
    
    <category term="Code Chronicles" scheme="https://chenhuiyu.github.io/categories/Code-Chronicles/"/>
    
    
    <category term="Python" scheme="https://chenhuiyu.github.io/tags/Python/"/>
    
    <category term="Leetcode" scheme="https://chenhuiyu.github.io/tags/Leetcode/"/>
    
    <category term="每日一题" scheme="https://chenhuiyu.github.io/tags/%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>Leetcode Python Solution - 2097. Valid Arrangement of Pairs</title>
    <link href="https://chenhuiyu.github.io/2024/12/01/Code%20Chronicles/[Leetcode%20Python%20Solution]%202097.%20Valid%20Arrangement%20of%20Pairs/"/>
    <id>https://chenhuiyu.github.io/2024/12/01/Code%20Chronicles/[Leetcode%20Python%20Solution]%202097.%20Valid%20Arrangement%20of%20Pairs/</id>
    <published>2024-11-30T19:00:00.000Z</published>
    <updated>2024-12-01T16:01:10.762Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Leetcode-Python-Solution-2097-Valid-Arrangement-of-Pairs"><a href="#Leetcode-Python-Solution-2097-Valid-Arrangement-of-Pairs" class="headerlink" title="[Leetcode Python Solution] 2097. Valid Arrangement of Pairs"></a>[Leetcode Python Solution] 2097. Valid Arrangement of Pairs</h1><p>In this technical blog, we’ll dive deep into Leetcode Problem 2097 — <em>Valid Arrangement of Pairs</em>. We will break down the solution step by step, from understanding the problem, modeling it as a graph theory problem, to implementing the solution.</p><p>Problem Link: <a href="https://leetcode.com/problems/valid-arrangement-of-pairs/description/">2097. Valid Arrangement of Pairs</a></p><hr><h2 id="Problem-Description"><a href="#Problem-Description" class="headerlink" title="Problem Description"></a>Problem Description</h2><p>Given a 2D array <code>pairs</code> where <code>pairs[i] = [start, end]</code>, you need to rearrange these pairs so that for adjacent pairs <code>[start1, end1]</code> and <code>[start2, end2]</code>, the condition <code>end1 == start2</code> holds.</p><p>It is guaranteed that at least one valid arrangement exists.</p><h3 id="Examples"><a href="#Examples" class="headerlink" title="Examples"></a>Examples</h3><h4 id="Example-1"><a href="#Example-1" class="headerlink" title="Example 1"></a>Example 1</h4><p><strong>Input:</strong><br></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pairs = [[<span class="number">5</span>,<span class="number">1</span>],[<span class="number">4</span>,<span class="number">5</span>],[<span class="number">11</span>,<span class="number">9</span>],[<span class="number">9</span>,<span class="number">4</span>]]</span><br></pre></td></tr></tbody></table></figure><p></p><p><strong>Output:</strong><br></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">11</span>,<span class="number">9</span>],[<span class="number">9</span>,<span class="number">4</span>],[<span class="number">4</span>,<span class="number">5</span>],[<span class="number">5</span>,<span class="number">1</span>]]</span><br></pre></td></tr></tbody></table></figure><p></p><p><strong>Explanation:</strong><br>The arrangement satisfies:</p><ul><li><code>end0 = 9 == 9 = start1</code></li><li><code>end1 = 4 == 4 = start2</code></li><li><code>end2 = 5 == 5 = start3</code></li></ul><h4 id="Example-2"><a href="#Example-2" class="headerlink" title="Example 2"></a>Example 2</h4><p><strong>Input:</strong><br></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pairs = [[<span class="number">1</span>,<span class="number">3</span>],[<span class="number">3</span>,<span class="number">2</span>],[<span class="number">2</span>,<span class="number">1</span>]]</span><br></pre></td></tr></tbody></table></figure><p></p><p><strong>Output:</strong><br></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">1</span>,<span class="number">3</span>],[<span class="number">3</span>,<span class="number">2</span>],[<span class="number">2</span>,<span class="number">1</span>]]</span><br></pre></td></tr></tbody></table></figure><p></p><hr><h2 id="Problem-Modeling-Eulerian-Path-Problem"><a href="#Problem-Modeling-Eulerian-Path-Problem" class="headerlink" title="Problem Modeling: Eulerian Path Problem"></a>Problem Modeling: Eulerian Path Problem</h2><p>This problem can be modeled as an <strong>Eulerian Path Problem</strong> in graph theory. Each <code>pair [start, end]</code> is treated as a directed edge from <code>start</code> to <code>end</code>, and we aim to find a path that traverses all edges while satisfying the given condition.</p><h3 id="What-is-an-Eulerian-Path"><a href="#What-is-an-Eulerian-Path" class="headerlink" title="What is an Eulerian Path?"></a>What is an Eulerian Path?</h3><ul><li><strong>Definition</strong>: An Eulerian path is a path in a graph that visits every edge exactly once.</li><li><strong>Conditions</strong>:<ol><li>An Eulerian path exists if and only if there are exactly two nodes in the graph with unbalanced in-degrees and out-degrees:<ul><li>Start node: the node where <code>out-degree - in-degree = 1</code>.</li><li>End node: the node where <code>in-degree - out-degree = 1</code>.</li></ul></li><li>If all nodes have equal in-degrees and out-degrees, the graph contains an Eulerian circuit, and the path can start at any node.</li></ol></li></ul><hr><h2 id="Solution-Approach"><a href="#Solution-Approach" class="headerlink" title="Solution Approach"></a>Solution Approach</h2><h3 id="1-Graph-Construction"><a href="#1-Graph-Construction" class="headerlink" title="1. Graph Construction"></a>1. Graph Construction</h3><p>Model each <code>pair [start, end]</code> as a directed edge:</p><ul><li>Nodes: <code>start</code> and <code>end</code>.</li><li>Edges: Directed edge from <code>start</code> to <code>end</code>.</li></ul><p>Simultaneously, calculate the <strong>in-degrees</strong> and <strong>out-degrees</strong> of each node to identify the starting node.</p><h3 id="2-Finding-the-Start-Node"><a href="#2-Finding-the-Start-Node" class="headerlink" title="2. Finding the Start Node"></a>2. Finding the Start Node</h3><p>Using the in-degree and out-degree counts:</p><ul><li>A node with <code>out-degree - in-degree = 1</code> is the start node.</li><li>If no such node exists, the graph contains an Eulerian circuit, and we can start from any node.</li></ul><h3 id="3-Using-Hierholzer’s-Algorithm-to-Find-the-Path"><a href="#3-Using-Hierholzer’s-Algorithm-to-Find-the-Path" class="headerlink" title="3. Using Hierholzer’s Algorithm to Find the Path"></a>3. Using Hierholzer’s Algorithm to Find the Path</h3><p><strong>Hierholzer’s Algorithm</strong> is used to find an Eulerian path:</p><ol><li>Start at the chosen node and follow any unvisited edge.</li><li>Continue until reaching a dead end (a node with no outgoing edges).</li><li>Backtrack and record the path.</li><li>Reverse the recorded path to get the correct order.</li></ol><hr><h2 id="Implementation"><a href="#Implementation" class="headerlink" title="Implementation"></a>Implementation</h2><p>Here’s the complete Python solution:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">validArrangement</span>(<span class="params">pairs</span>):</span><br><span class="line">    <span class="comment"># Construct the graph</span></span><br><span class="line">    graph = defaultdict(<span class="built_in">list</span>)</span><br><span class="line">    in_degree = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">    out_degree = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> start, end <span class="keyword">in</span> pairs:</span><br><span class="line">        graph[start].append(end)</span><br><span class="line">        out_degree[start] += <span class="number">1</span></span><br><span class="line">        in_degree[end] += <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Find the starting node</span></span><br><span class="line">    start_node = pairs[<span class="number">0</span>][<span class="number">0</span>]  <span class="comment"># Default start node</span></span><br><span class="line">    <span class="keyword">for</span> node <span class="keyword">in</span> graph:</span><br><span class="line">        <span class="keyword">if</span> out_degree[node] - in_degree[node] == <span class="number">1</span>:</span><br><span class="line">            start_node = node</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Hierholzer's Algorithm</span></span><br><span class="line">    path = []</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">current_node</span>):</span><br><span class="line">        <span class="keyword">while</span> graph[current_node]:</span><br><span class="line">            next_node = graph[current_node].pop()</span><br><span class="line">            dfs(next_node)</span><br><span class="line">            path.append([current_node, next_node])</span><br><span class="line">    </span><br><span class="line">    dfs(start_node)</span><br><span class="line">    <span class="keyword">return</span> path[::-<span class="number">1</span>]</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="Code-Explanation"><a href="#Code-Explanation" class="headerlink" title="Code Explanation"></a>Code Explanation</h2><h3 id="Graph-Construction"><a href="#Graph-Construction" class="headerlink" title="Graph Construction"></a>Graph Construction</h3><p>Using <code>defaultdict</code> to store the adjacency list and count the in-degrees and out-degrees:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">graph = defaultdict(<span class="built_in">list</span>)</span><br><span class="line">in_degree = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">out_degree = defaultdict(<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> start, end <span class="keyword">in</span> pairs:</span><br><span class="line">    graph[start].append(end)</span><br><span class="line">    out_degree[start] += <span class="number">1</span></span><br><span class="line">    in_degree[end] += <span class="number">1</span></span><br></pre></td></tr></tbody></table></figure><h3 id="Finding-the-Start-Node"><a href="#Finding-the-Start-Node" class="headerlink" title="Finding the Start Node"></a>Finding the Start Node</h3><p>Based on the rules for Eulerian paths:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">start_node = pairs[<span class="number">0</span>][<span class="number">0</span>]  <span class="comment"># Default value</span></span><br><span class="line"><span class="keyword">for</span> node <span class="keyword">in</span> graph:</span><br><span class="line">    <span class="keyword">if</span> out_degree[node] - in_degree[node] == <span class="number">1</span>:</span><br><span class="line">        start_node = node</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></tbody></table></figure><h3 id="Hierholzer’s-Algorithm"><a href="#Hierholzer’s-Algorithm" class="headerlink" title="Hierholzer’s Algorithm"></a>Hierholzer’s Algorithm</h3><p>Using a recursive DFS to find the Eulerian path:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">path = []  <span class="comment"># To store the path</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">current_node</span>):</span><br><span class="line">    <span class="keyword">while</span> graph[current_node]:  <span class="comment"># While there are outgoing edges</span></span><br><span class="line">        next_node = graph[current_node].pop()  <span class="comment"># Remove edge</span></span><br><span class="line">        dfs(next_node)</span><br><span class="line">        path.append([current_node, next_node])  <span class="comment"># Record path</span></span><br></pre></td></tr></tbody></table></figure><hr><h2 id="Example-Execution"><a href="#Example-Execution" class="headerlink" title="Example Execution"></a>Example Execution</h2><p>For <code>pairs = [[5,1],[4,5],[11,9],[9,4]]</code>:</p><h3 id="Graph-State"><a href="#Graph-State" class="headerlink" title="Graph State"></a>Graph State</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">11 → 9</span><br><span class="line"> 9 → 4</span><br><span class="line"> 4 → 5</span><br><span class="line"> 5 → 1</span><br></pre></td></tr></tbody></table></figure><h3 id="Execution-Process"><a href="#Execution-Process" class="headerlink" title="Execution Process"></a>Execution Process</h3><ol><li><p>Start DFS from node <code>11</code>:</p><ul><li>Visit <code>11 → 9</code> and remove the edge.</li><li>Visit <code>9 → 4</code> and remove the edge.</li><li>Visit <code>4 → 5</code> and remove the edge.</li><li>Visit <code>5 → 1</code> and remove the edge.</li></ul></li><li><p>Backtrack to record the path:</p><ul><li><code>path = [[5,1], [4,5], [9,4], [11,9]]</code>.</li></ul></li><li><p>Reverse the path for the final result:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">11</span>,<span class="number">9</span>], [<span class="number">9</span>,<span class="number">4</span>], [<span class="number">4</span>,<span class="number">5</span>], [<span class="number">5</span>,<span class="number">1</span>]]</span><br></pre></td></tr></tbody></table></figure></li></ol><hr><h2 id="Time-and-Space-Complexity"><a href="#Time-and-Space-Complexity" class="headerlink" title="Time and Space Complexity"></a>Time and Space Complexity</h2><ul><li><strong>Time Complexity</strong>: <code>O(E)</code>, where <code>E</code> is the number of edges.<ul><li>Constructing the graph: <code>O(E)</code>.</li><li>DFS traversal: <code>O(E)</code>.</li></ul></li><li><strong>Space Complexity</strong>: <code>O(E)</code> for storing the adjacency list and result path.</li></ul><hr><h2 id="Python-Tips-and-Tricks"><a href="#Python-Tips-and-Tricks" class="headerlink" title="Python Tips and Tricks"></a>Python Tips and Tricks</h2><h3 id="1-Closures"><a href="#1-Closures" class="headerlink" title="1. Closures"></a>1. Closures</h3><p>A <strong>closure</strong> allows inner functions to access variables from the outer function, even after the outer function has finished executing.</p><h4 id="Example"><a href="#Example" class="headerlink" title="Example:"></a>Example:</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">outer</span>():</span><br><span class="line">    x = <span class="number">10</span>  <span class="comment"># Outer variable</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">inner</span>():</span><br><span class="line">        <span class="keyword">nonlocal</span> x  <span class="comment"># Use the outer variable</span></span><br><span class="line">        x += <span class="number">1</span></span><br><span class="line">        <span class="built_in">print</span>(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> inner</span><br><span class="line"></span><br><span class="line">closure_func = outer()  <span class="comment"># Returns the inner function</span></span><br><span class="line">closure_func()  <span class="comment"># Outputs 11</span></span><br><span class="line">closure_func()  <span class="comment"># Outputs 12</span></span><br></pre></td></tr></tbody></table></figure><p>In the solution, <code>dfs()</code> uses a closure to access and modify the <code>path</code> list without passing it explicitly.</p><hr><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>This problem showcases how to model a graph problem as an Eulerian path and use Hierholzer’s algorithm for an efficient solution. Such graph theory techniques provide a robust framework for solving similar problems.</p><p>Feel free to leave questions or share your thoughts!</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Leetcode-Python-Solution-2097-Valid-Arrangement-of-Pairs&quot;&gt;&lt;a href=&quot;#Leetcode-Python-Solution-2097-Valid-Arrangement-of-Pairs&quot; class=</summary>
      
    
    
    
    <category term="Code Chronicles" scheme="https://chenhuiyu.github.io/categories/Code-Chronicles/"/>
    
    
    <category term="Python" scheme="https://chenhuiyu.github.io/tags/Python/"/>
    
    <category term="Leetcode" scheme="https://chenhuiyu.github.io/tags/Leetcode/"/>
    
    <category term="Daily Challenge" scheme="https://chenhuiyu.github.io/tags/Daily-Challenge/"/>
    
  </entry>
  
  <entry>
    <title>【Leetcode Python题解】「2097. Valid Arrangement of Pairs」</title>
    <link href="https://chenhuiyu.github.io/2024/12/01/Code%20Chronicles/%E3%80%90Leetcode%20Python%E9%A2%98%E8%A7%A3%E3%80%91%E3%80%8C2097.%20Valid%20Arrangement%20of%20Pairs%E3%80%8D/"/>
    <id>https://chenhuiyu.github.io/2024/12/01/Code%20Chronicles/%E3%80%90Leetcode%20Python%E9%A2%98%E8%A7%A3%E3%80%91%E3%80%8C2097.%20Valid%20Arrangement%20of%20Pairs%E3%80%8D/</id>
    <published>2024-11-30T19:00:00.000Z</published>
    <updated>2024-11-30T19:02:40.251Z</updated>
    
    <content type="html"><![CDATA[<h1 id="【Leetcode-Python题解】「2097-Valid-Arrangement-of-Pairs」"><a href="#【Leetcode-Python题解】「2097-Valid-Arrangement-of-Pairs」" class="headerlink" title="【Leetcode Python题解】「2097. Valid Arrangement of Pairs」"></a>【Leetcode Python题解】「2097. Valid Arrangement of Pairs」</h1><p>在这篇技术博客中，我们将深入解析 LeetCode 的第 2097 题 —— <em>Valid Arrangement of Pairs</em>，并全面介绍如何从题意理解、图论建模到算法实现逐步解决问题。</p><h2 id="题目：2097-Valid-Arrangement-of-Pairs"><a href="#题目：2097-Valid-Arrangement-of-Pairs" class="headerlink" title="题目：2097. Valid Arrangement of Pairs"></a>题目：<a href="https://leetcode.com/problems/valid-arrangement-of-pairs/description/">2097. Valid Arrangement of Pairs</a></h2><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>给定一个二维数组 <code>pairs</code>，其中 <code>pairs[i] = [start, end]</code>，我们需要重新排列这些数字对，使得相邻的两个数字对 <code>[start1, end1]</code> 和 <code>[start2, end2]</code> 满足以下条件：</p><ul><li><code>end1 == start2</code>。</li></ul><p>输入数据保证一定存在这样一种合法的排列方式。</p><h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><h4 id="示例-1"><a href="#示例-1" class="headerlink" title="示例 1"></a>示例 1</h4><p><strong>输入：</strong><br></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pairs = [[<span class="number">5</span>,<span class="number">1</span>],[<span class="number">4</span>,<span class="number">5</span>],[<span class="number">11</span>,<span class="number">9</span>],[<span class="number">9</span>,<span class="number">4</span>]]</span><br></pre></td></tr></tbody></table></figure><p></p><p><strong>输出：</strong><br></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">11</span>,<span class="number">9</span>],[<span class="number">9</span>,<span class="number">4</span>],[<span class="number">4</span>,<span class="number">5</span>],[<span class="number">5</span>,<span class="number">1</span>]]</span><br></pre></td></tr></tbody></table></figure><p></p><p><strong>解释：</strong><br>排列后满足条件：</p><ul><li><code>end0 = 9 == 9 = start1</code></li><li><code>end1 = 4 == 4 = start2</code></li><li><code>end2 = 5 == 5 = start3</code></li></ul><h4 id="示例-2"><a href="#示例-2" class="headerlink" title="示例 2"></a>示例 2</h4><p><strong>输入：</strong><br></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pairs = [[<span class="number">1</span>,<span class="number">3</span>],[<span class="number">3</span>,<span class="number">2</span>],[<span class="number">2</span>,<span class="number">1</span>]]</span><br></pre></td></tr></tbody></table></figure><p></p><p><strong>输出：</strong><br></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">1</span>,<span class="number">3</span>],[<span class="number">3</span>,<span class="number">2</span>],[<span class="number">2</span>,<span class="number">1</span>]]</span><br></pre></td></tr></tbody></table></figure><p></p><hr><h2 id="问题建模：欧拉路径问题"><a href="#问题建模：欧拉路径问题" class="headerlink" title="问题建模：欧拉路径问题"></a>问题建模：欧拉路径问题</h2><p>这道题的本质是一个图论中的欧拉路径问题。我们将每个 <code>pair [start, end]</code> 看作一条从 <code>start</code> 到 <code>end</code> 的有向边，并试图找到一条路径能遍历所有边且满足条件。</p><h3 id="什么是欧拉路径？"><a href="#什么是欧拉路径？" class="headerlink" title="什么是欧拉路径？"></a>什么是欧拉路径？</h3><ul><li><strong>定义</strong>：欧拉路径是一条路径，它能遍历图中每条边恰好一次。</li><li><strong>条件</strong>：<ol><li>如果图中有且仅有两个节点的入度和出度不相等，则可以存在欧拉路径。<ul><li>起点：出度比入度大 1 的节点。</li><li>终点：入度比出度大 1 的节点。</li></ul></li><li>如果所有节点的入度等于出度，则图中存在欧拉回路。</li></ol></li></ul><hr><h2 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h2><h3 id="1-图的构建"><a href="#1-图的构建" class="headerlink" title="1. 图的构建"></a>1. 图的构建</h3><p>将每个 <code>pair [start, end]</code> 建模为有向边：</p><ul><li>节点为 <code>start</code> 和 <code>end</code>。</li><li>边为从 <code>start</code> 到 <code>end</code>。</li></ul><p>同时统计每个节点的 <strong>入度</strong> 和 <strong>出度</strong>，用于后续判断起点。</p><h3 id="2-寻找起点"><a href="#2-寻找起点" class="headerlink" title="2. 寻找起点"></a>2. 寻找起点</h3><p>通过入度和出度的统计：</p><ul><li>出度 - 入度 = 1 的节点是路径的起点。</li><li>如果没有这样的节点，说明图中存在欧拉回路，可从任意节点开始。</li></ul><h3 id="3-Hierholzer-算法找路径"><a href="#3-Hierholzer-算法找路径" class="headerlink" title="3. Hierholzer 算法找路径"></a>3. Hierholzer 算法找路径</h3><p><strong>Hierholzer 算法</strong> 用于寻找欧拉路径，其核心步骤：</p><ol><li>从起点开始，任意选择一条未访问的边走。</li><li>一直走直到走到死胡同（当前节点没有出边）。</li><li>回溯过程中记录路径。</li><li>最终记录的路径需要反转才能得到正确的顺序。</li></ol><hr><h2 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a>算法实现</h2><p>下面是 Python 实现的完整代码：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">validArrangement</span>(<span class="params">pairs</span>):</span><br><span class="line">    <span class="comment"># 构建图</span></span><br><span class="line">    graph = defaultdict(<span class="built_in">list</span>)</span><br><span class="line">    in_degree = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">    out_degree = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> start, end <span class="keyword">in</span> pairs:</span><br><span class="line">        graph[start].append(end)</span><br><span class="line">        out_degree[start] += <span class="number">1</span></span><br><span class="line">        in_degree[end] += <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 寻找起点</span></span><br><span class="line">    start_node = pairs[<span class="number">0</span>][<span class="number">0</span>]  <span class="comment"># 默认起点</span></span><br><span class="line">    <span class="keyword">for</span> node <span class="keyword">in</span> graph:</span><br><span class="line">        <span class="keyword">if</span> out_degree[node] - in_degree[node] == <span class="number">1</span>:</span><br><span class="line">            start_node = node</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Hierholzer算法</span></span><br><span class="line">    path = []</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">current_node</span>):</span><br><span class="line">        <span class="keyword">while</span> graph[current_node]:</span><br><span class="line">            next_node = graph[current_node].pop()</span><br><span class="line">            dfs(next_node)</span><br><span class="line">            path.append([current_node, next_node])</span><br><span class="line">    </span><br><span class="line">    dfs(start_node)</span><br><span class="line">    <span class="keyword">return</span> path[::-<span class="number">1</span>]</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="代码解析"><a href="#代码解析" class="headerlink" title="代码解析"></a>代码解析</h2><h3 id="图的构建"><a href="#图的构建" class="headerlink" title="图的构建"></a>图的构建</h3><p>使用 <code>defaultdict</code> 来存储图的邻接表，以及统计每个节点的入度和出度：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">graph = defaultdict(<span class="built_in">list</span>)</span><br><span class="line">in_degree = defaultdict(<span class="built_in">int</span>)</span><br><span class="line">out_degree = defaultdict(<span class="built_in">int</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> start, end <span class="keyword">in</span> pairs:</span><br><span class="line">    graph[start].append(end)</span><br><span class="line">    out_degree[start] += <span class="number">1</span></span><br><span class="line">    in_degree[end] += <span class="number">1</span></span><br></pre></td></tr></tbody></table></figure><h3 id="寻找起点"><a href="#寻找起点" class="headerlink" title="寻找起点"></a>寻找起点</h3><p>根据入度和出度的统计规则：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">start_node = pairs[<span class="number">0</span>][<span class="number">0</span>]  <span class="comment"># 默认值</span></span><br><span class="line"><span class="keyword">for</span> node <span class="keyword">in</span> graph:</span><br><span class="line">    <span class="keyword">if</span> out_degree[node] - in_degree[node] == <span class="number">1</span>:</span><br><span class="line">        start_node = node</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></tbody></table></figure><h3 id="Hierholzer算法"><a href="#Hierholzer算法" class="headerlink" title="Hierholzer算法"></a>Hierholzer算法</h3><p>通过深度优先搜索（DFS）找到路径：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">path = []  <span class="comment"># 存储路径</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">current_node</span>):</span><br><span class="line">    <span class="keyword">while</span> graph[current_node]:  <span class="comment"># 当前节点还有出边</span></span><br><span class="line">        next_node = graph[current_node].pop()  <span class="comment"># 获取并删除边</span></span><br><span class="line">        dfs(next_node)</span><br><span class="line">        path.append([current_node, next_node])  <span class="comment"># 记录路径</span></span><br></pre></td></tr></tbody></table></figure><hr><h2 id="示例运行"><a href="#示例运行" class="headerlink" title="示例运行"></a>示例运行</h2><p>以 <code>pairs = [[5,1],[4,5],[11,9],[9,4]]</code> 为例：</p><h3 id="图的状态"><a href="#图的状态" class="headerlink" title="图的状态"></a>图的状态</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">11 → 9</span><br><span class="line"> 9 → 4</span><br><span class="line"> 4 → 5</span><br><span class="line"> 5 → 1</span><br></pre></td></tr></tbody></table></figure><h3 id="执行过程"><a href="#执行过程" class="headerlink" title="执行过程"></a>执行过程</h3><ol><li><p>从节点 <code>11</code> 开始，DFS 遍历：</p><ul><li>走 <code>11 → 9</code>，移除边。</li><li>走 <code>9 → 4</code>，移除边。</li><li>走 <code>4 → 5</code>，移除边。</li><li>走 <code>5 → 1</code>，移除边。</li></ul></li><li><p>回溯记录路径：</p><ul><li><code>path = [[5,1], [4,5], [9,4], [11,9]]</code></li></ul></li><li><p>反转路径得到最终结果：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">11</span>,<span class="number">9</span>], [<span class="number">9</span>,<span class="number">4</span>], [<span class="number">4</span>,<span class="number">5</span>], [<span class="number">5</span>,<span class="number">1</span>]]</span><br></pre></td></tr></tbody></table></figure></li></ol><hr><h2 id="算法复杂度"><a href="#算法复杂度" class="headerlink" title="算法复杂度"></a>算法复杂度</h2><ul><li><strong>时间复杂度</strong>：<code>O(E)</code>，其中 <code>E</code> 是边的数量。<ul><li>构建图需要 <code>O(E)</code>。</li><li>DFS 遍历每条边需要 <code>O(E)</code>。</li></ul></li><li><strong>空间复杂度</strong>：<code>O(E)</code>，用于存储图的邻接表和结果路径。</li></ul><hr><h2 id="Python-技巧补充"><a href="#Python-技巧补充" class="headerlink" title="Python 技巧补充"></a>Python 技巧补充</h2><p>最后，我们补充一些代码中用到的一些 Python 技巧。</p><h3 id="1-闭包（Closure）"><a href="#1-闭包（Closure）" class="headerlink" title="1. 闭包（Closure）"></a>1. 闭包（Closure）</h3><p><strong>闭包</strong> 是指在一个函数内部定义另一个函数时，内部函数可以访问外部函数的变量，即使外部函数已经执行完毕。 </p><p>在代码中，闭包的作用是通过内部函数访问和修改外部作用域的变量，而无需通过函数参数显式传递。</p><h4 id="示例："><a href="#示例：" class="headerlink" title="示例："></a>示例：</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">outer</span>():</span><br><span class="line">    x = <span class="number">10</span>  <span class="comment"># 外部变量</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">inner</span>():</span><br><span class="line">        <span class="keyword">nonlocal</span> x  <span class="comment"># 指定使用外部变量</span></span><br><span class="line">        x += <span class="number">1</span></span><br><span class="line">        <span class="built_in">print</span>(x)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> inner</span><br><span class="line"></span><br><span class="line">closure_func = outer()  <span class="comment"># 返回 inner 函数</span></span><br><span class="line">closure_func()  <span class="comment"># 输出 11</span></span><br><span class="line">closure_func()  <span class="comment"># 输出 12</span></span><br></pre></td></tr></tbody></table></figure><p>在本文的算法中，<code>dfs()</code> 函数利用闭包访问 <code>path</code> 列表，避免了通过参数显式传递路径的复杂操作。</p><h4 id="为什么不需要将-path-作为参数？"><a href="#为什么不需要将-path-作为参数？" class="headerlink" title="为什么不需要将 path 作为参数？"></a>为什么不需要将 <code>path</code> 作为参数？</h4><ul><li><strong>易读性</strong>：闭包让代码更加直观，避免传递多个参数。</li><li><strong>效率</strong>：闭包直接操作外部变量，避免在递归过程中传递和合并路径。</li></ul><h3 id="2-defaultdict-与-Counter"><a href="#2-defaultdict-与-Counter" class="headerlink" title="2. defaultdict 与 Counter"></a>2. <code>defaultdict</code> 与 <code>Counter</code></h3><p>Python 的 <code>collections</code> 模块提供了许多工具类，其中 <code>defaultdict</code> 和 <code>Counter</code> 是本题的核心工具。</p><h4 id="defaultdict"><a href="#defaultdict" class="headerlink" title="defaultdict"></a><code>defaultdict</code></h4><p><code>defaultdict</code> 是字典的一个子类，可以为不存在的键提供默认值，从而避免访问不存在键时抛出 <code>KeyError</code>。</p><h5 id="使用方式："><a href="#使用方式：" class="headerlink" title="使用方式："></a>使用方式：</h5><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line"><span class="comment"># 默认值是列表</span></span><br><span class="line">graph = defaultdict(<span class="built_in">list</span>)</span><br><span class="line">graph[<span class="number">1</span>].append(<span class="number">2</span>)</span><br><span class="line">graph[<span class="number">2</span>].append(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(graph)  <span class="comment"># 输出：{1: [2], 2: [3]}</span></span><br><span class="line"><span class="built_in">print</span>(graph[<span class="number">3</span>])  <span class="comment"># 输出：[]，不会报错</span></span><br></pre></td></tr></tbody></table></figure><p>在本文中，<code>defaultdict</code> 被用作图的邻接表，简化了图的构建和更新操作。</p><h4 id="Counter"><a href="#Counter" class="headerlink" title="Counter"></a><code>Counter</code></h4><p><code>Counter</code> 是字典的子类，用于统计元素的出现次数。它将每个元素作为键，出现次数作为值。</p><h5 id="使用方式：-1"><a href="#使用方式：-1" class="headerlink" title="使用方式："></a>使用方式：</h5><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"></span><br><span class="line">counts = Counter([<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(counts)  <span class="comment"># 输出：Counter({3: 3, 2: 2, 1: 1})</span></span><br></pre></td></tr></tbody></table></figure><p>在本文中，可以通过 <code>Counter</code> 快速统计每个节点的入度和出度。</p><hr><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这道题通过将数字对建模为图的欧拉路径问题，使用 Hierholzer 算法高效地找到合法排列。这种图论问题的建模方法不仅提升了题目理解，还为类似问题提供了通用解法。</p><p>希望这篇博客能帮助你彻底掌握这道题！如果你有其他问题，欢迎交流！</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;【Leetcode-Python题解】「2097-Valid-Arrangement-of-Pairs」&quot;&gt;&lt;a href=&quot;#【Leetcode-Python题解】「2097-Valid-Arrangement-of-Pairs」&quot; class=&quot;headerl</summary>
      
    
    
    
    <category term="Code Chronicles" scheme="https://chenhuiyu.github.io/categories/Code-Chronicles/"/>
    
    
    <category term="Python" scheme="https://chenhuiyu.github.io/tags/Python/"/>
    
    <category term="Leetcode" scheme="https://chenhuiyu.github.io/tags/Leetcode/"/>
    
    <category term="每日一题" scheme="https://chenhuiyu.github.io/tags/%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>Detailed Explanation of LoRA, DPO, KTO, and SFT Technologies</title>
    <link href="https://chenhuiyu.github.io/2024/10/23/NLP%20Insights/Introduction%20to%20LLM%20Training%20Terminology:%20LoRA,%20DPO,%20KTO,%20and%20SFT%20Technologies/"/>
    <id>https://chenhuiyu.github.io/2024/10/23/NLP%20Insights/Introduction%20to%20LLM%20Training%20Terminology:%20LoRA,%20DPO,%20KTO,%20and%20SFT%20Technologies/</id>
    <published>2024-10-23T08:26:29.000Z</published>
    <updated>2024-10-23T09:08:00.771Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Introduction-to-LLM-Training-Terminology-LoRA-DPO-KTO-and-SFT-Technologies"><a href="#Introduction-to-LLM-Training-Terminology-LoRA-DPO-KTO-and-SFT-Technologies" class="headerlink" title="Introduction to LLM Training Terminology:LoRA, DPO, KTO, and SFT Technologies"></a><strong>Introduction to LLM Training Terminology:LoRA, DPO, KTO, and SFT Technologies</strong></h3><p>This document provides a detailed introduction to several important techniques used in fine-tuning and optimizing large language models (such as LLAMA3), including <strong>SFT (Supervised Fine-Tuning)</strong>, <strong>LoRA (Low-Rank Adaptation)</strong>, <strong>Alignment</strong> technologies, <strong>KTO (Kahneman-Tversky Optimization)</strong>, and <strong>DPO (Direct Preference Optimization)</strong>. The document also elaborates on the principles of each technique, specific implementation methods, as well as the selection of corresponding loss functions and optimizers.</p><hr><h2 id="1-SFT-Supervised-Fine-Tuning"><a href="#1-SFT-Supervised-Fine-Tuning" class="headerlink" title="1. SFT (Supervised Fine-Tuning)"></a>1. <strong>SFT (Supervised Fine-Tuning)</strong></h2><h3 id="1-1-Principle"><a href="#1-1-Principle" class="headerlink" title="1.1 Principle"></a>1.1 <strong>Principle</strong></h3><p>SFT is a traditional fine-tuning method that adjusts the parameters of a pre-trained model through supervised learning to improve its performance on specific tasks. SFT is typically used to fine-tune models on specific labeled datasets, with the training process resembling standard supervised learning.</p><h3 id="1-2-Implementation-Method"><a href="#1-2-Implementation-Method" class="headerlink" title="1.2 Implementation Method"></a>1.2 <strong>Implementation Method</strong></h3><ul><li><strong>Select a Pre-trained Model</strong>: Such as GPT, BERT, and other language models.</li><li><strong>Prepare a Labeled Dataset</strong>: The dataset includes input-output pairs.</li><li><strong>Train the Model</strong>: Use a standard cross-entropy loss function to train the model, optimizing parameters through gradient descent.</li></ul><h3 id="1-3-Core-Code"><a href="#1-3-Core-Code" class="headerlink" title="1.3 Core Code"></a>1.3 <strong>Core Code</strong></h3><p>Using Hugging Face’s <code>Trainer</code> interface for SFT:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Trainer, TrainingArguments, AutoModelForSeq2SeqLM, AutoTokenizer</span><br><span class="line"></span><br><span class="line">model_name = <span class="string">"gpt2"</span></span><br><span class="line">model = AutoModelForSeq2SeqLM.from_pretrained(model_name)</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_name)</span><br><span class="line"></span><br><span class="line">train_dataset = load_dataset(<span class="string">"my_dataset"</span>, split=<span class="string">"train"</span>)</span><br><span class="line"></span><br><span class="line">training_args = TrainingArguments(</span><br><span class="line">    output_dir=<span class="string">"./results"</span>,</span><br><span class="line">    evaluation_strategy=<span class="string">"epoch"</span>,</span><br><span class="line">    learning_rate=<span class="number">2e-5</span>,</span><br><span class="line">    per_device_train_batch_size=<span class="number">16</span>,</span><br><span class="line">    num_train_epochs=<span class="number">3</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model=model,</span><br><span class="line">    args=training_args,</span><br><span class="line">    train_dataset=train_dataset,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer.train()</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="2-LoRA-Low-Rank-Adaptation"><a href="#2-LoRA-Low-Rank-Adaptation" class="headerlink" title="2. LoRA (Low-Rank Adaptation)"></a>2. <strong>LoRA (Low-Rank Adaptation)</strong></h2><h3 id="2-1-Principle"><a href="#2-1-Principle" class="headerlink" title="2.1 Principle"></a>2.1 <strong>Principle</strong></h3><p>LoRA is a parameter-efficient fine-tuning technique that performs low-rank decomposition of the weight matrices in large models. It decomposes the original weight matrix $W$ into two low-rank matrices $B$ and $A$, and only fine-tunes these low-rank matrices. The design goal of LoRA is to reduce the number of fine-tuning parameters while retaining the pre-trained model weights, optimizing model performance by adjusting the low-rank matrices.</p><h3 id="2-2-Implementation-Method"><a href="#2-2-Implementation-Method" class="headerlink" title="2.2 Implementation Method"></a>2.2 <strong>Implementation Method</strong></h3><ul><li><strong>Weight Decomposition</strong>: For the model’s linear layers (such as the <code>q_proj</code> and <code>v_proj</code> layers in the attention mechanism), decompose the weight matrix into two low-rank matrices $B$ and $A$.</li><li><strong>Fine-Tune Specific Layers</strong>: Apply LoRA only to these specific linear layers, keeping other layers in the model unchanged.</li></ul><h3 id="2-3-Layers-to-Fine-Tune-vs-Layers-to-Keep-Unchanged"><a href="#2-3-Layers-to-Fine-Tune-vs-Layers-to-Keep-Unchanged" class="headerlink" title="2.3 Layers to Fine-Tune vs. Layers to Keep Unchanged"></a>2.3 <strong>Layers to Fine-Tune vs. Layers to Keep Unchanged</strong></h3><h4 id="Layers-to-Fine-Tune"><a href="#Layers-to-Fine-Tune" class="headerlink" title="Layers to Fine-Tune"></a><strong>Layers to Fine-Tune</strong></h4><p>LoRA is typically applied to the linear projection layers in Transformer models, especially several key layers in the multi-head attention mechanism:</p><ul><li><strong>q_proj</strong> (Query Projection Layer)</li><li><strong>k_proj</strong> (Key Projection Layer)</li><li><strong>v_proj</strong> (Value Projection Layer)</li><li><strong>o_proj</strong> (Output Projection Layer)</li><li><strong>ffn_up_proj</strong> and <strong>ffn_down_proj</strong> (Up and Down Projection Layers of the Feedforward Neural Network)</li></ul><h4 id="Layers-to-Keep-Unchanged"><a href="#Layers-to-Keep-Unchanged" class="headerlink" title="Layers to Keep Unchanged"></a><strong>Layers to Keep Unchanged</strong></h4><ul><li><strong>Embedding Layers</strong>: Responsible for encoding inputs and outputs, usually do not require fine-tuning.</li><li><strong>LayerNorm Layers</strong>: These layers are mainly used for normalization, do not contain many parameters, and are typically kept unchanged.</li><li><strong>Activation Function Layers</strong>: Non-linear activation functions like ReLU or GELU do not involve parameters and do not require fine-tuning.</li></ul><h3 id="2-4-Loss-Function"><a href="#2-4-Loss-Function" class="headerlink" title="2.4 Loss Function"></a>2.4 <strong>Loss Function</strong></h3><p>The loss function for LoRA is usually task-specific. In language generation tasks, LoRA uses <strong>cross-entropy loss</strong> to measure the difference between the generated text and the target text:</p><script type="math/tex; mode=display">\mathcal{L}_{\text{LoRA}} = - \sum_{i} y_i \log(\hat{y}_i)</script><p>where $y_i$ is the true label, and $\hat{y}_i$ is the model’s output probability.</p><h3 id="2-5-Optimizer"><a href="#2-5-Optimizer" class="headerlink" title="2.5 Optimizer"></a>2.5 <strong>Optimizer</strong></h3><p>LoRA fine-tuning typically uses the <strong>AdamW</strong> optimizer, as shown in the following code:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.AdamW(lora_model.parameters(), lr=<span class="number">5e-5</span>)</span><br></pre></td></tr></tbody></table></figure><h3 id="2-6-Core-Code"><a href="#2-6-Core-Code" class="headerlink" title="2.6 Core Code"></a>2.6 <strong>Core Code</strong></h3><p>Implementing LoRA using the <code>peft</code> library:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> peft <span class="keyword">import</span> LoraConfig, get_peft_model</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForSeq2SeqLM</span><br><span class="line"></span><br><span class="line">model_name = <span class="string">"gpt2"</span></span><br><span class="line">model = AutoModelForSeq2SeqLM.from_pretrained(model_name)</span><br><span class="line"></span><br><span class="line">lora_config = LoraConfig(</span><br><span class="line">    r=<span class="number">8</span>,                </span><br><span class="line">    lora_alpha=<span class="number">32</span>,      </span><br><span class="line">    target_modules=[<span class="string">"q_proj"</span>, <span class="string">"v_proj"</span>],  </span><br><span class="line">    lora_dropout=<span class="number">0.1</span>,   </span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">lora_model = get_peft_model(model, lora_config)</span><br><span class="line">lora_model.train()</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="3-Alignment-Alignment-Techniques"><a href="#3-Alignment-Alignment-Techniques" class="headerlink" title="3. Alignment (Alignment Techniques)"></a>3. <strong>Alignment (Alignment Techniques)</strong></h2><p>Before introducing KL divergence, we first need to clarify how LLM alignment is achieved, along with the underlying principles and mathematical formulas.</p><h3 id="1-What-is-Model-Alignment"><a href="#1-What-is-Model-Alignment" class="headerlink" title="1. What is Model Alignment?"></a><strong>1. What is Model Alignment?</strong></h3><p>The core objective of model alignment is to ensure that the language model’s outputs meet human expectations or preferences. Typically, the model is initially trained through large-scale supervised learning (SFT, Supervised Fine-Tuning) to generate a model with basic capabilities. Subsequently, through alignment techniques, the model is further adjusted to ensure that its generated content better aligns with human preferences or avoids producing harmful or erroneous information.</p><p><strong>Core Mechanism of Alignment</strong>:</p><ul><li><strong>Positive Samples</strong>: Outputs that meet human expectations (e.g., correct answers).</li><li><strong>Negative Samples</strong>: Outputs that do not meet human expectations (e.g., incorrect answers).</li></ul><p>By using paired preference data or labels (correct/incorrect), the model’s outputs are further fine-tuned to generate more positive samples while reducing the probability of generating negative samples.</p><hr><h3 id="2-Mathematical-Principles-of-Model-Alignment"><a href="#2-Mathematical-Principles-of-Model-Alignment" class="headerlink" title="2. Mathematical Principles of Model Alignment"></a><strong>2. Mathematical Principles of Model Alignment</strong></h3><p>During the alignment process, the model generates outputs through a <strong>policy model</strong>, which is typically an SFT-trained language model used to generate outputs given an input. To optimize the model’s outputs to better align with human preferences, the following loss functions and optimization methods are commonly used:</p><h4 id="2-1-Policy-Model"><a href="#2-1-Policy-Model" class="headerlink" title="2.1 Policy Model"></a><strong>2.1 Policy Model</strong></h4><p>Assume the current policy of the model is $\pi_\theta$, which represents the probability of the model generating output $y$ given input $x$:</p><script type="math/tex; mode=display">\pi_\theta(y|x)</script><p>The objective of the policy model is to adjust the parameters $\theta$ to increase the probability of generating correct outputs (positive samples) and decrease the probability of generating incorrect outputs (negative samples).</p><h4 id="2-2-Mechanism-for-Increasing-Positive-Sample-Probability-and-Decreasing-Negative-Sample-Probability"><a href="#2-2-Mechanism-for-Increasing-Positive-Sample-Probability-and-Decreasing-Negative-Sample-Probability" class="headerlink" title="2.2 Mechanism for Increasing Positive Sample Probability and Decreasing Negative Sample Probability"></a><strong>2.2 Mechanism for Increasing Positive Sample Probability and Decreasing Negative Sample Probability</strong></h4><p>To achieve this goal, loss functions with preference comparisons or labels are typically used for optimization:</p><ol><li><p><strong>Optimization of Positive Samples</strong>: By increasing the loss weight of positive samples, the model is guided to generate positive samples with higher probability when faced with the same problem.</p><ul><li>The loss function for positive samples guides the model to produce more outputs that meet human expectations.</li></ul></li><li><p><strong>Penalty for Negative Samples</strong>: By applying higher loss weights to negative samples, the model learns to reduce the probability of generating these incorrect outputs.</p><ul><li>The loss function for negative samples aims to penalize the model more when it generates incorrect answers, thereby reducing the likelihood of such outputs.</li></ul></li></ol><p>In some methods, such as DPO and KTO, <strong>KL divergence</strong> between the current policy model and a reference model is calculated to prevent the model from deviating excessively from the original pre-trained model during optimization.</p><hr><h3 id="3-Role-of-Loss-Functions-and-KL-Divergence"><a href="#3-Role-of-Loss-Functions-and-KL-Divergence" class="headerlink" title="3. Role of Loss Functions and KL Divergence"></a><strong>3. Role of Loss Functions and KL Divergence</strong></h3><p>In the model alignment process, the loss function typically consists of two parts:</p><ol><li><strong>Preference Loss</strong> or <strong>Label Loss</strong>, used to optimize the model to generate outputs that meet human expectations.</li><li><strong>KL Divergence</strong>, used to constrain the model from deviating from the reference model.</li></ol><h4 id="3-1-Role-of-KL-Divergence"><a href="#3-1-Role-of-KL-Divergence" class="headerlink" title="3.1 Role of KL Divergence"></a><strong>3.1 Role of KL Divergence</strong></h4><p>KL divergence (Kullback-Leibler Divergence) measures the difference between two probability distributions. In model alignment, KL divergence is used to limit the distribution difference between the current model $\pi<em>\theta$ and the reference model $\pi</em>{\text{ref}}$, ensuring that the model’s outputs do not deviate excessively from the pre-trained model during optimization. The specific formula is:</p><script type="math/tex; mode=display">\text{KL}(\pi_\theta(y|x) \| \pi_{\text{ref}}(y|x)) = \sum_y \pi_\theta(y|x) \log \frac{\pi_\theta(y|x)}{\pi_{\text{ref}}(y|x)}</script><ul><li>If the KL divergence is large, it indicates that the current model’s generated distribution significantly differs from the reference model, which may mean the model is producing unreasonable outputs.</li><li>By minimizing KL divergence, the model can be further optimized while ensuring the reasonableness of its outputs.</li></ul><h4 id="3-2-Loss-Function-Formulas"><a href="#3-2-Loss-Function-Formulas" class="headerlink" title="3.2 Loss Function Formulas"></a><strong>3.2 Loss Function Formulas</strong></h4><p>Based on preferences or labels, the model’s loss function can be expressed in the following forms:</p><h5 id="Loss-Function-in-DPO"><a href="#Loss-Function-in-DPO" class="headerlink" title="Loss Function in DPO:"></a><strong>Loss Function in DPO</strong>:</h5><script type="math/tex; mode=display">L_{DPO} = -\mathbb{E}_{x, y_w, y_l \sim D} [\log \sigma(\beta (\log(\pi_\theta(y_w|x)) - \log(\pi_\theta(y_l|x))))]</script><ul><li>$y_w$: Higher-preference answer.</li><li>$y_l$: Lower-preference answer.</li></ul><p>In DPO, KL divergence can be introduced as a regularization term:</p><script type="math/tex; mode=display">L_{DPO} = -\log \sigma(\log(\pi_\theta(y_w|x)) - \log(\pi_\theta(y_l|x)) + \beta \cdot \text{KL}(\pi_\theta \| \pi_{\text{ref}}))</script><p>By controlling KL divergence, the model’s outputs do not deviate too much from the reference model.</p><h5 id="Loss-Function-in-KTO"><a href="#Loss-Function-in-KTO" class="headerlink" title="Loss Function in KTO:"></a><strong>Loss Function in KTO</strong>:</h5><p>The loss function in KTO is based on prospect theory and incorporates KL divergence as a core component:</p><script type="math/tex; mode=display">L_{KTO} = \lambda_U \cdot \sigma(\beta \cdot (\text{KL}(\pi_\theta(\text{Answer 2}) \| \pi_{\text{ref}}) - r_{\theta}(\text{Answer 2})))</script><ul><li>$r_{\theta}(x, y)$: The current policy’s confidence in negative samples (incorrect answers).</li><li>KL divergence is used to measure the difference between the current model and the reference model, ensuring that while reducing the generation of negative samples, the model does not deviate from the original reference model.</li></ul><p>By increasing the loss for negative samples (i.e., increasing the value of $\lambda_U$), the model reduces the confidence in negative samples, thereby decreasing the probability of generating similar incorrect answers in the future.</p><hr><h3 id="4-How-to-Optimize-the-Model"><a href="#4-How-to-Optimize-the-Model" class="headerlink" title="4. How to Optimize the Model"></a><strong>4. How to Optimize the Model</strong></h3><p>Through the loss functions introduced above, model optimization is typically performed using <strong>Gradient Descent</strong>. The gradients of the loss function reflect the differences between the model’s outputs and the expected outputs, and the optimization goal is to minimize the loss function.</p><p><strong>Gradient Update Formula</strong>:</p><script type="math/tex; mode=display">\theta_{\text{new}} = \theta_{\text{old}} - \eta \nabla_{\theta} L</script><p>where:</p><ul><li>$\eta$ is the learning rate, determining the step size of each parameter update.</li><li>$\nabla_{\theta} L$ is the gradient of the loss function with respect to the model parameters, indicating the contribution of the current parameters to the loss.</li></ul><p>Through continuous iteration, the model gradually increases the probability of generating positive samples and decreases the probability of generating negative samples, ultimately achieving model alignment.</p><ul><li>The core objective of <strong>Model Alignment</strong> is to optimize the model’s outputs to meet human expectations through preference or label data.</li><li>The <strong>Policy Model</strong> ($\pi_\theta$) generates outputs, and KL divergence is used to control the degree of deviation from the reference model, preventing unreasonable biases during optimization.</li><li>The <strong>Probability of Positive Samples</strong> is gradually increased through the optimization of the loss function, while the <strong>Probability of Negative Samples</strong> is reduced by increasing loss weights and lowering confidence.</li><li>Gradient descent is used to update model parameters, ultimately achieving model alignment.</li></ul><hr><h2 id="4-DPO-Direct-Preference-Optimization"><a href="#4-DPO-Direct-Preference-Optimization" class="headerlink" title="4. DPO (Direct Preference Optimization)"></a>4. <strong>DPO (Direct Preference Optimization)</strong></h2><h3 id="4-1-Principle"><a href="#4-1-Principle" class="headerlink" title="4.1 Principle"></a>4.1 <strong>Principle</strong></h3><p>DPO directly optimizes the model’s output preference function to make the model’s outputs more aligned with human preferences. It compares different outputs generated by the model and uses a preference function to evaluate which of the two outputs is better, thereby guiding the optimization of the model parameters.</p><h3 id="4-2-Loss-Function"><a href="#4-2-Loss-Function" class="headerlink" title="4.2 Loss Function"></a>4.2 <strong>Loss Function</strong></h3><p>DPO uses a preference loss function to compare the quality of two outputs:</p><script type="math/tex; mode=display">\mathcal{L}_{\text{DPO}} = \log(1 + \exp(-\sigma \cdot (\hat{y}_a - \hat{y}_b) \cdot p))</script><ul><li>$ \hat{y}_a $ and $ \hat{y}_b $ are the model’s predictions for two samples.</li><li>$ p $ is the human preference (1 indicates preference for $a$, -1 indicates preference for $b$).</li><li>$ \sigma $ is a smoothing parameter.</li></ul><h3 id="4-3-Optimizer"><a href="#4-3-Optimizer" class="headerlink" title="4.3 Optimizer"></a>4.3 <strong>Optimizer</strong></h3><p>DPO typically uses the <strong>AdamW</strong> optimizer, which is suitable for optimizing large-scale parameter models. The code is as follows:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.AdamW(model.parameters(), lr=<span class="number">1e-5</span>)</span><br></pre></td></tr></tbody></table></figure><h3 id="4-4-Core-Code"><a href="#4-4-Core-Code" class="headerlink" title="4.4 Core Code"></a>4.4 <strong>Core Code</strong></h3><p>The following are the training steps for DPO:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preference_loss</span>(<span class="params">output_a, output_b, human_preference</span>):</span><br><span class="line">    preference = human_preference(output_a, output_b)</span><br><span class="line">    loss = torch.log(<span class="number">1</span> + torch.exp(-preference * (output_a - output_b)))</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dpo_training_step</span>(<span class="params">model, data_a, data_b, human_preference</span>):</span><br><span class="line">    output_a = model(data_a)</span><br><span class="line">    output_b = model(data_b)</span><br><span class="line">    </span><br><span class="line">    loss = preference_loss(output_a, output_b, human_preference)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> batch_a, batch_b <span class="keyword">in</span> training_data:</span><br><span class="line">    dpo_training_step(model, batch_a, batch_b, human_preference_function)</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="5-KTO-Kahneman-Tversky-Optimization"><a href="#5-KTO-Kahneman-Tversky-Optimization" class="headerlink" title="5. KTO (Kahneman-Tversky Optimization)"></a>5. <strong>KTO (Kahneman-Tversky Optimization)</strong></h2><h3 id="5-1-Principle"><a href="#5-1-Principle" class="headerlink" title="5.1 Principle"></a>5.1 <strong>Principle</strong></h3><p>KTO is based on Kahneman and Tversky’s Prospect Theory, which uses an asymmetric utility function to measure the model’s gains and losses. It aims to optimize the model’s performance, especially in scenarios with asymmetric risks and rewards. The utility function is defined as follows:</p><script type="math/tex; mode=display">\mathcal{U}(x) =\begin{cases}x^{\alpha}, & x \geq 0 \\-\lambda (-x)^{\alpha}, & x < 0\end{cases}</script><ul><li>$x$ is the difference between the model’s prediction and the true value.</li><li>$\alpha$ is the non-linear coefficient, typically 0.88.</li><li>$\lambda$ is the loss penalty weight, typically 2.25.</li></ul><h3 id="5-2-Loss-Function"><a href="#5-2-Loss-Function" class="headerlink" title="5.2 Loss Function"></a>5.2 <strong>Loss Function</strong></h3><p>The loss function for KTO is based on the utility function from Prospect Theory and is used to penalize the model’s prediction errors:</p><script type="math/tex; mode=display">\mathcal{L}_{\text{KTO}} = -\mathbb{E}[\mathcal{U}(y_{\text{pred}} - y_{\text{true}})]</script><h3 id="5-3-Optimizer"><a href="#5-3-Optimizer" class="headerlink" title="5.3 Optimizer"></a>5.3 <strong>Optimizer</strong></h3><p>KTO commonly uses the <strong>AdamW</strong> optimizer to ensure stability during the training process:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.AdamW(model.parameters(), lr=<span class="number">3e-5</span>)</span><br></pre></td></tr></tbody></table></figure><h3 id="5-4-Core-Code"><a href="#5-4-Core-Code" class="headerlink" title="5.4 Core Code"></a>5.4 <strong>Core Code</strong></h3><p>The following is the code for calculating the KTO loss function:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">prospect_utility</span>(<span class="params">value, alpha=<span class="number">0.88</span>, lambda_=<span class="number">2.25</span></span>):</span><br><span class="line">    <span class="keyword">if</span> value &gt;= <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> torch.<span class="built_in">pow</span>(value, alpha)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> -lambda_ * torch.<span class="built_in">pow</span>(-value, alpha)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">kto_loss</span>(<span class="params">predictions, targets</span>):</span><br><span class="line">    value = predictions - targets</span><br><span class="line">    utility = prospect_utility(value)</span><br><span class="line">    loss = -torch.mean(utility)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">    predictions = model(data)</span><br><span class="line">    loss = kto_loss(predictions, targets)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a><strong>Summary</strong></h3><div class="table-container"><table><thead><tr><th>Method</th><th>Loss Function</th><th>Optimizer</th></tr></thead><tbody><tr><td><strong>SFT</strong></td><td>Cross-Entropy Loss</td><td>AdamW, RMSprop, SGD</td></tr><tr><td><strong>LoRA</strong></td><td>Cross-Entropy Loss</td><td>AdamW, RMSprop, SGD</td></tr><tr><td><strong>DPO</strong></td><td>Preference Loss Function: $\log(1 + \exp(-\sigma (\hat{y}_a - \hat{y}_b)p))$</td><td>AdamW</td></tr><tr><td><strong>KTO</strong></td><td>Prospect Theory Utility Function: $-\mathbb{E}[\mathcal{U}(y<em>{\text{pred}} - y</em>{\text{true}})]$</td><td>AdamW</td></tr></tbody></table></div><p>Through the organization of this document, readers can clearly understand the principles, specific implementation steps, loss function designs, and optimizer selections for technologies such as SFT, LoRA, DPO, and KTO, especially in the context of fine-tuning large-scale pre-trained models like LLAMA3.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;Introduction-to-LLM-Training-Terminology-LoRA-DPO-KTO-and-SFT-Technologies&quot;&gt;&lt;a href=&quot;#Introduction-to-LLM-Training-Terminology-LoRA-</summary>
      
    
    
    
    <category term="NLP Insights" scheme="https://chenhuiyu.github.io/categories/NLP-Insights/"/>
    
    
    <category term="LLM" scheme="https://chenhuiyu.github.io/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>LoRA, DPO, KTO 与 SFT 技术详解</title>
    <link href="https://chenhuiyu.github.io/2024/10/23/NLP%20Insights/LLM%E8%AE%AD%E7%BB%83%E6%9C%AF%E8%AF%AD%E4%BB%8B%E7%BB%8D%EF%BC%9ASFT%E3%80%81LoRA%E3%80%81Alignment%E3%80%81KTO%E5%92%8CDPO/"/>
    <id>https://chenhuiyu.github.io/2024/10/23/NLP%20Insights/LLM%E8%AE%AD%E7%BB%83%E6%9C%AF%E8%AF%AD%E4%BB%8B%E7%BB%8D%EF%BC%9ASFT%E3%80%81LoRA%E3%80%81Alignment%E3%80%81KTO%E5%92%8CDPO/</id>
    <published>2024-10-23T08:26:29.000Z</published>
    <updated>2024-10-23T09:10:14.320Z</updated>
    
    <content type="html"><![CDATA[<h3 id="LoRA-DPO-KTO-与-SFT-技术详解"><a href="#LoRA-DPO-KTO-与-SFT-技术详解" class="headerlink" title="LoRA, DPO, KTO 与 SFT 技术详解"></a><strong>LoRA, DPO, KTO 与 SFT 技术详解</strong></h3><p>本篇文档将详细介绍几种在大型语言模型（如 LLAMA3）微调和优化中的重要技术，包括 <strong>SFT（Supervised Fine-Tuning）</strong>、<strong>LoRA（Low-Rank Adaptation）</strong>、<strong>Alignment</strong> 技术、<strong>KTO（Kahneman-Tversky Optimization）</strong> 和 <strong>DPO（Direct Preference Optimization）</strong>。文中还将详细阐述每种技术的原理、具体实现方法以及相应的损失函数与优化器选择。</p><hr><h2 id="1-SFT（Supervised-Fine-Tuning）"><a href="#1-SFT（Supervised-Fine-Tuning）" class="headerlink" title="1. SFT（Supervised Fine-Tuning）"></a>1. <strong>SFT（Supervised Fine-Tuning）</strong></h2><h3 id="1-1-原理"><a href="#1-1-原理" class="headerlink" title="1.1 原理"></a>1.1 <strong>原理</strong></h3><p>SFT 是一种传统的微调方法，通过监督学习对预训练模型进行微调，调整模型的参数使其在特定任务上表现更好。SFT 通常用于针对特定的标注数据进行模型微调，训练的过程类似于常规的监督学习。</p><h3 id="1-2-实现方法"><a href="#1-2-实现方法" class="headerlink" title="1.2 实现方法"></a>1.2 <strong>实现方法</strong></h3><ul><li><strong>选择预训练模型</strong>：如 GPT、BERT 等语言模型。</li><li><strong>准备标注数据集</strong>：数据集包含输入和输出对。</li><li><strong>训练模型</strong>：使用标准的交叉熵损失函数对模型进行训练，通过梯度下降优化参数。</li></ul><h3 id="1-3-核心代码"><a href="#1-3-核心代码" class="headerlink" title="1.3 核心代码"></a>1.3 <strong>核心代码</strong></h3><p>使用 Hugging Face 的 <code>Trainer</code> 接口进行 SFT：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> Trainer, TrainingArguments, AutoModelForSeq2SeqLM, AutoTokenizer</span><br><span class="line"></span><br><span class="line">model_name = <span class="string">"gpt2"</span></span><br><span class="line">model = AutoModelForSeq2SeqLM.from_pretrained(model_name)</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(model_name)</span><br><span class="line"></span><br><span class="line">train_dataset = load_dataset(<span class="string">"my_dataset"</span>, split=<span class="string">"train"</span>)</span><br><span class="line"></span><br><span class="line">training_args = TrainingArguments(</span><br><span class="line">    output_dir=<span class="string">"./results"</span>,</span><br><span class="line">    evaluation_strategy=<span class="string">"epoch"</span>,</span><br><span class="line">    learning_rate=<span class="number">2e-5</span>,</span><br><span class="line">    per_device_train_batch_size=<span class="number">16</span>,</span><br><span class="line">    num_train_epochs=<span class="number">3</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model=model,</span><br><span class="line">    args=training_args,</span><br><span class="line">    train_dataset=train_dataset,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer.train()</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="2-LoRA（Low-Rank-Adaptation）"><a href="#2-LoRA（Low-Rank-Adaptation）" class="headerlink" title="2. LoRA（Low-Rank Adaptation）"></a>2. <strong>LoRA（Low-Rank Adaptation）</strong></h2><h3 id="2-1-原理"><a href="#2-1-原理" class="headerlink" title="2.1 原理"></a>2.1 <strong>原理</strong></h3><p>LoRA 是一种参数高效的微调技术，通过对大模型中的权重矩阵进行低秩分解，将原始权重矩阵 $W$ 分解为两个低秩矩阵 $B$ 和 $A$，并仅对这些低秩矩阵进行微调。LoRA 的设计目标是减少微调参数的数量，在保留预训练模型权重的同时，通过调整低秩矩阵来优化模型表现。</p><h3 id="2-2-实现方法"><a href="#2-2-实现方法" class="headerlink" title="2.2 实现方法"></a>2.2 <strong>实现方法</strong></h3><ul><li><strong>权重分解</strong>：对于模型的线性层（如注意力机制中的 <code>q_proj</code> 和 <code>v_proj</code> 层），将权重矩阵分解为两个低秩矩阵 $B$ 和 $A$。</li><li><strong>微调特定层</strong>：仅对这些特定的线性层应用 LoRA，而模型中的其他层保持不变。</li></ul><h3 id="2-3-可微调的层与不变的层"><a href="#2-3-可微调的层与不变的层" class="headerlink" title="2.3 可微调的层与不变的层"></a>2.3 <strong>可微调的层与不变的层</strong></h3><h4 id="可微调的层"><a href="#可微调的层" class="headerlink" title="可微调的层"></a><strong>可微调的层</strong></h4><p>LoRA 通常应用于 Transformer 模型中的线性投影层，尤其是多头注意力机制中的几个关键层：</p><ul><li><strong>q_proj</strong>（Query 投影层）</li><li><strong>k_proj</strong>（Key 投影层）</li><li><strong>v_proj</strong>（Value 投影层）</li><li><strong>o_proj</strong>（Output 投影层）</li><li><strong>ffn_up_proj</strong> 和 <strong>ffn_down_proj</strong>（前馈神经网络的上下投影层）</li></ul><h4 id="不变的层"><a href="#不变的层" class="headerlink" title="不变的层"></a><strong>不变的层</strong></h4><ul><li><strong>Embedding 层</strong>：负责输入和输出的编码，通常不需要微调。</li><li><strong>LayerNorm 层</strong>：这些层主要用于归一化，不含大量参数，通常保持不变。</li><li><strong>激活函数层</strong>：如 ReLU 或 GELU 等非线性激活函数不涉及参数，不需要进行微调。</li></ul><h3 id="2-4-损失函数"><a href="#2-4-损失函数" class="headerlink" title="2.4 损失函数"></a>2.4 <strong>损失函数</strong></h3><p>LoRA 的损失函数通常与具体任务相关。在语言生成任务中，LoRA 使用<strong>交叉熵损失</strong>来度量生成文本和目标文本之间的差异：</p><script type="math/tex; mode=display">\mathcal{L}_{\text{LoRA}} = - \sum_{i} y_i \log(\hat{y}_i)</script><p>其中 $y_i$ 是真实标签，$\hat{y}_i$ 是模型的输出概率。</p><h3 id="2-5-优化器"><a href="#2-5-优化器" class="headerlink" title="2.5 优化器"></a>2.5 <strong>优化器</strong></h3><p>LoRA 微调通常使用 <strong>AdamW</strong> 优化器，具体代码如下：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.AdamW(lora_model.parameters(), lr=<span class="number">5e-5</span>)</span><br></pre></td></tr></tbody></table></figure><h3 id="2-6-核心代码"><a href="#2-6-核心代码" class="headerlink" title="2.6 核心代码"></a>2.6 <strong>核心代码</strong></h3><p>使用 <code>peft</code> 库实现 LoRA：<br></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> peft <span class="keyword">import</span> LoraConfig, get_peft_model</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForSeq2SeqLM</span><br><span class="line"></span><br><span class="line">model_name = <span class="string">"gpt2"</span></span><br><span class="line">model = AutoModelForSeq2SeqLM.from_pretrained(model_name)</span><br><span class="line"></span><br><span class="line">lora_config = LoraConfig(</span><br><span class="line">    r=<span class="number">8</span>,                </span><br><span class="line">    lora_alpha=<span class="number">32</span>,      </span><br><span class="line">    target_modules=[<span class="string">"q_proj"</span>, <span class="string">"v_proj"</span>],  </span><br><span class="line">    lora_dropout=<span class="number">0.1</span>,   </span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">lora_model = get_peft_model(model, lora_config)</span><br><span class="line">lora_model.train()</span><br></pre></td></tr></tbody></table></figure><p></p><hr><h2 id="3-Alignment（对齐技术）"><a href="#3-Alignment（对齐技术）" class="headerlink" title="3. Alignment（对齐技术）"></a>3. <strong>Alignment（对齐技术）</strong></h2><p>在引入KL散度之前，我们首先需要明确LLM对齐（Alignment）是如何实现的，以及背后的原理和数学公式。</p><h3 id="1-什么是模型对齐（Alignment）？"><a href="#1-什么是模型对齐（Alignment）？" class="headerlink" title="1. 什么是模型对齐（Alignment）？"></a><strong>1. 什么是模型对齐（Alignment）？</strong></h3><p>模型对齐的核心目标是让语言模型的输出符合人类的期望或偏好。通常，模型最初通过大规模监督学习（SFT，Supervised Fine-Tuning）训练，生成具有基础能力的模型。接下来，通过对齐技术，进一步调整模型，使其生成的内容更符合人类偏好或避免产生有害、错误的信息。</p><p><strong>对齐的核心机制</strong>：</p><ul><li><strong>正样本</strong>：符合人类预期的输出（如正确回答）。</li><li><strong>负样本</strong>：不符合人类预期的输出（如错误回答）。</li></ul><p>通过使用成对偏好数据或标签（正确/错误），对模型的输出进行进一步微调，使模型能够生成更多的正样本，同时减少负样本的生成概率。</p><hr><h3 id="2-模型对齐的数学原理"><a href="#2-模型对齐的数学原理" class="headerlink" title="2. 模型对齐的数学原理"></a><strong>2. 模型对齐的数学原理</strong></h3><p>在对齐过程中，模型会通过<strong>策略模型</strong>（Policy Model）来生成输出，策略模型通常是经过SFT训练的语言模型，用来在给定输入下生成输出。为了优化模型的输出，使其更加符合人类偏好，常常使用以下损失函数和优化方法：</p><h4 id="2-1-策略模型"><a href="#2-1-策略模型" class="headerlink" title="2.1 策略模型"></a><strong>2.1 策略模型</strong></h4><p>假设当前模型的策略为 $\pi_\theta$，它表示在给定输入 $x$ 时，模型生成输出 $y$ 的概率：</p><script type="math/tex; mode=display">\pi_\theta(y|x)</script><p>策略模型的目标是通过调整参数 $\theta$，提高生成正确输出（正样本）的概率，降低生成错误输出（负样本）的概率。</p><h4 id="2-2-提高正样本概率与降低负样本概率的机制"><a href="#2-2-提高正样本概率与降低负样本概率的机制" class="headerlink" title="2.2 提高正样本概率与降低负样本概率的机制"></a><strong>2.2 提高正样本概率与降低负样本概率的机制</strong></h4><p>为了实现这个目标，通常使用带有偏好比较或标签的损失函数进行优化：</p><ol><li><p><strong>正样本的优化</strong>：通过增加正样本的损失权重，使得模型生成正样本的概率更高。</p><ul><li>正样本的损失函数会引导模型在面对相同问题时，生成更多符合人类期望的答案。</li></ul></li><li><p><strong>负样本的惩罚</strong>：对负样本施加更高的损失权重，模型会学习到减少这些错误输出的概率。</p><ul><li>负样本的损失函数旨在让模型在生成错误答案时感知到更大的惩罚，从而减少这些输出的生成。</li></ul></li></ol><p>在某些方法中，例如DPO和KTO，还会通过计算当前策略模型与参考模型之间的<strong>KL散度</strong>，来防止模型在优化过程中过度偏离原始预训练模型。</p><hr><h3 id="3-损失函数与KL散度的作用"><a href="#3-损失函数与KL散度的作用" class="headerlink" title="3. 损失函数与KL散度的作用"></a><strong>3. 损失函数与KL散度的作用</strong></h3><p>在模型对齐的过程中，损失函数通常包含两部分：</p><ol><li><strong>偏好损失</strong>或<strong>标签损失</strong>，用于优化模型生成符合人类期望的输出。</li><li><strong>KL散度</strong>，用于约束模型不要偏离参考模型。</li></ol><h4 id="3-1-KL散度的作用"><a href="#3-1-KL散度的作用" class="headerlink" title="3.1 KL散度的作用"></a><strong>3.1 KL散度的作用</strong></h4><p>KL散度（Kullback-Leibler Divergence）衡量的是两个概率分布之间的差异。在模型对齐中，KL散度用于限制当前模型 \(\pi<em>\theta\) 和参考模型 \(\pi</em>{\text{ref}}\) 的分布差异，确保在优化过程中模型的输出不会过度偏离预训练模型。具体公式为：</p><script type="math/tex; mode=display">\text{KL}(\pi_\theta(y|x) \| \pi_{\text{ref}}(y|x)) = \sum_y \pi_\theta(y|x) \log \frac{\pi_\theta(y|x)}{\pi_{\text{ref}}(y|x)}</script><ul><li>如果KL散度较大，表示当前模型生成的分布与参考模型有较大的差异，这可能意味着模型生成了不合理的输出。</li><li>通过最小化KL散度，模型能够在保证输出合理性的基础上，进行进一步的优化。</li></ul><h4 id="3-2-损失函数公式"><a href="#3-2-损失函数公式" class="headerlink" title="3.2 损失函数公式"></a><strong>3.2 损失函数公式</strong></h4><p>根据偏好或标签，模型的损失函数可以表达为以下形式：</p><h5 id="DPO中的损失函数："><a href="#DPO中的损失函数：" class="headerlink" title="DPO中的损失函数："></a><strong>DPO中的损失函数</strong>：</h5><script type="math/tex; mode=display">L_{DPO} = -\mathbb{E}_{x, y_w, y_l \sim D} [\log \sigma(\beta (\log(\pi_\theta(y_w|x)) - \log(\pi_\theta(y_l|x))))]</script><ul><li>$y_w$：偏好较高的答案。</li><li>$y_l$：偏好较低的答案。</li></ul><p>DPO中可以引入KL散度作为正则化项：</p><script type="math/tex; mode=display">L_{DPO} = -\log \sigma(\log(\pi_\theta(y_w|x)) - \log(\pi_\theta(y_l|x)) + \beta \cdot \text{KL}(\pi_\theta \| \pi_{\text{ref}}))</script><p>通过控制KL散度，模型的输出不会偏离参考模型太多。</p><h5 id="KTO中的损失函数："><a href="#KTO中的损失函数：" class="headerlink" title="KTO中的损失函数："></a><strong>KTO中的损失函数</strong>：</h5><p>KTO的损失函数基于前景理论，并将KL散度作为核心部分，表达为：</p><script type="math/tex; mode=display">L_{KTO} = \lambda_U \cdot \sigma(\beta \cdot (\text{KL}(\pi_\theta(\text{Answer 2}) \| \pi_{\text{ref}}) - r_{\theta}(\text{Answer 2})))</script><ul><li>$r_{\theta}(x, y)$：当前策略对负样本（错误答案）的置信度。</li><li>KL散度用于衡量当前模型与参考模型的差异，确保模型在减少负样本生成的同时，不偏离原始参考模型。</li></ul><p>通过增加负样本的损失（即增加 $\lambda_U$ 的值），模型会降低负样本的置信度，使未来生成类似错误答案的概率变小。</p><hr><h3 id="4-如何优化模型"><a href="#4-如何优化模型" class="headerlink" title="4. 如何优化模型"></a><strong>4. 如何优化模型</strong></h3><p>通过上面介绍的损失函数，模型的优化通常是通过<strong>梯度下降</strong>（Gradient Descent）来完成的。损失函数的梯度反映了模型输出与期望输出之间的差异，优化目标是最小化损失函数。</p><p><strong>梯度更新公式</strong>：</p><script type="math/tex; mode=display">\theta_{\text{new}} = \theta_{\text{old}} - \eta \nabla_{\theta} L</script><p>其中：</p><ul><li>$\eta$ 是学习率，决定每次参数更新的步长。</li><li>$\nabla_{\theta} L$ 是损失函数对模型参数的梯度，表示当前参数对损失的贡献。</li></ul><p>通过不断迭代，模型会逐渐提高生成正样本的概率，减少负样本的生成概率，最终实现模型对齐。</p><ul><li>模型对齐（Alignment）的核心目标是通过偏好或标签数据，优化模型的输出，使其符合人类期望。</li><li><strong>策略模型</strong>（$\pi_\theta$）生成输出，KL散度用于控制模型与参考模型的偏离程度，避免模型在优化过程中产生不合理的偏差。</li><li><strong>正样本的概率</strong>通过损失函数的优化逐步提升，<strong>负样本的概率</strong>通过增加损失权重和降低置信度来减少。</li><li>梯度下降用于更新模型参数，最终实现模型对齐</li></ul><hr><h2 id="4-DPO（Direct-Preference-Optimization）"><a href="#4-DPO（Direct-Preference-Optimization）" class="headerlink" title="4. DPO（Direct Preference Optimization）"></a>4. <strong>DPO（Direct Preference Optimization）</strong></h2><h3 id="4-1-原理"><a href="#4-1-原理" class="headerlink" title="4.1 原理"></a>4.1 <strong>原理</strong></h3><p>DPO 通过直接优化模型输出的偏好函数，使模型的输出更加符合人类偏好。它比较模型的不同输出，并通过偏好函数评估这两个输出哪个更好，从而指导模型参数的优化。</p><h3 id="4-2-损失函数"><a href="#4-2-损失函数" class="headerlink" title="4.2 损失函数"></a>4.2 <strong>损失函数</strong></h3><p>DPO 使用偏好损失函数（Preference Loss），用于比较两个输出的优劣：</p><script type="math/tex; mode=display">\mathcal{L}_{\text{DPO}} = \log(1 + \exp(-\sigma \cdot (\hat{y}_a - \hat{y}_b) \cdot p))</script><ul><li>$ \hat{y}_a $ 和 $ \hat{y}_b $ 是模型对两个样本的预测值。</li><li>$ p $ 是人类偏好（1 表示偏好 $a$，-1 表示偏好 $b$）。</li><li>$ \sigma $ 是平滑参数。</li></ul><h3 id="4-3-优化器"><a href="#4-3-优化器" class="headerlink" title="4.3 优化器"></a>4.3 <strong>优化器</strong></h3><p>DPO 通常使用 <strong>AdamW</strong> 优化器，适用于大规模参数模型的优化，代码如下：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.AdamW(model.parameters(), lr=<span class="number">1e-5</span>)</span><br></pre></td></tr></tbody></table></figure><h3 id="4-4-核心代码"><a href="#4-4-核心代码" class="headerlink" title="4.4 核心代码"></a>4.4 <strong>核心代码</strong></h3><p>以下是 DPO 的训练步骤：<br></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preference_loss</span>(<span class="params">output_a, output_b, human_preference</span>):</span><br><span class="line">    preference = human_preference(output_a, output_b)</span><br><span class="line">    loss = torch.log(<span class="number">1</span> + torch.exp(-preference * (output_a - output_b)))</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dpo_training_step</span>(<span class="params">model, data_a, data_b, human_preference</span>):</span><br><span class="line">    output_a = model(data_a)</span><br><span class="line">    output_b = model(data_b)</span><br><span class="line">    </span><br><span class="line">    loss = preference_loss(output_a, output_b, human_preference)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> batch_a, batch_b <span class="keyword">in</span> training_data:</span><br><span class="line">    dpo_training_step(model, batch_a, batch_b, human_preference_function)</span><br></pre></td></tr></tbody></table></figure><p></p><hr><h2 id="5-KTO（Kahneman-Tversky-Optimization）"><a href="#5-KTO（Kahneman-Tversky-Optimization）" class="headerlink" title="5. KTO（Kahneman-Tversky Optimization）"></a>5. <strong>KTO（Kahneman-Tversky Optimization）</strong></h2><h3 id="5-1-原理"><a href="#5-1-原理" class="headerlink" title="5.1 原理"></a>5.1 <strong>原理</strong></h3><p>KTO 基于 Kahneman 和 Tversky 的前景理论（Prospect Theory），通过非对称效用函数衡量模型的增益和损失，旨在优化模型的表现，尤其在风险和收益不对称的场景下。效用函数定义如下：</p><script type="math/tex; mode=display">\mathcal{U}(x) =\begin{cases}x^{\alpha}, & x \geq 0 \\-\lambda (-x)^{\alpha}, & x < 0\end{cases}</script><ul><li>$x$ 是模型预测与真实值的差异。</li><li>$\alpha$ 是非线性系数，通常为 0</li></ul><p>.88。</p><ul><li>$\lambda$ 是损失的惩罚权重，通常为 2.25。</li></ul><h3 id="5-2-损失函数"><a href="#5-2-损失函数" class="headerlink" title="5.2 损失函数"></a>5.2 <strong>损失函数</strong></h3><p>KTO 的损失函数基于前景理论的效用函数，用于惩罚模型的预测误差：</p><script type="math/tex; mode=display">\mathcal{L}_{\text{KTO}} = -\mathbb{E}[\mathcal{U}(y_{\text{pred}} - y_{\text{true}})]</script><h3 id="5-3-优化器"><a href="#5-3-优化器" class="headerlink" title="5.3 优化器"></a>5.3 <strong>优化器</strong></h3><p>KTO 常使用 <strong>AdamW</strong> 优化器，以确保训练过程的稳定性：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.AdamW(model.parameters(), lr=<span class="number">3e-5</span>)</span><br></pre></td></tr></tbody></table></figure><h3 id="5-4-核心代码"><a href="#5-4-核心代码" class="headerlink" title="5.4 核心代码"></a>5.4 <strong>核心代码</strong></h3><p>以下是 KTO 损失函数的计算代码：<br></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">prospect_utility</span>(<span class="params">value, alpha=<span class="number">0.88</span>, lambda_=<span class="number">2.25</span></span>):</span><br><span class="line">    <span class="keyword">if</span> value &gt;= <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> torch.<span class="built_in">pow</span>(value, alpha)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> -lambda_ * torch.<span class="built_in">pow</span>(-value, alpha)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">kto_loss</span>(<span class="params">predictions, targets</span>):</span><br><span class="line">    value = predictions - targets</span><br><span class="line">    utility = prospect_utility(value)</span><br><span class="line">    loss = -torch.mean(utility)</span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">    predictions = model(data)</span><br><span class="line">    loss = kto_loss(predictions, targets)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></tbody></table></figure><p></p><hr><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a><strong>总结</strong></h3><div class="table-container"><table><thead><tr><th>方法</th><th>损失函数</th><th>优化器</th></tr></thead><tbody><tr><td><strong>SFT</strong></td><td>交叉熵损失</td><td>AdamW，RMSprop，SGD</td></tr><tr><td><strong>LoRA</strong></td><td>交叉熵损失</td><td>AdamW，RMSprop，SGD</td></tr><tr><td><strong>DPO</strong></td><td>偏好损失函数： $\log(1 + \exp(-\sigma (\hat{y}_a - \hat{y}_b)p))$</td><td>AdamW</td></tr><tr><td><strong>KTO</strong></td><td>前景理论效用函数： $-\mathbb{E}[\mathcal{U}(y<em>{\text{pred}} - y</em>{\text{true}})]$</td><td>AdamW</td></tr></tbody></table></div><p>通过本文档的整理，读者能够清晰理解 SFT、LoRA、DPO 和 KTO 等技术的原理、具体实现步骤、损失函数设计和优化器选择，特别是在 LLAMA3 这种大规模预训练模型的微调场景下的实际应用。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;LoRA-DPO-KTO-与-SFT-技术详解&quot;&gt;&lt;a href=&quot;#LoRA-DPO-KTO-与-SFT-技术详解&quot; class=&quot;headerlink&quot; title=&quot;LoRA, DPO, KTO 与 SFT 技术详解&quot;&gt;&lt;/a&gt;&lt;strong&gt;LoRA, D</summary>
      
    
    
    
    <category term="NLP Insights" scheme="https://chenhuiyu.github.io/categories/NLP-Insights/"/>
    
    
    <category term="LLM" scheme="https://chenhuiyu.github.io/tags/LLM/"/>
    
  </entry>
  
  <entry>
    <title>使用压缩有限状态机进行本地 LLM 的快速 JSON 解码</title>
    <link href="https://chenhuiyu.github.io/2024/08/13/NLP%20Insights/%E4%BD%BF%E7%94%A8%E5%8E%8B%E7%BC%A9%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA%E8%BF%9B%E8%A1%8C%E6%9C%AC%E5%9C%B0%20LLM%20%E7%9A%84%E5%BF%AB%E9%80%9F%20JSON%20%E8%A7%A3%E7%A0%81/"/>
    <id>https://chenhuiyu.github.io/2024/08/13/NLP%20Insights/%E4%BD%BF%E7%94%A8%E5%8E%8B%E7%BC%A9%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA%E8%BF%9B%E8%A1%8C%E6%9C%AC%E5%9C%B0%20LLM%20%E7%9A%84%E5%BF%AB%E9%80%9F%20JSON%20%E8%A7%A3%E7%A0%81/</id>
    <published>2024-08-13T08:12:10.000Z</published>
    <updated>2024-08-13T08:30:07.807Z</updated>
    
    <content type="html"><![CDATA[<h2 id="使用压缩有限状态机进行本地-LLM-的快速-JSON-解码"><a href="#使用压缩有限状态机进行本地-LLM-的快速-JSON-解码" class="headerlink" title="使用压缩有限状态机进行本地 LLM 的快速 JSON 解码"></a>使用压缩有限状态机进行本地 LLM 的快速 JSON 解码</h2><p><strong>作者</strong>: Liangsheng Yin, Ying Sheng, Lianmin Zheng<br><strong>日期</strong>: 2024 年 2 月 5 日</p><hr><p>本文内容基于 LMSYS Org 发布的一篇博客文章，原文链接：<a href="https://lmsys.org/blog/2024-02-05-compressed-fsm/">LMSYS Org 博客</a>。相关的代码库可以在以下链接找到：<a href="https://github.com/sgl-project/sglang/tree/main?tab=readme-ov-file#json-decoding">SGLang 代码库</a>。</p><p>让一个 LLM 始终生成符合特定模式的有效 JSON 或 YAML，对于许多应用来说是一个关键特性。在这篇博客文章中，我们介绍了一种显著加速这种约束解码的优化方法。我们的方法利用了压缩的有限状态机，并且兼容任何正则表达式，因此可以适用于任何 JSON 或 YAML 模式。与现有系统逐步解码一个标记的方式不同，我们的方法分析了正则表达式的有限状态机，压缩了单一的转换路径，并在可能的情况下一次性解码多个标记。与最先进的系统（guidance + llama.cpp，outlines + vLLM）相比，我们的方法可以将延迟减少最多 2 倍，并提高吞吐量最多 2.5 倍。这一优化还使得约束解码比普通解码更快。你可以在 SGLang 上试用它。</p><p><img src="https://lmsys.org/images/blog/compressed_fsm/demo.gif" alt="图1：SGLang和Outlines + vLLM在JSON解码中的比较"></p><p>图一展示了 SGLang 和 Outlines + vLLM 在 JSON 解码任务中的性能比较。这是一个动态对比，目的是展示两者在相同任务下的速度差异。SGLang 采用了一种新的跳跃前进解码算法，通过压缩有限状态机来加速解码过程。相比之下，Outlines + vLLM 使用了传统的逐步解码方法。图中的动画演示了 SGLang 在处理多字符（或标记）解码时的优势，显著减少了解码时间。</p><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>JSON 是数据交换中最重要的格式之一。要求 LLM 始终生成有效的 JSON 可以使 LLM 的输出以结构化方式轻松解析。认识到其重要性，OpenAI 引入了 JSON 模式，它约束模型始终返回有效的 JSON 对象。然而，通常需要更细粒度的控制，以确保生成的 JSON 对象符合特定的模式，例如：</p><img src="/2024/08/13/NLP%20Insights/%E4%BD%BF%E7%94%A8%E5%8E%8B%E7%BC%A9%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA%E8%BF%9B%E8%A1%8C%E6%9C%AC%E5%9C%B0%20LLM%20%E7%9A%84%E5%BF%AB%E9%80%9F%20JSON%20%E8%A7%A3%E7%A0%81/image-1.png" class="" title="图2：遵循JSON模式的约束生成示例"><p>图二展示了一个受限生成的例子，利用大语言模型（LLMs）来生成符合特定 JSON 模式的对象。在这个例子中，左侧的 JSON 模式定义了一个对象，其中包含了 name、age 和 house 三个属性，分别是字符串和整数类型。右侧则显示了受限生成的输出对象，模型通过约束生成技术，生成了符合这些属性的具体实例，如“Harry”的名字、15 岁的年龄以及属于“Gryffindor”的房子。这展示了 LLMs 在生成结构化数据时的能力，同时确保了生成内容符合预定的格式。</p><p>对于本地 LLM，有两种主要方法来引导模型生成符合特定模式的 JSON 对象。</p><h3 id="方法-1：基于有限状态机"><a href="#方法-1：基于有限状态机" class="headerlink" title="方法 1：基于有限状态机"></a>方法 1：基于有限状态机</h3><p>这种方法涉及将 JSON 模式转换为正则表达式。然后，我们可以基于正则表达式构建一个有限状态机（FSM）。FSM 用于引导 LLM 的生成。在 FSM 的每个状态中，我们可以计算允许的转换并识别可接受的下一个标记。这使我们能够在解码过程中跟踪当前状态，并通过对输出应用 logit 偏差来过滤掉无效的标记。你可以在 outlines 论文中了解更多关于这种方法的信息。</p><img src="/2024/08/13/NLP%20Insights/%E4%BD%BF%E7%94%A8%E5%8E%8B%E7%BC%A9%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA%E8%BF%9B%E8%A1%8C%E6%9C%AC%E5%9C%B0%20LLM%20%E7%9A%84%E5%BF%AB%E9%80%9F%20JSON%20%E8%A7%A3%E7%A0%81/image-2.png" class="" title="图3：基于FSM和Logits屏蔽的约束解码。在第一次约束解码过程中，仅允许age。在第二次过程中，由于正则表达式需要数字，因此允许0和1，但LLM更有可能采样1"><p>图三展示了如何利用有限状态机（FSM）来实现受限解码。在这个过程中，首先将 JSON 模式转换为正则表达式，然后利用 FSM 来引导 LLM 的生成。在图中，FSM 状态图展示了 age 字段的受限生成过程，其中只有合法的数字（如 0-9）会被允许。每个状态的转换由正则表达式的规则定义，确保生成的 JSON 数据始终有效。这种方法通过在生成过程中施加限制，来控制 LLM 生成特定的输出。</p><p>FSM 方法利用广义的正则表达式来定义低层次规则，可以应用于广泛的语法，例如 JSON 模式、IP 地址和电子邮件。</p><h4 id="限制："><a href="#限制：" class="headerlink" title="限制："></a>限制：</h4><p>由于 FSM 是在标记级别构建的，因此它只能在每一步通过一个标记来转换状态。因此，它一次只能解码一个标记，导致解码速度较慢。</p><h3 id="方法-2：基于交织"><a href="#方法-2：基于交织" class="headerlink" title="方法 2：基于交织"></a>方法 2：基于交织</h3><p>除了将整个 JSON 模式转换为正则表达式之外，另一种方法是使用基于交织的解码。在这种方法中，给定的 JSON 模式可以分解为几个部分，每个部分包含一个分块预填充部分或一个约束解码部分。这些不同的部分由推理系统交织执行。由于分块预填充可以在一个前向传递中处理多个标记，它比逐标记解码更快。</p><p>Guidance 提供了一套基于交织解码的语法规则，使用 llama.cpp 作为后端。</p><img src="/2024/08/13/NLP%20Insights/%E4%BD%BF%E7%94%A8%E5%8E%8B%E7%BC%A9%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA%E8%BF%9B%E8%A1%8C%E6%9C%AC%E5%9C%B0%20LLM%20%E7%9A%84%E5%BF%AB%E9%80%9F%20JSON%20%E8%A7%A3%E7%A0%81/image-3.png" class="" title="图4：Guidance中的交织JSON解码"><p>图四展示了 Guidance 框架中的交织语法，如何利用交织语法来进行 JSON 的解码。图中的代码片段定义了一个函数，使用 Guidance 语法生成一个包含 name、age 和 house 的 JSON 对象。交织语法通过将不同部分的解码与预填充部分交替进行，能够提高解码速度。图下方展示了这一过程的工作原理，绿色和蓝色的条形代表不同部分的处理过程，展示了交织解码在不同阶段的执行情况。</p><h4 id="限制：-1"><a href="#限制：-1" class="headerlink" title="限制："></a>限制：</h4><ul><li>基于交织的方法需要自定义语法，使其不如单个正则表达式灵活和表达力强。</li><li>由于解码和分块预填充段之间可能存在冲突，处理标记边界时存在困难。</li><li>解释器与后端之间的频繁通信带来了额外的开销。</li></ul><h3 id="我们的方法：使用压缩有限状态机的跳跃前进解码"><a href="#我们的方法：使用压缩有限状态机的跳跃前进解码" class="headerlink" title="我们的方法：使用压缩有限状态机的跳跃前进解码"></a>我们的方法：使用压缩有限状态机的跳跃前进解码</h3><p>通过引入基于压缩有限状态机的新解码算法——跳跃前进解码，我们可以结合 FSM 和交织方法的优点。</p><p>在由 JSON 模式转换的正则表达式引导的解码过程中，当我们达到特定节点时，可以预测即将到来的字符串：</p><p>在图 3 中，解码开始时，根据正则表达式，我们可以预见到接下来的字符串是：</p><figure class="highlight json"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">{</span></span><br><span class="line">  <span class="attr">"name"</span><span class="punctuation">:</span></span><br></pre></td></tr></tbody></table></figure><p>然后进入实际的解码部分。<br>同样，当 LLM 在为角色填写房子属性时输出了 G，我们可以自信地预测下一个字符串将是 ryffindor，从而完成整个字符串为 Gryffindor。</p><p>这正是跳跃前进解码算法加速解码的方式。在跳跃前进算法中，我们检查给定正则表达式的有限状态机，识别所有单一的转换边，并将连续的转换路径压缩为单一路径。我们可以直接预填充（扩展）这些单一路径，跳过逐标记解码，直到下一个分支点。</p><img src="/2024/08/13/NLP%20Insights/%E4%BD%BF%E7%94%A8%E5%8E%8B%E7%BC%A9%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA%E8%BF%9B%E8%A1%8C%E6%9C%AC%E5%9C%B0%20LLM%20%E7%9A%84%E5%BF%AB%E9%80%9F%20JSON%20%E8%A7%A3%E7%A0%81/image-4.png" class="" title="图5：跳跃前进解码与压缩FSM和普通解码的比较"><p>图五展示了跳跃前进解码与普通解码的对比。跳跃前进解码利用压缩的有限状态机，通过提前预测并预填充可能的字符串，减少了逐标记解码的次数。例如，在为 house 字段生成值时，模型在解码过程中直接跳跃并预填充了“Gryffindor”这个字符串，而无需逐字符生成。图中的流程展示了如何通过这种方法提高解码效率，同时避免了不必要的重复计算。<br>图五展示了<strong>压缩有限状态机的跳跃前进解码</strong>与<strong>普通解码</strong>的对比，特别是在生成 JSON 数据时的性能差异。为了更详细地理解这张图，我们需要分步骤分析图中的各个部分。</p><ol><li><p><strong>输入提示</strong>（左侧的绿色部分）：提示模型生成一个符合 JSON 模式的对象。这里的 JSON 对象包括“name”、“age”和“house”三个属性，分别代表名字、年龄和学院。</p></li><li><p><strong>跳跃前进解码过程</strong>（中间部分的蓝色和橙色方块）：</p><ul><li><strong>橙色方块</strong>代表需要约束解码的部分。例如，生成“name”属性时，模型通过跳跃前进解码算法可以直接生成完整的字符串“Harry”。</li><li><strong>蓝色方块</strong>代表模型在跳跃前进过程中逐字符（或逐标记）解码的部分。这种解码方式在遇到非确定性时（例如多个可能的值）才会出现。</li></ul></li><li><p><strong>普通解码过程</strong>（中间部分的蓝色方块）：普通解码需要逐字符或逐标记地生成整个 JSON 对象。相比之下，普通解码方式在处理每一个字符或标记时都需要进行预测和选择，显著降低了解码速度。</p></li><li><p><strong>对比结果</strong>（右侧部分）：</p><ul><li><strong>跳跃前进解码</strong>生成的 JSON 对象展示在最上方，这种方法通过预测并预填充可能的字符串，大大加速了解码过程。例如，在生成“Gryffindor”这个字符串时，模型直接跳过了逐字符生成的步骤。</li><li><strong>普通解码</strong>生成的 JSON 对象展示在最下方，这种方法逐字符解码，虽然能够保证生成的准确性，但效率较低，尤其是在处理长字符串或复杂结构时。</li></ul></li></ol><h3 id="详细解读："><a href="#详细解读：" class="headerlink" title="详细解读："></a>详细解读：</h3><ol><li><p><strong>跳跃前进解码的工作原理</strong>：</p><ul><li>在解码的过程中，模型使用压缩后的有限状态机（FSM）来预测和识别即将生成的字符串。如果模型能在当前上下文中准确预测出接下来要生成的字符串，那么它可以跳过这些字符串的逐标记解码，直接生成整个字符串（例如“Gryffindor”）。</li><li>这种方法利用了正则表达式的结构特点，将连续的转换路径压缩成一个单一路径，从而避免了不必要的逐标记解码步骤。</li></ul></li><li><p><strong>普通解码的限制</strong>：</p><ul><li>普通解码方法需要逐步解码每一个字符或标记，因此在处理复杂的 JSON 对象时效率较低。每一步都需要模型重新计算可能的输出，并从中选择最优解，这会大幅增加解码时间。</li></ul></li><li><p><strong>性能差异</strong>：</p><ul><li>由于跳跃前进解码减少了逐字符解码的次数，并且利用了 FSM 的压缩特性，它在时间和计算资源上的开销都显著低于普通解码。尤其在需要生成大量数据或处理复杂结构时，跳跃前进解码的优势更加明显。</li></ul></li></ol><p>SGLang 的 RadixAttention 机制极大地简化了跳跃前进解码算法的实现。当执行跳跃前进时，我们可以简单地终止当前请求并排入新请求。SGLang 运行时的 RadixAttention 和高效的扩展原语将自动重用前一组标记的 KV 缓存，从而避免冗余计算。</p><h2 id="标记边界处理"><a href="#标记边界处理" class="headerlink" title="标记边界处理"></a>标记边界处理</h2><p>在实现约束解码时，由于字符与标记之间复杂的可能映射关系，处理标记边界总是很棘手。</p><p>在 LLM 解码过程中，它可能更倾向（意味着概率更高）于将多个字符组合成一个标记。例如，在 JSON 解码的上下文中解码”Hello”时，LLM 可能会输出如下标记：<br>“ He llo “，</p><p>而不是解码最后的” ，它总是倾向于将其与后续字符组合成更常见的标记”， 这种效果可能导致一些奇怪的行为。例如，在上述情况下，如果正则表达式设置为”[\w\d\s]*“（不包含最后的”， ），这可能会导致无限解码，因为 LLM 想要停止于”，但该标记是不允许的。</p><p>此外，在跳跃前进解码过程中，我们发现对跳跃前进部分使用不同的标记策略可能会导致后续标记的 logit 分布不同。简单地将标记化的跳跃前进部分附加到当前的标记序列中可能会产生意外的结果。</p><p>为了解决这些问题，我们提出了以下解决方案：</p><ul><li>我们在跳跃前进阶段实施了重新标记化机制。这包括附加字符串而不是标记，然后重新标记整个文本。这种方法有效地解决了大多数标记化问题，并且仅导致计算开销增加约 4%。</li><li><strong>建议</strong>使用综合正则表达式引导整个解码过程，而不是使用多个连接的正则表达式。这种方法确保 FSM 和 LLM 都了解整个解码过程，从而尽量减少与边界相关的问题。<br>你还可以在这篇博客文章中阅读一些额外的讨论。</li></ul><h2 id="基准测试结果"><a href="#基准测试结果" class="headerlink" title="基准测试结果"></a>基准测试结果</h2><p>我们在两个任务上对我们的跳跃前进解码进行了基准测试：</p><ol><li>使用简短的提示生成 JSON 格式的角色数据。</li><li>从长文档中提取城市信息并以 JSON 格式输出。</li></ol><p>我们在 NVIDIA A10 GPU（24GB）上测试了 llama-7B，使用了 vllm v0.2.7，guidance v0.1.0，outlines v0.2.5 和 llama.cpp v0.2.38（Python 绑定）。下图显示了这些方法的吞吐量（使用每个系统支持的最大批次大小）和延迟（批次大小为 1）：</p><img src="/2024/08/13/NLP%20Insights/%E4%BD%BF%E7%94%A8%E5%8E%8B%E7%BC%A9%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA%E8%BF%9B%E8%A1%8C%E6%9C%AC%E5%9C%B0%20LLM%20%E7%9A%84%E5%BF%AB%E9%80%9F%20JSON%20%E8%A7%A3%E7%A0%81/image-5.png" class="" title="图6：基准测试结果"><p>结果表明，使用我们的解码算法的 SGLang 显著优于所有其他系统。它可以将延迟减少最多 2 倍，并将吞吐量提高最多 2.5 倍。在角色生成任务中，即使不使用跳跃前进的 SGLang 也比 Outlines+vLLM 实现了更高的吞吐量；我们怀疑这是由于 Outlines 中的某些开销所致。</p><h2 id="用例"><a href="#用例" class="headerlink" title="用例"></a>用例</h2><p>我们已经与 Boson.ai 测试了这个功能两周，他们正在将这个功能引入他们的生产用例中，因为它保证了更高的解码吞吐量和可靠的响应。</p><p>此外，另一位用户使用此功能通过视觉语言模型 LLaVA 从图像中提取结构化信息。</p><img src="/2024/08/13/NLP%20Insights/%E4%BD%BF%E7%94%A8%E5%8E%8B%E7%BC%A9%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA%E8%BF%9B%E8%A1%8C%E6%9C%AC%E5%9C%B0%20LLM%20%E7%9A%84%E5%BF%AB%E9%80%9F%20JSON%20%E8%A7%A3%E7%A0%81/image-6.png" class="" title="图7：使用SGLang和LLaVA从图像中提取结构化信息">]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;使用压缩有限状态机进行本地-LLM-的快速-JSON-解码&quot;&gt;&lt;a href=&quot;#使用压缩有限状态机进行本地-LLM-的快速-JSON-解码&quot; class=&quot;headerlink&quot; title=&quot;使用压缩有限状态机进行本地 LLM 的快速 JSON 解码&quot;&gt;&lt;/a</summary>
      
    
    
    
    <category term="NLP Insights" scheme="https://chenhuiyu.github.io/categories/NLP-Insights/"/>
    
    
    <category term="SGLang" scheme="https://chenhuiyu.github.io/tags/SGLang/"/>
    
    <category term="Structured LLM" scheme="https://chenhuiyu.github.io/tags/Structured-LLM/"/>
    
  </entry>
  
  <entry>
    <title>Detailed Steps for Running Fine-tuned Gemma-2-2b-it with vLLM</title>
    <link href="https://chenhuiyu.github.io/2024/08/07/NLP%20Insights/Running%20Fine-tuned%20Gemma-2-2b-it%20with%20vLLM/"/>
    <id>https://chenhuiyu.github.io/2024/08/07/NLP%20Insights/Running%20Fine-tuned%20Gemma-2-2b-it%20with%20vLLM/</id>
    <published>2024-08-07T02:30:00.000Z</published>
    <updated>2024-08-07T11:24:45.021Z</updated>
    
    <content type="html"><![CDATA[<p>In this post, I will share the steps to run the fine-tuned Gemma-2-2b-it model using vLLM. This guide will cover the installation process, environment configuration, and common troubleshooting tips.</p><h2 id="Installation-and-Verification-of-vLLM"><a href="#Installation-and-Verification-of-vLLM" class="headerlink" title="Installation and Verification of vLLM"></a>Installation and Verification of vLLM</h2><p>First, ensure that you have installed and verified vLLM version 0.5.3.</p><ol><li><p>Install vLLM:</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!pip install vllm==0.5.3</span><br></pre></td></tr></tbody></table></figure></li><li><p>Verify the installation:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> vllm</span><br><span class="line"><span class="built_in">print</span>(vllm.__version__)</span><br><span class="line"><span class="comment"># Output: 0.5.3</span></span><br></pre></td></tr></tbody></table></figure></li></ol><h2 id="Installing-Flashinfer"><a href="#Installing-Flashinfer" class="headerlink" title="Installing Flashinfer"></a>Installing Flashinfer</h2><p>Follow these steps to install Flashinfer, ensuring compatibility with your torch version and CUDA.</p><ol><li><p>Check the torch version and CUDA compatibility:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="built_in">print</span>(torch.__version__)  <span class="comment"># Should output: 2.3.1+cu121</span></span><br><span class="line"><span class="built_in">print</span>(torch.version.cuda) <span class="comment"># Should output: 12.1</span></span><br></pre></td></tr></tbody></table></figure></li><li><p>Install Flashinfer:<br>According to the documentation, Gemma runs on version 0.0.8. vLLM requires FlashInfer v0.0.8 (refer to <a href="https://github.com/vllm-project/vllm/issues/7060">vLLM Version and Flashinfer Documentation</a> for details on Gemma 2).</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!pip install flashinfer==0.0.8 -i https://flashinfer.ai/whl/cu121/torch2.3/</span><br></pre></td></tr></tbody></table></figure></li></ol><h2 id="Updating-Environment-Variables-for-vLLM-Backend"><a href="#Updating-Environment-Variables-for-vLLM-Backend" class="headerlink" title="Updating Environment Variables for vLLM Backend"></a>Updating Environment Variables for vLLM Backend</h2><p>Ensure that Flashinfer is set as the attention mechanism backend for vLLM:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">"VLLM_ATTENTION_BACKEND"</span>] = <span class="string">"FLASHINFER"</span></span><br></pre></td></tr></tbody></table></figure><h2 id="Testing-vLLM"><a href="#Testing-vLLM" class="headerlink" title="Testing vLLM"></a>Testing vLLM</h2><p>Here is the test code to generate text using vLLM:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> vllm <span class="keyword">import</span> LLM, SamplingParams</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">llm = LLM(model=<span class="string">"gemma-2-2b-model"</span>, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">sampling_params = SamplingParams(</span><br><span class="line">    temperature=<span class="number">0.8</span>,</span><br><span class="line">    max_tokens=<span class="number">512</span>,</span><br><span class="line">    top_p=<span class="number">0.95</span>,</span><br><span class="line">    top_k=<span class="number">1</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Example test data</span></span><br><span class="line">test_data = [{<span class="string">"text"</span>: <span class="string">"Input test text 1"</span>}, {<span class="string">"text"</span>: <span class="string">"Input test text 2"</span>}]</span><br><span class="line"></span><br><span class="line">prompts = [</span><br><span class="line">    test_data[random.randint(<span class="number">0</span>, <span class="built_in">len</span>(test_data) - <span class="number">1</span>)][<span class="string">"text"</span>],</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">outputs = llm.generate(</span><br><span class="line">    prompts,</span><br><span class="line">    sampling_params</span><br><span class="line">)</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><p>By following these steps, you should be able to successfully run the fine-tuned Gemma-2-2b-it model.</p><h2 id="Common-Errors-and-Solutions"><a href="#Common-Errors-and-Solutions" class="headerlink" title="Common Errors and Solutions"></a>Common Errors and Solutions</h2><p>Here are some common errors you might encounter and their solutions:</p><ol><li><p><strong>RuntimeError: <code>CHECK_EQ(paged_kv_indptr.size(0), batch_size + 1) failed. 1 vs 257</code></strong></p><ul><li><strong>Cause</strong>: Incorrect Flashinfer version.</li><li><strong>Solution</strong>: Ensure you have installed the correct version of Flashinfer.</li></ul></li><li><p><strong>TypeError: <code>'NoneType' object is not callable</code></strong></p><ul><li><strong>Cause</strong>: Flashinfer is not installed.</li><li><strong>Solution</strong>: Install Flashinfer following the steps above.</li></ul></li><li><p><strong>ValueError: <code>Please use Flashinfer backend for models with logits_soft_cap (i.e., Gemma-2). Otherwise, the output might be wrong. Set Flashinfer backend by export VLLM_ATTENTION_BACKEND=FLASHINFER.</code></strong></p><ul><li><strong>Cause</strong>: Flashinfer backend is not set.</li><li><strong>Solution</strong>: Set the environment variable <code>VLLM_ATTENTION_BACKEND</code> to <code>FLASHINFER</code>.</li></ul></li></ol><p>By following these detailed steps and solutions, you should be able to successfully run and debug the fine-tuned Gemma-2-2b-it model. If you encounter any issues, refer to the relevant documentation or seek help from the community.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;In this post, I will share the steps to run the fine-tuned Gemma-2-2b-it model using vLLM. This guide will cover the installation process</summary>
      
    
    
    
    <category term="NLP Insights" scheme="https://chenhuiyu.github.io/categories/NLP-Insights/"/>
    
    
    <category term="vLLM" scheme="https://chenhuiyu.github.io/tags/vLLM/"/>
    
    <category term="Gemma-2-2b-it" scheme="https://chenhuiyu.github.io/tags/Gemma-2-2b-it/"/>
    
  </entry>
  
  <entry>
    <title>使用vLLM运行微调后的Gemma-2</title>
    <link href="https://chenhuiyu.github.io/2024/08/07/NLP%20Insights/%E4%BD%BF%E7%94%A8vLLM%E8%BF%90%E8%A1%8C%E5%BE%AE%E8%B0%83%E5%90%8E%E7%9A%84Gemma-2/"/>
    <id>https://chenhuiyu.github.io/2024/08/07/NLP%20Insights/%E4%BD%BF%E7%94%A8vLLM%E8%BF%90%E8%A1%8C%E5%BE%AE%E8%B0%83%E5%90%8E%E7%9A%84Gemma-2/</id>
    <published>2024-08-07T02:30:00.000Z</published>
    <updated>2024-08-07T11:30:32.047Z</updated>
    
    <content type="html"><![CDATA[<h1 id="使用vLLM运行微调后的Gemma-2-2b-it的详细步骤"><a href="#使用vLLM运行微调后的Gemma-2-2b-it的详细步骤" class="headerlink" title="使用vLLM运行微调后的Gemma-2-2b-it的详细步骤"></a>使用vLLM运行微调后的Gemma-2-2b-it的详细步骤</h1><p>在这里分享一下我运行微调后的Gemma-2-2b-it模型并使用vLLM的步骤，希望对其他人有所帮助。本文将详细介绍安装过程、环境配置以及常见问题的解决方法。</p><h2 id="安装和验证vLLM"><a href="#安装和验证vLLM" class="headerlink" title="安装和验证vLLM"></a>安装和验证vLLM</h2><p>首先，确保安装并验证vLLM的版本是0.5.3。</p><ol><li><p>安装vLLM：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install vllm==0.5.3</span><br></pre></td></tr></tbody></table></figure></li><li><p>验证安装：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> vllm</span><br><span class="line"><span class="built_in">print</span>(vllm.__version__)</span><br><span class="line"><span class="comment"># 输出: 0.5.3</span></span><br></pre></td></tr></tbody></table></figure></li></ol><h2 id="安装Flashinfer"><a href="#安装Flashinfer" class="headerlink" title="安装Flashinfer"></a>安装Flashinfer</h2><p>按照以下步骤安装Flashinfer，并确保您的torch版本和CUDA兼容性。</p><ol><li><p>检查torch版本和CUDA兼容性：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="built_in">print</span>(torch.__version__)  <span class="comment"># 应输出: 2.3.1+cu121</span></span><br><span class="line"><span class="built_in">print</span>(torch.version.cuda) <span class="comment"># 应输出: 12.1</span></span><br></pre></td></tr></tbody></table></figure></li><li><p>安装Flashinfer：<br>根据文档，Gemma运行在版本0.08。vLLM需要FlashInfer v0.0.8（请参阅<a href="https://github.com/vllm-project/vllm/issues/7060">vLLM版本和Flashinfer文档</a>中关于Gemma 2的部分）。</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">!pip install flashinfer==0.0.8 -i https://flashinfer.ai/whl/cu121/torch2.3/</span><br></pre></td></tr></tbody></table></figure></li></ol><h2 id="更新环境中的VLLM后端变量"><a href="#更新环境中的VLLM后端变量" class="headerlink" title="更新环境中的VLLM后端变量"></a>更新环境中的VLLM后端变量</h2><p>确保设置Flashinfer为vLLM的注意力机制后端：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">"VLLM_ATTENTION_BACKEND"</span>] = <span class="string">"FLASHINFER"</span></span><br></pre></td></tr></tbody></table></figure><h2 id="测试vLLM"><a href="#测试vLLM" class="headerlink" title="测试vLLM"></a>测试vLLM</h2><p>以下是使用vLLM生成文本的测试代码：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> vllm <span class="keyword">import</span> LLM, SamplingParams</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">llm = LLM(model=<span class="string">"gemma-2-2b-model"</span>, trust_remote_code=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">sampling_params = SamplingParams(</span><br><span class="line">    temperature=<span class="number">0.8</span>,</span><br><span class="line">    max_tokens=<span class="number">512</span>,</span><br><span class="line">    top_p=<span class="number">0.95</span>,</span><br><span class="line">    top_k=<span class="number">1</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例测试数据</span></span><br><span class="line">test_data = [{<span class="string">"text"</span>: <span class="string">"输入测试文本1"</span>}, {<span class="string">"text"</span>: <span class="string">"输入测试文本2"</span>}]</span><br><span class="line"></span><br><span class="line">prompts = [</span><br><span class="line">    test_data[random.randint(<span class="number">0</span>, <span class="built_in">len</span>(test_data) - <span class="number">1</span>)][<span class="string">"text"</span>],</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">outputs = llm.generate(</span><br><span class="line">    prompts,</span><br><span class="line">    sampling_params</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 预期输出:</span></span><br><span class="line"><span class="comment"># Processed prompts: 100%|██████████| 1/1 [00:01&lt;00:00,  1.24s/it, est. speed input: 991.44 toks/s, output: 87.79 toks/s]</span></span><br></pre></td></tr></tbody></table></figure><p>通过上述步骤，您应该能够成功运行微调后的Gemma-2-2b-it模型。</p><h2 id="常见错误及解决方法"><a href="#常见错误及解决方法" class="headerlink" title="常见错误及解决方法"></a>常见错误及解决方法</h2><p>在运行过程中，可能会遇到以下常见错误：</p><ol><li><p><strong>RuntimeError: <code>CHECK_EQ(paged_kv_indptr.size(0), batch_size + 1) failed. 1 vs 257</code></strong></p><ul><li><strong>原因</strong>：Flashinfer版本错误。</li><li><strong>解决方法</strong>：请确保安装了正确版本的Flashinfer。</li></ul></li><li><p><strong>TypeError: <code>'NoneType' object is not callable</code></strong></p><ul><li><strong>原因</strong>：没有安装Flashinfer。</li><li><strong>解决方法</strong>：按照上述步骤安装Flashinfer。</li></ul></li><li><p><strong>ValueError: <code>Please use Flashinfer backend for models with logits_soft_cap (i.e., Gemma-2). Otherwise, the output might be wrong. Set Flashinfer backend by export VLLM_ATTENTION_BACKEND=FLASHINFER.</code></strong></p><ul><li><strong>原因</strong>：未设置Flashinfer后端。</li><li><strong>解决方法</strong>：设置环境变量<code>VLLM_ATTENTION_BACKEND</code>为<code>FLASHINFER</code>。</li></ul></li></ol><p>通过上述详细步骤和解决方法，您应该能够成功运行并调试微调后的Gemma-2-2b-it模型。如果您在任何一步遇到问题，请参考相应的文档或在社区中寻求帮助。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;使用vLLM运行微调后的Gemma-2-2b-it的详细步骤&quot;&gt;&lt;a href=&quot;#使用vLLM运行微调后的Gemma-2-2b-it的详细步骤&quot; class=&quot;headerlink&quot; title=&quot;使用vLLM运行微调后的Gemma-2-2b-it的详细步骤&quot;&gt;</summary>
      
    
    
    
    <category term="NLP Insights" scheme="https://chenhuiyu.github.io/categories/NLP-Insights/"/>
    
    
    <category term="vLLM" scheme="https://chenhuiyu.github.io/tags/vLLM/"/>
    
    <category term="Gemma-2" scheme="https://chenhuiyu.github.io/tags/Gemma-2/"/>
    
  </entry>
  
  <entry>
    <title>如何准确计算固定长度模型的困惑度（PPL）</title>
    <link href="https://chenhuiyu.github.io/2024/04/17/NLP%20Insights/%20%E5%9B%B0%E6%83%91%E5%BA%A6(PPL)/"/>
    <id>https://chenhuiyu.github.io/2024/04/17/NLP%20Insights/%20%E5%9B%B0%E6%83%91%E5%BA%A6(PPL)/</id>
    <published>2024-04-17T04:00:00.000Z</published>
    <updated>2024-10-23T08:59:45.573Z</updated>
    
    <content type="html"><![CDATA[<h1 id="如何计算固定长度模型的困惑度（PPL）"><a href="#如何计算固定长度模型的困惑度（PPL）" class="headerlink" title="如何计算固定长度模型的困惑度（PPL）"></a>如何计算固定长度模型的困惑度（PPL）</h1><p>困惑度（PPL）是评估语言模型最常用的指标之一。在深入探讨之前，我们应该注意这个指标特别适用于传统语言模型（有时被称为自回归或因果语言模型），而对于像 BERT 这样的 masked language models 则没有明确定义（见<a href="https://huggingface.co/docs/transformers/main/en/model_summary">模型总结</a>）。</p><p>困惑度被定义为序列的指数化平均负对数似然。如果我们有一个标记化序列 $X = (x_0, x_1, \dots, x_t)$，那么 $X$ 的困惑度为，</p><script type="math/tex; mode=display">\text{PPL}(X) = \exp \left\{ -\frac{1}{t}\sum*{i=1}^t \log p*\theta (x*i|x*{<i}) \right\}</script><p>其中 $\log p<em>\theta (x_i|x</em>{&lt;i})$ 是第 i 个标记的对数似然，条件是根据我们的模型前面的标记 $x_{&lt;i}$。直观上，它可以被认为是评估模型在语料库中指定标记集合上预测均匀性的能力。重要的是，这意味着标记化程序直接影响模型的困惑度，这在比较不同模型时应始终考虑。</p><p>这也相当于数据和模型预测之间的交叉熵的指数化。想要了解更多关于困惑度及其与每字符位数（BPC）和数据压缩的关系的直觉，可以查看这篇在 The Gradient 上的<a href="https://thegradient.pub/understanding-evaluation-metrics-for-language-models/">精彩博客文章</a>。</p><h2 id="Calculating-PPL-with-fixed-length-models"><a href="#Calculating-PPL-with-fixed-length-models" class="headerlink" title="Calculating PPL with fixed-length models"></a>Calculating PPL with fixed-length models</h2><p>如果我们不受模型上下文大小的限制，我们会通过自回归地分解序列并在每一步都基于整个前序子序列来条件化，从而评估模型的困惑度，如下图所示。</p><p><img width="600" alt="Full decomposition of a sequence with unlimited context length" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/ppl_full.gif"></p><p>然而，在处理近似模型时，我们通常受到模型可以处理的标记数量的限制。例如，<a href="https://huggingface.co/docs/transformers/main/en/model_doc/gpt2">GPT-2</a>的最大版本有固定的 1024 个标记长度，所以当 $t$ 大于 1024 时，我们无法直接计算 $p<em>\theta(x_t|x</em>{&lt;t})$。</p><p>相反，序列通常被分解成等于模型最大输入大小的子序列。如果模型的最大输入大小是 $k$，那么我们通过只条件化前 $k-1$ 个标记（而不是整个上下文）来近似计算一个标记 $x_t$ 的似然。在评估模型序列的困惑度时，一种诱人但次优的方法是将序列分解成不相交的块，并独立地累加每个段的分解对数似然。</p><p><img width="600" alt="Suboptimal PPL not taking advantage of full available context" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/ppl_chunked.gif"></p><p>这种计算很快，因为每个段的困惑度可以在一次前向传递中计算出来，但这是一个较差的完全分解困惑度的近似，并且通常会产生更高（更差）的 PPL，因为模型在大多数预测步骤中的上下文较少。</p><p>相反，应该使用滑动窗口策略来评估固定长度模型的 PPL。这涉及到重复滑动上下文窗口，使模型在做出每个预测时拥有更多的上下文。</p><p><img width="600" alt="Sliding window PPL taking advantage of all available context" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/ppl_sliding.gif"></p><ol><li><p><strong>无限上下文分解：</strong> 如果没有对模型输入长度的限制，我们可以在每一步都使用整个前序子序列来预测下一个标记。这样可以最准确地评估模型的性能，因为每次预测都考虑了所有先前的信息。</p></li><li><p><strong>固定长度限制：</strong> 实际中，大多数模型如 GPT-2 有固定的输入长度限制（例如 1024 个标记）。当序列长度超过这个限制时，不能直接计算每个标记的条件概率，因为不能将整个序列作为条件。</p></li><li><p><strong>分块近似：</strong> 一种处理长序列的方法是将序列分解成多个与模型最大输入长度相等的子序列。每个子序列单独评估，但这种方法可能会因为没有使用完整的上下文而导致更高的困惑度。</p></li><li><p><strong>滑动窗口策略：</strong> 为了更好地利用可用的上下文，可以使用滑动窗口策略。这种方法通过不断移动上下文窗口来尝试在每次预测时为模型提供更多的上下文信息，从而更接近于使用完整上下文的理想情况。</p></li><li><p><strong>跨步滑动窗口：</strong> 一个实际的折中方法是使用跨步滑动窗口，这样可以在保证一定效率的同时，为每次模型预测提供足够的上下文，从而改善困惑度的计算和模型预测的准确性。</p></li></ol><p>这些方法都是为了解决因模型输入长度限制而不能直接评估整个序列的问题，试图通过不同的技术使评估更加准确，同时考虑到计算资源的有效使用。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;如何计算固定长度模型的困惑度（PPL）&quot;&gt;&lt;a href=&quot;#如何计算固定长度模型的困惑度（PPL）&quot; class=&quot;headerlink&quot; title=&quot;如何计算固定长度模型的困惑度（PPL）&quot;&gt;&lt;/a&gt;如何计算固定长度模型的困惑度（PPL）&lt;/h1&gt;&lt;p&gt;困惑</summary>
      
    
    
    
    <category term="NLP Insights" scheme="https://chenhuiyu.github.io/categories/NLP-Insights/"/>
    
    
    <category term="Language Modeling" scheme="https://chenhuiyu.github.io/tags/Language-Modeling/"/>
    
    <category term="Perplexity" scheme="https://chenhuiyu.github.io/tags/Perplexity/"/>
    
  </entry>
  
  <entry>
    <title>【Python题解】2834. 找出美丽数组的最小和</title>
    <link href="https://chenhuiyu.github.io/2024/03/08/Code%20Chronicles/2834.%20%E6%89%BE%E5%87%BA%E7%BE%8E%E4%B8%BD%E6%95%B0%E7%BB%84%E7%9A%84%E6%9C%80%E5%B0%8F%E5%92%8C/"/>
    <id>https://chenhuiyu.github.io/2024/03/08/Code%20Chronicles/2834.%20%E6%89%BE%E5%87%BA%E7%BE%8E%E4%B8%BD%E6%95%B0%E7%BB%84%E7%9A%84%E6%9C%80%E5%B0%8F%E5%92%8C/</id>
    <published>2024-03-08T15:31:44.000Z</published>
    <updated>2024-03-08T14:24:59.239Z</updated>
    
    <content type="html"><![CDATA[<h1 id="2834-找出美丽数组的最小和"><a href="#2834-找出美丽数组的最小和" class="headerlink" title="2834. 找出美丽数组的最小和"></a>2834. 找出美丽数组的最小和</h1><blockquote><p>Problem: <a href="https://leetcode.cn/problems/find-the-minimum-possible-sum-of-a-beautiful-array/description/">2834. 找出美丽数组的最小和</a></p><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2></blockquote><p>给定两个正整数 <code>n</code> 和 <code>target</code>，目标是找到一个长度为 <code>n</code> 的数组，满足以下条件：</p><ul><li>数组由两两不同的正整数组成。</li><li>不存在两个不同下标 <code>i</code> 和 <code>j</code> 使得 <code>nums[i] + nums[j] == target</code>。<br>返回符合条件的美丽数组所可能具备的最小和，并对结果进行 <code>10^9 + 7</code> 取模。</li></ul><h3 id="测试用例"><a href="#测试用例" class="headerlink" title="测试用例"></a>测试用例</h3><p>示例 1：</p><ul><li>输入：n = 2, target = 3</li><li>输出：4</li></ul><p>示例 2：</p><ul><li>输入：n = 3, target = 3</li><li>输出：8</li></ul><p>示例 3：</p><ul><li>输入：n = 1, target = 1</li><li>输出：1</li></ul><h2 id="原始思路"><a href="#原始思路" class="headerlink" title="原始思路"></a>原始思路</h2><h3 id="方案"><a href="#方案" class="headerlink" title="方案"></a>方案</h3><p>初始方案是从最小的数字开始，逐个检查每个数字是否可以被添加到数组中，同时确保不会存在两个数字之和等于 <code>target</code>。</p><ul><li>从 <code>1</code> 开始逐个尝试添加数字到数组。</li><li>对于每个数字，检查是否与数组中已有的数字相加会得到 <code>target</code>。</li><li>如果不会，将其添加到数组中。</li><li>继续此过程，直到数组长度达到 <code>n</code>。</li></ul><h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">minimumPossibleSum</span>(<span class="params">n, target</span>):</span><br><span class="line">    selected_nums = <span class="built_in">set</span>([<span class="number">1</span>])</span><br><span class="line">    total_sum = <span class="number">1</span></span><br><span class="line">    current_num = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> <span class="built_in">len</span>(selected_nums) &lt; n:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">all</span>((current_num + num != target) <span class="keyword">for</span> num <span class="keyword">in</span> selected_nums):</span><br><span class="line">            selected_nums.add(current_num)</span><br><span class="line">            total_sum += current_num</span><br><span class="line">        current_num += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> total_sum % (<span class="number">10</span>**<span class="number">9</span> + <span class="number">7</span>)</span><br></pre></td></tr></tbody></table></figure><h3 id="复杂度分析"><a href="#复杂度分析" class="headerlink" title="复杂度分析"></a>复杂度分析</h3><ul><li>时间复杂度：O(n^2)，因为每个数字的添加都需要遍历已选择的数字集合。</li><li>空间复杂度：O(n)，用于存储选择的数字集合。</li></ul><h2 id="贪心优化"><a href="#贪心优化" class="headerlink" title="贪心优化"></a>贪心优化</h2><h3 id="方案-1"><a href="#方案-1" class="headerlink" title="方案"></a>方案</h3><ul><li><p><strong>优化策略</strong>：</p><ul><li><strong>避免集合的使用</strong>：<ul><li>引入“避免”集合，存储所有与已选数字相加得到 <code>target</code> 的数字。</li><li>这样可以快速检查新数字是否会导致和为 <code>target</code> 的情况。</li></ul></li><li><strong>直接检查</strong>：<ul><li>每次选择一个新数字时，仅检查它是否在“避免”集合中。</li><li>不在集合中的数字被认为是安全的，可以直接添加。</li></ul></li><li><strong>动态更新避免集合</strong>：<ul><li>当新数字被添加到美丽数组时，相应的 <code>target - 新数字</code> 也被添加到“避免”集合中。</li><li>这确保任何可能与新数字组成 <code>target</code> 的数字在未来都会被避免。</li></ul></li></ul></li><li><p><strong>优化后的时间复杂度</strong>：</p><ul><li>每个数字只需进行一次集合检查。</li><li>时间复杂度降低为 O(n)，显著提高了算法效率。</li></ul></li></ul><h3 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">minimumPossibleSum_optimized</span>(<span class="params">n, target</span>):</span><br><span class="line">    selected_nums = <span class="built_in">set</span>()</span><br><span class="line">    avoid_nums = <span class="built_in">set</span>()</span><br><span class="line">    total_sum = <span class="number">0</span></span><br><span class="line">    current_num = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> <span class="built_in">len</span>(selected_nums) &lt; n:</span><br><span class="line">        <span class="keyword">if</span> current_num <span class="keyword">not</span> <span class="keyword">in</span> avoid_nums:</span><br><span class="line">            selected_nums.add(current_num)</span><br><span class="line">            total_sum += current_num</span><br><span class="line">            avoid_nums.add(target - current_num)</span><br><span class="line">        current_num += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> total_sum % (<span class="number">10</span>**<span class="number">9</span> + <span class="number">7</span>)</span><br></pre></td></tr></tbody></table></figure><h3 id="复杂度分析-1"><a href="#复杂度分析-1" class="headerlink" title="复杂度分析"></a>复杂度分析</h3><ul><li>时间复杂度：O(n)，因为每个数字只需检查一次。</li><li>空间复杂度：O(n)，用于存储选择的数字和避免数字集合。</li></ul><h2 id="数学方法"><a href="#数学方法" class="headerlink" title="数学方法"></a>数学方法</h2><h3 id="方案-2"><a href="#方案-2" class="headerlink" title="方案"></a>方案</h3><p>针对上述问题，我们采用了一种更高效的数学方法来解决这个问题。该方法通过分析问题的数学本质，减少了必要的计算量，特别适用于处理大规模数据。</p><ol><li><p><strong>问题分解</strong>：</p><ul><li>首先，我们将问题分解为两部分。由于数组中的数字都是唯一的，且两个不同的数字之和不能等于 <code>target</code>，我们首先从最小的数字开始选择，直到我们不能再选择更多的数字而不违反和的规则。</li></ul></li><li><p><strong>选择前半部分的数字</strong>：</p><ul><li>在 <code>1</code> 到 <code>target-1</code> 的范围内，某些数字不能同时出现。例如，如果 <code>target</code> 是 <code>6</code>，则 <code>1</code> 和 <code>5</code>、<code>2</code> 和 <code>4</code> 不能同时出现，因为它们的和等于 <code>6</code>。但是，<code>3</code>（当 <code>target</code> 是偶数）或 <code>3</code> 和 <code>2</code>（当 <code>target</code> 是奇数）是可以被选择的。</li><li>这意味着我们可以自由选择从 <code>1</code> 到 <code>m</code> 的数字，其中 <code>m = min(⌊target/2⌋, n)</code>。对于这部分数字，我们可以直接使用等差数列的求和公式来计算它们的总和，即 <code>m * (m + 1) / 2</code>。</li></ul></li><li><p><strong>选择后半部分的数字</strong>：</p><ul><li>一旦我们选择了前 <code>m</code> 个数字，剩下需要选择的数字的数量就是 <code>n - m</code>。由于我们已经选择了 <code>1</code> 到 <code>m</code>，我们现在需要从 <code>target</code> 开始选择剩下的数字。</li><li>如果 <code>n</code> 大于 <code>m</code>，那么我们将从 <code>target</code> 开始连续选择 <code>n - m</code> 个数字。这些数字的总和可以用等差数列的求和公式来计算，公式为：<code>(2 * target + n - m - 1) * (n - m) / 2</code>。</li></ul></li><li><p><strong>计算总和并取模</strong>：</p><ul><li>我们将两部分的和相加，并对结果进行 <code>10^9 + 7</code> 取模，以得到最终答案。</li></ul></li></ol><h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><ul><li>第一部分和：从 <code>1</code> 到 <code>min(target // 2, n)</code> 的和。</li><li>第二部分和（如果需要）：从 <code>target</code> 开始，选择的 <code>n - min(target // 2, n)</code> 个数字的和。</li><li>将这两部分的和相加，即得到符合条件的美丽数组的最小和。</li></ul><ol><li><p><strong>选择小于 <code>target // 2</code> 的数字</strong>：</p><ul><li>当我们从 1 开始逐渐增加数字，直到 <code>target // 2</code>，这些数字不可能与数组中的其他数字相加得到 <code>target</code>。</li><li>例如，如果 <code>target</code> 是 10，那么 <code>target // 2</code> 是 5。在这种情况下，1 到 5 之间的任何两个数字相加都不会等于 10。</li><li>因此，这部分的选择是安全的，并且由于我们需要最小和，所以我们从 1 开始逐一增加。</li></ul></li><li><p><strong>当 <code>n</code> 大于 <code>target // 2</code></strong>：</p><ul><li>如果 <code>n</code> 大于 <code>target // 2</code>，这意味着仅仅选择小于 <code>target // 2</code> 的数字不足以填满数组。</li><li>在这种情况下，我们需要继续选择更多的数字，但为了避免和为 <code>target</code> 的组合，我们需要从 <code>target</code> 本身开始选择。</li><li>我们继续逐一增加，直到数组长度达到 <code>n</code>。</li></ul></li><li><p><strong>计算总和</strong>：</p><ul><li>第一部分是从 1 到 <code>min(target // 2, n)</code> 的和。</li><li>第二部分（如果需要）是从 <code>target</code> 开始，选择剩下的 <code>n - min(target // 2, n)</code> 个数字。</li><li>最后，将这两部分的和加起来，就是我们要找的最小和。</li></ul></li></ol><p>这是一个通过数学方法来解决问题的典型例子，它避免了复杂的编程逻辑，提供了一种更简洁高效的解决方案。</p><h3 id="代码-2"><a href="#代码-2" class="headerlink" title="代码"></a>代码</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">minimumPossibleSum_math_approach</span>(<span class="params">n, target</span>):</span><br><span class="line">    MOD = <span class="number">10</span>**<span class="number">9</span> + <span class="number">7</span></span><br><span class="line">    m = <span class="built_in">min</span>(target // <span class="number">2</span>, n)</span><br><span class="line">    first_half_sum = m * (m + <span class="number">1</span>) // <span class="number">2</span></span><br><span class="line">    remaining = n - m</span><br><span class="line">    second_half_sum = (<span class="number">2</span> * target + remaining - <span class="number">1</span>) * remaining // <span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> (first_half_sum + second_half_sum) % MOD</span><br></pre></td></tr></tbody></table></figure><h3 id="复杂度分析-2"><a href="#复杂度分析-2" class="headerlink" title="复杂度分析"></a>复杂度分析</h3><ul><li>时间复杂度：O(1)，因为结果是通过直接计算得出的。</li><li>空间复杂度：O(1)，只使用了固定数量的变量。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;2834-找出美丽数组的最小和&quot;&gt;&lt;a href=&quot;#2834-找出美丽数组的最小和&quot; class=&quot;headerlink&quot; title=&quot;2834. 找出美丽数组的最小和&quot;&gt;&lt;/a&gt;2834. 找出美丽数组的最小和&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt;Pro</summary>
      
    
    
    
    <category term="Code Chronicles" scheme="https://chenhuiyu.github.io/categories/Code-Chronicles/"/>
    
    
    <category term="Python" scheme="https://chenhuiyu.github.io/tags/Python/"/>
    
    <category term="Leetcode" scheme="https://chenhuiyu.github.io/tags/Leetcode/"/>
    
    <category term="每日一题" scheme="https://chenhuiyu.github.io/tags/%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/"/>
    
  </entry>
  
  <entry>
    <title>跟着GPT老师学小聊：如何做一个好的捧哏</title>
    <link href="https://chenhuiyu.github.io/2024/03/05/Life%20Reflections/%E8%B7%9F%E7%9D%80GPT%E8%80%81%E5%B8%88%E5%AD%A6%E5%B0%8F%E8%81%8A/"/>
    <id>https://chenhuiyu.github.io/2024/03/05/Life%20Reflections/%E8%B7%9F%E7%9D%80GPT%E8%80%81%E5%B8%88%E5%AD%A6%E5%B0%8F%E8%81%8A/</id>
    <published>2024-03-05T14:31:00.000Z</published>
    <updated>2024-03-05T14:37:40.133Z</updated>
    
    <content type="html"><![CDATA[<p>英语小聊 (small talk) 是日常生活中不可或缺的交流形式，它不仅有助于打破沉默，也能在轻松的氛围中促进理解和友谊。在这篇文章中，我们将结合 10 个主题，提供相关的词汇、短语，并分享生活故事的开场白。同时，学习如何成为一名优秀的捧哏，通过提问和接话，让对话更加流畅和有趣。</p><h3 id="1-旅游体验-Travel-Experiences"><a href="#1-旅游体验-Travel-Experiences" class="headerlink" title="1. 旅游体验 (Travel Experiences)"></a>1. 旅游体验 (Travel Experiences)</h3><ul><li>词汇：Itinerary (行程), off the beaten path (人迹罕至), picturesque (如画的), excursion (远足), landmark (地标)。</li><li>短语：Cultural immersion (文化沉浸), travel off the beaten path (走偏僻的路), soak up the atmosphere (沉浸在气氛中)。</li><li>故事开场： “One place I really enjoyed visiting was…” (我非常喜欢去的一个地方是…)</li><li>追问： “That sounds amazing! What was the most unforgettable part of your trip?” (听起来太棒了！你旅行中最难忘的部分是什么？)</li><li>接话： “I’ve heard that place is beautiful. Did you take a lot of photos?” (我听说那个地方很美。你拍了很多照片吗？)</li></ul><h3 id="2-食物与美食-Food-and-Cuisine"><a href="#2-食物与美食-Food-and-Cuisine" class="headerlink" title="2. 食物与美食 (Food and Cuisine)"></a>2. 食物与美食 (Food and Cuisine)</h3><ul><li>词汇：Gastronomy (美食学), palate (味觉), savory (可口的), gourmet (美食家), culinary (烹饪的)。</li><li>短语：Acquired taste (后天品味), comfort food (安慰食物), fusion cuisine (融合菜肴), culinary delights (烹饪乐趣)。</li><li>故事开场： “I recently tried cooking…” (我最近尝试烹饪…)</li><li>追问： “Oh, how did it turn out? What ingredients did you use?” (哦，结果怎样？你用了哪些食材？)</li><li>接话： “I love trying new recipes too. Do you have any recommendations?” (我也喜欢尝试新食谱。你有什么推荐吗？)</li></ul><h3 id="3-爱好与兴趣-Hobbies-and-Interests"><a href="#3-爱好与兴趣-Hobbies-and-Interests" class="headerlink" title="3. 爱好与兴趣 (Hobbies and Interests)"></a>3. 爱好与兴趣 (Hobbies and Interests)</h3><ul><li>词汇：Amateur (业余爱好者), pastime (消遣), dabble (涉猎), proficiency (熟练), knack (诀窍)。</li><li>短语：Pursue a hobby (追求一个爱好), hone skills (磨练技能), leisure activities (休闲活动), broaden horizons (开阔视野)。</li><li>故事开场： “In my free time, I like to…” (在我空闲的时候，我喜欢…)</li><li>追问： “That’s interesting! How did you get started with that hobby?” (真有趣！你是怎么开始这个爱好的？)</li><li>接话： “It sounds like a great way to relax. I’ve been looking for a new hobby myself.” (听起来是放松的好方式。我自己也在找新的爱好。)</li></ul><h3 id="4-电影、电视节目和书籍-Movies-TV-Shows-and-Books"><a href="#4-电影、电视节目和书籍-Movies-TV-Shows-and-Books" class="headerlink" title="4. 电影、电视节目和书籍 (Movies, TV Shows, and Books)"></a>4. 电影、电视节目和书籍 (Movies, TV Shows, and Books)</h3><ul><li>词汇：Plot (情节), genre (类型), protagonist (主角), cliffhanger (悬念), screenplay (剧本)。</li><li>短语：Twist in the tale (故事的转折), page-turner (扣人心弦的书), critically acclaimed (广受好评), binge-watch (连续看剧)。</li><li>故事开场： “I watched a movie recently, and I found it…” (我最近看了一部电影，我觉得它…)</li><li>追问： “What did you like most about it? Any particular scene or character?” (你最喜欢它的哪个部分？有特别喜欢的场景或角色吗？)</li><li>接话： “I’ve been looking for something good to watch/read. Would you recommend it?” (我一直在找好看/好读的东西。你会推荐它吗？)</li></ul><h3 id="5-当前事件-Current-Events"><a href="#5-当前事件-Current-Events" class="headerlink" title="5. 当前事件 (Current Events)"></a>5. 当前事件 (Current Events)</h3><ul><li>词汇：Geopolitics (地缘政治), humanitarian (人道主义的), legislation (立法), diplomacy (外交), fiscal (财政的)。</li><li>短语：Political turmoil (政治动荡), economic sanctions (经济制裁), diplomatic relations (外交关系), social unrest (社会动乱)。</li><li>故事开场： “I read an interesting news article about…” (我读到了一个有趣的新闻文章，关于…)</li><li>追问： “That does sound interesting. How do you think it will affect us?” (那确实很有趣。你认为它会如何影响我们？)</li><li>接话： “I read something similar. It’s fascinating how quickly things are changing.” (我读过类似的东西。事物变化之快真是令人着迷。)</li></ul><h3 id="6-日常生活与日程-Daily-Life-and-Routine"><a href="#6-日常生活与日程-Daily-Life-and-Routine" class="headerlink" title="6. 日常生活与日程 (Daily Life and Routine)"></a>6. 日常生活与日程 (Daily Life and Routine)</h3><ul><li>词汇：Mundane (平凡的), routine (日常的), chores (杂务), errand (差事), regimen (规律)。</li><li>短语：Daily grind (日常琐事), run errands (做杂事), stick to a routine (遵守日常), day-to-day life (日常生活)。</li><li>故事开场： “A typical day for me involves…” (我的典型一天包括…)</li><li>追问： “Sounds like a busy day. What do you enjoy most in your daily routine?” (听起来是忙碌的一天。你最喜欢日常生活中的哪个部分？)</li><li>接话： “I can relate to that. My mornings are pretty similar. Do you have any morning rituals?” (我能理解。我的早晨也差不多。你有什么晨间仪式吗？)</li></ul><h3 id="7-语言学习-Language-Learning"><a href="#7-语言学习-Language-Learning" class="headerlink" title="7. 语言学习 (Language Learning)"></a>7. 语言学习 (Language Learning)</h3><ul><li>词汇：Fluency (流利), proficiency (精通), bilingual (双语的), immersion (沉浸式), linguistics (语言学)。</li><li>短语：Gain proficiency (提高熟练度), language barrier (语言障碍), mother tongue (母语), pick up a language (学习一种语言)。</li><li>故事开场： “One challenge I face in learning English is…” (我在学习英语时面临的一个挑战是…)</li><li>追问： “I see. What strategies are you using to overcome that challenge?” (我明白了。你用什么策略来克服这个挑战？)</li><li>接话： “Learning a language can be tough. I’m also trying to improve my [language].” (学习一门语言可能很难。我也在努力提高我的[语言]水平。)</li></ul><h3 id="8-文化差异-Cultural-Differences"><a href="#8-文化差异-Cultural-Differences" class="headerlink" title="8. 文化差异 (Cultural Differences)"></a>8. 文化差异 (Cultural Differences)</h3><ul><li>词汇：Etiquette (礼仪), customs (风俗), heritage (遗产), assimilate (同化), diversity (多样性)。</li><li>短语：Cultural exchange (文化交流), societal norms (社会规范), cross-cultural (跨文化), traditional values (传统价值)。</li><li>故事开场： “One thing I find different here compared to my home country is…” (我发现这里和我的祖国相比有一点不同是…)</li><li>追问： “That’s quite interesting. How do you feel about that difference?” (这很有趣。你对这种差异有什么感觉？)</li><li>接话： “Cultural differences are so intriguing. I’ve noticed something similar when I traveled to [country].” (文化差异真的很有趣。我在去[国家]旅行时也注意到了类似的事情。)</li></ul><h3 id="9-科技与趋势-Technology-and-Trends"><a href="#9-科技与趋势-Technology-and-Trends" class="headerlink" title="9. 科技与趋势 (Technology and Trends)"></a>9. 科技与趋势 (Technology and Trends)</h3><ul><li>词汇：Innovative (创新的), cutting-edge (尖端的), algorithm (算法), virtual reality (虚拟现实), automation (自动化)。</li><li>短语：Stay ahead of the curve (保持领先), technological advancements (技术进步), digital age (数字时代), the latest trend (最新趋势)。</li><li>故事开场： “I’m curious about how…” (我对…感到好奇)</li><li>追问： “Why does that interest you? Have you tried it out yourself?” (为什么那会引起你的兴趣？你自己试过了吗？)</li><li>接话： “Technology is advancing so fast. I’m also curious about [specific technology or trend].” (科技进步太快了。我也对[特定科技或趋势]感到好奇。)</li></ul><h3 id="10-个人发展-Personal-Development"><a href="#10-个人发展-Personal-Development" class="headerlink" title="10. 个人发展 (Personal Development)"></a>10. 个人发展 (Personal Development)</h3><ul><li>词汇：Self-improvement (自我提升), mindfulness (正念), resilience (韧性), aspiration (抱负), introspection (反省)。</li><li>短语：Set goals (设定目标), personal growth(个人成长), step out of comfort zone (走出舒适区), life-long learning (终身学习)。</li><li>故事开场： “I’ve been trying to…” (我一直在尝试…)</li><li>追问： “That’s a great goal. How are you planning to achieve it?” (那是个很好的目标。你打算如何实现它？)</li><li>接话： “Self-improvement is so important. I’m also working on [your goal or habit].” (自我提升非常重要。我也在努力[你的目标或习惯]。)</li></ul><p>成为一个好的捧哏不仅能使对话更加深入和有意义，还能展现你的倾听和理解能力。这些问题和评论可以帮助你在各种话题中更好地参与和维持英语对话。试试看，你会发现每次小聊都充满新的发现和乐趣！🌟🗣️💬</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;英语小聊 (small talk) 是日常生活中不可或缺的交流形式，它不仅有助于打破沉默，也能在轻松的氛围中促进理解和友谊。在这篇文章中，我们将结合 10 个主题，提供相关的词汇、短语，并分享生活故事的开场白。同时，学习如何成为一名优秀的捧哏，通过提问和接话，让对话更加流畅</summary>
      
    
    
    
    <category term="Life Reflections" scheme="https://chenhuiyu.github.io/categories/Life-Reflections/"/>
    
    
    <category term="Language Learning" scheme="https://chenhuiyu.github.io/tags/Language-Learning/"/>
    
    <category term="Small Talk" scheme="https://chenhuiyu.github.io/tags/Small-Talk/"/>
    
  </entry>
  
  <entry>
    <title>【Python题解】100226. 在带权树网络中统计可连接服务器对数目</title>
    <link href="https://chenhuiyu.github.io/2024/03/04/Code%20Chronicles/%E3%80%90Python%E9%A2%98%E8%A7%A3%E3%80%91100226.%20%E5%9C%A8%E5%B8%A6%E6%9D%83%E6%A0%91%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%BB%9F%E8%AE%A1%E5%8F%AF%E8%BF%9E%E6%8E%A5%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AF%B9%E6%95%B0%E7%9B%AE/"/>
    <id>https://chenhuiyu.github.io/2024/03/04/Code%20Chronicles/%E3%80%90Python%E9%A2%98%E8%A7%A3%E3%80%91100226.%20%E5%9C%A8%E5%B8%A6%E6%9D%83%E6%A0%91%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%BB%9F%E8%AE%A1%E5%8F%AF%E8%BF%9E%E6%8E%A5%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AF%B9%E6%95%B0%E7%9B%AE/</id>
    <published>2024-03-04T15:31:44.000Z</published>
    <updated>2024-03-05T14:36:12.984Z</updated>
    
    <content type="html"><![CDATA[<h3 id="题目：100226-在带权树网络中统计可连接服务器对数目"><a href="#题目：100226-在带权树网络中统计可连接服务器对数目" class="headerlink" title="题目：100226. 在带权树网络中统计可连接服务器对数目"></a>题目：100226. 在带权树网络中统计可连接服务器对数目</h3><h4 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h4><p>你被给定一个未定根的加权树，它有 <code>n</code> 个顶点，代表从 0 到 <code>n - 1</code> 编号的服务器，一个数组 <code>edges</code>，其中 <code>edges[i] = [ai, bi, weighti]</code> 代表顶点 <code>ai</code> 和 <code>bi</code> 之间的双向边，边的权重为 <code>weighti</code>。你还被给定一个整数 <code>signalSpeed</code>。</p><p>如果满足以下条件，两个服务器 <code>a</code> 和 <code>b</code> 可以通过服务器 <code>c</code> 连接：</p><ul><li><code>a &lt; b</code>，<code>a != c</code> 且 <code>b != c</code>。</li><li>从 <code>c</code> 到 <code>a</code> 的距离可被 <code>signalSpeed</code> 整除。</li><li>从 <code>c</code> 到 <code>b</code> 的距离可被 <code>signalSpeed</code> 整除。</li><li>从 <code>c</code> 到 <code>b</code> 和从 <code>c</code> 到 <code>a</code> 的路径不共享任何边。</li></ul><p>返回一个整数数组 <code>count</code>，长度为 <code>n</code>，其中 <code>count[i]</code> 是通过服务器 <code>i</code> 可连接的服务器对数。</p><h4 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h4><p><strong>示例 1</strong>:</p><p>输入: <code>edges = [[0,1,1],[1,2,5],[2,3,13],[3,4,9],[4,5,2]]</code>, <code>signalSpeed = 1</code><br>输出: <code>[0,4,6,6,4,0]</code><br>解释: 由于 <code>signalSpeed</code> 为 1，<code>count[c]</code> 等于从 <code>c</code> 出发且不共享任何边的路径对数。<br>在给定的路径图中，<code>count[c]</code> 等于 <code>c</code> 左侧的服务器数乘以 <code>c</code> 右侧的服务器数。</p><p><strong>示例 2</strong>:</p><p>输入: <code>edges = [[0,6,3],[6,5,3],[0,3,1],[3,2,7],[3,1,6],[3,4,2]]</code>, <code>signalSpeed = 3</code><br>输出: <code>[2,0,0,0,0,0,2]</code><br>解释: 通过服务器 0，有 2 对可连接服务器：(4, 5) 和 (4, 6)。<br>通过服务器 6，有 2 对可连接服务器：(4, 5) 和 (0, 5)。<br>可以证明，除了 0 和 6 之外的服务器无法连接任何两个服务器。</p><h4 id="限制条件"><a href="#限制条件" class="headerlink" title="限制条件"></a>限制条件</h4><ul><li><code>2 &lt;= n &lt;= 1000</code></li><li><code>edges.length == n - 1</code></li><li><code>edges[i].length == 3</code></li><li><code>0 &lt;= ai, bi &lt; n</code></li><li><code>1 &lt;= weighti &lt;= 10^6</code></li><li><code>1 &lt;= signalSpeed &lt;= 10^6</code></li><li>输入保证 <code>edges</code> 表示一个有效的树。</li></ul><hr><h3 id="问题概述"><a href="#问题概述" class="headerlink" title="问题概述"></a>问题概述</h3><p>给定一个表示服务器网络的树结构。每个服务器通过带权重的边与其他服务器连接。目标是计算在树中的每个服务器通过的可连接服务器对的数量，这些条件由<code>signalSpeed</code>定义。</p><h3 id="可连接服务器的条件"><a href="#可连接服务器的条件" class="headerlink" title="可连接服务器的条件"></a>可连接服务器的条件</h3><p>如果满足以下条件，两个服务器 <code>a</code> 和 <code>b</code> 可以通过服务器 <code>c</code> 连接：</p><ul><li><code>a &lt; b</code></li><li><code>a</code> 和 <code>b</code> 都不同于 <code>c</code>。</li><li>从 <code>c</code> 到 <code>a</code> 和从 <code>c</code> 到 <code>b</code> 的距离都能被 <code>signalSpeed</code> 整除。</li><li>从 <code>c</code> 到 <code>a</code> 和从 <code>c</code> 到 <code>b</code> 的路径不共享任何边。</li></ul><h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><ol><li><p><strong>树表示</strong>：树使用邻接表表示，每个服务器连接到其邻居以及连接边的权重。</p></li><li><p><strong>深度优先搜索（DFS）</strong>：</p><ul><li>使用修改后的 DFS 算法从每个服务器开始遍历树。</li><li>该算法计算所有其他服务器与当前服务器的距离。</li><li>DFS 确保在路径中不考虑共享边。</li></ul></li><li><p><strong>计算可连接对</strong>：</p><ul><li>对于每个服务器 <code>c</code>，该算法识别通过移除 <code>c</code> 形成的所有可能的子树。</li><li>它计算每个子树中与 <code>c</code> 的距离能被 <code>signalSpeed</code> 整除的服务器数量。</li><li>通过服务器 <code>c</code> 的可连接对的总数是通过考虑来自不同子树的对的所有可能组合计算出来的。</li></ul></li></ol><h3 id="Python-实现"><a href="#Python-实现" class="headerlink" title="Python 实现"></a>Python 实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">count_connectable_servers</span>(<span class="params">edges, signal_speed</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">build_tree</span>(<span class="params">edges</span>):</span><br><span class="line">        tree = defaultdict(<span class="built_in">list</span>)</span><br><span class="line">        <span class="keyword">for</span> a, b, weight <span class="keyword">in</span> edges:</span><br><span class="line">            tree[a].append((b, weight))</span><br><span class="line">            tree[b].append((a, weight))</span><br><span class="line">        <span class="keyword">return</span> tree</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">dfs_count_nodes</span>(<span class="params">server, parent, distance</span>):</span><br><span class="line">        <span class="keyword">if</span> distance % signal_speed == <span class="number">0</span>:</span><br><span class="line">            count[<span class="number">0</span>] += <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> neighbor, weight <span class="keyword">in</span> tree[server]:</span><br><span class="line">            <span class="keyword">if</span> neighbor != parent:</span><br><span class="line">                dfs_count_nodes(neighbor, server, distance + weight)</span><br><span class="line"></span><br><span class="line">    n = <span class="built_in">len</span>(edges) + <span class="number">1</span></span><br><span class="line">    tree = build_tree(edges)</span><br><span class="line">    counts = [<span class="number">0</span>] * n</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> c <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        subtree_counts = []</span><br><span class="line">        <span class="keyword">for</span> neighbor, weight <span class="keyword">in</span> tree[c]:</span><br><span class="line">            count = [<span class="number">0</span>]</span><br><span class="line">            dfs_count_nodes(neighbor, c, weight)</span><br><span class="line">            subtree_counts.append(count[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(subtree_counts)):</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i + <span class="number">1</span>, <span class="built_in">len</span>(subtree_counts)):</span><br><span class="line">                counts[c] += subtree_counts[i] * subtree_counts[j]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> counts</span><br></pre></td></tr></tbody></table></figure><h3 id="复杂度分析"><a href="#复杂度分析" class="headerlink" title="复杂度分析"></a>复杂度分析</h3><ul><li><strong>时间复杂度</strong>：该算法对树中的每个服务器执行一次 DFS。由于每条边在每次 DFS 中被访问一次，并且有 <code>n</code> 个服务器，所以总体时间复杂度为 O(n^2)，其中 <code>n</code> 是服务器的数量。</li><li><strong>空间复杂度</strong>：由于存储树结构和在 DFS 过程中使用的辅助数据结构，空间复杂度为 O(n)。</li></ul><h3 id="测试用例"><a href="#测试用例" class="headerlink" title="测试用例"></a>测试用例</h3><p>该解决方案已经通过提供的示例和其他自定义测试用例进行了测试，以确保其正确性。</p><h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>这个解决方案有效地计算了在树形网络中，每个服务器通过的可连接服务器对的数量，考虑到了与 <code>signalSpeed</code> 和连接规则相关的给定限制。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;题目：100226-在带权树网络中统计可连接服务器对数目&quot;&gt;&lt;a href=&quot;#题目：100226-在带权树网络中统计可连接服务器对数目&quot; class=&quot;headerlink&quot; title=&quot;题目：100226. 在带权树网络中统计可连接服务器对数目&quot;&gt;&lt;/a&gt;题</summary>
      
    
    
    
    <category term="Code Chronicles" scheme="https://chenhuiyu.github.io/categories/Code-Chronicles/"/>
    
    
    <category term="Python" scheme="https://chenhuiyu.github.io/tags/Python/"/>
    
    <category term="Leetcode" scheme="https://chenhuiyu.github.io/tags/Leetcode/"/>
    
    <category term="双周赛" scheme="https://chenhuiyu.github.io/tags/%E5%8F%8C%E5%91%A8%E8%B5%9B/"/>
    
  </entry>
  
</feed>
