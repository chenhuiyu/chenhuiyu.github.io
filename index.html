<!DOCTYPE html><html lang="[&quot;zh-CN&quot;,&quot;en&quot;,&quot;default&quot;]" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>黑头呆鱼进化之旅 - 只身打码过草原</title><meta name="author" content="Huiyu Chen"><meta name="copyright" content="Huiyu Chen"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta property="og:type" content="website">
<meta property="og:title" content="黑头呆鱼进化之旅">
<meta property="og:url" content="https://chenhuiyu.github.io/index.html">
<meta property="og:site_name" content="黑头呆鱼进化之旅">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://chenhuiyu.github.io/img/butterfly-icon.png">
<meta property="article:author" content="Huiyu Chen">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://chenhuiyu.github.io/img/butterfly-icon.png"><script type="application/ld+json"></script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://chenhuiyu.github.io/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=5.5.4"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.13.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '黑头呆鱼进化之旅',
  isHighlightShrink: false,
  isToc: false,
  pageType: 'home'
}</script><meta name="generator" content="Hexo 6.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="黑头呆鱼进化之旅" type="application/atom+xml">
</head><body><div class="page" id="body-wrap"><header class="full_page" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">黑头呆鱼进化之旅</span></a></span><div id="menus"></div></nav><div id="site-info"><h1 id="site-title">黑头呆鱼进化之旅</h1></div><div id="scroll-down"><i class="fas fa-angle-down scroll-down-effects"></i></div></header><main class="layout" id="content-inner"><div class="recent-posts nc" id="recent-posts"><div class="recent-post-items"><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2026/02/21/NLP%20Insights/paper-deep-dive-2026-02-21.zh-CN/" title="论文深读｜SLA2: Sparse-Linear Attention with Learnable Routing and QAT">论文深读｜SLA2: Sparse-Linear Attention with Learnable Routing and QAT</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2026-02-21T04:28:28.000Z" title="发表于 2026-02-21 12:28:28">2026-02-21</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/NLP-Insights/">NLP Insights</a></span></div><div class="content">今天这篇按“口试级精读模板”来拆。  论文：SLA2: Sparse-Linear Attention with Learnable Routing and QAT arXiv：https://arxiv.org/abs/2602.12675 新鲜度：7 天内 关注度：Hugging Face Daily Papers 点赞 47  0) 先归类：这篇论文属于哪一类？我先把它归为：系统/推理优化 + 架构改进（偏 attention 机制）。 这类论文的读法重点：  结构图与信息流是否真的更简洁 复杂度与推理成本是否可量化下降 吞吐/延迟/显存是否同时受益，而不是只优化一个指标  1) 10分钟快读：先拿主结论按顺序只读四块：Abstract → Method Overview 图 → Contributions → 主实验表。 What / Why / How / How much What：提出 SLA2，对稀疏-线性注意力做可学习路由与更直接的组合方式。 Why：原始 SLA 的启发式分流可能次优，且分解形式与目标存在误差。 How（一句话）：用可学习 router 决定稀...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2026/02/21/NLP%20Insights/evaluation-of-generation-based-large-language-models-llms-opportunities-and-challenges-from-generation-to-judgment.zh-CN/" title="evaluation-of-generation-based-large-language-models-llms-opportunities-and-challenges-from-generation-to-judgment">evaluation-of-generation-based-large-language-models-llms-opportunities-and-challenges-from-generation-to-judgment</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2026-02-20T16:00:00.000Z" title="发表于 2026-02-21 00:00:00">2026-02-21</time></span></div><div class="content">Evaluation of Generation-Based Large Language Models (LLMs): Opportunities and Challenges from Generation to JudgmentAbstractEvaluation tasks in artificial intelligence (AI) and natural language processing (NLP) have long been challenging. Traditional evaluation methods, such as those based on matching or embeddings, are limited in assessing complex attributes. The recent development of large language models (LLMs) has given rise to the “LLM-as-a-Judge” paradigm, which utilizes LLMs for scori...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2025/06/24/NLP%20Insights/secom-redefining-memory-management-in-conversational-ai.zh-CN/" title="SeCom: 重新定义对话AI的记忆管理">SeCom: 重新定义对话AI的记忆管理</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2025-06-24T08:00:00.000Z" title="发表于 2025-06-24 16:00:00">2025-06-24</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/NLP-Insights/">NLP Insights</a></span></div><div class="content">SeCom: 重新定义对话AI的记忆管理写在前面最近笔者一直在研究对话AI中的内存管理问题，特别是长期对话场景下的记忆构建与检索技术。发现了一篇令人眼前一亮的ICLR 2025论文——《SeCom: On Memory Construction and Retrieval for Personalized Conversational Agents》，由Microsoft和清华大学的研究团队联合发表。 这篇论文提出的SeCom方法巧妙地解决了一个核心问题：如何在长期对话中有效管理和检索历史信息？今天想和大家分享一下这个方法的技术细节和创新点，希望能为从事相关研究的朋友们提供一些启发。 1. 为什么我们需要关注对话内存管理？1.1 长期对话的现实挑战在与LLMs的日常交互中，相信大家都遇到过这样的困扰：当对话变得很长时，AI似乎”忘记”了之前讨论的内容，或者给出的回答与前面的上下文不够连贯。这背后反映的正是长期对话中的内存管理挑战。 随着大语言模型技术的成熟，基于LLM的对话代理已经深入到我们生活的方方面面。但是，当我们希望与AI进行真正的长期、个性化交互时——比如跨越数天、数周的...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2025/03/06/NLP%20Insights/differences-in-padding-strategies-between-decoder-only-and-encoder-only-models.zh-CN/" title="Decoder-only与Encoder-only模型Padding策略的差异">Decoder-only与Encoder-only模型Padding策略的差异</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2025-03-06T09:43:10.000Z" title="发表于 2025-03-06 17:43:10">2025-03-06</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/NLP-Insights/">NLP Insights</a></span></div><div class="content">📌 Padding 的含义在大模型 (LLM) 中，padding 是用于将不同长度的序列调整为同一长度的方法，以便于批量 (batch) 处理。 例如： 12句子1: "I love NLP"句子2: "Padding is useful in LLM training"  使用 &lt;pad&gt; token 进行对齐： 12"I love NLP &lt;pad&gt; &lt;pad&gt; &lt;pad&gt;""Padding is useful in LLM training"   📌 Padding 位置的选择：Left vs RightPadding 有两种常见方式：  Right padding（右填充）： 1"I love NLP &lt;pad&gt; &lt;pad&gt;"  Left padding（左填充）： 1"&lt;pad&gt; &lt;pad&gt; I love NLP"  通常：  Decoder-only 模型（如 GPT, Llama）：采用 Left padding Encoder-only 模型（如 BERT）：采用...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2025/02/11/NLP%20Insights/moe%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97-%E6%8F%AD%E7%A7%98-moe-%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2.zh-CN/" title="MoE模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色">MoE模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2025-02-11T03:50:29.000Z" title="发表于 2025-02-11 11:50:29">2025-02-11</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/NLP-Insights/">NLP Insights</a></span></div><div class="content">MoE 模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色原文地址：A Visual Guide to Mixture of Experts (MoE) 📅 作者：Maarten Grootendorst 📆 日期：2024 年 10 月 7 日  探索语言模型：混合专家模型（MoE）可视化指南目录 MoE 模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色 探索语言模型：混合专家模型（MoE）可视化指南 目录 什么是混合专家（MoE）模型？ Experts Dense Layers Sparse Layers What does an Expert Learn? 专家的架构（Architecture of Experts）      当我们查看最新发布的大型语言模型（LLMs，Large Language Models）时，常常会在标题中看到 “MoE”。这个 “MoE” 代表什么？为什么这么多 LLM 都在使用它？ 在这份可视化指南中，我们会通过 50 多个可视化图示，逐步探索这一关键组件：**Mixture of Experts (MoE)**。   图示内...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86-llm-%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97-%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E-deepseek-r1.zh-CN/" title="推理 LLM 的可视化指南：探索推理时计算技术与 DeepSeek-R1">推理 LLM 的可视化指南：探索推理时计算技术与 DeepSeek-R1</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2025-02-11T03:50:29.000Z" title="发表于 2025-02-11 11:50:29">2025-02-11</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/NLP-Insights/">NLP Insights</a></span></div><div class="content">推理 LLM 的可视化指南：探索推理时计算技术与 DeepSeek-R1原文地址：A Visual Guide to Reasoning LLMs 📅 作者：Maarten Grootendorst 📆 日期：2025 年 2 月 3 日  📌 引言DeepSeek-R1、OpenAI o3-mini 和 Google Gemini 2.0 Flash Thinking 是如何通过“推理”框架将 LLM（大型语言模型, Large Language Models） 扩展到新高度的典型示例。 它们标志着从 扩展训练时计算（train-time compute） 到 扩展推理时计算（test-time compute） 的范式转变。 在本篇文章中，我们提供了 超过 40 张定制可视化图表，带你深入探索：  推理 LLM（Reasoning LLMs） 领域 推理时计算（Test-Time Compute） 机制 DeepSeek-R1 的核心思想  我们将逐步介绍相关概念，帮助你建立对这一新范式的直觉理解。    📖 什么是推理 LLM？与普通 LLM（Large Langu...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/12/11/Life%20Reflections/%E8%BF%BD%E9%80%90%E4%B8%8E%E5%80%92%E5%BD%B1.zh-CN/" title="追逐与倒影">追逐与倒影</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-12-10T18:29:06.000Z" title="发表于 2024-12-11 02:29:06">2024-12-11</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Life-Reflections/">Life Reflections</a></span></div><div class="content">追逐与倒影在清晨的第一缕光洒下之前，世间一切尚未显形。光与影的边界模糊，彷佛可以交叠，又彷佛注定分离。人们常说，朝阳是希望的象征，可它升起时，必将抛下一地影子。光和影之间，究竟是追逐还是相伴？这样的思考让我想起一则古老的寓言：一匹马在沙漠中追逐远方的绿洲，却不知道那不过是海市蜃楼，它每前进一步，绿洲也随之远去。 有时我们追寻的目标，如同沙漠中的绿洲一般，它并非虚无，但也不完全真实。它是一种存在于心中的映像，一个无法企及的彼岸。无论我们怎样靠近，那份距离似乎总是恒定，甚至在我们伸手触碰的一刹那，它便如烟雾般消散。是目标变了，还是我们的执念让它愈加模糊？ 镜中的倒影也是如此。当你站在镜前凝视自己时，你看见的那个“你”，究竟是谁？是一个忠实的再现，还是一场温柔的欺骗？镜中的倒影总会回应你的动作，可是你永远无法拥抱它，甚至连碰触都无法做到。这种触不可及的关系，既令人惋惜，又教人思索。倘若生命中许多事物都像这面镜子，是否意味着我们注定只能遥望，却无法真正拥有？ “人类最大的悲剧在于，他们注定要追求那些不可得之物。”起初，我对这句话嗤之以鼻。世界这么大，怎么可能所有的追求都是徒劳？然而，当经...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/12/06/NLP%20Insights/%E5%9F%BA%E4%BA%8E%E7%94%9F%E6%88%90%E7%9A%84%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B-llm-%E8%AF%84%E4%BC%B0-%E4%BB%8E%E7%94%9F%E6%88%90%E5%88%B0%E5%88%A4%E6%96%AD%E7%9A%84%E6%9C%BA%E9%81%87%E4%B8%8E%E6%8C%91%E6%88%98.zh-CN/" title="基于生成的大语言模型（LLM）评估：从生成到判断的机遇与挑战">基于生成的大语言模型（LLM）评估：从生成到判断的机遇与挑战</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-12-06T06:34:18.000Z" title="发表于 2024-12-06 14:34:18">2024-12-06</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/NLP-Insights/">NLP Insights</a></span></div><div class="content"> 基于生成的大语言模型（LLM）评估：从生成到判断的机遇与挑战摘要人工智能（AI）与自然语言处理（NLP）领域中的评估任务长期面临挑战。传统的评估方法（如基于匹配或嵌入的技术）在判断复杂属性时效果有限。近期大语言模型（LLM）的发展催生了“LLM-as-a-Judge”范式，利用LLM对任务进行评分、排序或选择。本论文对LLM评估方法进行了全面综述，包括其定义、分类框架、评估基准，以及未来的研究方向。  1. 引言1.1 背景评估是机器学习和NLP的核心问题之一，传统评估方法如BLEU和ROUGE通常基于文本重叠，缺乏对复杂场景的适用性。随着深度学习和LLM的发展（如GPT-4），研究者提出了“LLM-as-a-Judge”模式，以解决传统评估的局限。 1.2 研究问题本论文旨在探讨以下问题：  评估内容：LLM评估什么？ 评估方法：如何进行评估？ 应用场景：LLM在哪里评估？   2. 预备知识2.1 输入格式评估输入可分为：  点对点（Point-Wise）：单个样本评估。 对/列表评估（Pair/List-Wise）：多个样本的比较评估。  2.2 输出格式评估输出包括： ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/12/03/Life%20Reflections/reflections-on-identity-and-subjectivity.zh-CN/" title="身份与主体性的反思">身份与主体性的反思</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-12-03T06:11:06.000Z" title="发表于 2024-12-03 14:11:06">2024-12-03</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Life-Reflections/">Life Reflections</a></span></div><div class="content">永居申请被拒：身份与主体性的反思当我接到永居申请被拒的消息时，短暂的愣神之后，内心涌动的却并非单纯的挫败，而是一种奇异的“生存论困境”感。表面上，这似乎只是一次行政结果的体现，但其背后却深刻折射了当代全球流动性结构与主体性建构之间的多重张力。  在全球化与国家主权的张力下，个体身份的确认究竟是否可能？ 当永居申请被拒时，是否意味着个体被象征性地排除在某种集体意义之外？   永居申请：从权利幻想到身份迷宫在吉登斯的“现代性与自我认同”理论框架下，永居申请不仅是一种居留权的争取，更是一种对身份稳定性与未来可能性的符号化追求。然而，在全球化语境下，这种追求往往陷入德里达所描述的“延异”结构：权利的实现总是被推迟，身份的确认总是悬置。 在此情境中，申请被拒的结果无异于一种符号暴力。它不仅断裂了我对未来的规划，也撕裂了我在这一场域中的主体性幻象。  主体性与制度规训的对抗布尔迪厄的场域理论揭示了权力在社会实践中的分布方式，而永居申请这一制度实践正是权力规训个体的具体化场域。拒绝不仅是一种行政结果，更是一种对主体的隐形规训，暗示了平台资本主义时代个体与制度之间的权力失衡。 福柯的规训视角让我...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/12/02/Code%20Chronicles/leetcode-python%E9%A2%98%E8%A7%A3-1346-check-if-n-and-its-double-exist.zh-CN/" title="【Leetcode Python题解】「1346. Check If N and Its Double Exist」">【Leetcode Python题解】「1346. Check If N and Its Double Exist」</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-12-01T16:00:00.000Z" title="发表于 2024-12-02 00:00:00">2024-12-02</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Code-Chronicles/">Code Chronicles</a></span></div><div class="content">【Leetcode Python题解】「1346. Check If N and Its Double Exist」题目：1346. Check If N and Its Double Exist题目描述给定一个整数数组 arr，检查是否存在两个不同的索引 i 和 j，满足：  i != j 0 &lt;= i, j &lt; arr.length arr[i] == 2 * arr[j]  示例示例 1: 123输入：arr = [10,2,5,3]输出：true解释：对于 i = 0 和 j = 2，arr[i] = 10 等于 2 * 5 = 2 * arr[j]  示例 2: 123输入：arr = [3,1,7,11]输出：false解释：不存在满足条件的 i 和 j。  约束条件 2 &lt;= arr.length &lt;= 500 -10³ &lt;= arr[i] &lt;= 10³  解题思路这道题可以用多种方法解决，我们来分析两种主要的解法：暴力解法和哈希表解法。 1. 暴力解法最直观的解法是使用两层循环，遍历所有可能的数对。 1234567def che...</div></div></div></div><nav id="pagination"><div class="pagination"><span class="page-number current">1</span><a class="page-number" href="/page/2/#content-inner">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/6/#content-inner">6</a><a class="extend next" rel="next" href="/page/2/#content-inner"><i class="fas fa-chevron-right fa-fw"></i></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Huiyu Chen</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">102</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">49</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/21/NLP%20Insights/paper-deep-dive-2026-02-21.en/" title="Paper Deep Dive | SLA2: Sparse-Linear Attention with Learnable Routing and QAT">Paper Deep Dive | SLA2: Sparse-Linear Attention with Learnable Routing and QAT</a><time datetime="2026-02-21T04:28:28.000Z" title="发表于 2026-02-21 12:28:28">2026-02-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/21/NLP%20Insights/paper-deep-dive-2026-02-21.zh-CN/" title="论文深读｜SLA2: Sparse-Linear Attention with Learnable Routing and QAT">论文深读｜SLA2: Sparse-Linear Attention with Learnable Routing and QAT</a><time datetime="2026-02-21T04:28:28.000Z" title="发表于 2026-02-21 12:28:28">2026-02-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/21/NLP%20Insights/evaluation-of-generation-based-large-language-models-llms-opportunities-and-challenges-from-generation-to-judgment.en/" title="evaluation-of-generation-based-large-language-models-llms-opportunities-and-challenges-from-generation-to-judgment">evaluation-of-generation-based-large-language-models-llms-opportunities-and-challenges-from-generation-to-judgment</a><time datetime="2026-02-20T16:00:00.000Z" title="发表于 2026-02-21 00:00:00">2026-02-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/21/NLP%20Insights/evaluation-of-generation-based-large-language-models-llms-opportunities-and-challenges-from-generation-to-judgment.zh-CN/" title="evaluation-of-generation-based-large-language-models-llms-opportunities-and-challenges-from-generation-to-judgment">evaluation-of-generation-based-large-language-models-llms-opportunities-and-challenges-from-generation-to-judgment</a><time datetime="2026-02-20T16:00:00.000Z" title="发表于 2026-02-21 00:00:00">2026-02-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/24/NLP%20Insights/secom-redefining-memory-management-in-conversational-ai.en/" title="SeCom: Redefining Memory Management in Conversational AI">SeCom: Redefining Memory Management in Conversational AI</a><time datetime="2025-06-24T08:00:00.000Z" title="发表于 2025-06-24 16:00:00">2025-06-24</time></div></div></div></div><div class="card-widget card-categories"><div class="item-headline">
            <i class="fas fa-folder-open"></i>
            <span>分类</span>
            
          </div>
          <ul class="card-category-list" id="aside-cat-list">
            <li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Code-Chronicles/"><span class="card-category-list-name">Code Chronicles</span><span class="card-category-list-count">30</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Debugging-Diaries/"><span class="card-category-list-name">Debugging Diaries</span><span class="card-category-list-count">6</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Life-Reflections/"><span class="card-category-list-name">Life Reflections</span><span class="card-category-list-count">14</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/NLP-Insights/"><span class="card-category-list-name">NLP Insights</span><span class="card-category-list-count">40</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Tech-Toolbox/"><span class="card-category-list-name">Tech Toolbox</span><span class="card-category-list-count">8</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Wanderlust-Adventures/"><span class="card-category-list-name">Wanderlust Adventures</span><span class="card-category-list-count">2</span></a></li>
          </ul></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>标签</span></div><div class="card-tag-cloud"><a href="/tags/Memory-Management/" style="font-size: 1.16em; color: #999b9e">Memory Management</a> <a href="/tags/Tool-Use/" style="font-size: 1.16em; color: #999b9e">Tool Use</a> <a href="/tags/Daily-Challenge/" style="font-size: 1.1em; color: #999">Daily Challenge</a> <a href="/tags/%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/" style="font-size: 1.33em; color: #99a2af">每日一题</a> <a href="/tags/Deep-Learning/" style="font-size: 1.16em; color: #999b9e">Deep Learning</a> <a href="/tags/vLLM/" style="font-size: 1.16em; color: #999b9e">vLLM</a> <a href="/tags/Onnx/" style="font-size: 1.21em; color: #999ea4">Onnx</a> <a href="/tags/SGLang/" style="font-size: 1.16em; color: #999b9e">SGLang</a> <a href="/tags/Language-Learning/" style="font-size: 1.27em; color: #99a0a9">Language Learning</a> <a href="/tags/Python-Basic/" style="font-size: 1.39em; color: #99a4b4">Python Basic</a> <a href="/tags/Python/" style="font-size: 1.44em; color: #99a7ba">Python</a> <a href="/tags/Chatbot/" style="font-size: 1.16em; color: #999b9e">Chatbot</a> <a href="/tags/FastChat/" style="font-size: 1.16em; color: #999b9e">FastChat</a> <a href="/tags/Gradient-Descent/" style="font-size: 1.16em; color: #999b9e">Gradient Descent</a> <a href="/tags/Perplexity/" style="font-size: 1.16em; color: #999b9e">Perplexity</a> <a href="/tags/Prompt/" style="font-size: 1.16em; color: #999b9e">Prompt</a> <a href="/tags/%E6%9D%82%E8%B0%88/" style="font-size: 1.16em; color: #999b9e">杂谈</a> <a href="/tags/K8s/" style="font-size: 1.16em; color: #999b9e">K8s</a> <a href="/tags/Language-Modeling/" style="font-size: 1.16em; color: #999b9e">Language Modeling</a> <a href="/tags/Gorilla/" style="font-size: 1.16em; color: #999b9e">Gorilla</a> <a href="/tags/DSSM/" style="font-size: 1.16em; color: #999b9e">DSSM</a> <a href="/tags/LLM/" style="font-size: 1.5em; color: #99a9bf">LLM</a> <a href="/tags/Structured-LLM/" style="font-size: 1.16em; color: #999b9e">Structured LLM</a> <a href="/tags/Paper-Deep-Dive/" style="font-size: 1.16em; color: #999b9e">Paper Deep Dive</a> <a href="/tags/Living-in-Singapore/" style="font-size: 1.1em; color: #999">Living in Singapore</a> <a href="/tags/IssueFix/" style="font-size: 1.27em; color: #99a0a9">IssueFix</a> <a href="/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/" style="font-size: 1.16em; color: #999b9e">动态规划</a> <a href="/tags/English-Vocabulary/" style="font-size: 1.21em; color: #999ea4">English Vocabulary</a> <a href="/tags/Train/" style="font-size: 1.16em; color: #999b9e">Train</a> <a href="/tags/arXiv/" style="font-size: 1.16em; color: #999b9e">arXiv</a> <a href="/tags/FAISS/" style="font-size: 1.16em; color: #999b9e">FAISS</a> <a href="/tags/Research/" style="font-size: 1.16em; color: #999b9e">Research</a> <a href="/tags/Blog/" style="font-size: 1.21em; color: #999ea4">Blog</a> <a href="/tags/Gemma-2/" style="font-size: 1.1em; color: #999">Gemma-2</a> <a href="/tags/Gemma-2-2b-it/" style="font-size: 1.1em; color: #999">Gemma-2-2b-it</a> <a href="/tags/Sports/" style="font-size: 1.21em; color: #999ea4">Sports</a> <a href="/tags/SeCom/" style="font-size: 1.16em; color: #999b9e">SeCom</a> <a href="/tags/Small-Talk/" style="font-size: 1.16em; color: #999b9e">Small Talk</a> <a href="/tags/%E5%8F%8C%E5%91%A8%E8%B5%9B/" style="font-size: 1.27em; color: #99a0a9">双周赛</a> <a href="/tags/Guide-to-Living-on-Singapore-Island/" style="font-size: 1.1em; color: #999">Guide to Living on Singapore Island</a></div></div><div class="card-widget card-archives">
    <div class="item-headline">
      <i class="fas fa-archive"></i>
      <span>归档</span>
      <a class="card-more-btn" href="/archives/"
            title="查看更多">
            <i class="fas fa-angle-right"></i>
          </a>
    </div>
  
    <ul class="card-archive-list">
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2026/02/">
            <span class="card-archive-list-date">
              二月 2026
            </span>
            <span class="card-archive-list-count">4</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/06/">
            <span class="card-archive-list-date">
              六月 2025
            </span>
            <span class="card-archive-list-count">2</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/03/">
            <span class="card-archive-list-date">
              三月 2025
            </span>
            <span class="card-archive-list-count">2</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/02/">
            <span class="card-archive-list-date">
              二月 2025
            </span>
            <span class="card-archive-list-count">4</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2024/12/">
            <span class="card-archive-list-date">
              十二月 2024
            </span>
            <span class="card-archive-list-count">10</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2024/10/">
            <span class="card-archive-list-date">
              十月 2024
            </span>
            <span class="card-archive-list-count">2</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2024/08/">
            <span class="card-archive-list-date">
              八月 2024
            </span>
            <span class="card-archive-list-count">4</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2024/04/">
            <span class="card-archive-list-date">
              四月 2024
            </span>
            <span class="card-archive-list-count">2</span>
          </a>
        </li>
      
    </ul>
  </div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>网站信息</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">文章数目 :</div><div class="item-count">102</div></div><div class="webinfo-item"><div class="item-name">本站访客数 :</div><div class="item-count" id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">本站总浏览量 :</div><div class="item-count" id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">最后更新时间 :</div><div class="item-count" id="last-push-date" data-lastPushDate="2026-02-21T04:28:32.213Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 - 2026 By Huiyu Chen</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 6.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.5.4</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.4"></script><script src="/js/main.js?v=5.5.4"></script><div class="js-pjax"></div><script src="/js/lang-switch.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>