<!DOCTYPE html><html lang="[&quot;zh-CN&quot;,&quot;en&quot;,&quot;default&quot;]" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>LONGNET - Scaling Transformers to 1,000,000,000 Tokens | 黑头呆鱼进化之旅</title><meta name="author" content="Huiyu Chen"><meta name="copyright" content="Huiyu Chen"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="LONGNET：将Transformer扩展到10亿个标记在本篇文章中，我们将详细讨论一个近期发布的先进模型——“LongNet”。该模型由微软亚洲研究院研发，于大约两周前正式公布。LongNet基于Transformer模型构建，其核心理念在于拓展Transformer的应用规模。值得一提的是，研究团队成功地将其扩展至处理10亿个令牌的规模。对于熟悉语言模型的人来说，会明白序列长度对模型性能的影">
<meta property="og:type" content="article">
<meta property="og:title" content="LONGNET - Scaling Transformers to 1,000,000,000 Tokens">
<meta property="og:url" content="https://chenhuiyu.github.io/2023/07/28/NLP%20Insights/longnet-scaling-transformers-to-1-000-000-000-tokens.en/index.html">
<meta property="og:site_name" content="黑头呆鱼进化之旅">
<meta property="og:description" content="LONGNET：将Transformer扩展到10亿个标记在本篇文章中，我们将详细讨论一个近期发布的先进模型——“LongNet”。该模型由微软亚洲研究院研发，于大约两周前正式公布。LongNet基于Transformer模型构建，其核心理念在于拓展Transformer的应用规模。值得一提的是，研究团队成功地将其扩展至处理10亿个令牌的规模。对于熟悉语言模型的人来说，会明白序列长度对模型性能的影">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://chenhuiyu.github.io/img/butterfly-icon.png">
<meta property="article:published_time" content="2023-07-28T06:43:10.000Z">
<meta property="article:modified_time" content="2026-02-20T22:19:17.272Z">
<meta property="article:author" content="Huiyu Chen">
<meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://chenhuiyu.github.io/img/butterfly-icon.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "LONGNET - Scaling Transformers to 1,000,000,000 Tokens",
  "url": "https://chenhuiyu.github.io/2023/07/28/NLP%20Insights/longnet-scaling-transformers-to-1-000-000-000-tokens.en/",
  "image": "https://chenhuiyu.github.io/img/butterfly-icon.png",
  "datePublished": "2023-07-28T06:43:10.000Z",
  "dateModified": "2026-02-20T22:19:17.272Z",
  "author": [
    {
      "@type": "Person",
      "name": "Huiyu Chen",
      "url": "https://chenhuiyu.github.io"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://chenhuiyu.github.io/2023/07/28/NLP%20Insights/longnet-scaling-transformers-to-1-000-000-000-tokens.en/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=5.5.4"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.13.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'LONGNET - Scaling Transformers to 1,000,000,000 Tokens',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 6.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="黑头呆鱼进化之旅" type="application/atom+xml">
</head><body><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">黑头呆鱼进化之旅</span></a><a class="nav-page-title" href="/"><span class="site-name">LONGNET - Scaling Transformers to 1,000,000,000 Tokens</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  Back to Home</span></span></a></span><div id="menus"></div></nav><div id="post-info"><h1 class="post-title">LONGNET - Scaling Transformers to 1,000,000,000 Tokens</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-07-28T06:43:10.000Z" title="Created 2023-07-28 14:43:10">2023-07-28</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2026-02-20T22:19:17.272Z" title="Updated 2026-02-21 06:19:17">2026-02-21</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/NLP-Insights/">NLP Insights</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="LONGNET：将Transformer扩展到10亿个标记"><a href="#LONGNET：将Transformer扩展到10亿个标记" class="headerlink" title="LONGNET：将Transformer扩展到10亿个标记"></a>LONGNET：将Transformer扩展到10亿个标记</h1><p>在本篇文章中，我们将详细讨论一个近期发布的先进模型——“LongNet”。该模型由微软亚洲研究院研发，于大约两周前正式公布。LongNet基于Transformer模型构建，其核心理念在于拓展Transformer的应用规模。值得一提的是，研究团队成功地将其扩展至处理10亿个令牌的规模。对于熟悉语言模型的人来说，会明白序列长度对模型性能的影响，因为序列长度决定了在执行注意力机制时，能够关联的令牌数量，从而影响模型可以获取的上下文信息长度。例如，我们希望像GPT这样的模型能拥有更长的上下文，使得模型可以参考更久之前的单词来预测下一个令牌。而LongNet就成功地将这个能力扩展到了10亿个令牌。以下图为例，可以清晰看出，GPT的序列长度仅为512，而Power Transformer的序列长度可扩展至12、000、64、262、000、甚至1000万，然而LongNet将序列长度扩展至惊人的10亿个令牌。试想一下，我们可以将所有维基百科的文本信息输入到模型中，模型可以利用所有这些令牌进行注意力计算。接下来，让我们首先来了解一下LongNet的工作原理。</p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>在大型语言模型时代，扩展序列长度已成为一个重要需求。然而，现有方法在处理计算复杂性或模型表达能力时遇到困难，导致最大序列长度受限。为了解决这个问题，我们引入了LONGNET，这是一种Transformer变体，可以将序列长度扩展到10亿个标记以上，同时不损害对较短序列的性能。具体而言，我们提出了扩张注意力（dilated attention），随着距离增加，它以指数级扩展注意力范围。LONGNET有显著的优势：1）它具有线性计算复杂性，并且序列中任意两个标记之间存在对数依赖关系；2）它可以作为分布式训练器用于非常长的序列；3）它的扩张注意力可以无缝替换标准注意力，并且可以与现有基于Transformer的优化方法无缝集成。实验结果表明，LONGNET在长序列建模和一般语言任务上表现出强大的性能。我们的工作为建模非常长的序列打开了新的可能性，例如将整个语料库甚至整个互联网视为一个序列。</p>

<h3 id="LongNet的优点"><a href="#LongNet的优点" class="headerlink" title="LongNet的优点"></a>LongNet的优点</h3><p>LongNet具有多种优点。首先，其计算复杂度与序列长度呈线性关系，稍后将具体解释原因。其次，令牌之间存在对数依赖，也就是说，两个距离较远的令牌之间的依赖性较弱，而距离较近的令牌之间的依赖性较强。此外，它可在分布式网络中进行训练，这意味着我们可以利用分布式系统计算该注意力机制，如使用多个GPU或多台计算机。同时，LongNet可以作为标准注意力的替代品，这意味着如果我们已经有一个使用注意力机制的模型，我们只需将注意力机制替换为LongNet的机制，无需改变模型的其他部分，模型仍然能够像以前一样运行，但通过使用这种改进的注意力机制，可以处理更长的序列长度。</p>
<h3 id="关于Transformer模型的梳理"><a href="#关于Transformer模型的梳理" class="headerlink" title="关于Transformer模型的梳理"></a>关于Transformer模型的梳理</h3><p>自注意力机制，我们使用了被称为“Q、K、V”的矩阵。其中，“Q”矩阵代表查询，其规模为“序列长度乘以模型大小”，模型大小指的是每个词嵌入的向量表示。当我们计算查询与键（K）的乘积，或者查询与K的转置的乘积来产生此矩阵时，所需的操作次数是“序列长度的平方乘以模型大小”，因为我们需要为矩阵中的每个元素计算点积。这就是为什么自注意力的复杂度是“序列长度的平方乘以模型大小”。这个比较在相关论文中也有详细描述，常规的注意力复杂度是“序列长度的平方乘以模型大小”，然而LongNet这种新模型，其注意力机制复杂度仅为“序列长度乘以模型大小”，下文我将说明如何实现这种线性复杂度。</p>


<h3 id="LongNet的注意力分配原理"><a href="#LongNet的注意力分配原理" class="headerlink" title="LongNet的注意力分配原理"></a>LongNet的注意力分配原理</h3><p>LongNet的核心原理是，令牌间的注意力分配会随着它们之间距离的增加而呈指数级地减小。让我们参照图表来理解它的运作方式。在传统方式中，我们计算所有令牌与其他所有令牌之间的注意力，但LongNet并未如此操作。它采用了一种将序列切分为不同大小窗口的方法。首先，以4为窗口大小为例，这里的“N”是序列的令牌数，我们将其分成四个大小为4的段，并计算这个小窗口内的所有词与其他词之间的注意力。然后，我们对所有这些小段中的词执行同样的操作，接着使用更大的窗口，这次窗口大小为8。如此类推，直到覆盖整个序列长度，然后我们再以增加窗口大小的方式进行操作，同时我们也增加了跳过的令牌数，即“R”。例如，我们可以先计算大小为8的窗口，然后将“R”设为2，这意味着我们会跳过一个令牌，然后计算注意力，再跳过一个令牌，继续计算注意力。以这种方式，随着窗口大小和跳过的令牌数的增加，计算的复杂度变得更小，因为我们并不是计算每个令牌与所有其他令牌之间的注意力，而是只计算在有限范围内的注意力。这样，LongNet的注意力分配就遵循了对数依赖的原则，即，距离较远的令牌之间的依赖性较弱，而距离较近的令牌之间的依赖性较强。这是LongNet能在更大序列长度上进行工作的关键。<br>扩展注意力由一系列用于建模短程和长程依赖关系的注意力模式组成，注意力模式的数量可以根据序列长度进行扩展。在每个注意力模式中，查询向量和键向量之间的点积被分解为多个子点积，每个子点积仅涉及到一小部分的键向量。这种分解方式可以减少计算复杂度，同时也可以使模型更好地处理长序列。具体如下图所示：</p>


<p>扩张注意力还引入了“多头”机制，可以在不同的头之间分别计算注意力。每个头都有自己的偏移量，这样就可以在不同的位置上计算注意力，从而更好地捕捉序列中的信息。通过这种方式，扩张注意力可以更好地处理长序列，同时保持较短序列的性能。具体如下图所示：</p>


<h3 id="计算复杂度的优化"><a href="#计算复杂度的优化" class="headerlink" title="计算复杂度的优化"></a>计算复杂度的优化</h3><p>现在我们来看看为什么LongNet的计算复杂度是线性的。在传统的自注意力机制中，我们需要执行序列长度平方次数的点积操作，而LongNet通过使用窗口和跳过的方式，将计算的复杂度降低到了线性。如果我们假设窗口大小是固定的，例如为4，然后“R”也是固定的，例如为2，那么计算复杂度将是“O(N)”。当然，在实际操作中，窗口大小和跳过的令牌数可能会根据实际情况进行调整，但是它们是常数，不随序列长度增加而增加。这就是为什么LongNet的计算复杂度是线性的。</p>


<h3 id="Token扩展10亿"><a href="#Token扩展10亿" class="headerlink" title="Token扩展10亿+"></a>Token扩展10亿+</h3><p>分布式训练方法，利用LONGNET的线性计算复杂度，将序列维度分布式地进行训练。具体而言，算法首先将输入序列沿着序列维度进行切分，每个序列片段被分配到不同的设备上进行计算。然后，每个设备将序列片段投影为查询、键和值，并使用本地计算得到局部的注意力权重。对于超出本地设备序列长度的部分，键和值将被发送到其他设备上进行计算。最后，所有设备将局部的注意力权重进行汇总，得到全局的注意力权重，并使用全局的注意力权重计算每个标记的表示。具体如下图所示：</p>


<p>该算法可以在任意数量的设备上进行扩展，并且可以通过并行计算来加速训练过程。由于LONGNET具有线性计算复杂度，因此该算法可以有效地处理超长序列，而不会牺牲训练速度和模型性能。此外，该算法还支持标准Transformer的优化技术，例如内核融合、量化和分布式训练，从而使得LONGNET可以无缝地与现有的深度学习框架进行集成。</p>
<h3 id="LongNet的应用前景"><a href="#LongNet的应用前景" class="headerlink" title="LongNet的应用前景"></a>LongNet的应用前景</h3><p>LongNet的发布为自然语言处理领域带来了诸多潜在的应用前景。首先，它可以应用于更长文本的生成任务，如生成长篇小说或长篇新闻报道。其次，它可以应用于更复杂的对话任务，因为在对话中，我们往往需要处理大量的历史信息和上下文。另外，它还可能在翻译任务中发挥更大的作用，因为翻译往往涉及到处理长句子或长段落的情况。总的来说，LongNet的发布为我们提供了处理更长文本的新工具和可能性。</p>
<h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><p>总的来说，LongNet是一个基于Transformer的模型，它成功地将自注意力机制扩展到了10亿个令牌，实现了处理更长文本的能力。它的优势包括计算复杂度是线性的、遵循对数依赖原则，以及可以在分布式系统上进行训练。通过LongNet，我们可以探索更多自然语言处理任务，并处理那些过去由于序列长度限制而难以处理的任务。这个新模型的发布为我们带来了更多可能性。</p>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2307.02486.pdf">LONGNET: Scaling Transformers to 1,000,000,000 Tokens</a></li>
<li><a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=nC2nU9j9DVQ">Youtube Tutorial by Umar Jamil</a></li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="https://chenhuiyu.github.io">Huiyu Chen</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="https://chenhuiyu.github.io/2023/07/28/NLP%20Insights/longnet-scaling-transformers-to-1-000-000-000-tokens.en/">https://chenhuiyu.github.io/2023/07/28/NLP%20Insights/longnet-scaling-transformers-to-1-000-000-000-tokens.en/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles on this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless otherwise stated.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/LLM/">LLM</a></div><div class="post-share"><div class="social-share" data-image="/img/butterfly-icon.png" data-sites="facebook,x,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2023/08/02/NLP%20Insights/training-llama-2-model-on-single-gpu-with-int8-quantization-and-lora.zh-CN/" title="Training Llama 2 Model on Single GPU with int8 Quantization and LoRA"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">Previous</div><div class="info-item-2">Training Llama 2 Model on Single GPU with int8 Quantization and LoRA</div></div><div class="info-2"><div class="info-item-1">Training Llama 2 Model on Single GPU with int8 Quantization and LoRALlama 2概述Llama 2 是一个包含预训练和微调的生成式文本模型的集合，其规模从 70 亿到 700 亿个参数不等。Llama2模型是由Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert等人在Llama 2: Open Foundation and Fine-Tuned Chat Models中提出的。 该论文的摘要如下： 在这项工作中，我们开发并发布了Llama 2，这是一组从70亿到700亿参数的预训练和微调的大型语言模型（LLMs）。我们的微调LLMs，称为Llama 2-Chat，针对对话用例进行了优化。我们的模型在我们测试的大多数基准上胜过开源聊天模型，并且基于我们对有用性和安全性的人类评估，可能是闭源模型的合适替代品。我们提供了关于微调和改进Llama 2-Chat安全性的方法的详细描述，以便社区能够在我们的工作基础上构建，并有助于LLMs的负责任发展。 在此处查看所有L...</div></div></div></a><a class="pagination-related" href="/2023/07/28/NLP%20Insights/longnet-scaling-transformers-to-1-000-000-000-tokens.zh-CN/" title="LONGNET - Scaling Transformers to 1,000,000,000 Tokens"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">Next</div><div class="info-item-2">LONGNET - Scaling Transformers to 1,000,000,000 Tokens</div></div><div class="info-2"><div class="info-item-1">LONGNET：将Transformer扩展到10亿个标记在本篇文章中，我们将详细讨论一个近期发布的先进模型——“LongNet”。该模型由微软亚洲研究院研发，于大约两周前正式公布。LongNet基于Transformer模型构建，其核心理念在于拓展Transformer的应用规模。值得一提的是，研究团队成功地将其扩展至处理10亿个令牌的规模。对于熟悉语言模型的人来说，会明白序列长度对模型性能的影响，因为序列长度决定了在执行注意力机制时，能够关联的令牌数量，从而影响模型可以获取的上下文信息长度。例如，我们希望像GPT这样的模型能拥有更长的上下文，使得模型可以参考更久之前的单词来预测下一个令牌。而LongNet就成功地将这个能力扩展到了10亿个令牌。以下图为例，可以清晰看出，GPT的序列长度仅为512，而Power Transformer的序列长度可扩展至12、000、64、262、000、甚至1000万，然而LongNet将序列长度扩展至惊人的10亿个令牌。试想一下，我们可以将所有维基百科的文本信息输入到模型中，模型可以利用所有这些令牌进行注意力计算。接下来，让我们首先来了解一下...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2023/08/02/NLP%20Insights/training-llama-2-model-on-single-gpu-with-int8-quantization-and-lora.zh-CN/" title="Training Llama 2 Model on Single GPU with int8 Quantization and LoRA"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2023-08-02</div><div class="info-item-2">Training Llama 2 Model on Single GPU with int8 Quantization and LoRA</div></div><div class="info-2"><div class="info-item-1">Training Llama 2 Model on Single GPU with int8 Quantization and LoRALlama 2概述Llama 2 是一个包含预训练和微调的生成式文本模型的集合，其规模从 70 亿到 700 亿个参数不等。Llama2模型是由Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert等人在Llama 2: Open Foundation and Fine-Tuned Chat Models中提出的。 该论文的摘要如下： 在这项工作中，我们开发并发布了Llama 2，这是一组从70亿到700亿参数的预训练和微调的大型语言模型（LLMs）。我们的微调LLMs，称为Llama 2-Chat，针对对话用例进行了优化。我们的模型在我们测试的大多数基准上胜过开源聊天模型，并且基于我们对有用性和安全性的人类评估，可能是闭源模型的合适替代品。我们提供了关于微调和改进Llama 2-Chat安全性的方法的详细描述，以便社区能够在我们的工作基础上构建，并有助于LLMs的负责任发展。 在此处查看所有L...</div></div></div></a><a class="pagination-related" href="/2025/02/11/NLP%20Insights/moe%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97-%E6%8F%AD%E7%A7%98-moe-%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2.zh-CN/" title="MoE模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-02-11</div><div class="info-item-2">MoE模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色</div></div><div class="info-2"><div class="info-item-1">MoE 模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色原文地址：A Visual Guide to Mixture of Experts (MoE) 📅 作者：Maarten Grootendorst 📆 日期：2024 年 10 月 7 日  探索语言模型：混合专家模型（MoE）可视化指南目录 MoE 模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色 探索语言模型：混合专家模型（MoE）可视化指南 目录 什么是混合专家（MoE）模型？ Experts Dense Layers Sparse Layers What does an Expert Learn? 专家的架构（Architecture of Experts）      当我们查看最新发布的大型语言模型（LLMs，Large Language Models）时，常常会在标题中看到 “MoE”。这个 “MoE” 代表什么？为什么这么多 LLM 都在使用它？ 在这份可视化指南中，我们会通过 50 多个可视化图示，逐步探索这一关键组件：**Mixture of Experts (MoE)**。   图示内...</div></div></div></a><a class="pagination-related" href="/2023/07/28/NLP%20Insights/longnet-scaling-transformers-to-1-000-000-000-tokens.zh-CN/" title="LONGNET - Scaling Transformers to 1,000,000,000 Tokens"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2023-07-28</div><div class="info-item-2">LONGNET - Scaling Transformers to 1,000,000,000 Tokens</div></div><div class="info-2"><div class="info-item-1">LONGNET：将Transformer扩展到10亿个标记在本篇文章中，我们将详细讨论一个近期发布的先进模型——“LongNet”。该模型由微软亚洲研究院研发，于大约两周前正式公布。LongNet基于Transformer模型构建，其核心理念在于拓展Transformer的应用规模。值得一提的是，研究团队成功地将其扩展至处理10亿个令牌的规模。对于熟悉语言模型的人来说，会明白序列长度对模型性能的影响，因为序列长度决定了在执行注意力机制时，能够关联的令牌数量，从而影响模型可以获取的上下文信息长度。例如，我们希望像GPT这样的模型能拥有更长的上下文，使得模型可以参考更久之前的单词来预测下一个令牌。而LongNet就成功地将这个能力扩展到了10亿个令牌。以下图为例，可以清晰看出，GPT的序列长度仅为512，而Power Transformer的序列长度可扩展至12、000、64、262、000、甚至1000万，然而LongNet将序列长度扩展至惊人的10亿个令牌。试想一下，我们可以将所有维基百科的文本信息输入到模型中，模型可以利用所有这些令牌进行注意力计算。接下来，让我们首先来了解一下...</div></div></div></a><a class="pagination-related" href="/2025/02/11/NLP%20Insights/moe%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97-%E6%8F%AD%E7%A7%98-moe-%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2.en/" title="MoE模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-02-11</div><div class="info-item-2">MoE模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色</div></div><div class="info-2"><div class="info-item-1">MoE 模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色原文地址：A Visual Guide to Mixture of Experts (MoE) 📅 作者：Maarten Grootendorst 📆 日期：2024 年 10 月 7 日  探索语言模型：混合专家模型（MoE）可视化指南目录 MoE 模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色 探索语言模型：混合专家模型（MoE）可视化指南 目录 什么是混合专家（MoE）模型？ Experts Dense Layers Sparse Layers What does an Expert Learn? 专家的架构（Architecture of Experts）      当我们查看最新发布的大型语言模型（LLMs，Large Language Models）时，常常会在标题中看到 “MoE”。这个 “MoE” 代表什么？为什么这么多 LLM 都在使用它？ 在这份可视化指南中，我们会通过 50 多个可视化图示，逐步探索这一关键组件：**Mixture of Experts (MoE)**。   图示内...</div></div></div></a><a class="pagination-related" href="/2024/02/28/NLP%20Insights/gorilla-llm-%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B.zh-CN/" title="Gorilla LLM 大语言模型简介"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-28</div><div class="info-item-2">Gorilla LLM 大语言模型简介</div></div><div class="info-2"><div class="info-item-1">Gorilla LLM 大语言模型简介🦍 Gorilla: Large Language Model Connected with Massive APIsLink: https://gorilla.cs.berkeley.edu/blogs/7_open_functions_v2.html  Berkeley 功能调用排行榜Berkeley 功能调用排行榜 在线体验模型：Gorilla OpenFunctions-v2 网络演示 项目详情：GitHub 模型（7B 参数）在 HuggingFace 上的页面：gorilla-llm/gorilla-openfunctions-v2  1. 伯克利函数调用排行榜自 2022 年底以来，大语言模型（LLMs）凭借其执行通用任务的强大能力，成为众人关注的焦点。不仅限于聊天应用，将这些模型应用于开发各类 AI 应用和软件（如 Langchain, Llama Index, AutoGPT, Voyager）已成为一种趋势。GPT, Gemini, Llama, Mistral 等模型通过与外部世界的交互，如函数调用和执行，展现了其巨大...</div></div></div></a><a class="pagination-related" href="/2025/03/06/NLP%20Insights/differences-in-padding-strategies-between-decoder-only-and-encoder-only-models.en/" title="Differences in Padding Strategies Between Decoder-only and Encoder-only Models"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-06</div><div class="info-item-2">Differences in Padding Strategies Between Decoder-only and Encoder-only Models</div></div><div class="info-2"><div class="info-item-1">📌 What is Padding?In Large Language Models (LLMs), padding is a method used to standardize sequence lengths for batch processing. For example: 12Sentence 1: "I love NLP"Sentence 2: "Padding is useful in LLM training"  Using the &lt;pad&gt; token for alignment: 12"I love NLP &lt;pad&gt; &lt;pad&gt; &lt;pad&gt;""Padding is useful in LLM training"   📌 Padding Positioning: Left vs RightThere are two common padding strategies:  Right padding: 1"I love NLP &lt;pad&gt; &lt;pad&gt;"  Left padding: ...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Huiyu Chen</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">100</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">46</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#LONGNET%EF%BC%9A%E5%B0%86Transformer%E6%89%A9%E5%B1%95%E5%88%B010%E4%BA%BF%E4%B8%AA%E6%A0%87%E8%AE%B0"><span class="toc-number">1.</span> <span class="toc-text">LONGNET：将Transformer扩展到10亿个标记</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-number">1.1.</span> <span class="toc-text">摘要</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#LongNet%E7%9A%84%E4%BC%98%E7%82%B9"><span class="toc-number">1.1.1.</span> <span class="toc-text">LongNet的优点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B3%E4%BA%8ETransformer%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%A2%B3%E7%90%86"><span class="toc-number">1.1.2.</span> <span class="toc-text">关于Transformer模型的梳理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LongNet%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%88%86%E9%85%8D%E5%8E%9F%E7%90%86"><span class="toc-number">1.1.3.</span> <span class="toc-text">LongNet的注意力分配原理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E5%A4%8D%E6%9D%82%E5%BA%A6%E7%9A%84%E4%BC%98%E5%8C%96"><span class="toc-number">1.1.4.</span> <span class="toc-text">计算复杂度的优化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Token%E6%89%A9%E5%B1%9510%E4%BA%BF"><span class="toc-number">1.1.5.</span> <span class="toc-text">Token扩展10亿+</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#LongNet%E7%9A%84%E5%BA%94%E7%94%A8%E5%89%8D%E6%99%AF"><span class="toc-number">1.1.6.</span> <span class="toc-text">LongNet的应用前景</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%93%E8%AE%BA"><span class="toc-number">1.1.7.</span> <span class="toc-text">结论</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#References"><span class="toc-number">2.</span> <span class="toc-text">References</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/21/NLP%20Insights/evaluation-of-generation-based-large-language-models-llms-opportunities-and-challenges-from-generation-to-judgment.en/" title="evaluation-of-generation-based-large-language-models-llms-opportunities-and-challenges-from-generation-to-judgment">evaluation-of-generation-based-large-language-models-llms-opportunities-and-challenges-from-generation-to-judgment</a><time datetime="2026-02-20T16:00:00.000Z" title="发表于 2026-02-21 00:00:00">2026-02-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/21/NLP%20Insights/evaluation-of-generation-based-large-language-models-llms-opportunities-and-challenges-from-generation-to-judgment.zh-CN/" title="evaluation-of-generation-based-large-language-models-llms-opportunities-and-challenges-from-generation-to-judgment">evaluation-of-generation-based-large-language-models-llms-opportunities-and-challenges-from-generation-to-judgment</a><time datetime="2026-02-20T16:00:00.000Z" title="发表于 2026-02-21 00:00:00">2026-02-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/24/NLP%20Insights/secom-redefining-memory-management-in-conversational-ai.en/" title="SeCom: Redefining Memory Management in Conversational AI">SeCom: Redefining Memory Management in Conversational AI</a><time datetime="2025-06-24T08:00:00.000Z" title="发表于 2025-06-24 16:00:00">2025-06-24</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/24/NLP%20Insights/secom-redefining-memory-management-in-conversational-ai.zh-CN/" title="SeCom: 重新定义对话AI的记忆管理">SeCom: 重新定义对话AI的记忆管理</a><time datetime="2025-06-24T08:00:00.000Z" title="发表于 2025-06-24 16:00:00">2025-06-24</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/03/06/NLP%20Insights/differences-in-padding-strategies-between-decoder-only-and-encoder-only-models.en/" title="Differences in Padding Strategies Between Decoder-only and Encoder-only Models">Differences in Padding Strategies Between Decoder-only and Encoder-only Models</a><time datetime="2025-03-06T09:43:10.000Z" title="发表于 2025-03-06 17:43:10">2025-03-06</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 - 2026 By Huiyu Chen</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 6.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.5.4</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Reading Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light and Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle Between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Settings"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back to Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.4"></script><script src="/js/main.js?v=5.5.4"></script><div class="js-pjax"></div><script src="/js/lang-switch.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>