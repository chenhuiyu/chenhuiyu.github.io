<!DOCTYPE html><html lang="[&quot;zh-CN&quot;,&quot;en&quot;,&quot;default&quot;]" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Training Llama 2 Model on Single GPU with int8 Quantization and LoRA | é»‘å¤´å‘†é±¼è¿›åŒ–ä¹‹æ—…</title><meta name="author" content="Huiyu Chen"><meta name="copyright" content="Huiyu Chen"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Training Llama 2 Model on Single GPU with int8 Quantization and LoRALlama 2æ¦‚è¿°Llama 2 æ˜¯ä¸€ä¸ªåŒ…å«é¢„è®­ç»ƒå’Œå¾®è°ƒçš„ç”Ÿæˆå¼æ–‡æœ¬æ¨¡å‹çš„é›†åˆï¼Œå…¶è§„æ¨¡ä» 70 äº¿åˆ° 700 äº¿ä¸ªå‚æ•°ä¸ç­‰ã€‚Llama2æ¨¡å‹æ˜¯ç”±Hugo Touvron, Louis Martin, Kevin Stone, Peter Albertç­‰äººåœ¨Llam">
<meta property="og:type" content="article">
<meta property="og:title" content="Training Llama 2 Model on Single GPU with int8 Quantization and LoRA">
<meta property="og:url" content="https://chenhuiyu.github.io/2023/08/02/NLP%20Insights/training-llama-2-model-on-single-gpu-with-int8-quantization-and-lora.zh-CN/index.html">
<meta property="og:site_name" content="é»‘å¤´å‘†é±¼è¿›åŒ–ä¹‹æ—…">
<meta property="og:description" content="Training Llama 2 Model on Single GPU with int8 Quantization and LoRALlama 2æ¦‚è¿°Llama 2 æ˜¯ä¸€ä¸ªåŒ…å«é¢„è®­ç»ƒå’Œå¾®è°ƒçš„ç”Ÿæˆå¼æ–‡æœ¬æ¨¡å‹çš„é›†åˆï¼Œå…¶è§„æ¨¡ä» 70 äº¿åˆ° 700 äº¿ä¸ªå‚æ•°ä¸ç­‰ã€‚Llama2æ¨¡å‹æ˜¯ç”±Hugo Touvron, Louis Martin, Kevin Stone, Peter Albertç­‰äººåœ¨Llam">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://chenhuiyu.github.io/img/butterfly-icon.png">
<meta property="article:published_time" content="2023-08-02T07:38:29.000Z">
<meta property="article:modified_time" content="2026-02-20T22:19:17.270Z">
<meta property="article:author" content="Huiyu Chen">
<meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://chenhuiyu.github.io/img/butterfly-icon.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Training Llama 2 Model on Single GPU with int8 Quantization and LoRA",
  "url": "https://chenhuiyu.github.io/2023/08/02/NLP%20Insights/training-llama-2-model-on-single-gpu-with-int8-quantization-and-lora.zh-CN/",
  "image": "https://chenhuiyu.github.io/img/butterfly-icon.png",
  "datePublished": "2023-08-02T07:38:29.000Z",
  "dateModified": "2026-02-20T22:19:17.270Z",
  "author": [
    {
      "@type": "Person",
      "name": "Huiyu Chen",
      "url": "https://chenhuiyu.github.io"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://chenhuiyu.github.io/2023/08/02/NLP%20Insights/training-llama-2-model-on-single-gpu-with-int8-quantization-and-lora.zh-CN/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=5.5.4"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: 'å¤åˆ¶æˆåŠŸ',
    error: 'å¤åˆ¶å¤±è´¥',
    noSupport: 'æµè§ˆå™¨ä¸æ”¯æŒ'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'åˆšåˆš',
    min: 'åˆ†é’Ÿå‰',
    hour: 'å°æ—¶å‰',
    day: 'å¤©å‰',
    month: 'ä¸ªæœˆå‰'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.13.0/dist/infinitegrid.min.js',
    buttonText: 'åŠ è½½æ›´å¤š'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Training Llama 2 Model on Single GPU with int8 Quantization and LoRA',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 6.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="é»‘å¤´å‘†é±¼è¿›åŒ–ä¹‹æ—…" type="application/atom+xml">
</head><body><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">é»‘å¤´å‘†é±¼è¿›åŒ–ä¹‹æ—…</span></a><a class="nav-page-title" href="/"><span class="site-name">Training Llama 2 Model on Single GPU with int8 Quantization and LoRA</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  è¿”å›é¦–é¡µ</span></span></a></span><div id="menus"></div></nav><div id="post-info"><h1 class="post-title">Training Llama 2 Model on Single GPU with int8 Quantization and LoRA</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">å‘è¡¨äº</span><time class="post-meta-date-created" datetime="2023-08-02T07:38:29.000Z" title="å‘è¡¨äº 2023-08-02 15:38:29">2023-08-02</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">æ›´æ–°äº</span><time class="post-meta-date-updated" datetime="2026-02-20T22:19:17.270Z" title="æ›´æ–°äº 2026-02-21 06:19:17">2026-02-21</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/NLP-Insights/">NLP Insights</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">æµè§ˆé‡:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="Training-Llama-2-Model-on-Single-GPU-with-int8-Quantization-and-LoRA"><a href="#Training-Llama-2-Model-on-Single-GPU-with-int8-Quantization-and-LoRA" class="headerlink" title="Training Llama 2 Model on Single GPU with int8 Quantization and LoRA"></a>Training Llama 2 Model on Single GPU with int8 Quantization and LoRA</h1><h1 id="Llama-2"><a href="#Llama-2" class="headerlink" title="Llama 2"></a>Llama 2</h1><h2 id="æ¦‚è¿°"><a href="#æ¦‚è¿°" class="headerlink" title="æ¦‚è¿°"></a>æ¦‚è¿°</h2><p><em>Llama 2</em> æ˜¯ä¸€ä¸ªåŒ…å«é¢„è®­ç»ƒå’Œå¾®è°ƒçš„ç”Ÿæˆå¼æ–‡æœ¬æ¨¡å‹çš„é›†åˆï¼Œå…¶è§„æ¨¡ä» 70 äº¿åˆ° 700 äº¿ä¸ªå‚æ•°ä¸ç­‰ã€‚Llama2æ¨¡å‹æ˜¯ç”±Hugo Touvron, Louis Martin, Kevin Stone, Peter Albertç­‰äººåœ¨<a target="_blank" rel="noopener" href="https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/">Llama 2: Open Foundation and Fine-Tuned Chat Models</a>ä¸­æå‡ºçš„ã€‚</p>
<p>è¯¥è®ºæ–‡çš„æ‘˜è¦å¦‚ä¸‹ï¼š</p>
<p>åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼€å‘å¹¶å‘å¸ƒäº†Llama 2ï¼Œè¿™æ˜¯ä¸€ç»„ä»70äº¿åˆ°700äº¿å‚æ•°çš„é¢„è®­ç»ƒå’Œå¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€‚æˆ‘ä»¬çš„å¾®è°ƒLLMsï¼Œç§°ä¸ºLlama 2-Chatï¼Œé’ˆå¯¹å¯¹è¯ç”¨ä¾‹è¿›è¡Œäº†ä¼˜åŒ–ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨æˆ‘ä»¬æµ‹è¯•çš„å¤§å¤šæ•°åŸºå‡†ä¸Šèƒœè¿‡å¼€æºèŠå¤©æ¨¡å‹ï¼Œå¹¶ä¸”åŸºäºæˆ‘ä»¬å¯¹æœ‰ç”¨æ€§å’Œå®‰å…¨æ€§çš„äººç±»è¯„ä¼°ï¼Œå¯èƒ½æ˜¯é—­æºæ¨¡å‹çš„åˆé€‚æ›¿ä»£å“ã€‚æˆ‘ä»¬æä¾›äº†å…³äºå¾®è°ƒå’Œæ”¹è¿›Llama 2-Chatå®‰å…¨æ€§çš„æ–¹æ³•çš„è¯¦ç»†æè¿°ï¼Œä»¥ä¾¿ç¤¾åŒºèƒ½å¤Ÿåœ¨æˆ‘ä»¬çš„å·¥ä½œåŸºç¡€ä¸Šæ„å»ºï¼Œå¹¶æœ‰åŠ©äºLLMsçš„è´Ÿè´£ä»»å‘å±•ã€‚</p>
<p><a target="_blank" rel="noopener" href="https://huggingface.co/models?search=llama2">åœ¨æ­¤å¤„æŸ¥çœ‹æ‰€æœ‰Llama2æ¨¡å‹</a></p>
<h3 id="æç¤ºï¼š"><a href="#æç¤ºï¼š" class="headerlink" title="æç¤ºï¼š"></a>æç¤ºï¼š</h3><ul>
<li>é€šè¿‡å¡«å†™<a target="_blank" rel="noopener" href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/">æ­¤è¡¨æ ¼</a>å¯ä»¥è·å¾—Llama2æ¨¡å‹çš„æƒé‡</li>
<li>è¯¥æ¶æ„ä¸ç¬¬ä¸€ä¸ªLlamaéå¸¸ç›¸ä¼¼ï¼Œå¢åŠ äº†Groupe Query Attentionï¼ˆGQAï¼‰<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2305.13245.pdf">æ­¤è®ºæ–‡</a>ä¹‹å</li>
<li>å°†<code>config.pretraining_tp</code>è®¾ç½®ä¸ºä¸åŒäº1çš„å€¼å°†æ¿€æ´»çº¿æ€§å±‚çš„æ›´å‡†ç¡®ä½†æ›´æ…¢çš„è®¡ç®—ï¼Œè¿™åº”æ›´å¥½åœ°åŒ¹é…åŸå§‹logitsã€‚</li>
<li>åŸå§‹æ¨¡å‹ä½¿ç”¨<code>pad_id = -1</code>ï¼Œè¿™æ„å‘³ç€æ²¡æœ‰å¡«å……ä»¤ç‰Œã€‚æˆ‘ä»¬ä¸èƒ½ä½¿ç”¨ç›¸åŒçš„é€»è¾‘ï¼Œè¯·ç¡®ä¿ä½¿ç”¨<code>tokenizer.add_special_tokens({"pad_token":"&lt;pad&gt;"})</code>æ·»åŠ å¡«å……ä»¤ç‰Œï¼Œå¹¶ç›¸åº”åœ°è°ƒæ•´ä»¤ç‰ŒåµŒå…¥å¤§å°ã€‚æ‚¨è¿˜åº”è®¾ç½®<code>model.config.pad_token_id</code>ã€‚æ¨¡å‹çš„embed_tokenså±‚ç”¨<code>self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.config.padding_idx)</code>åˆå§‹åŒ–ï¼Œç¡®ä¿ç¼–ç å¡«å……ä»¤ç‰Œå°†è¾“å‡ºé›¶ï¼Œå› æ­¤åœ¨åˆå§‹åŒ–æ—¶ä¼ é€’å®ƒæ˜¯æ¨èçš„ã€‚</li>
<li>å¡«å†™è¡¨æ ¼å¹¶è·å¾—æ¨¡å‹æ£€æŸ¥ç‚¹çš„è®¿é—®æƒé™åï¼Œæ‚¨åº”è¯¥èƒ½å¤Ÿä½¿ç”¨å·²è½¬æ¢çš„æ£€æŸ¥ç‚¹ã€‚å¦åˆ™ï¼Œå¦‚æœæ‚¨æ­£åœ¨è½¬æ¢è‡ªå·±çš„æ¨¡å‹ï¼Œè¯·éšæ—¶ä½¿ç”¨è½¬æ¢è„šæœ¬ã€‚å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ï¼ˆç¤ºä¾‹ï¼‰å‘½ä»¤è°ƒç”¨è„šæœ¬ï¼š<figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python src/transformers/models/llama/convert_llama_weights_to_hf.py \</span><br><span class="line">    --input_dir /path/to/downloaded/llama/weights --model_size 7B --output_dir /output/path</span><br></pre></td></tr></tbody></table></figure></li>
</ul>
<h2 id="æ¨¡å‹è¯¦æƒ…"><a href="#æ¨¡å‹è¯¦æƒ…" class="headerlink" title="æ¨¡å‹è¯¦æƒ…"></a>æ¨¡å‹è¯¦æƒ…</h2><p>æ³¨æ„ï¼šä½¿ç”¨è¯¥æ¨¡å‹å— Meta è®¸å¯è¯çš„çº¦æŸã€‚ä¸ºäº†ä¸‹è½½æ¨¡å‹æƒé‡å’Œåˆ†è¯å™¨ï¼Œè¯·è®¿é—®ç½‘ç«™å¹¶åœ¨è¯·æ±‚è®¿é—®ä¹‹å‰æ¥å—è®¸å¯è¯ã€‚</p>
<p>Meta å¼€å‘å¹¶å…¬å¼€å‘å¸ƒäº† Llama 2 ç³»åˆ—å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œè¿™æ˜¯ä¸€ç³»åˆ—è§„æ¨¡ä» 70 äº¿åˆ° 700 äº¿å‚æ•°çš„é¢„è®­ç»ƒå’Œå¾®è°ƒçš„ç”Ÿæˆå¼æ–‡æœ¬æ¨¡å‹ã€‚æˆ‘ä»¬çš„å¾®è°ƒ LLMsï¼Œç§°ä¸º Llama-2-Chatï¼Œç»è¿‡ä¼˜åŒ–ç”¨äºå¯¹è¯åº”ç”¨åœºæ™¯ã€‚Llama-2-Chat æ¨¡å‹åœ¨æˆ‘ä»¬æµ‹è¯•çš„å¤§å¤šæ•°åŸºå‡†æµ‹è¯•ä¸­ä¼˜äºå¼€æºèŠå¤©æ¨¡å‹ï¼Œå¹¶åœ¨æˆ‘ä»¬çš„äººå·¥è¯„ä¼°ä¸­åœ¨æœ‰ç”¨æ€§å’Œå®‰å…¨æ€§æ–¹é¢ä¸ä¸€äº›æµè¡Œçš„é—­æºæ¨¡å‹ï¼ˆå¦‚ChatGPTå’ŒPaLMï¼‰æŒå¹³ã€‚</p>
<h2 id="æ¨¡å‹å¼€å‘è€…"><a href="#æ¨¡å‹å¼€å‘è€…" class="headerlink" title="æ¨¡å‹å¼€å‘è€…"></a>æ¨¡å‹å¼€å‘è€…</h2><p>Model Developers Meta</p>
<h2 id="ä¸åŒç‰ˆæœ¬"><a href="#ä¸åŒç‰ˆæœ¬" class="headerlink" title="ä¸åŒç‰ˆæœ¬"></a>ä¸åŒç‰ˆæœ¬</h2><p>Llama 2 æœ‰ä¸åŒè§„æ¨¡çš„å‚æ•°ç‰ˆæœ¬ï¼ŒåŒ…æ‹¬ 7Bã€13B å’Œ 70Bï¼Œä»¥åŠé¢„è®­ç»ƒå’Œå¾®è°ƒçš„å˜ä½“ã€‚</p>
<h2 id="è¾“å…¥è¾“å‡º"><a href="#è¾“å…¥è¾“å‡º" class="headerlink" title="è¾“å…¥è¾“å‡º"></a>è¾“å…¥è¾“å‡º</h2><p>è¾“å…¥æ¨¡å‹ä»…æ”¯æŒæ–‡æœ¬è¾“å…¥ã€‚</p>
<p>è¾“å‡ºæ¨¡å‹ä»…ç”Ÿæˆæ–‡æœ¬ã€‚</p>
<h2 id="æ¨¡å‹æ¶æ„"><a href="#æ¨¡å‹æ¶æ„" class="headerlink" title="æ¨¡å‹æ¶æ„"></a>æ¨¡å‹æ¶æ„</h2><p>Llama 2 æ˜¯ä¸€ç§è‡ªå›å½’è¯­è¨€æ¨¡å‹ï¼Œé‡‡ç”¨äº†ä¼˜åŒ–çš„ Transformer æ¶æ„ã€‚å¾®è°ƒç‰ˆæœ¬ä½¿ç”¨æœ‰ç›‘ç£çš„å¾®è°ƒï¼ˆSFTï¼‰å’ŒåŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰æ¥ä¸äººç±»å¯¹ helpfulness å’Œ safety çš„åå¥½ä¿æŒä¸€è‡´ã€‚</p>
<h2 id="è®­ç»ƒæ•°æ®"><a href="#è®­ç»ƒæ•°æ®" class="headerlink" title="è®­ç»ƒæ•°æ®"></a>è®­ç»ƒæ•°æ®</h2><table>
<thead>
<tr>
<th>æ¨¡å‹åç§°</th>
<th>è®­ç»ƒæ•°æ®</th>
<th>å‚æ•°è§„æ¨¡</th>
<th>å†…å®¹é•¿åº¦</th>
<th>GQA</th>
<th>Tokens</th>
<th>LR</th>
</tr>
</thead>
<tbody><tr>
<td>Llama 2</td>
<td>ä¸€ç§æ–°çš„å…¬å¼€å¯ç”¨çš„åœ¨çº¿æ•°æ®æ··åˆ</td>
<td>7B</td>
<td>4k</td>
<td>âœ—</td>
<td>2.0T</td>
<td>3.0 x 10-4</td>
</tr>
<tr>
<td>Llama 2</td>
<td>ä¸€ç§æ–°çš„å…¬å¼€å¯ç”¨çš„åœ¨çº¿æ•°æ®æ··åˆ</td>
<td>13B</td>
<td>4k</td>
<td>âœ—</td>
<td>2.0T</td>
<td>3.0 x 10-4</td>
</tr>
<tr>
<td>Llama 2</td>
<td>ä¸€ç§æ–°çš„å…¬å¼€å¯ç”¨çš„åœ¨çº¿æ•°æ®æ··åˆ</td>
<td>70B</td>
<td>4k</td>
<td>âœ”</td>
<td>2.0T</td>
<td>1.5 x 10-4</td>
</tr>
</tbody></table>
<p>æ³¨ï¼šToken counts ä»…æŒ‡é¢„è®­ç»ƒæ•°æ®ã€‚æ‰€æœ‰æ¨¡å‹éƒ½ä½¿ç”¨å…¨å±€ batch-size ä¸º 4M tokens è¿›è¡Œè®­ç»ƒã€‚è§„æ¨¡æ›´å¤§çš„æ¨¡å‹ï¼ˆ70Bï¼‰ä½¿ç”¨ Grouped-Query Attentionï¼ˆGQAï¼‰æ¥æé«˜æ¨ç†å¯ä¼¸ç¼©æ€§ã€‚</p>
<h2 id="æ¨¡å‹è®­ç»ƒæ—¥æœŸ"><a href="#æ¨¡å‹è®­ç»ƒæ—¥æœŸ" class="headerlink" title="æ¨¡å‹è®­ç»ƒæ—¥æœŸ"></a>æ¨¡å‹è®­ç»ƒæ—¥æœŸ</h2><p>Llama 2 åœ¨2023å¹´1æœˆè‡³2023å¹´7æœˆä¹‹é—´è¿›è¡Œè®­ç»ƒã€‚</p>
<h2 id="çŠ¶æ€"><a href="#çŠ¶æ€" class="headerlink" title="çŠ¶æ€"></a>çŠ¶æ€</h2><p>è¿™æ˜¯ä¸€ä¸ªåœ¨ç¦»çº¿æ•°æ®é›†ä¸Šè®­ç»ƒçš„é™æ€æ¨¡å‹ã€‚éšç€æˆ‘ä»¬æ ¹æ®ç¤¾åŒºåé¦ˆæ”¹è¿›æ¨¡å‹çš„å®‰å…¨æ€§ï¼Œå°†å‘å¸ƒå¾®è°ƒç‰ˆæœ¬çš„æœªæ¥ç‰ˆæœ¬ã€‚</p>
<h2 id="è®¸å¯è¯"><a href="#è®¸å¯è¯" class="headerlink" title="è®¸å¯è¯"></a>è®¸å¯è¯</h2><p>å®šåˆ¶çš„å•†ä¸šè®¸å¯è¯å¯åœ¨ä»¥ä¸‹ç½‘å€è·å–ï¼š<a target="_blank" rel="noopener" href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/">https://ai.meta.com/resources/models-and-libraries/llama-downloads/</a></p>
<h2 id="ç ”ç©¶è®ºæ–‡"><a href="#ç ”ç©¶è®ºæ–‡" class="headerlink" title="ç ”ç©¶è®ºæ–‡"></a>ç ”ç©¶è®ºæ–‡</h2><p>ã€ŠLlama-2: Open Foundation and Fine-tuned Chat Modelsã€‹</p>
<h2 id="ä½¿ç”¨ç›®çš„"><a href="#ä½¿ç”¨ç›®çš„" class="headerlink" title="ä½¿ç”¨ç›®çš„"></a>ä½¿ç”¨ç›®çš„</h2><h3 id="é¢„æœŸç”¨é€”"><a href="#é¢„æœŸç”¨é€”" class="headerlink" title="é¢„æœŸç”¨é€”"></a>é¢„æœŸç”¨é€”</h3><p>Llama 2 æ—¨åœ¨ç”¨äºè‹±è¯­å•†ä¸šå’Œç ”ç©¶ç”¨é€”ã€‚å¾®è°ƒæ¨¡å‹é€‚ç”¨äºç±»ä¼¼åŠ©ç†çš„èŠå¤©åº”ç”¨ï¼Œè€Œé¢„è®­ç»ƒæ¨¡å‹å¯é€‚åº”å¤šç§è‡ªç„¶è¯­è¨€ç”Ÿæˆä»»åŠ¡ã€‚</p>
<p>è¦è·å¾—èŠå¤©ç‰ˆæœ¬çš„é¢„æœŸåŠŸèƒ½å’Œæ€§èƒ½ï¼Œéœ€è¦éµå¾ªç‰¹å®šçš„æ ¼å¼ï¼ŒåŒ…æ‹¬ INST å’Œ &lt;<sys>&gt; æ ‡ç­¾ã€BOS å’Œ EOS tokensï¼Œä»¥åŠå®ƒä»¬ä¹‹é—´çš„ç©ºæ ¼å’Œæ¢è¡Œç¬¦ï¼ˆæˆ‘ä»¬å»ºè®®å¯¹è¾“å…¥è°ƒç”¨ strip() æ–¹æ³•ï¼Œä»¥é¿å…åŒç©ºæ ¼ï¼‰ã€‚æœ‰å…³è¯¦æƒ…ï¼Œè¯·å‚é˜…æˆ‘ä»¬åœ¨ GitHub ä¸Šçš„å‚è€ƒä»£ç ï¼šchat_completionã€‚</sys></p>
<h3 id="ä¸åœ¨èŒƒå›´å†…çš„ç”¨é€”"><a href="#ä¸åœ¨èŒƒå›´å†…çš„ç”¨é€”" class="headerlink" title="ä¸åœ¨èŒƒå›´å†…çš„ç”¨é€”"></a>ä¸åœ¨èŒƒå›´å†…çš„ç”¨é€”</h3><ul>
<li>ç”¨äºè¿åé€‚ç”¨æ³•å¾‹æ³•è§„ï¼ˆåŒ…æ‹¬è´¸æ˜“åˆè§„æ³•ï¼‰çš„ä»»ä½•æ–¹å¼ã€‚</li>
<li>ç”¨äºé™¤è‹±è¯­ä»¥å¤–çš„å…¶ä»–è¯­è¨€ã€‚</li>
<li>ç”¨äº Llama 2 å¯æ¥å—ä½¿ç”¨æ”¿ç­–å’Œè®¸å¯åè®®æ‰€ç¦æ­¢çš„ä»»ä½•å…¶ä»–æ–¹å¼ã€‚</li>
</ul>
<h2 id="ç¡¬ä»¶å’Œè½¯ä»¶"><a href="#ç¡¬ä»¶å’Œè½¯ä»¶" class="headerlink" title="ç¡¬ä»¶å’Œè½¯ä»¶"></a>ç¡¬ä»¶å’Œè½¯ä»¶</h2><h3 id="è®­ç»ƒå› ç´ "><a href="#è®­ç»ƒå› ç´ " class="headerlink" title="è®­ç»ƒå› ç´ "></a>è®­ç»ƒå› ç´ </h3><p>æˆ‘ä»¬ä½¿ç”¨è‡ªå®šä¹‰è®­ç»ƒåº“ã€Meta çš„ Research Super Cluster ä»¥åŠç”Ÿäº§é›†ç¾¤è¿›è¡Œé¢„è®­ç»ƒã€‚å¾®è°ƒã€æ ‡æ³¨å’Œè¯„ä¼°ä¹Ÿæ˜¯åœ¨ç¬¬ä¸‰æ–¹äº‘è®¡ç®—ä¸Šæ‰§è¡Œçš„ã€‚</p>
<h3 id="ç¢³è¶³è¿¹"><a href="#ç¢³è¶³è¿¹" class="headerlink" title="ç¢³è¶³è¿¹"></a>ç¢³è¶³è¿¹</h3><p>é¢„è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨äº†ç´¯è®¡ 330 ä¸‡ GPU å°æ—¶çš„è®¡ç®—ï¼Œä½¿ç”¨çš„ç¡¬ä»¶ç±»å‹ä¸º A100-80GBï¼ˆTDP ä¸º 350-400Wï¼‰ã€‚é¢„è®¡æ€»æ’æ”¾é‡ä¸º 539 tCO2eqï¼Œå…¶ä¸­ 100% ç”± Meta çš„å¯æŒç»­æ€§è®¡åˆ’æŠµæ¶ˆã€‚</p>
<table>
<thead>
<tr>
<th>æ¨¡å‹</th>
<th>æ—¶é—´ï¼ˆGPU å°æ—¶ï¼‰</th>
<th>åŠŸè€—ï¼ˆç“¦ï¼‰</th>
<th>æ’æ”¾ç¢³é‡ï¼ˆtCO2eqï¼‰</th>
</tr>
</thead>
<tbody><tr>
<td>Llama 2 7B</td>
<td>184,320</td>
<td>400</td>
<td>31.22</td>
</tr>
<tr>
<td>Llama 2 13B</td>
<td>368,640</td>
<td>400</td>
<td>62.44</td>
</tr>
<tr>
<td>Llama 2 70B</td>
<td>1,720,320</td>
<td>400</td>
<td>291.42</td>
</tr>
<tr>
<td>æ€»è®¡</td>
<td>3,311,616</td>
<td></td>
<td>539.00</td>
</tr>
</tbody></table>
<p>é¢„è®­ç»ƒæœŸé—´çš„äºŒæ°§åŒ–ç¢³æ’æ”¾é‡ã€‚æ—¶é—´ï¼šæ¯ä¸ªæ¨¡å‹è®­ç»ƒæ‰€éœ€çš„æ€» GPU æ—¶é—´ã€‚åŠŸè€—ï¼šç”¨äºæ‰€ä½¿ç”¨çš„ GPU è®¾å¤‡çš„æ¯ä¸ª GPU çš„å³°å€¼åŠŸç‡å®¹é‡ï¼Œè°ƒæ•´åçš„</p>
<p>åŠŸè€—ä½¿ç”¨æ•ˆç‡ã€‚100% çš„æ’æ”¾ç›´æ¥ç”± Meta çš„å¯æŒç»­æ€§è®¡åˆ’æŠµæ¶ˆï¼Œå› ä¸ºæˆ‘ä»¬æ­£åœ¨å…¬å¼€å‘å¸ƒè¿™äº›æ¨¡å‹ï¼Œé¢„è®­ç»ƒæˆæœ¬ä¸éœ€è¦ç”±ä»–äººæ‰¿æ‹…ã€‚</p>
<h2 id="è®­ç»ƒæ•°æ®-1"><a href="#è®­ç»ƒæ•°æ®-1" class="headerlink" title="è®­ç»ƒæ•°æ®"></a>è®­ç»ƒæ•°æ®</h2><h3 id="æ¦‚è¿°-1"><a href="#æ¦‚è¿°-1" class="headerlink" title="æ¦‚è¿°"></a>æ¦‚è¿°</h3><p>Llama 2 åœ¨æ¥è‡ªå…¬å¼€æ¥æºçš„æ•°æ®ä¸­é¢„è®­ç»ƒäº† 2 ä¸‡äº¿ä¸ª tokensã€‚å¾®è°ƒæ•°æ®åŒ…æ‹¬å…¬å¼€å¯ç”¨çš„æŒ‡å¯¼æ•°æ®é›†ï¼Œä»¥åŠä¸€ç™¾ä¸‡ä¸ªæ–°çš„äººå·¥æ ‡æ³¨ç¤ºä¾‹ã€‚é¢„è®­ç»ƒå’Œå¾®è°ƒæ•°æ®é›†å‡ä¸åŒ…å« Meta ç”¨æˆ·æ•°æ®ã€‚</p>
<h3 id="æ•°æ®æ–°é²œåº¦"><a href="#æ•°æ®æ–°é²œåº¦" class="headerlink" title="æ•°æ®æ–°é²œåº¦"></a>æ•°æ®æ–°é²œåº¦</h3><p>é¢„è®­ç»ƒæ•°æ®æˆªæ­¢æ—¥æœŸä¸º 2022 å¹´ 9 æœˆï¼Œä½†ä¸€äº›å¾®è°ƒæ•°æ®æ›´è¿‘ï¼Œæœ€å¤šè‡³ 2023 å¹´ 7 æœˆã€‚</p>
<h2 id="è¯„ä¼°ç»“æœ"><a href="#è¯„ä¼°ç»“æœ" class="headerlink" title="è¯„ä¼°ç»“æœ"></a>è¯„ä¼°ç»“æœ</h2><p>åœ¨æ­¤éƒ¨åˆ†ï¼Œæˆ‘ä»¬æŠ¥å‘Šäº† Llama 1 å’Œ Llama 2 æ¨¡å‹åœ¨æ ‡å‡†å­¦æœ¯åŸºå‡†æµ‹è¯•ä¸Šçš„ç»“æœã€‚å¯¹äºæ‰€æœ‰è¯„ä¼°ï¼Œæˆ‘ä»¬ä½¿ç”¨æˆ‘ä»¬çš„å†…éƒ¨è¯„ä¼°åº“ã€‚</p>
<table>
<thead>
<tr>
<th>æ¨¡å‹</th>
<th>è§„æ¨¡</th>
<th>Code</th>
<th>å¸¸è¯†æ¨ç†</th>
<th>ä¸–ç•ŒçŸ¥è¯†</th>
<th>é˜…è¯»ç†è§£</th>
<th>æ•°å­¦</th>
<th>MMLU</th>
<th>BBH</th>
<th>AGI Eval</th>
</tr>
</thead>
<tbody><tr>
<td>Llama 1</td>
<td>7B</td>
<td>14.1</td>
<td>60.8</td>
<td>46.2</td>
<td>58.5</td>
<td>6.95</td>
<td>35.1</td>
<td>30.3</td>
<td>23.9</td>
</tr>
<tr>
<td>Llama 1</td>
<td>13B</td>
<td>18.9</td>
<td>66.1</td>
<td>52.6</td>
<td>62.3</td>
<td>10.9</td>
<td>46.9</td>
<td>37.0</td>
<td>33.9</td>
</tr>
<tr>
<td>Llama 1</td>
<td>33B</td>
<td>26.0</td>
<td>70.0</td>
<td>58.4</td>
<td>67.6</td>
<td>21.4</td>
<td>57.8</td>
<td>39.8</td>
<td>41.7</td>
</tr>
<tr>
<td>Llama 1</td>
<td>65B</td>
<td>30.7</td>
<td>70.7</td>
<td>60.5</td>
<td>68.6</td>
<td>30.8</td>
<td>63.4</td>
<td>43.5</td>
<td>47.6</td>
</tr>
<tr>
<td>Llama 2</td>
<td>7B</td>
<td>16.8</td>
<td>63.9</td>
<td>48.9</td>
<td>61.3</td>
<td>14.6</td>
<td>45.3</td>
<td>32.6</td>
<td>29.3</td>
</tr>
<tr>
<td>Llama 2</td>
<td>13B</td>
<td>24.5</td>
<td>66.9</td>
<td>55.4</td>
<td>65.8</td>
<td>28.7</td>
<td>54.8</td>
<td>39.4</td>
<td>39.1</td>
</tr>
<tr>
<td>Llama 2</td>
<td>70B</td>
<td>37.5</td>
<td>71.9</td>
<td>63.6</td>
<td>69.4</td>
<td>35.2</td>
<td>68.9</td>
<td>51.2</td>
<td>54.2</td>
</tr>
</tbody></table>
<p>æ¨¡å‹åœ¨ grouped academic benchmarks ä¸Šçš„æ•´ä½“è¡¨ç°ã€‚Codeï¼šæˆ‘ä»¬æŠ¥å‘Šæ¨¡å‹åœ¨ HumanEval å’Œ MBPP ä¸Šçš„å¹³å‡ pass@1 åˆ†æ•°ã€‚å¸¸è¯†æ¨ç†ï¼šæˆ‘ä»¬æŠ¥å‘Š PIQAã€SIQAã€HellaSwagã€WinoGrandeã€ARC easy å’Œ challengeã€OpenBookQA å’Œ CommonsenseQA çš„å¹³å‡åˆ†æ•°ã€‚æˆ‘ä»¬å¯¹ CommonSenseQA è¿›è¡Œäº† 7-shot ç»“æœè¯„ä¼°ï¼Œå¯¹å…¶ä»–æ‰€æœ‰åŸºå‡†æµ‹è¯•è¿›è¡Œäº† 0-shot ç»“æœè¯„ä¼°ã€‚ä¸–ç•ŒçŸ¥è¯†ï¼šæˆ‘ä»¬åœ¨ NaturalQuestions å’Œ TriviaQA ä¸Šè¿›è¡Œ 5-shot æ€§èƒ½è¯„ä¼°å¹¶æŠ¥å‘Šå¹³å‡åˆ†æ•°ã€‚é˜…è¯»ç†è§£ï¼šå¯¹äºé˜…è¯»ç†è§£ï¼Œæˆ‘ä»¬æŠ¥å‘Š SQuADã€QuAC å’Œ BoolQ çš„ 0-shot å¹³å‡åˆ†æ•°ã€‚æ•°å­¦ï¼šæˆ‘ä»¬æŠ¥å‘Š GSM8Kï¼ˆ8-shotï¼‰å’Œ MATHï¼ˆ4-shotï¼‰åŸºå‡†æµ‹è¯•çš„å¹³å‡åˆ†æ•°ã€‚</p>
<h3 id="TruthfulQA-å’Œ-Toxigen"><a href="#TruthfulQA-å’Œ-Toxigen" class="headerlink" title="TruthfulQA å’Œ Toxigen"></a>TruthfulQA å’Œ Toxigen</h3><table>
<thead>
<tr>
<th>æ¨¡å‹</th>
<th>è§„æ¨¡</th>
<th>TruthfulQA</th>
<th>Toxigen</th>
</tr>
</thead>
<tbody><tr>
<td>Llama 1</td>
<td>7B</td>
<td>27.42</td>
<td>23.00</td>
</tr>
<tr>
<td>Llama 1</td>
<td>13B</td>
<td>41.74</td>
<td>23.08</td>
</tr>
<tr>
<td>Llama 1</td>
<td>33B</td>
<td>44.19</td>
<td>22.57</td>
</tr>
<tr>
<td>Llama 1</td>
<td>65B</td>
<td>48.71</td>
<td>21.77</td>
</tr>
<tr>
<td>Llama 2</td>
<td>7B</td>
<td>33.29</td>
<td>21.25</td>
</tr>
<tr>
<td>Llama 2</td>
<td>13B</td>
<td>41.86</td>
<td>26.10</td>
</tr>
<tr>
<td>Llama 2</td>
<td>70B</td>
<td>50.18</td>
<td>24.60</td>
</tr>
</tbody></table>
<p>é¢„è®­ç»ƒ LLMs åœ¨è‡ªåŠ¨å®‰å…¨åŸºå‡†æµ‹è¯•ä¸Šçš„è¯„ä¼°ç»“æœã€‚å¯¹äº TruthfulQAï¼Œæˆ‘ä»¬å‘ˆç°åŒæ—¶å…·æœ‰çœŸå®æ€§å’Œä¿¡æ¯é‡çš„ç”Ÿæˆç™¾åˆ†æ¯”ï¼ˆç™¾åˆ†æ¯”è¶Šé«˜è¶Šå¥½ï¼‰ã€‚å¯¹äº ToxiGenï¼Œæˆ‘ä»¬å‘ˆç°æœ‰å®³ç”Ÿæˆçš„ç™¾åˆ†æ¯”ï¼ˆç™¾åˆ†æ¯”è¶Šå°è¶Šå¥½ï¼‰ã€‚</p>
<h3 id="TruthfulQA-å’Œ-Toxigenï¼ˆå¾®è°ƒç‰ˆæœ¬-LLMsï¼‰"><a href="#TruthfulQA-å’Œ-Toxigenï¼ˆå¾®è°ƒç‰ˆæœ¬-LLMsï¼‰" class="headerlink" title="TruthfulQA å’Œ Toxigenï¼ˆå¾®è°ƒç‰ˆæœ¬ LLMsï¼‰"></a>TruthfulQA å’Œ Toxigenï¼ˆå¾®è°ƒç‰ˆæœ¬ LLMsï¼‰</h3><table>
<thead>
<tr>
<th>æ¨¡å‹</th>
<th>è§„æ¨¡</th>
<th>TruthfulQA</th>
<th>Toxigen</th>
</tr>
</thead>
<tbody><tr>
<td>Llama-2-Chat</td>
<td>7B</td>
<td>57.04</td>
<td>0.00</td>
</tr>
<tr>
<td>Llama-2-Chat</td>
<td>13B</td>
<td>62.18</td>
<td>0.00</td>
</tr>
<tr>
<td>Llama-2-Chat</td>
<td>70B</td>
<td>64.14</td>
<td>0.01</td>
</tr>
</tbody></table>
<p>ä¸åŒå®‰å…¨æ•°æ®é›†ä¸Šå¾®è°ƒ LLMs çš„è¯„ä¼°ç»“æœã€‚åº¦é‡æ ‡å‡†å®šä¹‰åŒä¸Šã€‚</p>
<h2 id="é“å¾·è€ƒè™‘å’Œå±€é™æ€§"><a href="#é“å¾·è€ƒè™‘å’Œå±€é™æ€§" class="headerlink" title="é“å¾·è€ƒè™‘å’Œå±€é™æ€§"></a>é“å¾·è€ƒè™‘å’Œå±€é™æ€§</h2><p>Llama 2 æ˜¯ä¸€é¡¹å…·æœ‰é£é™©çš„æ–°æŠ€æœ¯ã€‚è¿„ä»Šä¸ºæ­¢çš„æµ‹è¯•ä»…æ¶µç›–äº†è‹±è¯­ï¼Œå¹¶ä¸”æ— æ³•è¦†ç›–æ‰€æœ‰åœºæ™¯ã€‚å› æ­¤ï¼Œä¸æ‰€æœ‰ LLMs ä¸€æ ·ï¼ŒLlama 2 çš„æ½œåœ¨è¾“å‡ºæ— æ³•äº‹å…ˆé¢„æµ‹ï¼Œå¹¶ä¸”åœ¨æŸäº›æƒ…å†µä¸‹å¯èƒ½ä¼šäº§ç”Ÿä¸å‡†ç¡®ã€å¸¦åè§æˆ–å…¶ä»–ä¸å¯å–çš„å“åº”ã€‚å› æ­¤ï¼Œåœ¨éƒ¨ç½²ä»»ä½• Llama 2 åº”ç”¨ç¨‹åºä¹‹å‰ï¼Œå¼€å‘äººå‘˜åº”æ ¹æ®å…¶ç‰¹å®šçš„æ¨¡å‹åº”ç”¨è¿›è¡Œå®‰å…¨æµ‹è¯•å’Œè°ƒæ•´ã€‚</p>
<p>è¯·å‚é˜…â€œè´Ÿè´£ä»»ä½¿ç”¨æŒ‡å—â€ï¼Œç½‘å€ä¸ºï¼š<a target="_blank" rel="noopener" href="https://ai.meta.com/llama/responsible-use-guide/">https://ai.meta.com/llama/responsible-use-guide/</a></p>
<h2 id="æŠ¥å‘Šé—®é¢˜"><a href="#æŠ¥å‘Šé—®é¢˜" class="headerlink" title="æŠ¥å‘Šé—®é¢˜"></a>æŠ¥å‘Šé—®é¢˜</h2><p>è¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¹‹ä¸€æŠ¥å‘Šä»»ä½•è½¯ä»¶â€œbugâ€æˆ–æ¨¡å‹çš„å…¶ä»–é—®é¢˜ï¼š</p>
<ul>
<li>æŠ¥å‘Šæ¨¡å‹é—®é¢˜ï¼š[github.com/facebookresearch/llama](<a target="_blank" rel="noopener" href="https://github/">https://github</a></li>
</ul>
<p>.com/facebookresearch/llama)</p>
<ul>
<li>æŠ¥å‘Šæ¨¡å‹ç”Ÿæˆçš„æœ‰é—®é¢˜å†…å®¹ï¼š<a target="_blank" rel="noopener" href="https://developers.facebook.com/llama_output_feedback">developers.facebook.com/llama_output_feedback</a></li>
<li>æŠ¥å‘Š bug å’Œå®‰å…¨é—®é¢˜ï¼š<a target="_blank" rel="noopener" href="https://facebook.com/whitehat/info">facebook.com/whitehat/info</a></li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>æ–‡ç« ä½œè€…: </span><span class="post-copyright-info"><a href="https://chenhuiyu.github.io">Huiyu Chen</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>æ–‡ç« é“¾æ¥: </span><span class="post-copyright-info"><a href="https://chenhuiyu.github.io/2023/08/02/NLP%20Insights/training-llama-2-model-on-single-gpu-with-int8-quantization-and-lora.zh-CN/">https://chenhuiyu.github.io/2023/08/02/NLP%20Insights/training-llama-2-model-on-single-gpu-with-int8-quantization-and-lora.zh-CN/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>ç‰ˆæƒå£°æ˜: </span><span class="post-copyright-info">æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ«å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨ <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº <a href="https://chenhuiyu.github.io" target="_blank">é»‘å¤´å‘†é±¼è¿›åŒ–ä¹‹æ—…</a>ï¼</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/LLM/">LLM</a></div><div class="post-share"><div class="social-share" data-image="/img/butterfly-icon.png" data-sites="facebook,x,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2023/08/21/NLP%20Insights/conver-pytorch-model-to-onnx-format.en/" title="Conver Pytorch Model to ONNX Format"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">ä¸Šä¸€ç¯‡</div><div class="info-item-2">Conver Pytorch Model to ONNX Format</div></div><div class="info-2"><div class="info-item-1">ä½¿ç”¨ PyTorch å’Œ ONNX æ£€æŸ¥æ¨¡å‹ä¸€è‡´æ€§åœ¨æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ çš„å¼€å‘è¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹çš„äº’æ“ä½œæ€§å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚ONNX (Open Neural Network Exchange) æ˜¯ä¸€ç§å¼€æ”¾æ ¼å¼ï¼Œç”¨äºè¡¨ç¤ºæœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚å®ƒå…è®¸å¼€å‘è€…åœ¨å„ç§æ·±åº¦å­¦ä¹ æ¡†æ¶ä¹‹é—´è½»æ¾åœ°å…±äº«æ¨¡å‹ï¼Œä»è€Œæé«˜äº†æ¨¡å‹çš„å¯ç§»æ¤æ€§å’Œäº’æ“ä½œæ€§ã€‚ æœ¬æ•™ç¨‹å°†æŒ‡å¯¼æ‚¨å®Œæˆä»¥ä¸‹æ­¥éª¤ï¼š  å°† PyTorch æ¨¡å‹è½¬æ¢ä¸º ONNX æ ¼å¼ã€‚ éªŒè¯è½¬æ¢åçš„ ONNX æ¨¡å‹ä¸åŸå§‹ PyTorch æ¨¡å‹çš„è¾“å‡ºæ˜¯å¦ä¸€è‡´ã€‚  1. å¯¼å…¥å¿…è¦çš„åº“é¦–å…ˆï¼Œæˆ‘ä»¬å¯¼å…¥ä¸ºæ¨¡å‹è½¬æ¢å’ŒéªŒè¯æ‰€éœ€çš„æ‰€æœ‰åº“ã€‚ 123456import osimport sysimport torchimport onnximport onnxruntimeimport numpy as np  2. å®šä¹‰æ¨¡å‹è½¬æ¢å‡½æ•°ä¸ºäº†å°† PyTorch æ¨¡å‹è½¬æ¢ä¸º ONNX æ ¼å¼ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªåä¸º convert_onnx çš„å‡½æ•°ã€‚æ­¤å‡½æ•°ä½¿ç”¨ PyTorch çš„å†…ç½®å‡½æ•° torch.onnx.export å°†æ¨¡å‹è½¬æ¢ä¸º ONNX æ ¼å¼ã€‚ 12345678910def conver...</div></div></div></a><a class="pagination-related" href="/2023/08/02/NLP%20Insights/training-llama-2-model-on-single-gpu-with-int8-quantization-and-lora.en/" title="Training Llama 2 Model on Single GPU with int8 Quantization and LoRA"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">ä¸‹ä¸€ç¯‡</div><div class="info-item-2">Training Llama 2 Model on Single GPU with int8 Quantization and LoRA</div></div><div class="info-2"><div class="info-item-1">Training Llama 2 Model on Single GPU with int8 Quantization and LoRALlama 2æ¦‚è¿°Llama 2 æ˜¯ä¸€ä¸ªåŒ…å«é¢„è®­ç»ƒå’Œå¾®è°ƒçš„ç”Ÿæˆå¼æ–‡æœ¬æ¨¡å‹çš„é›†åˆï¼Œå…¶è§„æ¨¡ä» 70 äº¿åˆ° 700 äº¿ä¸ªå‚æ•°ä¸ç­‰ã€‚Llama2æ¨¡å‹æ˜¯ç”±Hugo Touvron, Louis Martin, Kevin Stone, Peter Albertç­‰äººåœ¨Llama 2: Open Foundation and Fine-Tuned Chat Modelsä¸­æå‡ºçš„ã€‚ è¯¥è®ºæ–‡çš„æ‘˜è¦å¦‚ä¸‹ï¼š åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼€å‘å¹¶å‘å¸ƒäº†Llama 2ï¼Œè¿™æ˜¯ä¸€ç»„ä»70äº¿åˆ°700äº¿å‚æ•°çš„é¢„è®­ç»ƒå’Œå¾®è°ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ã€‚æˆ‘ä»¬çš„å¾®è°ƒLLMsï¼Œç§°ä¸ºLlama 2-Chatï¼Œé’ˆå¯¹å¯¹è¯ç”¨ä¾‹è¿›è¡Œäº†ä¼˜åŒ–ã€‚æˆ‘ä»¬çš„æ¨¡å‹åœ¨æˆ‘ä»¬æµ‹è¯•çš„å¤§å¤šæ•°åŸºå‡†ä¸Šèƒœè¿‡å¼€æºèŠå¤©æ¨¡å‹ï¼Œå¹¶ä¸”åŸºäºæˆ‘ä»¬å¯¹æœ‰ç”¨æ€§å’Œå®‰å…¨æ€§çš„äººç±»è¯„ä¼°ï¼Œå¯èƒ½æ˜¯é—­æºæ¨¡å‹çš„åˆé€‚æ›¿ä»£å“ã€‚æˆ‘ä»¬æä¾›äº†å…³äºå¾®è°ƒå’Œæ”¹è¿›Llama 2-Chatå®‰å…¨æ€§çš„æ–¹æ³•çš„è¯¦ç»†æè¿°ï¼Œä»¥ä¾¿ç¤¾åŒºèƒ½å¤Ÿåœ¨æˆ‘ä»¬çš„å·¥ä½œåŸºç¡€ä¸Šæ„å»ºï¼Œå¹¶æœ‰åŠ©äºLLMsçš„è´Ÿè´£ä»»å‘å±•ã€‚ åœ¨æ­¤å¤„æŸ¥çœ‹æ‰€æœ‰L...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>ç›¸å…³æ¨è</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2024/02/27/NLP%20Insights/fastchat-training-script-code-analysis-train-py-fastchat-series-part-1.zh-CN/" title="FastChat è®­ç»ƒè„šæœ¬ä»£ç é€è¡Œè§£æ-Train.py ã€FastChat ç³»åˆ—ç¬¬ 1 ç¯‡ã€‘"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-27</div><div class="info-item-2">FastChat è®­ç»ƒè„šæœ¬ä»£ç é€è¡Œè§£æ-Train.py ã€FastChat ç³»åˆ—ç¬¬ 1 ç¯‡ã€‘</div></div><div class="info-2"><div class="info-item-1">FastChat è®­ç»ƒè„šæœ¬ä»£ç é€è¡Œè§£æ-Train.py ã€FastChat ç³»åˆ—ç¬¬ 1 ç¯‡ã€‘åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥æ¢è®¨ FastChat çš„ train.py è„šæœ¬ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè®­ç»ƒå’Œä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹çš„å…³é”®ç»„ä»¶ã€‚FastChat æ˜¯ä¸€ä¸ªå…ˆè¿›çš„å¼€æºå¹³å°ï¼Œä¸“æ³¨äºå¼€å‘ã€éƒ¨ç½²å’Œè¯„ä¼°åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èŠå¤©æœºå™¨äººã€‚è¯¥å¹³å°ä¸ä»…æä¾›å¯¹é¡¶å°–æ¨¡å‹å¦‚ Vicuna å’Œ MT-Bench çš„æ”¯æŒï¼Œè¿˜åŒ…æ‹¬ä¸€ä¸ªåˆ†å¸ƒå¼çš„å¤šæ¨¡å‹æœåŠ¡ç³»ç»Ÿï¼Œé…å¤‡äº† Web UI å’Œä¸ OpenAI å…¼å®¹çš„ RESTful APIï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿé«˜æ•ˆåœ°è®­ç»ƒå’Œè¯„ä¼°ä»–ä»¬çš„æ¨¡å‹ã€‚ æœ¬æ–‡çš„æ·±å…¥åˆ†æå°†èšç„¦äº train.py è„šæœ¬çš„æºä»£ç ã€‚è¿™ä¸ªè„šæœ¬æ˜¯åŸºäº transformers åº“çš„è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹è®­ç»ƒè„šæœ¬ï¼Œæ¶µç›–äº†æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒå’Œä¿å­˜ç­‰å…³é”®æ­¥éª¤ã€‚æˆ‘ä»¬æ—¨åœ¨æä¾›å¯¹ train.py ä¸­æ¯ä¸ªç±»å’Œå‡½æ•°çš„è¯¦ç»†è§£é‡Šï¼ŒåŒ…æ‹¬å®ƒä»¬çš„åŠŸèƒ½å’Œåœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­çš„ä½œç”¨ã€‚ 1. å¯¼å…¥æ¨¡å—1. å†…ç½®æ¨¡å—è¿™äº›æ˜¯ Python è‡ªå¸¦çš„æ ‡å‡†åº“æ¨¡å—ï¼Œæ— éœ€é¢å¤–å®‰è£…ã€‚ 1from dataclasses import dataclass, field  å¯¼å…¥ Pytho...</div></div></div></a><a class="pagination-related" href="/2024/02/19/NLP%20Insights/understanding-the-differences-between-fine-tuning-and-further-pretraining-in-large-language-models.en/" title="Understanding the Differences Between Fine-tuning and Further Pretraining in Large Language Models"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-19</div><div class="info-item-2">Understanding the Differences Between Fine-tuning and Further Pretraining in Large Language Models</div></div><div class="info-2"><div class="info-item-1">Understanding the Differences Between Fine-tuning and Further Pretraining in Large Language ModelsIn the world of Natural Language Processing (NLP), the advent of large language models like GPT and BERT has revolutionized how we approach tasks such as text classification, sentiment analysis, and question-answering. Two pivotal techniques in leveraging these models are Fine-tuning and Further Pretraining. While they may seem similar at a glance, they cater to different needs and scenarios in t...</div></div></div></a><a class="pagination-related" href="/2025/03/06/NLP%20Insights/differences-in-padding-strategies-between-decoder-only-and-encoder-only-models.zh-CN/" title="Decoder-onlyä¸Encoder-onlyæ¨¡å‹Paddingç­–ç•¥çš„å·®å¼‚"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-06</div><div class="info-item-2">Decoder-onlyä¸Encoder-onlyæ¨¡å‹Paddingç­–ç•¥çš„å·®å¼‚</div></div><div class="info-2"><div class="info-item-1">ğŸ“Œ Padding çš„å«ä¹‰åœ¨å¤§æ¨¡å‹ (LLM) ä¸­ï¼Œpadding æ˜¯ç”¨äºå°†ä¸åŒé•¿åº¦çš„åºåˆ—è°ƒæ•´ä¸ºåŒä¸€é•¿åº¦çš„æ–¹æ³•ï¼Œä»¥ä¾¿äºæ‰¹é‡ (batch) å¤„ç†ã€‚ ä¾‹å¦‚ï¼š 12å¥å­1: "I love NLP"å¥å­2: "Padding is useful in LLM training"  ä½¿ç”¨ &lt;pad&gt; token è¿›è¡Œå¯¹é½ï¼š 12"I love NLP &lt;pad&gt; &lt;pad&gt; &lt;pad&gt;""Padding is useful in LLM training"   ğŸ“Œ Padding ä½ç½®çš„é€‰æ‹©ï¼šLeft vs RightPadding æœ‰ä¸¤ç§å¸¸è§æ–¹å¼ï¼š  Right paddingï¼ˆå³å¡«å……ï¼‰ï¼š 1"I love NLP &lt;pad&gt; &lt;pad&gt;"  Left paddingï¼ˆå·¦å¡«å……ï¼‰ï¼š 1"&lt;pad&gt; &lt;pad&gt; I love NLP"  é€šå¸¸ï¼š  Decoder-only æ¨¡å‹ï¼ˆå¦‚ GPT, Llamaï¼‰ï¼šé‡‡ç”¨ Left padding Encoder-only æ¨¡å‹ï¼ˆå¦‚ BERTï¼‰ï¼šé‡‡ç”¨...</div></div></div></a><a class="pagination-related" href="/2024/02/28/NLP%20Insights/gorilla-llm-%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B.en/" title="Gorilla LLM å¤§è¯­è¨€æ¨¡å‹ç®€ä»‹"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-28</div><div class="info-item-2">Gorilla LLM å¤§è¯­è¨€æ¨¡å‹ç®€ä»‹</div></div><div class="info-2"><div class="info-item-1">Gorilla LLM å¤§è¯­è¨€æ¨¡å‹ç®€ä»‹ğŸ¦ Gorilla: Large Language Model Connected with Massive APIsLink: https://gorilla.cs.berkeley.edu/blogs/7_open_functions_v2.html  Berkeley åŠŸèƒ½è°ƒç”¨æ’è¡Œæ¦œBerkeley åŠŸèƒ½è°ƒç”¨æ’è¡Œæ¦œ åœ¨çº¿ä½“éªŒæ¨¡å‹ï¼šGorilla OpenFunctions-v2 ç½‘ç»œæ¼”ç¤º é¡¹ç›®è¯¦æƒ…ï¼šGitHub æ¨¡å‹ï¼ˆ7B å‚æ•°ï¼‰åœ¨ HuggingFace ä¸Šçš„é¡µé¢ï¼šgorilla-llm/gorilla-openfunctions-v2  1. ä¼¯å…‹åˆ©å‡½æ•°è°ƒç”¨æ’è¡Œæ¦œè‡ª 2022 å¹´åº•ä»¥æ¥ï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰å‡­å€Ÿå…¶æ‰§è¡Œé€šç”¨ä»»åŠ¡çš„å¼ºå¤§èƒ½åŠ›ï¼Œæˆä¸ºä¼—äººå…³æ³¨çš„ç„¦ç‚¹ã€‚ä¸ä»…é™äºèŠå¤©åº”ç”¨ï¼Œå°†è¿™äº›æ¨¡å‹åº”ç”¨äºå¼€å‘å„ç±» AI åº”ç”¨å’Œè½¯ä»¶ï¼ˆå¦‚ Langchain, Llama Index, AutoGPT, Voyagerï¼‰å·²æˆä¸ºä¸€ç§è¶‹åŠ¿ã€‚GPT, Gemini, Llama, Mistral ç­‰æ¨¡å‹é€šè¿‡ä¸å¤–éƒ¨ä¸–ç•Œçš„äº¤äº’ï¼Œå¦‚å‡½æ•°è°ƒç”¨å’Œæ‰§è¡Œï¼Œå±•ç°äº†å…¶å·¨å¤§...</div></div></div></a><a class="pagination-related" href="/2024/02/27/NLP%20Insights/fastchat-training-script-code-analysis-train-py-fastchat-series-part-1.en/" title="FastChat Training Script Code Analysis - Train.py ã€FastChat Series Part 1ã€‘"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-27</div><div class="info-item-2">FastChat Training Script Code Analysis - Train.py ã€FastChat Series Part 1ã€‘</div></div><div class="info-2"><div class="info-item-1">FastChat Training Script Code Analysis - Train.py ã€FastChat Series Part 1ã€‘In this article, we delve into the train.py script of FastChat (https://github.com/lm-sys/FastChat) (https://github.com/lm-sys/FastChat/blob/main/fastchat/train/train.py), a key component for training and optimizing large language models (LLMs). FastChat is an advanced open-source platform focused on developing, deploying, and evaluating chatbots based on LLMs. The platform not only supports top-tier models like Vicuna ...</div></div></div></a><a class="pagination-related" href="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86-llm-%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97-%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E-deepseek-r1.en/" title="æ¨ç† LLM çš„å¯è§†åŒ–æŒ‡å—ï¼šæ¢ç´¢æ¨ç†æ—¶è®¡ç®—æŠ€æœ¯ä¸ DeepSeek-R1"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-02-11</div><div class="info-item-2">æ¨ç† LLM çš„å¯è§†åŒ–æŒ‡å—ï¼šæ¢ç´¢æ¨ç†æ—¶è®¡ç®—æŠ€æœ¯ä¸ DeepSeek-R1</div></div><div class="info-2"><div class="info-item-1">æ¨ç† LLM çš„å¯è§†åŒ–æŒ‡å—ï¼šæ¢ç´¢æ¨ç†æ—¶è®¡ç®—æŠ€æœ¯ä¸ DeepSeek-R1åŸæ–‡åœ°å€ï¼šA Visual Guide to Reasoning LLMs ğŸ“… ä½œè€…ï¼šMaarten Grootendorst ğŸ“† æ—¥æœŸï¼š2025 å¹´ 2 æœˆ 3 æ—¥  ğŸ“Œ å¼•è¨€DeepSeek-R1ã€OpenAI o3-mini å’Œ Google Gemini 2.0 Flash Thinking æ˜¯å¦‚ä½•é€šè¿‡â€œæ¨ç†â€æ¡†æ¶å°† LLMï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹, Large Language Modelsï¼‰ æ‰©å±•åˆ°æ–°é«˜åº¦çš„å…¸å‹ç¤ºä¾‹ã€‚ å®ƒä»¬æ ‡å¿—ç€ä» æ‰©å±•è®­ç»ƒæ—¶è®¡ç®—ï¼ˆtrain-time computeï¼‰ åˆ° æ‰©å±•æ¨ç†æ—¶è®¡ç®—ï¼ˆtest-time computeï¼‰ çš„èŒƒå¼è½¬å˜ã€‚ åœ¨æœ¬ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬æä¾›äº† è¶…è¿‡ 40 å¼ å®šåˆ¶å¯è§†åŒ–å›¾è¡¨ï¼Œå¸¦ä½ æ·±å…¥æ¢ç´¢ï¼š  æ¨ç† LLMï¼ˆReasoning LLMsï¼‰ é¢†åŸŸ æ¨ç†æ—¶è®¡ç®—ï¼ˆTest-Time Computeï¼‰ æœºåˆ¶ DeepSeek-R1 çš„æ ¸å¿ƒæ€æƒ³  æˆ‘ä»¬å°†é€æ­¥ä»‹ç»ç›¸å…³æ¦‚å¿µï¼Œå¸®åŠ©ä½ å»ºç«‹å¯¹è¿™ä¸€æ–°èŒƒå¼çš„ç›´è§‰ç†è§£ã€‚    ğŸ“– ä»€ä¹ˆæ˜¯æ¨ç† LLMï¼Ÿä¸æ™®é€š LLMï¼ˆLarge Langu...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Huiyu Chen</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">æ–‡ç« </div><div class="length-num">102</div></a><a href="/tags/"><div class="headline">æ ‡ç­¾</div><div class="length-num">49</div></a><a href="/categories/"><div class="headline">åˆ†ç±»</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>å…¬å‘Š</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>ç›®å½•</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Training-Llama-2-Model-on-Single-GPU-with-int8-Quantization-and-LoRA"><span class="toc-number">1.</span> <span class="toc-text">Training Llama 2 Model on Single GPU with int8 Quantization and LoRA</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Llama-2"><span class="toc-number">2.</span> <span class="toc-text">Llama 2</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A6%82%E8%BF%B0"><span class="toc-number">2.1.</span> <span class="toc-text">æ¦‚è¿°</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8F%90%E7%A4%BA%EF%BC%9A"><span class="toc-number">2.1.1.</span> <span class="toc-text">æç¤ºï¼š</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AF%A6%E6%83%85"><span class="toc-number">2.2.</span> <span class="toc-text">æ¨¡å‹è¯¦æƒ…</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%BC%80%E5%8F%91%E8%80%85"><span class="toc-number">2.3.</span> <span class="toc-text">æ¨¡å‹å¼€å‘è€…</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%8D%E5%90%8C%E7%89%88%E6%9C%AC"><span class="toc-number">2.4.</span> <span class="toc-text">ä¸åŒç‰ˆæœ¬</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA"><span class="toc-number">2.5.</span> <span class="toc-text">è¾“å…¥è¾“å‡º</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84"><span class="toc-number">2.6.</span> <span class="toc-text">æ¨¡å‹æ¶æ„</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE"><span class="toc-number">2.7.</span> <span class="toc-text">è®­ç»ƒæ•°æ®</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%97%A5%E6%9C%9F"><span class="toc-number">2.8.</span> <span class="toc-text">æ¨¡å‹è®­ç»ƒæ—¥æœŸ</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%8A%B6%E6%80%81"><span class="toc-number">2.9.</span> <span class="toc-text">çŠ¶æ€</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%B8%E5%8F%AF%E8%AF%81"><span class="toc-number">2.10.</span> <span class="toc-text">è®¸å¯è¯</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A0%94%E7%A9%B6%E8%AE%BA%E6%96%87"><span class="toc-number">2.11.</span> <span class="toc-text">ç ”ç©¶è®ºæ–‡</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E7%9B%AE%E7%9A%84"><span class="toc-number">2.12.</span> <span class="toc-text">ä½¿ç”¨ç›®çš„</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A2%84%E6%9C%9F%E7%94%A8%E9%80%94"><span class="toc-number">2.12.1.</span> <span class="toc-text">é¢„æœŸç”¨é€”</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8D%E5%9C%A8%E8%8C%83%E5%9B%B4%E5%86%85%E7%9A%84%E7%94%A8%E9%80%94"><span class="toc-number">2.12.2.</span> <span class="toc-text">ä¸åœ¨èŒƒå›´å†…çš„ç”¨é€”</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A1%AC%E4%BB%B6%E5%92%8C%E8%BD%AF%E4%BB%B6"><span class="toc-number">2.13.</span> <span class="toc-text">ç¡¬ä»¶å’Œè½¯ä»¶</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E5%9B%A0%E7%B4%A0"><span class="toc-number">2.13.1.</span> <span class="toc-text">è®­ç»ƒå› ç´ </span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A2%B3%E8%B6%B3%E8%BF%B9"><span class="toc-number">2.13.2.</span> <span class="toc-text">ç¢³è¶³è¿¹</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE-1"><span class="toc-number">2.14.</span> <span class="toc-text">è®­ç»ƒæ•°æ®</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A6%82%E8%BF%B0-1"><span class="toc-number">2.14.1.</span> <span class="toc-text">æ¦‚è¿°</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%96%B0%E9%B2%9C%E5%BA%A6"><span class="toc-number">2.14.2.</span> <span class="toc-text">æ•°æ®æ–°é²œåº¦</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%84%E4%BC%B0%E7%BB%93%E6%9E%9C"><span class="toc-number">2.15.</span> <span class="toc-text">è¯„ä¼°ç»“æœ</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#TruthfulQA-%E5%92%8C-Toxigen"><span class="toc-number">2.15.1.</span> <span class="toc-text">TruthfulQA å’Œ Toxigen</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#TruthfulQA-%E5%92%8C-Toxigen%EF%BC%88%E5%BE%AE%E8%B0%83%E7%89%88%E6%9C%AC-LLMs%EF%BC%89"><span class="toc-number">2.15.2.</span> <span class="toc-text">TruthfulQA å’Œ Toxigenï¼ˆå¾®è°ƒç‰ˆæœ¬ LLMsï¼‰</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%81%93%E5%BE%B7%E8%80%83%E8%99%91%E5%92%8C%E5%B1%80%E9%99%90%E6%80%A7"><span class="toc-number">2.16.</span> <span class="toc-text">é“å¾·è€ƒè™‘å’Œå±€é™æ€§</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8A%A5%E5%91%8A%E9%97%AE%E9%A2%98"><span class="toc-number">2.17.</span> <span class="toc-text">æŠ¥å‘Šé—®é¢˜</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>æœ€æ–°æ–‡ç« </span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/21/NLP%20Insights/paper-deep-dive-2026-02-21.en/" title="Paper Deep Dive | SLA2: Sparse-Linear Attention with Learnable Routing and QAT">Paper Deep Dive | SLA2: Sparse-Linear Attention with Learnable Routing and QAT</a><time datetime="2026-02-21T04:28:28.000Z" title="å‘è¡¨äº 2026-02-21 12:28:28">2026-02-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/21/NLP%20Insights/paper-deep-dive-2026-02-21.zh-CN/" title="è®ºæ–‡æ·±è¯»ï½œSLA2: Sparse-Linear Attention with Learnable Routing and QAT">è®ºæ–‡æ·±è¯»ï½œSLA2: Sparse-Linear Attention with Learnable Routing and QAT</a><time datetime="2026-02-21T04:28:28.000Z" title="å‘è¡¨äº 2026-02-21 12:28:28">2026-02-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/21/NLP%20Insights/evaluation-of-generation-based-large-language-models-llms-opportunities-and-challenges-from-generation-to-judgment.en/" title="evaluation-of-generation-based-large-language-models-llms-opportunities-and-challenges-from-generation-to-judgment">evaluation-of-generation-based-large-language-models-llms-opportunities-and-challenges-from-generation-to-judgment</a><time datetime="2026-02-20T16:00:00.000Z" title="å‘è¡¨äº 2026-02-21 00:00:00">2026-02-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/21/NLP%20Insights/evaluation-of-generation-based-large-language-models-llms-opportunities-and-challenges-from-generation-to-judgment.zh-CN/" title="evaluation-of-generation-based-large-language-models-llms-opportunities-and-challenges-from-generation-to-judgment">evaluation-of-generation-based-large-language-models-llms-opportunities-and-challenges-from-generation-to-judgment</a><time datetime="2026-02-20T16:00:00.000Z" title="å‘è¡¨äº 2026-02-21 00:00:00">2026-02-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/24/NLP%20Insights/secom-redefining-memory-management-in-conversational-ai.en/" title="SeCom: Redefining Memory Management in Conversational AI">SeCom: Redefining Memory Management in Conversational AI</a><time datetime="2025-06-24T08:00:00.000Z" title="å‘è¡¨äº 2025-06-24 16:00:00">2025-06-24</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 - 2026 By Huiyu Chen</span><span class="framework-info"><span>æ¡†æ¶ </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 6.3.0</a><span class="footer-separator">|</span><span>ä¸»é¢˜ </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.5.4</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="é˜…è¯»æ¨¡å¼"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="æ—¥é—´å’Œå¤œé—´æ¨¡å¼åˆ‡æ¢"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="å•æ å’ŒåŒæ åˆ‡æ¢"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="è®¾ç½®"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="ç›®å½•"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="å›åˆ°é¡¶éƒ¨"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.4"></script><script src="/js/main.js?v=5.5.4"></script><div class="js-pjax"></div><script src="/js/lang-switch.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>