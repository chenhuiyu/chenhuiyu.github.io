<!DOCTYPE html><html lang="[&quot;zh-CN&quot;,&quot;en&quot;,&quot;default&quot;]" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Training Llama 2 Model on Single GPU with int8 Quantization and LoRA | 黑头呆鱼进化之旅</title><meta name="author" content="Huiyu Chen"><meta name="copyright" content="Huiyu Chen"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Training Llama 2 Model on Single GPU with int8 Quantization and LoRALlama 2概述Llama 2 是一个包含预训练和微调的生成式文本模型的集合，其规模从 70 亿到 700 亿个参数不等。Llama2模型是由Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert等人在Llam">
<meta property="og:type" content="article">
<meta property="og:title" content="Training Llama 2 Model on Single GPU with int8 Quantization and LoRA">
<meta property="og:url" content="https://chenhuiyu.github.io/2023/08/02/NLP%20Insights/training-llama-2-model-on-single-gpu-with-int8-quantization-and-lora.zh-CN/index.html">
<meta property="og:site_name" content="黑头呆鱼进化之旅">
<meta property="og:description" content="Training Llama 2 Model on Single GPU with int8 Quantization and LoRALlama 2概述Llama 2 是一个包含预训练和微调的生成式文本模型的集合，其规模从 70 亿到 700 亿个参数不等。Llama2模型是由Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert等人在Llam">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://chenhuiyu.github.io/img/butterfly-icon.png">
<meta property="article:published_time" content="2023-08-02T07:38:29.000Z">
<meta property="article:modified_time" content="2026-02-20T22:19:17.270Z">
<meta property="article:author" content="Huiyu Chen">
<meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://chenhuiyu.github.io/img/butterfly-icon.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Training Llama 2 Model on Single GPU with int8 Quantization and LoRA",
  "url": "https://chenhuiyu.github.io/2023/08/02/NLP%20Insights/training-llama-2-model-on-single-gpu-with-int8-quantization-and-lora.zh-CN/",
  "image": "https://chenhuiyu.github.io/img/butterfly-icon.png",
  "datePublished": "2023-08-02T07:38:29.000Z",
  "dateModified": "2026-02-20T22:19:17.270Z",
  "author": [
    {
      "@type": "Person",
      "name": "Huiyu Chen",
      "url": "https://chenhuiyu.github.io"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://chenhuiyu.github.io/2023/08/02/NLP%20Insights/training-llama-2-model-on-single-gpu-with-int8-quantization-and-lora.zh-CN/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=5.5.4"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.13.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Training Llama 2 Model on Single GPU with int8 Quantization and LoRA',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 6.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="黑头呆鱼进化之旅" type="application/atom+xml">
</head><body><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">黑头呆鱼进化之旅</span></a><a class="nav-page-title" href="/"><span class="site-name">Training Llama 2 Model on Single GPU with int8 Quantization and LoRA</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  返回首页</span></span></a></span><div id="menus"></div></nav><div id="post-info"><h1 class="post-title">Training Llama 2 Model on Single GPU with int8 Quantization and LoRA</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-08-02T07:38:29.000Z" title="发表于 2023-08-02 15:38:29">2023-08-02</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2026-02-20T22:19:17.270Z" title="更新于 2026-02-21 06:19:17">2026-02-21</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/NLP-Insights/">NLP Insights</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">浏览量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h1 id="Training-Llama-2-Model-on-Single-GPU-with-int8-Quantization-and-LoRA"><a href="#Training-Llama-2-Model-on-Single-GPU-with-int8-Quantization-and-LoRA" class="headerlink" title="Training Llama 2 Model on Single GPU with int8 Quantization and LoRA"></a>Training Llama 2 Model on Single GPU with int8 Quantization and LoRA</h1><h1 id="Llama-2"><a href="#Llama-2" class="headerlink" title="Llama 2"></a>Llama 2</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p><em>Llama 2</em> 是一个包含预训练和微调的生成式文本模型的集合，其规模从 70 亿到 700 亿个参数不等。Llama2模型是由Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert等人在<a target="_blank" rel="noopener" href="https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/">Llama 2: Open Foundation and Fine-Tuned Chat Models</a>中提出的。</p>
<p>该论文的摘要如下：</p>
<p>在这项工作中，我们开发并发布了Llama 2，这是一组从70亿到700亿参数的预训练和微调的大型语言模型（LLMs）。我们的微调LLMs，称为Llama 2-Chat，针对对话用例进行了优化。我们的模型在我们测试的大多数基准上胜过开源聊天模型，并且基于我们对有用性和安全性的人类评估，可能是闭源模型的合适替代品。我们提供了关于微调和改进Llama 2-Chat安全性的方法的详细描述，以便社区能够在我们的工作基础上构建，并有助于LLMs的负责任发展。</p>
<p><a target="_blank" rel="noopener" href="https://huggingface.co/models?search=llama2">在此处查看所有Llama2模型</a></p>
<h3 id="提示："><a href="#提示：" class="headerlink" title="提示："></a>提示：</h3><ul>
<li>通过填写<a target="_blank" rel="noopener" href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/">此表格</a>可以获得Llama2模型的权重</li>
<li>该架构与第一个Llama非常相似，增加了Groupe Query Attention（GQA）<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2305.13245.pdf">此论文</a>之后</li>
<li>将<code>config.pretraining_tp</code>设置为不同于1的值将激活线性层的更准确但更慢的计算，这应更好地匹配原始logits。</li>
<li>原始模型使用<code>pad_id = -1</code>，这意味着没有填充令牌。我们不能使用相同的逻辑，请确保使用<code>tokenizer.add_special_tokens({"pad_token":"&lt;pad&gt;"})</code>添加填充令牌，并相应地调整令牌嵌入大小。您还应设置<code>model.config.pad_token_id</code>。模型的embed_tokens层用<code>self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.config.padding_idx)</code>初始化，确保编码填充令牌将输出零，因此在初始化时传递它是推荐的。</li>
<li>填写表格并获得模型检查点的访问权限后，您应该能够使用已转换的检查点。否则，如果您正在转换自己的模型，请随时使用转换脚本。可以使用以下（示例）命令调用脚本：<figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python src/transformers/models/llama/convert_llama_weights_to_hf.py \</span><br><span class="line">    --input_dir /path/to/downloaded/llama/weights --model_size 7B --output_dir /output/path</span><br></pre></td></tr></tbody></table></figure></li>
</ul>
<h2 id="模型详情"><a href="#模型详情" class="headerlink" title="模型详情"></a>模型详情</h2><p>注意：使用该模型受 Meta 许可证的约束。为了下载模型权重和分词器，请访问网站并在请求访问之前接受许可证。</p>
<p>Meta 开发并公开发布了 Llama 2 系列大型语言模型（LLMs），这是一系列规模从 70 亿到 700 亿参数的预训练和微调的生成式文本模型。我们的微调 LLMs，称为 Llama-2-Chat，经过优化用于对话应用场景。Llama-2-Chat 模型在我们测试的大多数基准测试中优于开源聊天模型，并在我们的人工评估中在有用性和安全性方面与一些流行的闭源模型（如ChatGPT和PaLM）持平。</p>
<h2 id="模型开发者"><a href="#模型开发者" class="headerlink" title="模型开发者"></a>模型开发者</h2><p>Model Developers Meta</p>
<h2 id="不同版本"><a href="#不同版本" class="headerlink" title="不同版本"></a>不同版本</h2><p>Llama 2 有不同规模的参数版本，包括 7B、13B 和 70B，以及预训练和微调的变体。</p>
<h2 id="输入输出"><a href="#输入输出" class="headerlink" title="输入输出"></a>输入输出</h2><p>输入模型仅支持文本输入。</p>
<p>输出模型仅生成文本。</p>
<h2 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h2><p>Llama 2 是一种自回归语言模型，采用了优化的 Transformer 架构。微调版本使用有监督的微调（SFT）和基于人类反馈的强化学习（RLHF）来与人类对 helpfulness 和 safety 的偏好保持一致。</p>
<h2 id="训练数据"><a href="#训练数据" class="headerlink" title="训练数据"></a>训练数据</h2><table>
<thead>
<tr>
<th>模型名称</th>
<th>训练数据</th>
<th>参数规模</th>
<th>内容长度</th>
<th>GQA</th>
<th>Tokens</th>
<th>LR</th>
</tr>
</thead>
<tbody><tr>
<td>Llama 2</td>
<td>一种新的公开可用的在线数据混合</td>
<td>7B</td>
<td>4k</td>
<td>✗</td>
<td>2.0T</td>
<td>3.0 x 10-4</td>
</tr>
<tr>
<td>Llama 2</td>
<td>一种新的公开可用的在线数据混合</td>
<td>13B</td>
<td>4k</td>
<td>✗</td>
<td>2.0T</td>
<td>3.0 x 10-4</td>
</tr>
<tr>
<td>Llama 2</td>
<td>一种新的公开可用的在线数据混合</td>
<td>70B</td>
<td>4k</td>
<td>✔</td>
<td>2.0T</td>
<td>1.5 x 10-4</td>
</tr>
</tbody></table>
<p>注：Token counts 仅指预训练数据。所有模型都使用全局 batch-size 为 4M tokens 进行训练。规模更大的模型（70B）使用 Grouped-Query Attention（GQA）来提高推理可伸缩性。</p>
<h2 id="模型训练日期"><a href="#模型训练日期" class="headerlink" title="模型训练日期"></a>模型训练日期</h2><p>Llama 2 在2023年1月至2023年7月之间进行训练。</p>
<h2 id="状态"><a href="#状态" class="headerlink" title="状态"></a>状态</h2><p>这是一个在离线数据集上训练的静态模型。随着我们根据社区反馈改进模型的安全性，将发布微调版本的未来版本。</p>
<h2 id="许可证"><a href="#许可证" class="headerlink" title="许可证"></a>许可证</h2><p>定制的商业许可证可在以下网址获取：<a target="_blank" rel="noopener" href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/">https://ai.meta.com/resources/models-and-libraries/llama-downloads/</a></p>
<h2 id="研究论文"><a href="#研究论文" class="headerlink" title="研究论文"></a>研究论文</h2><p>《Llama-2: Open Foundation and Fine-tuned Chat Models》</p>
<h2 id="使用目的"><a href="#使用目的" class="headerlink" title="使用目的"></a>使用目的</h2><h3 id="预期用途"><a href="#预期用途" class="headerlink" title="预期用途"></a>预期用途</h3><p>Llama 2 旨在用于英语商业和研究用途。微调模型适用于类似助理的聊天应用，而预训练模型可适应多种自然语言生成任务。</p>
<p>要获得聊天版本的预期功能和性能，需要遵循特定的格式，包括 INST 和 &lt;<sys>&gt; 标签、BOS 和 EOS tokens，以及它们之间的空格和换行符（我们建议对输入调用 strip() 方法，以避免双空格）。有关详情，请参阅我们在 GitHub 上的参考代码：chat_completion。</sys></p>
<h3 id="不在范围内的用途"><a href="#不在范围内的用途" class="headerlink" title="不在范围内的用途"></a>不在范围内的用途</h3><ul>
<li>用于违反适用法律法规（包括贸易合规法）的任何方式。</li>
<li>用于除英语以外的其他语言。</li>
<li>用于 Llama 2 可接受使用政策和许可协议所禁止的任何其他方式。</li>
</ul>
<h2 id="硬件和软件"><a href="#硬件和软件" class="headerlink" title="硬件和软件"></a>硬件和软件</h2><h3 id="训练因素"><a href="#训练因素" class="headerlink" title="训练因素"></a>训练因素</h3><p>我们使用自定义训练库、Meta 的 Research Super Cluster 以及生产集群进行预训练。微调、标注和评估也是在第三方云计算上执行的。</p>
<h3 id="碳足迹"><a href="#碳足迹" class="headerlink" title="碳足迹"></a>碳足迹</h3><p>预训练过程中使用了累计 330 万 GPU 小时的计算，使用的硬件类型为 A100-80GB（TDP 为 350-400W）。预计总排放量为 539 tCO2eq，其中 100% 由 Meta 的可持续性计划抵消。</p>
<table>
<thead>
<tr>
<th>模型</th>
<th>时间（GPU 小时）</th>
<th>功耗（瓦）</th>
<th>排放碳量（tCO2eq）</th>
</tr>
</thead>
<tbody><tr>
<td>Llama 2 7B</td>
<td>184,320</td>
<td>400</td>
<td>31.22</td>
</tr>
<tr>
<td>Llama 2 13B</td>
<td>368,640</td>
<td>400</td>
<td>62.44</td>
</tr>
<tr>
<td>Llama 2 70B</td>
<td>1,720,320</td>
<td>400</td>
<td>291.42</td>
</tr>
<tr>
<td>总计</td>
<td>3,311,616</td>
<td></td>
<td>539.00</td>
</tr>
</tbody></table>
<p>预训练期间的二氧化碳排放量。时间：每个模型训练所需的总 GPU 时间。功耗：用于所使用的 GPU 设备的每个 GPU 的峰值功率容量，调整后的</p>
<p>功耗使用效率。100% 的排放直接由 Meta 的可持续性计划抵消，因为我们正在公开发布这些模型，预训练成本不需要由他人承担。</p>
<h2 id="训练数据-1"><a href="#训练数据-1" class="headerlink" title="训练数据"></a>训练数据</h2><h3 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h3><p>Llama 2 在来自公开来源的数据中预训练了 2 万亿个 tokens。微调数据包括公开可用的指导数据集，以及一百万个新的人工标注示例。预训练和微调数据集均不包含 Meta 用户数据。</p>
<h3 id="数据新鲜度"><a href="#数据新鲜度" class="headerlink" title="数据新鲜度"></a>数据新鲜度</h3><p>预训练数据截止日期为 2022 年 9 月，但一些微调数据更近，最多至 2023 年 7 月。</p>
<h2 id="评估结果"><a href="#评估结果" class="headerlink" title="评估结果"></a>评估结果</h2><p>在此部分，我们报告了 Llama 1 和 Llama 2 模型在标准学术基准测试上的结果。对于所有评估，我们使用我们的内部评估库。</p>
<table>
<thead>
<tr>
<th>模型</th>
<th>规模</th>
<th>Code</th>
<th>常识推理</th>
<th>世界知识</th>
<th>阅读理解</th>
<th>数学</th>
<th>MMLU</th>
<th>BBH</th>
<th>AGI Eval</th>
</tr>
</thead>
<tbody><tr>
<td>Llama 1</td>
<td>7B</td>
<td>14.1</td>
<td>60.8</td>
<td>46.2</td>
<td>58.5</td>
<td>6.95</td>
<td>35.1</td>
<td>30.3</td>
<td>23.9</td>
</tr>
<tr>
<td>Llama 1</td>
<td>13B</td>
<td>18.9</td>
<td>66.1</td>
<td>52.6</td>
<td>62.3</td>
<td>10.9</td>
<td>46.9</td>
<td>37.0</td>
<td>33.9</td>
</tr>
<tr>
<td>Llama 1</td>
<td>33B</td>
<td>26.0</td>
<td>70.0</td>
<td>58.4</td>
<td>67.6</td>
<td>21.4</td>
<td>57.8</td>
<td>39.8</td>
<td>41.7</td>
</tr>
<tr>
<td>Llama 1</td>
<td>65B</td>
<td>30.7</td>
<td>70.7</td>
<td>60.5</td>
<td>68.6</td>
<td>30.8</td>
<td>63.4</td>
<td>43.5</td>
<td>47.6</td>
</tr>
<tr>
<td>Llama 2</td>
<td>7B</td>
<td>16.8</td>
<td>63.9</td>
<td>48.9</td>
<td>61.3</td>
<td>14.6</td>
<td>45.3</td>
<td>32.6</td>
<td>29.3</td>
</tr>
<tr>
<td>Llama 2</td>
<td>13B</td>
<td>24.5</td>
<td>66.9</td>
<td>55.4</td>
<td>65.8</td>
<td>28.7</td>
<td>54.8</td>
<td>39.4</td>
<td>39.1</td>
</tr>
<tr>
<td>Llama 2</td>
<td>70B</td>
<td>37.5</td>
<td>71.9</td>
<td>63.6</td>
<td>69.4</td>
<td>35.2</td>
<td>68.9</td>
<td>51.2</td>
<td>54.2</td>
</tr>
</tbody></table>
<p>模型在 grouped academic benchmarks 上的整体表现。Code：我们报告模型在 HumanEval 和 MBPP 上的平均 pass@1 分数。常识推理：我们报告 PIQA、SIQA、HellaSwag、WinoGrande、ARC easy 和 challenge、OpenBookQA 和 CommonsenseQA 的平均分数。我们对 CommonSenseQA 进行了 7-shot 结果评估，对其他所有基准测试进行了 0-shot 结果评估。世界知识：我们在 NaturalQuestions 和 TriviaQA 上进行 5-shot 性能评估并报告平均分数。阅读理解：对于阅读理解，我们报告 SQuAD、QuAC 和 BoolQ 的 0-shot 平均分数。数学：我们报告 GSM8K（8-shot）和 MATH（4-shot）基准测试的平均分数。</p>
<h3 id="TruthfulQA-和-Toxigen"><a href="#TruthfulQA-和-Toxigen" class="headerlink" title="TruthfulQA 和 Toxigen"></a>TruthfulQA 和 Toxigen</h3><table>
<thead>
<tr>
<th>模型</th>
<th>规模</th>
<th>TruthfulQA</th>
<th>Toxigen</th>
</tr>
</thead>
<tbody><tr>
<td>Llama 1</td>
<td>7B</td>
<td>27.42</td>
<td>23.00</td>
</tr>
<tr>
<td>Llama 1</td>
<td>13B</td>
<td>41.74</td>
<td>23.08</td>
</tr>
<tr>
<td>Llama 1</td>
<td>33B</td>
<td>44.19</td>
<td>22.57</td>
</tr>
<tr>
<td>Llama 1</td>
<td>65B</td>
<td>48.71</td>
<td>21.77</td>
</tr>
<tr>
<td>Llama 2</td>
<td>7B</td>
<td>33.29</td>
<td>21.25</td>
</tr>
<tr>
<td>Llama 2</td>
<td>13B</td>
<td>41.86</td>
<td>26.10</td>
</tr>
<tr>
<td>Llama 2</td>
<td>70B</td>
<td>50.18</td>
<td>24.60</td>
</tr>
</tbody></table>
<p>预训练 LLMs 在自动安全基准测试上的评估结果。对于 TruthfulQA，我们呈现同时具有真实性和信息量的生成百分比（百分比越高越好）。对于 ToxiGen，我们呈现有害生成的百分比（百分比越小越好）。</p>
<h3 id="TruthfulQA-和-Toxigen（微调版本-LLMs）"><a href="#TruthfulQA-和-Toxigen（微调版本-LLMs）" class="headerlink" title="TruthfulQA 和 Toxigen（微调版本 LLMs）"></a>TruthfulQA 和 Toxigen（微调版本 LLMs）</h3><table>
<thead>
<tr>
<th>模型</th>
<th>规模</th>
<th>TruthfulQA</th>
<th>Toxigen</th>
</tr>
</thead>
<tbody><tr>
<td>Llama-2-Chat</td>
<td>7B</td>
<td>57.04</td>
<td>0.00</td>
</tr>
<tr>
<td>Llama-2-Chat</td>
<td>13B</td>
<td>62.18</td>
<td>0.00</td>
</tr>
<tr>
<td>Llama-2-Chat</td>
<td>70B</td>
<td>64.14</td>
<td>0.01</td>
</tr>
</tbody></table>
<p>不同安全数据集上微调 LLMs 的评估结果。度量标准定义同上。</p>
<h2 id="道德考虑和局限性"><a href="#道德考虑和局限性" class="headerlink" title="道德考虑和局限性"></a>道德考虑和局限性</h2><p>Llama 2 是一项具有风险的新技术。迄今为止的测试仅涵盖了英语，并且无法覆盖所有场景。因此，与所有 LLMs 一样，Llama 2 的潜在输出无法事先预测，并且在某些情况下可能会产生不准确、带偏见或其他不可取的响应。因此，在部署任何 Llama 2 应用程序之前，开发人员应根据其特定的模型应用进行安全测试和调整。</p>
<p>请参阅“负责任使用指南”，网址为：<a target="_blank" rel="noopener" href="https://ai.meta.com/llama/responsible-use-guide/">https://ai.meta.com/llama/responsible-use-guide/</a></p>
<h2 id="报告问题"><a href="#报告问题" class="headerlink" title="报告问题"></a>报告问题</h2><p>请通过以下方式之一报告任何软件“bug”或模型的其他问题：</p>
<ul>
<li>报告模型问题：[github.com/facebookresearch/llama](<a target="_blank" rel="noopener" href="https://github/">https://github</a></li>
</ul>
<p>.com/facebookresearch/llama)</p>
<ul>
<li>报告模型生成的有问题内容：<a target="_blank" rel="noopener" href="https://developers.facebook.com/llama_output_feedback">developers.facebook.com/llama_output_feedback</a></li>
<li>报告 bug 和安全问题：<a target="_blank" rel="noopener" href="https://facebook.com/whitehat/info">facebook.com/whitehat/info</a></li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="https://chenhuiyu.github.io">Huiyu Chen</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="https://chenhuiyu.github.io/2023/08/02/NLP%20Insights/training-llama-2-model-on-single-gpu-with-int8-quantization-and-lora.zh-CN/">https://chenhuiyu.github.io/2023/08/02/NLP%20Insights/training-llama-2-model-on-single-gpu-with-int8-quantization-and-lora.zh-CN/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来源 <a href="https://chenhuiyu.github.io" target="_blank">黑头呆鱼进化之旅</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/LLM/">LLM</a></div><div class="post-share"><div class="social-share" data-image="/img/butterfly-icon.png" data-sites="facebook,x,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2023/08/21/NLP%20Insights/conver-pytorch-model-to-onnx-format.en/" title="Conver Pytorch Model to ONNX Format"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">上一篇</div><div class="info-item-2">Conver Pytorch Model to ONNX Format</div></div><div class="info-2"><div class="info-item-1">使用 PyTorch 和 ONNX 检查模型一致性在机器学习和深度学习的开发过程中，模型的互操作性变得越来越重要。ONNX (Open Neural Network Exchange) 是一种开放格式，用于表示机器学习和深度学习模型。它允许开发者在各种深度学习框架之间轻松地共享模型，从而提高了模型的可移植性和互操作性。 本教程将指导您完成以下步骤：  将 PyTorch 模型转换为 ONNX 格式。 验证转换后的 ONNX 模型与原始 PyTorch 模型的输出是否一致。  1. 导入必要的库首先，我们导入为模型转换和验证所需的所有库。 123456import osimport sysimport torchimport onnximport onnxruntimeimport numpy as np  2. 定义模型转换函数为了将 PyTorch 模型转换为 ONNX 格式，我们定义了一个名为 convert_onnx 的函数。此函数使用 PyTorch 的内置函数 torch.onnx.export 将模型转换为 ONNX 格式。 12345678910def conver...</div></div></div></a><a class="pagination-related" href="/2023/08/02/NLP%20Insights/training-llama-2-model-on-single-gpu-with-int8-quantization-and-lora.en/" title="Training Llama 2 Model on Single GPU with int8 Quantization and LoRA"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">下一篇</div><div class="info-item-2">Training Llama 2 Model on Single GPU with int8 Quantization and LoRA</div></div><div class="info-2"><div class="info-item-1">Training Llama 2 Model on Single GPU with int8 Quantization and LoRALlama 2概述Llama 2 是一个包含预训练和微调的生成式文本模型的集合，其规模从 70 亿到 700 亿个参数不等。Llama2模型是由Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert等人在Llama 2: Open Foundation and Fine-Tuned Chat Models中提出的。 该论文的摘要如下： 在这项工作中，我们开发并发布了Llama 2，这是一组从70亿到700亿参数的预训练和微调的大型语言模型（LLMs）。我们的微调LLMs，称为Llama 2-Chat，针对对话用例进行了优化。我们的模型在我们测试的大多数基准上胜过开源聊天模型，并且基于我们对有用性和安全性的人类评估，可能是闭源模型的合适替代品。我们提供了关于微调和改进Llama 2-Chat安全性的方法的详细描述，以便社区能够在我们的工作基础上构建，并有助于LLMs的负责任发展。 在此处查看所有L...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2024/02/27/NLP%20Insights/fastchat-training-script-code-analysis-train-py-fastchat-series-part-1.en/" title="FastChat Training Script Code Analysis - Train.py 【FastChat Series Part 1】"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-27</div><div class="info-item-2">FastChat Training Script Code Analysis - Train.py 【FastChat Series Part 1】</div></div><div class="info-2"><div class="info-item-1">FastChat Training Script Code Analysis - Train.py 【FastChat Series Part 1】In this article, we delve into the train.py script of FastChat (https://github.com/lm-sys/FastChat) (https://github.com/lm-sys/FastChat/blob/main/fastchat/train/train.py), a key component for training and optimizing large language models (LLMs). FastChat is an advanced open-source platform focused on developing, deploying, and evaluating chatbots based on LLMs. The platform not only supports top-tier models like Vicuna ...</div></div></div></a><a class="pagination-related" href="/2025/02/11/NLP%20Insights/moe%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97-%E6%8F%AD%E7%A7%98-moe-%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2.zh-CN/" title="MoE模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-02-11</div><div class="info-item-2">MoE模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色</div></div><div class="info-2"><div class="info-item-1">MoE 模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色原文地址：A Visual Guide to Mixture of Experts (MoE) 📅 作者：Maarten Grootendorst 📆 日期：2024 年 10 月 7 日  探索语言模型：混合专家模型（MoE）可视化指南目录 MoE 模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色 探索语言模型：混合专家模型（MoE）可视化指南 目录 什么是混合专家（MoE）模型？ Experts Dense Layers Sparse Layers What does an Expert Learn? 专家的架构（Architecture of Experts）      当我们查看最新发布的大型语言模型（LLMs，Large Language Models）时，常常会在标题中看到 “MoE”。这个 “MoE” 代表什么？为什么这么多 LLM 都在使用它？ 在这份可视化指南中，我们会通过 50 多个可视化图示，逐步探索这一关键组件：**Mixture of Experts (MoE)**。   图示内...</div></div></div></a><a class="pagination-related" href="/2023/07/27/NLP%20Insights/prompt-engineering.zh-CN/" title="Prompt Engineering"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2023-07-27</div><div class="info-item-2">Prompt Engineering</div></div><div class="info-2"><div class="info-item-1">Prompt EngineeringPrompt Engineering, 也被称为上下文提示，是指在不更新模型权重的情况下，与LLM（语言模型）进行交互以引导其产生期望输出的方法。它是一门实证科学，提示工程方法的效果在不同模型之间可能会有很大的差异，因此需要进行大量的实验和试探。 本文仅关注自回归语言模型的提示工程，不涉及填空测试、图像生成或多模态模型。在本质上，提示工程的目标是实现模型的对齐和可操控性。您可以查阅我之前关于可控文本生成的帖子。 基本提示方法zero-shot学习和few-shot学习是两种最基本的提示模型方法，这些方法由许多LLM论文首创，并且通常用于评估LLM性能。 zero-shot学习zero-shot学习是将任务文本直接输入模型并要求获得结果。 （所有情感分析示例来自于SST-2数据集） 12Text: i'll bet the video game is a lot more fun than the film.Sentiment: few-shot学习few-shot学习通过提供一组高质量的示例演示，每个示例都包含目标任务的输入和期望输出。当模型首...</div></div></div></a><a class="pagination-related" href="/2025/02/11/NLP%20Insights/moe%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97-%E6%8F%AD%E7%A7%98-moe-%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2.en/" title="MoE模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-02-11</div><div class="info-item-2">MoE模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色</div></div><div class="info-2"><div class="info-item-1">MoE 模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色原文地址：A Visual Guide to Mixture of Experts (MoE) 📅 作者：Maarten Grootendorst 📆 日期：2024 年 10 月 7 日  探索语言模型：混合专家模型（MoE）可视化指南目录 MoE 模型的的可视化指南：揭秘 MoE 在大型语言模型中的角色 探索语言模型：混合专家模型（MoE）可视化指南 目录 什么是混合专家（MoE）模型？ Experts Dense Layers Sparse Layers What does an Expert Learn? 专家的架构（Architecture of Experts）      当我们查看最新发布的大型语言模型（LLMs，Large Language Models）时，常常会在标题中看到 “MoE”。这个 “MoE” 代表什么？为什么这么多 LLM 都在使用它？ 在这份可视化指南中，我们会通过 50 多个可视化图示，逐步探索这一关键组件：**Mixture of Experts (MoE)**。   图示内...</div></div></div></a><a class="pagination-related" href="/2025/03/06/NLP%20Insights/differences-in-padding-strategies-between-decoder-only-and-encoder-only-models.zh-CN/" title="Decoder-only与Encoder-only模型Padding策略的差异"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-06</div><div class="info-item-2">Decoder-only与Encoder-only模型Padding策略的差异</div></div><div class="info-2"><div class="info-item-1">📌 Padding 的含义在大模型 (LLM) 中，padding 是用于将不同长度的序列调整为同一长度的方法，以便于批量 (batch) 处理。 例如： 12句子1: "I love NLP"句子2: "Padding is useful in LLM training"  使用 &lt;pad&gt; token 进行对齐： 12"I love NLP &lt;pad&gt; &lt;pad&gt; &lt;pad&gt;""Padding is useful in LLM training"   📌 Padding 位置的选择：Left vs RightPadding 有两种常见方式：  Right padding（右填充）： 1"I love NLP &lt;pad&gt; &lt;pad&gt;"  Left padding（左填充）： 1"&lt;pad&gt; &lt;pad&gt; I love NLP"  通常：  Decoder-only 模型（如 GPT, Llama）：采用 Left padding Encoder-only 模型（如 BERT）：采用...</div></div></div></a><a class="pagination-related" href="/2024/02/28/NLP%20Insights/gorilla-llm-%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%AE%80%E4%BB%8B.en/" title="Gorilla LLM 大语言模型简介"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-28</div><div class="info-item-2">Gorilla LLM 大语言模型简介</div></div><div class="info-2"><div class="info-item-1">Gorilla LLM 大语言模型简介🦍 Gorilla: Large Language Model Connected with Massive APIsLink: https://gorilla.cs.berkeley.edu/blogs/7_open_functions_v2.html  Berkeley 功能调用排行榜Berkeley 功能调用排行榜 在线体验模型：Gorilla OpenFunctions-v2 网络演示 项目详情：GitHub 模型（7B 参数）在 HuggingFace 上的页面：gorilla-llm/gorilla-openfunctions-v2  1. 伯克利函数调用排行榜自 2022 年底以来，大语言模型（LLMs）凭借其执行通用任务的强大能力，成为众人关注的焦点。不仅限于聊天应用，将这些模型应用于开发各类 AI 应用和软件（如 Langchain, Llama Index, AutoGPT, Voyager）已成为一种趋势。GPT, Gemini, Llama, Mistral 等模型通过与外部世界的交互，如函数调用和执行，展现了其巨大...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Huiyu Chen</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">102</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">49</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Training-Llama-2-Model-on-Single-GPU-with-int8-Quantization-and-LoRA"><span class="toc-number">1.</span> <span class="toc-text">Training Llama 2 Model on Single GPU with int8 Quantization and LoRA</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Llama-2"><span class="toc-number">2.</span> <span class="toc-text">Llama 2</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A6%82%E8%BF%B0"><span class="toc-number">2.1.</span> <span class="toc-text">概述</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8F%90%E7%A4%BA%EF%BC%9A"><span class="toc-number">2.1.1.</span> <span class="toc-text">提示：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AF%A6%E6%83%85"><span class="toc-number">2.2.</span> <span class="toc-text">模型详情</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%BC%80%E5%8F%91%E8%80%85"><span class="toc-number">2.3.</span> <span class="toc-text">模型开发者</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%8D%E5%90%8C%E7%89%88%E6%9C%AC"><span class="toc-number">2.4.</span> <span class="toc-text">不同版本</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA"><span class="toc-number">2.5.</span> <span class="toc-text">输入输出</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84"><span class="toc-number">2.6.</span> <span class="toc-text">模型架构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE"><span class="toc-number">2.7.</span> <span class="toc-text">训练数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%97%A5%E6%9C%9F"><span class="toc-number">2.8.</span> <span class="toc-text">模型训练日期</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%8A%B6%E6%80%81"><span class="toc-number">2.9.</span> <span class="toc-text">状态</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%B8%E5%8F%AF%E8%AF%81"><span class="toc-number">2.10.</span> <span class="toc-text">许可证</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A0%94%E7%A9%B6%E8%AE%BA%E6%96%87"><span class="toc-number">2.11.</span> <span class="toc-text">研究论文</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E7%9B%AE%E7%9A%84"><span class="toc-number">2.12.</span> <span class="toc-text">使用目的</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A2%84%E6%9C%9F%E7%94%A8%E9%80%94"><span class="toc-number">2.12.1.</span> <span class="toc-text">预期用途</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8D%E5%9C%A8%E8%8C%83%E5%9B%B4%E5%86%85%E7%9A%84%E7%94%A8%E9%80%94"><span class="toc-number">2.12.2.</span> <span class="toc-text">不在范围内的用途</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A1%AC%E4%BB%B6%E5%92%8C%E8%BD%AF%E4%BB%B6"><span class="toc-number">2.13.</span> <span class="toc-text">硬件和软件</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E5%9B%A0%E7%B4%A0"><span class="toc-number">2.13.1.</span> <span class="toc-text">训练因素</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A2%B3%E8%B6%B3%E8%BF%B9"><span class="toc-number">2.13.2.</span> <span class="toc-text">碳足迹</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE-1"><span class="toc-number">2.14.</span> <span class="toc-text">训练数据</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A6%82%E8%BF%B0-1"><span class="toc-number">2.14.1.</span> <span class="toc-text">概述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%96%B0%E9%B2%9C%E5%BA%A6"><span class="toc-number">2.14.2.</span> <span class="toc-text">数据新鲜度</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%84%E4%BC%B0%E7%BB%93%E6%9E%9C"><span class="toc-number">2.15.</span> <span class="toc-text">评估结果</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#TruthfulQA-%E5%92%8C-Toxigen"><span class="toc-number">2.15.1.</span> <span class="toc-text">TruthfulQA 和 Toxigen</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#TruthfulQA-%E5%92%8C-Toxigen%EF%BC%88%E5%BE%AE%E8%B0%83%E7%89%88%E6%9C%AC-LLMs%EF%BC%89"><span class="toc-number">2.15.2.</span> <span class="toc-text">TruthfulQA 和 Toxigen（微调版本 LLMs）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%81%93%E5%BE%B7%E8%80%83%E8%99%91%E5%92%8C%E5%B1%80%E9%99%90%E6%80%A7"><span class="toc-number">2.16.</span> <span class="toc-text">道德考虑和局限性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8A%A5%E5%91%8A%E9%97%AE%E9%A2%98"><span class="toc-number">2.17.</span> <span class="toc-text">报告问题</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/21/NLP%20Insights/memory-daily-2026-02-21.en/" title="Memory Daily Open-Source Paper &amp; Code Picks (2026-02-21)">Memory Daily Open-Source Paper &amp; Code Picks (2026-02-21)</a><time datetime="2026-02-21T04:13:43.000Z" title="发表于 2026-02-21 12:13:43">2026-02-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/21/NLP%20Insights/memory-daily-2026-02-21.zh-CN/" title="Memory Daily｜大厂开源论文代码精选（2026-02-21）">Memory Daily｜大厂开源论文代码精选（2026-02-21）</a><time datetime="2026-02-21T04:13:43.000Z" title="发表于 2026-02-21 12:13:43">2026-02-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/21/NLP%20Insights/evaluation-of-generation-based-large-language-models-llms-opportunities-and-challenges-from-generation-to-judgment.en/" title="evaluation-of-generation-based-large-language-models-llms-opportunities-and-challenges-from-generation-to-judgment">evaluation-of-generation-based-large-language-models-llms-opportunities-and-challenges-from-generation-to-judgment</a><time datetime="2026-02-20T16:00:00.000Z" title="发表于 2026-02-21 00:00:00">2026-02-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/21/NLP%20Insights/evaluation-of-generation-based-large-language-models-llms-opportunities-and-challenges-from-generation-to-judgment.zh-CN/" title="evaluation-of-generation-based-large-language-models-llms-opportunities-and-challenges-from-generation-to-judgment">evaluation-of-generation-based-large-language-models-llms-opportunities-and-challenges-from-generation-to-judgment</a><time datetime="2026-02-20T16:00:00.000Z" title="发表于 2026-02-21 00:00:00">2026-02-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/24/NLP%20Insights/secom-redefining-memory-management-in-conversational-ai.en/" title="SeCom: Redefining Memory Management in Conversational AI">SeCom: Redefining Memory Management in Conversational AI</a><time datetime="2025-06-24T08:00:00.000Z" title="发表于 2025-06-24 16:00:00">2025-06-24</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 - 2026 By Huiyu Chen</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 6.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.5.4</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.4"></script><script src="/js/main.js?v=5.5.4"></script><div class="js-pjax"></div><script src="/js/lang-switch.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>