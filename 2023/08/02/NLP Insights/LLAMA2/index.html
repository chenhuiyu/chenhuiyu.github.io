<!DOCTYPE html><html class="appearance-auto" lang="[&quot;en&quot;]"><head><meta charset="UTF-8"><title>Training Llama 2 Model on Single GPU with int8 Quantization and LoRA</title><meta name="description" content="May the Force be with you"><meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no, initial-scale=1"><!-- Google Analytics --><!-- End Google Analytics -->
<!-- Baidu Analytics --><!-- End Baidu Analytics --><link rel="icon" href="/images/favicon.ico"><link rel="stylesheet" href="/style/common/bulma.css"><link rel="stylesheet" href="/style/base.css"><link rel="stylesheet" href="/style/common/helper.css"><script src="/js/common.js"></script><link rel="stylesheet" href="/style/post.css"><link rel="stylesheet" href="/style/themes/highlight-theme-light.css"><link rel="stylesheet" href="/style/common/jquery.fancybox.min.css"><script src="/js/highlight.pack.js"></script><meta name="description" content="Training Llama 2 Model on Single GPU with int8 Quantization and LoRALlama 2概述Llama 2 是一个包含预训练和微调的生成式文本模型的集合，其规模从 70 亿到 700 亿个参数不等。Llama2模型是由Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert等人在Llama 2: Open Foundation and Fine-Tuned Chat Models中提出的。
该论文的摘要如下：
在这项工作中，我们开发并发布了Llama 2，这是一组从70亿到700亿参数的预训练和微调的大型语言模型（LLMs）。我们的微调LLMs，称为Llama 2-Chat，针对对话用例进行了优化.."><meta name="generator" content="Hexo 6.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="黑头呆鱼进化之旅" type="application/atom+xml">
</head><body class="is-flex is-flex-direction-column"><header class="header-widget is-flex-shrink-0 is-hidden-mobile"><div class="container is-fullhd is-flex is-justify-content-space-between is-align-items-center is-full-height"><section class="is-hidden-mobile is-flex-shrink-0"><h2><a href="/">user's blog</a></h2></section><h3 class="is-hidden-mobile is-family-serif is-full-height is-flex is-align-items-center is-flex-shrink-0"><div class="is-full-height" id="postTopic"><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">Training Llama 2 Model on Single GPU with int8 Quantization and..</p><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">Click back to the top</p></div></h3><aside class="is-flex-shrink-0"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></aside></div></header><header class="is-flex header-widget is-flex-shrink-0 is-align-items-center is-justify-content-center is-hidden-tablet"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></header><main><main class="container is-max-widescreen content section post-page pt-4 px-4"><div class="columns is-flex-desktop is-justify-content-center is-flex-direction-row-reverse"><div class="column is-3 is-hidden-mobile"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Training-Llama-2-Model-on-Single-GPU-with-int8-Quantization-and-LoRA"><span class="toc-text">Training Llama 2 Model on Single GPU with int8 Quantization and LoRA</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Llama-2"><span class="toc-text">Llama 2</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A6%82%E8%BF%B0"><span class="toc-text">概述</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8F%90%E7%A4%BA%EF%BC%9A"><span class="toc-text">提示：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AF%A6%E6%83%85"><span class="toc-text">模型详情</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%BC%80%E5%8F%91%E8%80%85"><span class="toc-text">模型开发者</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%8D%E5%90%8C%E7%89%88%E6%9C%AC"><span class="toc-text">不同版本</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA"><span class="toc-text">输入输出</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84"><span class="toc-text">模型架构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE"><span class="toc-text">训练数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%97%A5%E6%9C%9F"><span class="toc-text">模型训练日期</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%8A%B6%E6%80%81"><span class="toc-text">状态</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%B8%E5%8F%AF%E8%AF%81"><span class="toc-text">许可证</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A0%94%E7%A9%B6%E8%AE%BA%E6%96%87"><span class="toc-text">研究论文</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E7%9B%AE%E7%9A%84"><span class="toc-text">使用目的</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A2%84%E6%9C%9F%E7%94%A8%E9%80%94"><span class="toc-text">预期用途</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8D%E5%9C%A8%E8%8C%83%E5%9B%B4%E5%86%85%E7%9A%84%E7%94%A8%E9%80%94"><span class="toc-text">不在范围内的用途</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A1%AC%E4%BB%B6%E5%92%8C%E8%BD%AF%E4%BB%B6"><span class="toc-text">硬件和软件</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E5%9B%A0%E7%B4%A0"><span class="toc-text">训练因素</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A2%B3%E8%B6%B3%E8%BF%B9"><span class="toc-text">碳足迹</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE-1"><span class="toc-text">训练数据</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A6%82%E8%BF%B0-1"><span class="toc-text">概述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%96%B0%E9%B2%9C%E5%BA%A6"><span class="toc-text">数据新鲜度</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%84%E4%BC%B0%E7%BB%93%E6%9E%9C"><span class="toc-text">评估结果</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#TruthfulQA-%E5%92%8C-Toxigen"><span class="toc-text">TruthfulQA 和 Toxigen</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#TruthfulQA-%E5%92%8C-Toxigen%EF%BC%88%E5%BE%AE%E8%B0%83%E7%89%88%E6%9C%AC-LLMs%EF%BC%89"><span class="toc-text">TruthfulQA 和 Toxigen（微调版本 LLMs）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%81%93%E5%BE%B7%E8%80%83%E8%99%91%E5%92%8C%E5%B1%80%E9%99%90%E6%80%A7"><span class="toc-text">道德考虑和局限性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8A%A5%E5%91%8A%E9%97%AE%E9%A2%98"><span class="toc-text">报告问题</span></a></li></ol></li></ol></div><div class="column is-9"><header class="my-4"><a href="/tags/LLM"><i class="tag post-item-tag">LLM</i></a></header><h1 class="mt-0 mb-1 is-family-serif" id="postTitle">Training Llama 2 Model on Single GPU with int8 Quantization and LoRA</h1><time class="has-text-grey" datetime="2023-08-02T07:38:29.000Z">2023-08-02</time><article class="mt-2 post-content"><h1 id="Training-Llama-2-Model-on-Single-GPU-with-int8-Quantization-and-LoRA"><a href="#Training-Llama-2-Model-on-Single-GPU-with-int8-Quantization-and-LoRA" class="headerlink" title="Training Llama 2 Model on Single GPU with int8 Quantization and LoRA"></a>Training Llama 2 Model on Single GPU with int8 Quantization and LoRA</h1><h1 id="Llama-2"><a href="#Llama-2" class="headerlink" title="Llama 2"></a>Llama 2</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p><em>Llama 2</em> 是一个包含预训练和微调的生成式文本模型的集合，其规模从 70 亿到 700 亿个参数不等。Llama2模型是由Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert等人在<a target="_blank" rel="noopener" href="https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/">Llama 2: Open Foundation and Fine-Tuned Chat Models</a>中提出的。</p>
<p>该论文的摘要如下：</p>
<p>在这项工作中，我们开发并发布了Llama 2，这是一组从70亿到700亿参数的预训练和微调的大型语言模型（LLMs）。我们的微调LLMs，称为Llama 2-Chat，针对对话用例进行了优化。我们的模型在我们测试的大多数基准上胜过开源聊天模型，并且基于我们对有用性和安全性的人类评估，可能是闭源模型的合适替代品。我们提供了关于微调和改进Llama 2-Chat安全性的方法的详细描述，以便社区能够在我们的工作基础上构建，并有助于LLMs的负责任发展。</p>
<p><a target="_blank" rel="noopener" href="https://huggingface.co/models?search=llama2">在此处查看所有Llama2模型</a></p>
<h3 id="提示："><a href="#提示：" class="headerlink" title="提示："></a>提示：</h3><ul>
<li>通过填写<a target="_blank" rel="noopener" href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/">此表格</a>可以获得Llama2模型的权重</li>
<li>该架构与第一个Llama非常相似，增加了Groupe Query Attention（GQA）<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2305.13245.pdf">此论文</a>之后</li>
<li>将<code>config.pretraining_tp</code>设置为不同于1的值将激活线性层的更准确但更慢的计算，这应更好地匹配原始logits。</li>
<li>原始模型使用<code>pad_id = -1</code>，这意味着没有填充令牌。我们不能使用相同的逻辑，请确保使用<code>tokenizer.add_special_tokens({"pad_token":"&lt;pad&gt;"})</code>添加填充令牌，并相应地调整令牌嵌入大小。您还应设置<code>model.config.pad_token_id</code>。模型的embed_tokens层用<code>self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.config.padding_idx)</code>初始化，确保编码填充令牌将输出零，因此在初始化时传递它是推荐的。</li>
<li>填写表格并获得模型检查点的访问权限后，您应该能够使用已转换的检查点。否则，如果您正在转换自己的模型，请随时使用转换脚本。可以使用以下（示例）命令调用脚本：<figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python src/transformers/models/llama/convert_llama_weights_to_hf.py \</span><br><span class="line">    --input_dir /path/to/downloaded/llama/weights --model_size 7B --output_dir /output/path</span><br></pre></td></tr></tbody></table></figure></li>
</ul>
<h2 id="模型详情"><a href="#模型详情" class="headerlink" title="模型详情"></a>模型详情</h2><p>注意：使用该模型受 Meta 许可证的约束。为了下载模型权重和分词器，请访问网站并在请求访问之前接受许可证。</p>
<p>Meta 开发并公开发布了 Llama 2 系列大型语言模型（LLMs），这是一系列规模从 70 亿到 700 亿参数的预训练和微调的生成式文本模型。我们的微调 LLMs，称为 Llama-2-Chat，经过优化用于对话应用场景。Llama-2-Chat 模型在我们测试的大多数基准测试中优于开源聊天模型，并在我们的人工评估中在有用性和安全性方面与一些流行的闭源模型（如ChatGPT和PaLM）持平。</p>
<h2 id="模型开发者"><a href="#模型开发者" class="headerlink" title="模型开发者"></a>模型开发者</h2><p>Model Developers Meta</p>
<h2 id="不同版本"><a href="#不同版本" class="headerlink" title="不同版本"></a>不同版本</h2><p>Llama 2 有不同规模的参数版本，包括 7B、13B 和 70B，以及预训练和微调的变体。</p>
<h2 id="输入输出"><a href="#输入输出" class="headerlink" title="输入输出"></a>输入输出</h2><p>输入模型仅支持文本输入。</p>
<p>输出模型仅生成文本。</p>
<h2 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h2><p>Llama 2 是一种自回归语言模型，采用了优化的 Transformer 架构。微调版本使用有监督的微调（SFT）和基于人类反馈的强化学习（RLHF）来与人类对 helpfulness 和 safety 的偏好保持一致。</p>
<h2 id="训练数据"><a href="#训练数据" class="headerlink" title="训练数据"></a>训练数据</h2><table>
<thead>
<tr>
<th>模型名称</th>
<th>训练数据</th>
<th>参数规模</th>
<th>内容长度</th>
<th>GQA</th>
<th>Tokens</th>
<th>LR</th>
</tr>
</thead>
<tbody><tr>
<td>Llama 2</td>
<td>一种新的公开可用的在线数据混合</td>
<td>7B</td>
<td>4k</td>
<td>✗</td>
<td>2.0T</td>
<td>3.0 x 10-4</td>
</tr>
<tr>
<td>Llama 2</td>
<td>一种新的公开可用的在线数据混合</td>
<td>13B</td>
<td>4k</td>
<td>✗</td>
<td>2.0T</td>
<td>3.0 x 10-4</td>
</tr>
<tr>
<td>Llama 2</td>
<td>一种新的公开可用的在线数据混合</td>
<td>70B</td>
<td>4k</td>
<td>✔</td>
<td>2.0T</td>
<td>1.5 x 10-4</td>
</tr>
</tbody></table>
<p>注：Token counts 仅指预训练数据。所有模型都使用全局 batch-size 为 4M tokens 进行训练。规模更大的模型（70B）使用 Grouped-Query Attention（GQA）来提高推理可伸缩性。</p>
<h2 id="模型训练日期"><a href="#模型训练日期" class="headerlink" title="模型训练日期"></a>模型训练日期</h2><p>Llama 2 在2023年1月至2023年7月之间进行训练。</p>
<h2 id="状态"><a href="#状态" class="headerlink" title="状态"></a>状态</h2><p>这是一个在离线数据集上训练的静态模型。随着我们根据社区反馈改进模型的安全性，将发布微调版本的未来版本。</p>
<h2 id="许可证"><a href="#许可证" class="headerlink" title="许可证"></a>许可证</h2><p>定制的商业许可证可在以下网址获取：<a target="_blank" rel="noopener" href="https://ai.meta.com/resources/models-and-libraries/llama-downloads/">https://ai.meta.com/resources/models-and-libraries/llama-downloads/</a></p>
<h2 id="研究论文"><a href="#研究论文" class="headerlink" title="研究论文"></a>研究论文</h2><p>《Llama-2: Open Foundation and Fine-tuned Chat Models》</p>
<h2 id="使用目的"><a href="#使用目的" class="headerlink" title="使用目的"></a>使用目的</h2><h3 id="预期用途"><a href="#预期用途" class="headerlink" title="预期用途"></a>预期用途</h3><p>Llama 2 旨在用于英语商业和研究用途。微调模型适用于类似助理的聊天应用，而预训练模型可适应多种自然语言生成任务。</p>
<p>要获得聊天版本的预期功能和性能，需要遵循特定的格式，包括 INST 和 &lt;<sys>&gt; 标签、BOS 和 EOS tokens，以及它们之间的空格和换行符（我们建议对输入调用 strip() 方法，以避免双空格）。有关详情，请参阅我们在 GitHub 上的参考代码：chat_completion。</sys></p>
<h3 id="不在范围内的用途"><a href="#不在范围内的用途" class="headerlink" title="不在范围内的用途"></a>不在范围内的用途</h3><ul>
<li>用于违反适用法律法规（包括贸易合规法）的任何方式。</li>
<li>用于除英语以外的其他语言。</li>
<li>用于 Llama 2 可接受使用政策和许可协议所禁止的任何其他方式。</li>
</ul>
<h2 id="硬件和软件"><a href="#硬件和软件" class="headerlink" title="硬件和软件"></a>硬件和软件</h2><h3 id="训练因素"><a href="#训练因素" class="headerlink" title="训练因素"></a>训练因素</h3><p>我们使用自定义训练库、Meta 的 Research Super Cluster 以及生产集群进行预训练。微调、标注和评估也是在第三方云计算上执行的。</p>
<h3 id="碳足迹"><a href="#碳足迹" class="headerlink" title="碳足迹"></a>碳足迹</h3><p>预训练过程中使用了累计 330 万 GPU 小时的计算，使用的硬件类型为 A100-80GB（TDP 为 350-400W）。预计总排放量为 539 tCO2eq，其中 100% 由 Meta 的可持续性计划抵消。</p>
<table>
<thead>
<tr>
<th>模型</th>
<th>时间（GPU 小时）</th>
<th>功耗（瓦）</th>
<th>排放碳量（tCO2eq）</th>
</tr>
</thead>
<tbody><tr>
<td>Llama 2 7B</td>
<td>184,320</td>
<td>400</td>
<td>31.22</td>
</tr>
<tr>
<td>Llama 2 13B</td>
<td>368,640</td>
<td>400</td>
<td>62.44</td>
</tr>
<tr>
<td>Llama 2 70B</td>
<td>1,720,320</td>
<td>400</td>
<td>291.42</td>
</tr>
<tr>
<td>总计</td>
<td>3,311,616</td>
<td></td>
<td>539.00</td>
</tr>
</tbody></table>
<p>预训练期间的二氧化碳排放量。时间：每个模型训练所需的总 GPU 时间。功耗：用于所使用的 GPU 设备的每个 GPU 的峰值功率容量，调整后的</p>
<p>功耗使用效率。100% 的排放直接由 Meta 的可持续性计划抵消，因为我们正在公开发布这些模型，预训练成本不需要由他人承担。</p>
<h2 id="训练数据-1"><a href="#训练数据-1" class="headerlink" title="训练数据"></a>训练数据</h2><h3 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h3><p>Llama 2 在来自公开来源的数据中预训练了 2 万亿个 tokens。微调数据包括公开可用的指导数据集，以及一百万个新的人工标注示例。预训练和微调数据集均不包含 Meta 用户数据。</p>
<h3 id="数据新鲜度"><a href="#数据新鲜度" class="headerlink" title="数据新鲜度"></a>数据新鲜度</h3><p>预训练数据截止日期为 2022 年 9 月，但一些微调数据更近，最多至 2023 年 7 月。</p>
<h2 id="评估结果"><a href="#评估结果" class="headerlink" title="评估结果"></a>评估结果</h2><p>在此部分，我们报告了 Llama 1 和 Llama 2 模型在标准学术基准测试上的结果。对于所有评估，我们使用我们的内部评估库。</p>
<table>
<thead>
<tr>
<th>模型</th>
<th>规模</th>
<th>Code</th>
<th>常识推理</th>
<th>世界知识</th>
<th>阅读理解</th>
<th>数学</th>
<th>MMLU</th>
<th>BBH</th>
<th>AGI Eval</th>
</tr>
</thead>
<tbody><tr>
<td>Llama 1</td>
<td>7B</td>
<td>14.1</td>
<td>60.8</td>
<td>46.2</td>
<td>58.5</td>
<td>6.95</td>
<td>35.1</td>
<td>30.3</td>
<td>23.9</td>
</tr>
<tr>
<td>Llama 1</td>
<td>13B</td>
<td>18.9</td>
<td>66.1</td>
<td>52.6</td>
<td>62.3</td>
<td>10.9</td>
<td>46.9</td>
<td>37.0</td>
<td>33.9</td>
</tr>
<tr>
<td>Llama 1</td>
<td>33B</td>
<td>26.0</td>
<td>70.0</td>
<td>58.4</td>
<td>67.6</td>
<td>21.4</td>
<td>57.8</td>
<td>39.8</td>
<td>41.7</td>
</tr>
<tr>
<td>Llama 1</td>
<td>65B</td>
<td>30.7</td>
<td>70.7</td>
<td>60.5</td>
<td>68.6</td>
<td>30.8</td>
<td>63.4</td>
<td>43.5</td>
<td>47.6</td>
</tr>
<tr>
<td>Llama 2</td>
<td>7B</td>
<td>16.8</td>
<td>63.9</td>
<td>48.9</td>
<td>61.3</td>
<td>14.6</td>
<td>45.3</td>
<td>32.6</td>
<td>29.3</td>
</tr>
<tr>
<td>Llama 2</td>
<td>13B</td>
<td>24.5</td>
<td>66.9</td>
<td>55.4</td>
<td>65.8</td>
<td>28.7</td>
<td>54.8</td>
<td>39.4</td>
<td>39.1</td>
</tr>
<tr>
<td>Llama 2</td>
<td>70B</td>
<td>37.5</td>
<td>71.9</td>
<td>63.6</td>
<td>69.4</td>
<td>35.2</td>
<td>68.9</td>
<td>51.2</td>
<td>54.2</td>
</tr>
</tbody></table>
<p>模型在 grouped academic benchmarks 上的整体表现。Code：我们报告模型在 HumanEval 和 MBPP 上的平均 pass@1 分数。常识推理：我们报告 PIQA、SIQA、HellaSwag、WinoGrande、ARC easy 和 challenge、OpenBookQA 和 CommonsenseQA 的平均分数。我们对 CommonSenseQA 进行了 7-shot 结果评估，对其他所有基准测试进行了 0-shot 结果评估。世界知识：我们在 NaturalQuestions 和 TriviaQA 上进行 5-shot 性能评估并报告平均分数。阅读理解：对于阅读理解，我们报告 SQuAD、QuAC 和 BoolQ 的 0-shot 平均分数。数学：我们报告 GSM8K（8-shot）和 MATH（4-shot）基准测试的平均分数。</p>
<h3 id="TruthfulQA-和-Toxigen"><a href="#TruthfulQA-和-Toxigen" class="headerlink" title="TruthfulQA 和 Toxigen"></a>TruthfulQA 和 Toxigen</h3><table>
<thead>
<tr>
<th>模型</th>
<th>规模</th>
<th>TruthfulQA</th>
<th>Toxigen</th>
</tr>
</thead>
<tbody><tr>
<td>Llama 1</td>
<td>7B</td>
<td>27.42</td>
<td>23.00</td>
</tr>
<tr>
<td>Llama 1</td>
<td>13B</td>
<td>41.74</td>
<td>23.08</td>
</tr>
<tr>
<td>Llama 1</td>
<td>33B</td>
<td>44.19</td>
<td>22.57</td>
</tr>
<tr>
<td>Llama 1</td>
<td>65B</td>
<td>48.71</td>
<td>21.77</td>
</tr>
<tr>
<td>Llama 2</td>
<td>7B</td>
<td>33.29</td>
<td>21.25</td>
</tr>
<tr>
<td>Llama 2</td>
<td>13B</td>
<td>41.86</td>
<td>26.10</td>
</tr>
<tr>
<td>Llama 2</td>
<td>70B</td>
<td>50.18</td>
<td>24.60</td>
</tr>
</tbody></table>
<p>预训练 LLMs 在自动安全基准测试上的评估结果。对于 TruthfulQA，我们呈现同时具有真实性和信息量的生成百分比（百分比越高越好）。对于 ToxiGen，我们呈现有害生成的百分比（百分比越小越好）。</p>
<h3 id="TruthfulQA-和-Toxigen（微调版本-LLMs）"><a href="#TruthfulQA-和-Toxigen（微调版本-LLMs）" class="headerlink" title="TruthfulQA 和 Toxigen（微调版本 LLMs）"></a>TruthfulQA 和 Toxigen（微调版本 LLMs）</h3><table>
<thead>
<tr>
<th>模型</th>
<th>规模</th>
<th>TruthfulQA</th>
<th>Toxigen</th>
</tr>
</thead>
<tbody><tr>
<td>Llama-2-Chat</td>
<td>7B</td>
<td>57.04</td>
<td>0.00</td>
</tr>
<tr>
<td>Llama-2-Chat</td>
<td>13B</td>
<td>62.18</td>
<td>0.00</td>
</tr>
<tr>
<td>Llama-2-Chat</td>
<td>70B</td>
<td>64.14</td>
<td>0.01</td>
</tr>
</tbody></table>
<p>不同安全数据集上微调 LLMs 的评估结果。度量标准定义同上。</p>
<h2 id="道德考虑和局限性"><a href="#道德考虑和局限性" class="headerlink" title="道德考虑和局限性"></a>道德考虑和局限性</h2><p>Llama 2 是一项具有风险的新技术。迄今为止的测试仅涵盖了英语，并且无法覆盖所有场景。因此，与所有 LLMs 一样，Llama 2 的潜在输出无法事先预测，并且在某些情况下可能会产生不准确、带偏见或其他不可取的响应。因此，在部署任何 Llama 2 应用程序之前，开发人员应根据其特定的模型应用进行安全测试和调整。</p>
<p>请参阅“负责任使用指南”，网址为：<a target="_blank" rel="noopener" href="https://ai.meta.com/llama/responsible-use-guide/">https://ai.meta.com/llama/responsible-use-guide/</a></p>
<h2 id="报告问题"><a href="#报告问题" class="headerlink" title="报告问题"></a>报告问题</h2><p>请通过以下方式之一报告任何软件“bug”或模型的其他问题：</p>
<ul>
<li>报告模型问题：[github.com/facebookresearch/llama](<a target="_blank" rel="noopener" href="https://github/">https://github</a></li>
</ul>
<p>.com/facebookresearch/llama)</p>
<ul>
<li>报告模型生成的有问题内容：<a target="_blank" rel="noopener" href="https://developers.facebook.com/llama_output_feedback">developers.facebook.com/llama_output_feedback</a></li>
<li>报告 bug 和安全问题：<a target="_blank" rel="noopener" href="https://facebook.com/whitehat/info">facebook.com/whitehat/info</a></li>
</ul>
</article><section class="jump-container is-flex is-justify-content-space-between my-6"><!-- em is empty placeholder--><a class="button is-default" href="/2023/08/21/NLP%20Insights/Conver%20Pytorch%20Model%20to%20ONNX%20Format/" title="Conver Pytorch Model to ONNX Format"><i class="iconfont icon-prev mr-2 has-text-grey"></i><span class="has-text-weight-semibold">Previous: Conver Pytorch Model to ONNX Format</span></a><a class="button is-default" href="/2023/07/28/NLP%20Insights/LONGNET/" title="LONGNET - Scaling Transformers to 1,000,000,000 Tokens"><span class="has-text-weight-semibold">Next: LONGNET - Scaling Transformers to 1,000,000,000 Tokens</span><i class="iconfont icon-next ml-2 has-text-grey"></i></a></section><article class="mt-6 comment-container"><script async repo="Haojen/Claudia-theme-blog" src="https://utteranc.es/client.js" issue-term="pathname" theme="preferred-color-scheme"></script></article></div></div></main></main><footer class="is-flex is-flex-direction-column is-align-items-center is-flex-shrink-0 is-family-serif"><section class="sns-container"><a title="twitter" target="_blank" rel="noopener nofollow" href="//twitter.com//"><i class="iconfont icon-twitter"></i></a><!-- Github--><a title="github" target="_blank" rel="noopener nofollow" href="//github.com/haojen"><i class="iconfont icon-github"></i></a><!-- Ins--><a title="instagram" target="_blank" rel="noopener nofollow" href="//www.instagram.com//"><i class="iconfont icon-ins"></i></a><!-- RSS--><!-- 知乎--><!-- 领英--><!-- 脸书--><a title="facebook" target="_blank" rel="noopener nofollow" href="//www.facebook.com//"><i class="iconfont icon-tian7_facebook"></i></a></section><p><span>Copyright ©</span><span> user 2026</span></p><div class="is-flex is-justify-content-center is-flex-wrap-wrap"><p>Powered by Hexo &verbar;&nbsp;</p><p class="is-flex is-justify-content-center"><a title="Hexo theme author" target="_blank" rel="noopener" href="//github.com/haojen">Theme by Haojen&nbsp;</a></p><div style="margin-top: 2px"><a class="github-button" title="github-button" target="_blank" rel="noopener" href="https://github.com/haojen/hexo-theme-Claudia" data-color-scheme="no-preference: light; light: light; dark: dark;" data-show-count="true"></a></div></div><div><span></span></div></footer><script async defer src="https://buttons.github.io/buttons.js"></script><script src="/js/jquery-3.6.1.min.js"></script><script src="/js/jquery-fancybox.min.js"></script><script src="/js/img_zoom.js"></script><script src="/js/post.js"></script></body></html>