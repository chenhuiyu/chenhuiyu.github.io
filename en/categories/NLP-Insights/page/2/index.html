<!DOCTYPE html><html class="appearance-auto" lang="en"><head><meta charset="UTF-8"><title>user's blog</title><meta name="description" content="May the Force be with you"><meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no, initial-scale=1"><!-- Google Analytics --><!-- End Google Analytics -->
<!-- Baidu Analytics --><!-- End Baidu Analytics --><link rel="icon" href="/images/favicon.ico"><link rel="stylesheet" href="/style/common/bulma.css"><link rel="stylesheet" href="/style/base.css"><link rel="stylesheet" href="/style/common/helper.css"><script src="/js/common.js"></script><link rel="stylesheet" href="/style/widget-post-list.css"><meta name="generator" content="Hexo 6.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="黑头呆鱼进化之旅" type="application/atom+xml">
</head><body class="is-flex is-flex-direction-column"><header class="header-widget is-flex-shrink-0 is-hidden-mobile"><div class="container is-fullhd is-flex is-justify-content-space-between is-align-items-center is-full-height"><section class="is-hidden-mobile is-flex-shrink-0"><h2><a href="/">user's blog</a></h2></section><h3 class="is-hidden-mobile is-family-serif is-full-height is-flex is-align-items-center is-flex-shrink-0"><span>Categories · NLP Insights</span></h3><aside class="is-flex-shrink-0"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></aside></div></header><header class="is-flex header-widget is-flex-shrink-0 is-align-items-center is-justify-content-center is-hidden-tablet"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></header><main><article class="post-container is-flex is-justify-content-center section container is-max-widescreen pt-4 px-2"><div class="columns is-variable is-1-tablet is-3-desktop-only is-2-widescreen is-full-width"><section class="column"><article class="post-item-card"><header class="is-relative is-flex"><div class="post-cover-backdrop is-hidden"><img src="https://user-images.githubusercontent.com/your-image-url.png" alt="loading.."></div><a class="post-cover-link has-text-centered skeleton" href="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1.zh-CN/"><img class="post-cover-img js-img-fadeIn" src="https://user-images.githubusercontent.com/your-image-url.png" alt="loading.." data-backdrop="true"></a></header><section class="content post-card-content p-4 pb-5"><header><a href="/tags/LLM"><i class="tag post-item-tag">LLM</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1.zh-CN/">推理 LLM 的可视化指南：探索推理时计算技术与 DeepSeek-R1</a></h2><time class="has-text-grey" datetime="2025-02-11T03:50:29.000Z">2025-02-11</time><p class="is-flex-grow-2 mt-2">推理 LLM 的可视化指南：探索推理时计算技术与 DeepSeek-R1原文地址：A Visual Guide to Reasoning LLMs
📅 作者：Maarten Grootendorst
📆 日期：2025 年 2 月 3 日

📌 引言DeepSeek-R1、OpenAI o3-mini 和 Google Gemini 2.0 Flash Thinking 是如何通过“推理”框架将 LLM（大型语言模型, Large Language Models） 扩展到新高度的典型示例。
它们标志着从 扩展训练时计算（train-time compute） 到 扩展推理时计算（test-time compute） 的范式转变。
在本篇文章中，我们提供了 超过 40 张定制可视化图表，带你深入探索：
..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1.zh-CN/">Read more</a></section></article><article class="post-item-card"><header class="is-relative is-flex"><div class="post-cover-backdrop is-hidden"><img src="https://user-images.githubusercontent.com/your-image-url.png" alt="loading.."></div><a class="post-cover-link has-text-centered skeleton" href="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1.en/"><img class="post-cover-img js-img-fadeIn" src="https://user-images.githubusercontent.com/your-image-url.png" alt="loading.." data-backdrop="true"></a></header><section class="content post-card-content p-4 pb-5"><header><a href="/tags/LLM"><i class="tag post-item-tag">LLM</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1.en/">推理 LLM 的可视化指南：探索推理时计算技术与 DeepSeek-R1</a></h2><time class="has-text-grey" datetime="2025-02-11T03:50:29.000Z">2025-02-11</time><p class="is-flex-grow-2 mt-2">推理 LLM 的可视化指南：探索推理时计算技术与 DeepSeek-R1原文地址：A Visual Guide to Reasoning LLMs
📅 作者：Maarten Grootendorst
📆 日期：2025 年 2 月 3 日

📌 引言DeepSeek-R1、OpenAI o3-mini 和 Google Gemini 2.0 Flash Thinking 是如何通过“推理”框架将 LLM（大型语言模型, Large Language Models） 扩展到新高度的典型示例。
它们标志着从 扩展训练时计算（train-time compute） 到 扩展推理时计算（test-time compute） 的范式转变。
在本篇文章中，我们提供了 超过 40 张定制可视化图表，带你深入探索：
..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1.en/">Read more</a></section></article><article class="post-item-card"><section class="content post-card-content p-4 pb-5"><header><a href="/tags/Onnx"><i class="tag post-item-tag">Onnx</i></a><a href="/tags/Deployment"><i class="tag post-item-tag">Deployment</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2024/12/06/NLP%20Insights/%E5%9F%BA%E4%BA%8E%E7%94%9F%E6%88%90%E7%9A%84%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%88LLM%EF%BC%89%E8%AF%84%E4%BC%B0%EF%BC%9A%E4%BB%8E%E7%94%9F%E6%88%90%E5%88%B0%E5%88%A4%E6%96%AD%E7%9A%84%E6%9C%BA%E9%81%87%E4%B8%8E%E6%8C%91%E6%88%98.en/">基于生成的大语言模型（LLM）评估：从生成到判断的机遇与挑战</a></h2><time class="has-text-grey" datetime="2024-12-06T06:34:18.000Z">2024-12-06</time><p class="is-flex-grow-2 mt-2">
基于生成的大语言模型（LLM）评估：从生成到判断的机遇与挑战摘要人工智能（AI）与自然语言处理（NLP）领域中的评估任务长期面临挑战。传统的评估方法（如基于匹配或嵌入的技术）在判断复杂属性时效果有限。近期大语言模型（LLM）的发展催生了“LLM-as-a-Judge”范式，利用LLM对任务进行评分、排序或选择。本论文对LLM评估方法进行了全面综述，包括其定义、分类框架、评估基准，以及未来的研究方向。

1. 引言1.1 背景评估是机器学习和NLP的核心问题之一，传统评估方法如BLEU和ROUGE通常基于文本重叠，缺乏对复杂场景的适用性。随着深度学习和LLM的发展（如GPT-4），研究者提出了“LLM-as-a-Judge”模式，以解决传统评估的局限。
1.2 研究问题本论文旨在探讨以下问题：

评估内容：..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2024/12/06/NLP%20Insights/%E5%9F%BA%E4%BA%8E%E7%94%9F%E6%88%90%E7%9A%84%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%88LLM%EF%BC%89%E8%AF%84%E4%BC%B0%EF%BC%9A%E4%BB%8E%E7%94%9F%E6%88%90%E5%88%B0%E5%88%A4%E6%96%AD%E7%9A%84%E6%9C%BA%E9%81%87%E4%B8%8E%E6%8C%91%E6%88%98.en/">Read more</a></section></article><article class="post-item-card"><section class="content post-card-content p-4 pb-5"><header><a href="/tags/Onnx"><i class="tag post-item-tag">Onnx</i></a><a href="/tags/Deployment"><i class="tag post-item-tag">Deployment</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2024/12/06/NLP%20Insights/%E5%9F%BA%E4%BA%8E%E7%94%9F%E6%88%90%E7%9A%84%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%88LLM%EF%BC%89%E8%AF%84%E4%BC%B0%EF%BC%9A%E4%BB%8E%E7%94%9F%E6%88%90%E5%88%B0%E5%88%A4%E6%96%AD%E7%9A%84%E6%9C%BA%E9%81%87%E4%B8%8E%E6%8C%91%E6%88%98.zh-CN/">基于生成的大语言模型（LLM）评估：从生成到判断的机遇与挑战</a></h2><time class="has-text-grey" datetime="2024-12-06T06:34:18.000Z">2024-12-06</time><p class="is-flex-grow-2 mt-2">
基于生成的大语言模型（LLM）评估：从生成到判断的机遇与挑战摘要人工智能（AI）与自然语言处理（NLP）领域中的评估任务长期面临挑战。传统的评估方法（如基于匹配或嵌入的技术）在判断复杂属性时效果有限。近期大语言模型（LLM）的发展催生了“LLM-as-a-Judge”范式，利用LLM对任务进行评分、排序或选择。本论文对LLM评估方法进行了全面综述，包括其定义、分类框架、评估基准，以及未来的研究方向。

1. 引言1.1 背景评估是机器学习和NLP的核心问题之一，传统评估方法如BLEU和ROUGE通常基于文本重叠，缺乏对复杂场景的适用性。随着深度学习和LLM的发展（如GPT-4），研究者提出了“LLM-as-a-Judge”模式，以解决传统评估的局限。
1.2 研究问题本论文旨在探讨以下问题：

评估内容：..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2024/12/06/NLP%20Insights/%E5%9F%BA%E4%BA%8E%E7%94%9F%E6%88%90%E7%9A%84%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%EF%BC%88LLM%EF%BC%89%E8%AF%84%E4%BC%B0%EF%BC%9A%E4%BB%8E%E7%94%9F%E6%88%90%E5%88%B0%E5%88%A4%E6%96%AD%E7%9A%84%E6%9C%BA%E9%81%87%E4%B8%8E%E6%8C%91%E6%88%98.zh-CN/">Read more</a></section></article><article class="post-item-card"><section class="content post-card-content p-4 pb-5"><header><a href="/tags/LLM"><i class="tag post-item-tag">LLM</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2024/10/23/NLP%20Insights/Introduction%20to%20LLM%20Training%20Terminology:%20LoRA,%20DPO,%20KTO,%20and%20SFT%20Technologies.en/">Detailed Explanation of LoRA, DPO, KTO, and SFT Technologies</a></h2><time class="has-text-grey" datetime="2024-10-23T08:26:29.000Z">2024-10-23</time><p class="is-flex-grow-2 mt-2">Introduction to LLM Training Terminology:LoRA, DPO, KTO, and SFT TechnologiesThis document provides a detailed introduction to several important techniques used in fine-tuning and optimizing large language models (such as LLAMA3), including SFT (Supervised Fine-Tuning), LoRA (Low-Rank Adaptation), Alignment technologies, KTO (Kahneman-Tversky Optimization)..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2024/10/23/NLP%20Insights/Introduction%20to%20LLM%20Training%20Terminology:%20LoRA,%20DPO,%20KTO,%20and%20SFT%20Technologies.en/">Read more</a></section></article><article class="post-item-card"><section class="content post-card-content p-4 pb-5"><header><a href="/tags/LLM"><i class="tag post-item-tag">LLM</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2024/10/23/NLP%20Insights/Introduction%20to%20LLM%20Training%20Terminology:%20LoRA,%20DPO,%20KTO,%20and%20SFT%20Technologies.zh-CN/">Detailed Explanation of LoRA, DPO, KTO, and SFT Technologies</a></h2><time class="has-text-grey" datetime="2024-10-23T08:26:29.000Z">2024-10-23</time><p class="is-flex-grow-2 mt-2">Introduction to LLM Training Terminology:LoRA, DPO, KTO, and SFT TechnologiesThis document provides a detailed introduction to several important techniques used in fine-tuning and optimizing large language models (such as LLAMA3), including SFT (Supervised Fine-Tuning), LoRA (Low-Rank Adaptation), Alignment technologies, KTO (Kahneman-Tversky Optimization)..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2024/10/23/NLP%20Insights/Introduction%20to%20LLM%20Training%20Terminology:%20LoRA,%20DPO,%20KTO,%20and%20SFT%20Technologies.zh-CN/">Read more</a></section></article><article class="post-item-card"><section class="content post-card-content p-4 pb-5"><header><a href="/tags/LLM"><i class="tag post-item-tag">LLM</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2024/10/23/NLP%20Insights/LLM%E8%AE%AD%E7%BB%83%E6%9C%AF%E8%AF%AD%E4%BB%8B%E7%BB%8D%EF%BC%9ASFT%E3%80%81LoRA%E3%80%81Alignment%E3%80%81KTO%E5%92%8CDPO.en/">LoRA, DPO, KTO 与 SFT 技术详解</a></h2><time class="has-text-grey" datetime="2024-10-23T08:26:29.000Z">2024-10-23</time><p class="is-flex-grow-2 mt-2">LoRA, DPO, KTO 与 SFT 技术详解本篇文档将详细介绍几种在大型语言模型（如 LLAMA3）微调和优化中的重要技术，包括 SFT（Supervised Fine-Tuning）、LoRA（Low-Rank Adaptation）、Alignment 技术、KTO（Kahneman-Tversky Optimization） 和 DPO（Direct Preference Optimization）。文中还将详细阐述每种技术的原理、具体实现方法以及相应的损失函数与优化器选择。

1. SFT（Supervised Fine-Tuning）1.1 原理SFT 是一种传统的微调方法，通过监督学习对预训练模型进行微调，调整模型的参数使其在特定任务上表现更好。SFT 通常用于针对特定的标注数据进行模型微..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2024/10/23/NLP%20Insights/LLM%E8%AE%AD%E7%BB%83%E6%9C%AF%E8%AF%AD%E4%BB%8B%E7%BB%8D%EF%BC%9ASFT%E3%80%81LoRA%E3%80%81Alignment%E3%80%81KTO%E5%92%8CDPO.en/">Read more</a></section></article><article class="post-item-card"><section class="content post-card-content p-4 pb-5"><header><a href="/tags/LLM"><i class="tag post-item-tag">LLM</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2024/10/23/NLP%20Insights/LLM%E8%AE%AD%E7%BB%83%E6%9C%AF%E8%AF%AD%E4%BB%8B%E7%BB%8D%EF%BC%9ASFT%E3%80%81LoRA%E3%80%81Alignment%E3%80%81KTO%E5%92%8CDPO.zh-CN/">LoRA, DPO, KTO 与 SFT 技术详解</a></h2><time class="has-text-grey" datetime="2024-10-23T08:26:29.000Z">2024-10-23</time><p class="is-flex-grow-2 mt-2">LoRA, DPO, KTO 与 SFT 技术详解本篇文档将详细介绍几种在大型语言模型（如 LLAMA3）微调和优化中的重要技术，包括 SFT（Supervised Fine-Tuning）、LoRA（Low-Rank Adaptation）、Alignment 技术、KTO（Kahneman-Tversky Optimization） 和 DPO（Direct Preference Optimization）。文中还将详细阐述每种技术的原理、具体实现方法以及相应的损失函数与优化器选择。

1. SFT（Supervised Fine-Tuning）1.1 原理SFT 是一种传统的微调方法，通过监督学习对预训练模型进行微调，调整模型的参数使其在特定任务上表现更好。SFT 通常用于针对特定的标注数据进行模型微..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2024/10/23/NLP%20Insights/LLM%E8%AE%AD%E7%BB%83%E6%9C%AF%E8%AF%AD%E4%BB%8B%E7%BB%8D%EF%BC%9ASFT%E3%80%81LoRA%E3%80%81Alignment%E3%80%81KTO%E5%92%8CDPO.zh-CN/">Read more</a></section></article><article class="post-item-card"><header class="is-relative is-flex"><div class="post-cover-backdrop is-hidden"><img src="https://lmsys.org/images/blog/compressed_fsm/demo.gif" alt="loading.."></div><a class="post-cover-link has-text-centered skeleton" href="/2024/08/13/NLP%20Insights/%E4%BD%BF%E7%94%A8%E5%8E%8B%E7%BC%A9%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA%E8%BF%9B%E8%A1%8C%E6%9C%AC%E5%9C%B0%20LLM%20%E7%9A%84%E5%BF%AB%E9%80%9F%20JSON%20%E8%A7%A3%E7%A0%81.en/"><img class="post-cover-img js-img-fadeIn" src="https://lmsys.org/images/blog/compressed_fsm/demo.gif" alt="loading.." data-backdrop="true"></a></header><section class="content post-card-content p-4 pb-5"><header><a href="/tags/SGLang"><i class="tag post-item-tag">SGLang</i></a><a href="/tags/Structured%20LLM"><i class="tag post-item-tag">Structured LLM</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2024/08/13/NLP%20Insights/%E4%BD%BF%E7%94%A8%E5%8E%8B%E7%BC%A9%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA%E8%BF%9B%E8%A1%8C%E6%9C%AC%E5%9C%B0%20LLM%20%E7%9A%84%E5%BF%AB%E9%80%9F%20JSON%20%E8%A7%A3%E7%A0%81.en/">使用压缩有限状态机进行本地 LLM 的快速 JSON 解码</a></h2><time class="has-text-grey" datetime="2024-08-13T08:12:10.000Z">2024-08-13</time><p class="is-flex-grow-2 mt-2">使用压缩有限状态机进行本地 LLM 的快速 JSON 解码作者: Liangsheng Yin, Ying Sheng, Lianmin Zheng日期: 2024 年 2 月 5 日

本文内容基于 LMSYS Org 发布的一篇博客文章，原文链接：LMSYS Org 博客。相关的代码库可以在以下链接找到：SGLang 代码库。
让一个 LLM 始终生成符合特定模式的有效 JSON 或 YAML，对于许多应用来说是一个关键特性。在这篇博客文章中，我们介绍了一种显著加速这种约束解码的优化方法。我们的方法利用了压缩的有限状态机，并且兼容任何正则表达式，因此可以适用于任何 JSON 或 YAML 模式。与现有系统逐步解码一个标记的方式不同，我们的方法分析了正则表达式的有限状态机，压缩了单一的转换路径，并在可能的..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2024/08/13/NLP%20Insights/%E4%BD%BF%E7%94%A8%E5%8E%8B%E7%BC%A9%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA%E8%BF%9B%E8%A1%8C%E6%9C%AC%E5%9C%B0%20LLM%20%E7%9A%84%E5%BF%AB%E9%80%9F%20JSON%20%E8%A7%A3%E7%A0%81.en/">Read more</a></section></article><article class="post-item-card"><header class="is-relative is-flex"><div class="post-cover-backdrop is-hidden"><img src="https://lmsys.org/images/blog/compressed_fsm/demo.gif" alt="loading.."></div><a class="post-cover-link has-text-centered skeleton" href="/2024/08/13/NLP%20Insights/%E4%BD%BF%E7%94%A8%E5%8E%8B%E7%BC%A9%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA%E8%BF%9B%E8%A1%8C%E6%9C%AC%E5%9C%B0%20LLM%20%E7%9A%84%E5%BF%AB%E9%80%9F%20JSON%20%E8%A7%A3%E7%A0%81.zh-CN/"><img class="post-cover-img js-img-fadeIn" src="https://lmsys.org/images/blog/compressed_fsm/demo.gif" alt="loading.." data-backdrop="true"></a></header><section class="content post-card-content p-4 pb-5"><header><a href="/tags/SGLang"><i class="tag post-item-tag">SGLang</i></a><a href="/tags/Structured%20LLM"><i class="tag post-item-tag">Structured LLM</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2024/08/13/NLP%20Insights/%E4%BD%BF%E7%94%A8%E5%8E%8B%E7%BC%A9%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA%E8%BF%9B%E8%A1%8C%E6%9C%AC%E5%9C%B0%20LLM%20%E7%9A%84%E5%BF%AB%E9%80%9F%20JSON%20%E8%A7%A3%E7%A0%81.zh-CN/">使用压缩有限状态机进行本地 LLM 的快速 JSON 解码</a></h2><time class="has-text-grey" datetime="2024-08-13T08:12:10.000Z">2024-08-13</time><p class="is-flex-grow-2 mt-2">使用压缩有限状态机进行本地 LLM 的快速 JSON 解码作者: Liangsheng Yin, Ying Sheng, Lianmin Zheng日期: 2024 年 2 月 5 日

本文内容基于 LMSYS Org 发布的一篇博客文章，原文链接：LMSYS Org 博客。相关的代码库可以在以下链接找到：SGLang 代码库。
让一个 LLM 始终生成符合特定模式的有效 JSON 或 YAML，对于许多应用来说是一个关键特性。在这篇博客文章中，我们介绍了一种显著加速这种约束解码的优化方法。我们的方法利用了压缩的有限状态机，并且兼容任何正则表达式，因此可以适用于任何 JSON 或 YAML 模式。与现有系统逐步解码一个标记的方式不同，我们的方法分析了正则表达式的有限状态机，压缩了单一的转换路径，并在可能的..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2024/08/13/NLP%20Insights/%E4%BD%BF%E7%94%A8%E5%8E%8B%E7%BC%A9%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA%E8%BF%9B%E8%A1%8C%E6%9C%AC%E5%9C%B0%20LLM%20%E7%9A%84%E5%BF%AB%E9%80%9F%20JSON%20%E8%A7%A3%E7%A0%81.zh-CN/">Read more</a></section></article><section class="paginator is-flex is-justify-content-flex-end is-flex-wrap-wrap mt-5"><a class="extend prev" rel="prev" href="/en/categories/NLP-Insights/"><i class="iconfont icon-prev has-text-grey"></i></a><a class="page-number" href="/en/categories/NLP-Insights/">1</a><span class="page-number current">2</span><a class="page-number" href="/en/categories/NLP-Insights/page/3/">3</a><a class="page-number" href="/en/categories/NLP-Insights/page/4/">4</a><a class="page-number" href="/en/categories/NLP-Insights/page/5/">5</a><a class="extend next" rel="next" href="/en/categories/NLP-Insights/page/3/"><i class="iconfont icon-next has-text-grey"></i></a></section></section><aside class="column is-hidden-mobile is-4-tablet is-3-widescreen"><div style="position: sticky; top: 50px;"><main class="aside-card-container categories-widget category-page"><h3>Categories</h3><section><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Code-Chronicles/">Code Chronicles</a><span class="category-list-count">34</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Debugging-Diaries/">Debugging Diaries</a><span class="category-list-count">10</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Life-Reflections/">Life Reflections</a><span class="category-list-count">19</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP-Insights/">NLP Insights</a><span class="category-list-count">50</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tech-Toolbox/">Tech Toolbox</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Wanderlust-Adventures/">Wanderlust Adventures</a><span class="category-list-count">2</span></li></ul></section></main></div></aside></div></article><script>$claudia.fadeInImage(null, $claudia.blurBackdropImg)

window.addEventListener('resize', $claudia.throttle(function () {
    var images = document.querySelectorAll('.js-img-fadeIn')

    images.forEach($claudia.blurBackdropImg)
}, 150))</script></main><footer class="is-flex is-flex-direction-column is-align-items-center is-flex-shrink-0 is-family-serif"><section class="sns-container"><a title="twitter" target="_blank" rel="noopener nofollow" href="//twitter.com//"><i class="iconfont icon-twitter"></i></a><!-- Github--><a title="github" target="_blank" rel="noopener nofollow" href="//github.com/haojen"><i class="iconfont icon-github"></i></a><!-- Ins--><a title="instagram" target="_blank" rel="noopener nofollow" href="//www.instagram.com//"><i class="iconfont icon-ins"></i></a><!-- RSS--><!-- 知乎--><!-- 领英--><!-- 脸书--><a title="facebook" target="_blank" rel="noopener nofollow" href="//www.facebook.com//"><i class="iconfont icon-tian7_facebook"></i></a></section><p><span>Copyright ©</span><span> user 2026</span></p><div class="is-flex is-justify-content-center is-flex-wrap-wrap"><p>Powered by Hexo &verbar;&nbsp;</p><p class="is-flex is-justify-content-center"><a title="Hexo theme author" target="_blank" rel="noopener" href="//github.com/haojen">Theme by Haojen&nbsp;</a></p><div style="margin-top: 2px"><a class="github-button" title="github-button" target="_blank" rel="noopener" href="https://github.com/haojen/hexo-theme-Claudia" data-color-scheme="no-preference: light; light: light; dark: dark;" data-show-count="true"></a></div></div><div><span></span></div></footer><script src="/js/lang-switch.js"></script><script async defer src="https://buttons.github.io/buttons.js"></script><script>$claudia.fadeInImage(null, $claudia.blurBackdropImg)

window.addEventListener('resize', $claudia.throttle(function () {
    var images = document.querySelectorAll('.js-img-fadeIn')

    images.forEach($claudia.blurBackdropImg)
}, 150))</script></body></html>