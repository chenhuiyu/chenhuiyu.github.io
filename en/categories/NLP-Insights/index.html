<!DOCTYPE html><html class="appearance-auto" lang="en"><head><meta charset="UTF-8"><title>user's blog</title><meta name="description" content="May the Force be with you"><meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no, initial-scale=1"><!-- Google Analytics --><!-- End Google Analytics -->
<!-- Baidu Analytics --><!-- End Baidu Analytics --><link rel="icon" href="/images/favicon.ico"><link rel="stylesheet" href="/style/common/bulma.css"><link rel="stylesheet" href="/style/base.css"><link rel="stylesheet" href="/style/common/helper.css"><script src="/js/common.js"></script><link rel="stylesheet" href="/style/widget-post-list.css"><meta name="generator" content="Hexo 6.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="é»‘å¤´å‘†é±¼è¿›åŒ–ä¹‹æ—…" type="application/atom+xml">
</head><body class="is-flex is-flex-direction-column"><header class="header-widget is-flex-shrink-0 is-hidden-mobile"><div class="container is-fullhd is-flex is-justify-content-space-between is-align-items-center is-full-height"><section class="is-hidden-mobile is-flex-shrink-0"><h2><a href="/">user's blog</a></h2></section><h3 class="is-hidden-mobile is-family-serif is-full-height is-flex is-align-items-center is-flex-shrink-0"><span>Categories Â· NLP Insights</span></h3><aside class="is-flex-shrink-0"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></aside></div></header><header class="is-flex header-widget is-flex-shrink-0 is-align-items-center is-justify-content-center is-hidden-tablet"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></header><main><article class="post-container is-flex is-justify-content-center section container is-max-widescreen pt-4 px-2"><div class="columns is-variable is-1-tablet is-3-desktop-only is-2-widescreen is-full-width"><section class="column"><article class="post-item-card"><section class="content post-card-content p-4 pb-5"><header><a href="/tags/Conversational%20AI"><i class="tag post-item-tag">Conversational AI</i></a><a href="/tags/Memory%20Management"><i class="tag post-item-tag">Memory Management</i></a><a href="/tags/SeCom"><i class="tag post-item-tag">SeCom</i></a><a href="/tags/RAG"><i class="tag post-item-tag">RAG</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2025/06/24/NLP%20Insights/SeCom%20Redefining%20Memory%20Management%20in%20Conversational%20AI.en/">SeCom: Redefining Memory Management in Conversational AI</a></h2><time class="has-text-grey" datetime="2025-06-24T08:00:00.000Z">2025-06-24</time><p class="is-flex-grow-2 mt-2">SeCom: Redefining Memory Management in Conversational AIForewordIâ€™ve recently been diving into memory management for dialog-based AI, especially how to construct and retrieve memories in long-term conversations. During my exploration I came across an eye-opening ICLR 2025 paperâ€”**â€SeCom: On Memory Construction and Retrieval for Personalized Conversational ..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2025/06/24/NLP%20Insights/SeCom%20Redefining%20Memory%20Management%20in%20Conversational%20AI.en/">Read more</a></section></article><article class="post-item-card"><section class="content post-card-content p-4 pb-5"><header><a href="/tags/Conversational%20AI"><i class="tag post-item-tag">Conversational AI</i></a><a href="/tags/Memory%20Management"><i class="tag post-item-tag">Memory Management</i></a><a href="/tags/SeCom"><i class="tag post-item-tag">SeCom</i></a><a href="/tags/RAG"><i class="tag post-item-tag">RAG</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2025/06/24/NLP%20Insights/SeCom%20Redefining%20Memory%20Management%20in%20Conversational%20AI.zh-CN/">SeCom: Redefining Memory Management in Conversational AI</a></h2><time class="has-text-grey" datetime="2025-06-24T08:00:00.000Z">2025-06-24</time><p class="is-flex-grow-2 mt-2">SeCom: Redefining Memory Management in Conversational AIForewordIâ€™ve recently been diving into memory management for dialog-based AI, especially how to construct and retrieve memories in long-term conversations. During my exploration I came across an eye-opening ICLR 2025 paperâ€”**â€SeCom: On Memory Construction and Retrieval for Personalized Conversational ..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2025/06/24/NLP%20Insights/SeCom%20Redefining%20Memory%20Management%20in%20Conversational%20AI.zh-CN/">Read more</a></section></article><article class="post-item-card"><section class="content post-card-content p-4 pb-5"><header><a href="/tags/Conversational%20AI"><i class="tag post-item-tag">Conversational AI</i></a><a href="/tags/Memory%20Management"><i class="tag post-item-tag">Memory Management</i></a><a href="/tags/SeCom"><i class="tag post-item-tag">SeCom</i></a><a href="/tags/RAG"><i class="tag post-item-tag">RAG</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2025/06/24/NLP%20Insights/SeCom:%20%E9%87%8D%E6%96%B0%E5%AE%9A%E4%B9%89%E5%AF%B9%E8%AF%9DAI%E7%9A%84%E8%AE%B0%E5%BF%86%E7%AE%A1%E7%90%86.en/">SeCom: é‡æ–°å®šä¹‰å¯¹è¯AIçš„è®°å¿†ç®¡ç†</a></h2><time class="has-text-grey" datetime="2025-06-24T08:00:00.000Z">2025-06-24</time><p class="is-flex-grow-2 mt-2">SeCom: é‡æ–°å®šä¹‰å¯¹è¯AIçš„è®°å¿†ç®¡ç†å†™åœ¨å‰é¢æœ€è¿‘ç¬”è€…ä¸€ç›´åœ¨ç ”ç©¶å¯¹è¯AIä¸­çš„å†…å­˜ç®¡ç†é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯é•¿æœŸå¯¹è¯åœºæ™¯ä¸‹çš„è®°å¿†æ„å»ºä¸æ£€ç´¢æŠ€æœ¯ã€‚å‘ç°äº†ä¸€ç¯‡ä»¤äººçœ¼å‰ä¸€äº®çš„ICLR 2025è®ºæ–‡â€”â€”ã€ŠSeCom: On Memory Construction and Retrieval for Personalized Conversational Agentsã€‹ï¼Œç”±Microsoftå’Œæ¸…åå¤§å­¦çš„ç ”ç©¶å›¢é˜Ÿè”åˆå‘è¡¨ã€‚
è¿™ç¯‡è®ºæ–‡æå‡ºçš„SeComæ–¹æ³•å·§å¦™åœ°è§£å†³äº†ä¸€ä¸ªæ ¸å¿ƒé—®é¢˜ï¼šå¦‚ä½•åœ¨é•¿æœŸå¯¹è¯ä¸­æœ‰æ•ˆç®¡ç†å’Œæ£€ç´¢å†å²ä¿¡æ¯ï¼Ÿä»Šå¤©æƒ³å’Œå¤§å®¶åˆ†äº«ä¸€ä¸‹è¿™ä¸ªæ–¹æ³•çš„æŠ€æœ¯ç»†èŠ‚å’Œåˆ›æ–°ç‚¹ï¼Œå¸Œæœ›èƒ½ä¸ºä»äº‹ç›¸å…³ç ”ç©¶çš„æœ‹å‹ä»¬æä¾›ä¸€äº›å¯å‘ã€‚
1. ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦å…³æ³¨å¯¹è¯å†…å­˜ç®¡ç†ï¼Ÿ1.1 é•¿æœŸå¯¹è¯çš„ç°å®æŒ‘æˆ˜åœ¨ä¸LLMsçš„æ—¥å¸¸äº¤äº’ä¸­ï¼Œç›¸ä¿¡å¤§å®¶éƒ½é‡åˆ°è¿‡è¿™æ ·çš„å›°æ‰°ï¼šå½“å¯¹è¯..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2025/06/24/NLP%20Insights/SeCom:%20%E9%87%8D%E6%96%B0%E5%AE%9A%E4%B9%89%E5%AF%B9%E8%AF%9DAI%E7%9A%84%E8%AE%B0%E5%BF%86%E7%AE%A1%E7%90%86.en/">Read more</a></section></article><article class="post-item-card"><section class="content post-card-content p-4 pb-5"><header><a href="/tags/Conversational%20AI"><i class="tag post-item-tag">Conversational AI</i></a><a href="/tags/Memory%20Management"><i class="tag post-item-tag">Memory Management</i></a><a href="/tags/SeCom"><i class="tag post-item-tag">SeCom</i></a><a href="/tags/RAG"><i class="tag post-item-tag">RAG</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2025/06/24/NLP%20Insights/SeCom:%20%E9%87%8D%E6%96%B0%E5%AE%9A%E4%B9%89%E5%AF%B9%E8%AF%9DAI%E7%9A%84%E8%AE%B0%E5%BF%86%E7%AE%A1%E7%90%86.zh-CN/">SeCom: é‡æ–°å®šä¹‰å¯¹è¯AIçš„è®°å¿†ç®¡ç†</a></h2><time class="has-text-grey" datetime="2025-06-24T08:00:00.000Z">2025-06-24</time><p class="is-flex-grow-2 mt-2">SeCom: é‡æ–°å®šä¹‰å¯¹è¯AIçš„è®°å¿†ç®¡ç†å†™åœ¨å‰é¢æœ€è¿‘ç¬”è€…ä¸€ç›´åœ¨ç ”ç©¶å¯¹è¯AIä¸­çš„å†…å­˜ç®¡ç†é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯é•¿æœŸå¯¹è¯åœºæ™¯ä¸‹çš„è®°å¿†æ„å»ºä¸æ£€ç´¢æŠ€æœ¯ã€‚å‘ç°äº†ä¸€ç¯‡ä»¤äººçœ¼å‰ä¸€äº®çš„ICLR 2025è®ºæ–‡â€”â€”ã€ŠSeCom: On Memory Construction and Retrieval for Personalized Conversational Agentsã€‹ï¼Œç”±Microsoftå’Œæ¸…åå¤§å­¦çš„ç ”ç©¶å›¢é˜Ÿè”åˆå‘è¡¨ã€‚
è¿™ç¯‡è®ºæ–‡æå‡ºçš„SeComæ–¹æ³•å·§å¦™åœ°è§£å†³äº†ä¸€ä¸ªæ ¸å¿ƒé—®é¢˜ï¼šå¦‚ä½•åœ¨é•¿æœŸå¯¹è¯ä¸­æœ‰æ•ˆç®¡ç†å’Œæ£€ç´¢å†å²ä¿¡æ¯ï¼Ÿä»Šå¤©æƒ³å’Œå¤§å®¶åˆ†äº«ä¸€ä¸‹è¿™ä¸ªæ–¹æ³•çš„æŠ€æœ¯ç»†èŠ‚å’Œåˆ›æ–°ç‚¹ï¼Œå¸Œæœ›èƒ½ä¸ºä»äº‹ç›¸å…³ç ”ç©¶çš„æœ‹å‹ä»¬æä¾›ä¸€äº›å¯å‘ã€‚
1. ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦å…³æ³¨å¯¹è¯å†…å­˜ç®¡ç†ï¼Ÿ1.1 é•¿æœŸå¯¹è¯çš„ç°å®æŒ‘æˆ˜åœ¨ä¸LLMsçš„æ—¥å¸¸äº¤äº’ä¸­ï¼Œç›¸ä¿¡å¤§å®¶éƒ½é‡åˆ°è¿‡è¿™æ ·çš„å›°æ‰°ï¼šå½“å¯¹è¯..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2025/06/24/NLP%20Insights/SeCom:%20%E9%87%8D%E6%96%B0%E5%AE%9A%E4%B9%89%E5%AF%B9%E8%AF%9DAI%E7%9A%84%E8%AE%B0%E5%BF%86%E7%AE%A1%E7%90%86.zh-CN/">Read more</a></section></article><article class="post-item-card"><section class="content post-card-content p-4 pb-5"><header><a href="/tags/LLM"><i class="tag post-item-tag">LLM</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2025/03/06/NLP%20Insights/Decoder%E6%A8%A1%E5%9E%8B%E5%92%8CEncoder%E6%A8%A1%E5%9E%8B%E5%9C%A8Padding%E4%B8%AD%E7%9A%84%E4%B8%8D%E5%90%8C.zh-CN/">Decoder-onlyä¸Encoder-onlyæ¨¡å‹Paddingç­–ç•¥çš„å·®å¼‚</a></h2><time class="has-text-grey" datetime="2025-03-06T09:43:10.000Z">2025-03-06</time><p class="is-flex-grow-2 mt-2">ğŸ“Œ Padding çš„å«ä¹‰åœ¨å¤§æ¨¡å‹ (LLM) ä¸­ï¼Œpadding æ˜¯ç”¨äºå°†ä¸åŒé•¿åº¦çš„åºåˆ—è°ƒæ•´ä¸ºåŒä¸€é•¿åº¦çš„æ–¹æ³•ï¼Œä»¥ä¾¿äºæ‰¹é‡ (batch) å¤„ç†ã€‚
ä¾‹å¦‚ï¼š
12å¥å­1: &quot;I love NLP&quot;å¥å­2: &quot;Padding is useful in LLM training&quot;

ä½¿ç”¨ &amp;lt;pad&amp;gt; token è¿›è¡Œå¯¹é½ï¼š
12&quot;I love NLP &amp;lt;pad&amp;gt; &amp;lt;pad&amp;gt; &amp;lt;pad&amp;gt;&quot;&quot;Padding is useful in LLM training&quot;


ğŸ“Œ Padding ä½ç½®çš„é€‰æ‹©ï¼šLeft vs RightPadding æœ‰ä¸¤ç§å¸¸è§æ–¹å¼ï¼š

Right paddingï¼ˆå³å¡«å……ï¼‰ï¼š
1&quot;I love NLP &amp;lt;pad&amp;gt; &amp;lt;pad&amp;gt..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2025/03/06/NLP%20Insights/Decoder%E6%A8%A1%E5%9E%8B%E5%92%8CEncoder%E6%A8%A1%E5%9E%8B%E5%9C%A8Padding%E4%B8%AD%E7%9A%84%E4%B8%8D%E5%90%8C.zh-CN/">Read more</a></section></article><article class="post-item-card"><section class="content post-card-content p-4 pb-5"><header><a href="/tags/LLM"><i class="tag post-item-tag">LLM</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2025/03/06/NLP%20Insights/Differences%20in%20Padding%20Strategies%20Between%20Decoder-only%20and%20Encoder-only%20Models.en/">Differences in Padding Strategies Between Decoder-only and Encoder-only Models</a></h2><time class="has-text-grey" datetime="2025-03-06T09:43:10.000Z">2025-03-06</time><p class="is-flex-grow-2 mt-2">ğŸ“Œ What is Padding?In Large Language Models (LLMs), padding is a method used to standardize sequence lengths for batch processing.
For example:
12Sentence 1: &quot;I love NLP&quot;Sentence 2: &quot;Padding is useful in LLM training&quot;

Using the &amp;lt;pad&amp;gt; token for alignment:
12&quot;I love NLP &amp;lt;pad&amp;gt; &amp;lt;pad&amp;gt; &amp;lt;pad&amp;gt;&quot;&quot;Padding is useful in LLM training&quot;


ğŸ“Œ Paddi..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2025/03/06/NLP%20Insights/Differences%20in%20Padding%20Strategies%20Between%20Decoder-only%20and%20Encoder-only%20Models.en/">Read more</a></section></article><article class="post-item-card"><section class="content post-card-content p-4 pb-5"><header><a href="/tags/LLM"><i class="tag post-item-tag">LLM</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2025/03/06/NLP%20Insights/Decoder%E6%A8%A1%E5%9E%8B%E5%92%8CEncoder%E6%A8%A1%E5%9E%8B%E5%9C%A8Padding%E4%B8%AD%E7%9A%84%E4%B8%8D%E5%90%8C.en/">Decoder-onlyä¸Encoder-onlyæ¨¡å‹Paddingç­–ç•¥çš„å·®å¼‚</a></h2><time class="has-text-grey" datetime="2025-03-06T09:43:10.000Z">2025-03-06</time><p class="is-flex-grow-2 mt-2">ğŸ“Œ Padding çš„å«ä¹‰åœ¨å¤§æ¨¡å‹ (LLM) ä¸­ï¼Œpadding æ˜¯ç”¨äºå°†ä¸åŒé•¿åº¦çš„åºåˆ—è°ƒæ•´ä¸ºåŒä¸€é•¿åº¦çš„æ–¹æ³•ï¼Œä»¥ä¾¿äºæ‰¹é‡ (batch) å¤„ç†ã€‚
ä¾‹å¦‚ï¼š
12å¥å­1: &quot;I love NLP&quot;å¥å­2: &quot;Padding is useful in LLM training&quot;

ä½¿ç”¨ &amp;lt;pad&amp;gt; token è¿›è¡Œå¯¹é½ï¼š
12&quot;I love NLP &amp;lt;pad&amp;gt; &amp;lt;pad&amp;gt; &amp;lt;pad&amp;gt;&quot;&quot;Padding is useful in LLM training&quot;


ğŸ“Œ Padding ä½ç½®çš„é€‰æ‹©ï¼šLeft vs RightPadding æœ‰ä¸¤ç§å¸¸è§æ–¹å¼ï¼š

Right paddingï¼ˆå³å¡«å……ï¼‰ï¼š
1&quot;I love NLP &amp;lt;pad&amp;gt; &amp;lt;pad&amp;gt..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2025/03/06/NLP%20Insights/Decoder%E6%A8%A1%E5%9E%8B%E5%92%8CEncoder%E6%A8%A1%E5%9E%8B%E5%9C%A8Padding%E4%B8%AD%E7%9A%84%E4%B8%8D%E5%90%8C.en/">Read more</a></section></article><article class="post-item-card"><section class="content post-card-content p-4 pb-5"><header><a href="/tags/LLM"><i class="tag post-item-tag">LLM</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2025/03/06/NLP%20Insights/Differences%20in%20Padding%20Strategies%20Between%20Decoder-only%20and%20Encoder-only%20Models.zh-CN/">Differences in Padding Strategies Between Decoder-only and Encoder-only Models</a></h2><time class="has-text-grey" datetime="2025-03-06T09:43:10.000Z">2025-03-06</time><p class="is-flex-grow-2 mt-2">ğŸ“Œ What is Padding?In Large Language Models (LLMs), padding is a method used to standardize sequence lengths for batch processing.
For example:
12Sentence 1: &quot;I love NLP&quot;Sentence 2: &quot;Padding is useful in LLM training&quot;

Using the &amp;lt;pad&amp;gt; token for alignment:
12&quot;I love NLP &amp;lt;pad&amp;gt; &amp;lt;pad&amp;gt; &amp;lt;pad&amp;gt;&quot;&quot;Padding is useful in LLM training&quot;


ğŸ“Œ Paddi..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2025/03/06/NLP%20Insights/Differences%20in%20Padding%20Strategies%20Between%20Decoder-only%20and%20Encoder-only%20Models.zh-CN/">Read more</a></section></article><article class="post-item-card"><section class="content post-card-content p-4 pb-5"><header><a href="/tags/LLM"><i class="tag post-item-tag">LLM</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2.en/">MoEæ¨¡å‹çš„çš„å¯è§†åŒ–æŒ‡å—ï¼šæ­ç§˜ MoE åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„è§’è‰²</a></h2><time class="has-text-grey" datetime="2025-02-11T03:50:29.000Z">2025-02-11</time><p class="is-flex-grow-2 mt-2">MoE æ¨¡å‹çš„çš„å¯è§†åŒ–æŒ‡å—ï¼šæ­ç§˜ MoE åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„è§’è‰²åŸæ–‡åœ°å€ï¼šA Visual Guide to Mixture of Experts (MoE)
ğŸ“… ä½œè€…ï¼šMaarten Grootendorst
ğŸ“† æ—¥æœŸï¼š2024 å¹´ 10 æœˆ 7 æ—¥

æ¢ç´¢è¯­è¨€æ¨¡å‹ï¼šæ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMoEï¼‰å¯è§†åŒ–æŒ‡å—ç›®å½•
MoE æ¨¡å‹çš„çš„å¯è§†åŒ–æŒ‡å—ï¼šæ­ç§˜ MoE åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„è§’è‰²
æ¢ç´¢è¯­è¨€æ¨¡å‹ï¼šæ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMoEï¼‰å¯è§†åŒ–æŒ‡å—
ç›®å½•
ä»€ä¹ˆæ˜¯æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¨¡å‹ï¼Ÿ
Experts
Dense Layers
Sparse Layers
What does an Expert Learn?
ä¸“å®¶çš„æ¶æ„ï¼ˆArchitecture of Expertsï¼‰





å½“æˆ‘ä»¬æŸ¥çœ‹æœ€æ–°å‘å¸ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼ŒLarge ..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2.en/">Read more</a></section></article><article class="post-item-card"><section class="content post-card-content p-4 pb-5"><header><a href="/tags/LLM"><i class="tag post-item-tag">LLM</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2.zh-CN/">MoEæ¨¡å‹çš„çš„å¯è§†åŒ–æŒ‡å—ï¼šæ­ç§˜ MoE åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„è§’è‰²</a></h2><time class="has-text-grey" datetime="2025-02-11T03:50:29.000Z">2025-02-11</time><p class="is-flex-grow-2 mt-2">MoE æ¨¡å‹çš„çš„å¯è§†åŒ–æŒ‡å—ï¼šæ­ç§˜ MoE åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„è§’è‰²åŸæ–‡åœ°å€ï¼šA Visual Guide to Mixture of Experts (MoE)
ğŸ“… ä½œè€…ï¼šMaarten Grootendorst
ğŸ“† æ—¥æœŸï¼š2024 å¹´ 10 æœˆ 7 æ—¥

æ¢ç´¢è¯­è¨€æ¨¡å‹ï¼šæ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMoEï¼‰å¯è§†åŒ–æŒ‡å—ç›®å½•
MoE æ¨¡å‹çš„çš„å¯è§†åŒ–æŒ‡å—ï¼šæ­ç§˜ MoE åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„è§’è‰²
æ¢ç´¢è¯­è¨€æ¨¡å‹ï¼šæ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMoEï¼‰å¯è§†åŒ–æŒ‡å—
ç›®å½•
ä»€ä¹ˆæ˜¯æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¨¡å‹ï¼Ÿ
Experts
Dense Layers
Sparse Layers
What does an Expert Learn?
ä¸“å®¶çš„æ¶æ„ï¼ˆArchitecture of Expertsï¼‰





å½“æˆ‘ä»¬æŸ¥çœ‹æœ€æ–°å‘å¸ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼ŒLarge ..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2.zh-CN/">Read more</a></section></article><section class="paginator is-flex is-justify-content-flex-end is-flex-wrap-wrap mt-5"><span class="page-number current">1</span><a class="page-number" href="/en/categories/NLP-Insights/page/2/">2</a><a class="page-number" href="/en/categories/NLP-Insights/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/en/categories/NLP-Insights/page/5/">5</a><a class="extend next" rel="next" href="/en/categories/NLP-Insights/page/2/"><i class="iconfont icon-next has-text-grey"></i></a></section></section><aside class="column is-hidden-mobile is-4-tablet is-3-widescreen"><div style="position: sticky; top: 50px;"><main class="aside-card-container categories-widget category-page"><h3>Categories</h3><section><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Code-Chronicles/">Code Chronicles</a><span class="category-list-count">34</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Debugging-Diaries/">Debugging Diaries</a><span class="category-list-count">10</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Life-Reflections/">Life Reflections</a><span class="category-list-count">19</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/NLP-Insights/">NLP Insights</a><span class="category-list-count">50</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Tech-Toolbox/">Tech Toolbox</a><span class="category-list-count">8</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/Wanderlust-Adventures/">Wanderlust Adventures</a><span class="category-list-count">2</span></li></ul></section></main></div></aside></div></article><script>$claudia.fadeInImage(null, $claudia.blurBackdropImg)

window.addEventListener('resize', $claudia.throttle(function () {
    var images = document.querySelectorAll('.js-img-fadeIn')

    images.forEach($claudia.blurBackdropImg)
}, 150))</script></main><footer class="is-flex is-flex-direction-column is-align-items-center is-flex-shrink-0 is-family-serif"><section class="sns-container"><a title="twitter" target="_blank" rel="noopener nofollow" href="//twitter.com//"><i class="iconfont icon-twitter"></i></a><!-- Github--><a title="github" target="_blank" rel="noopener nofollow" href="//github.com/haojen"><i class="iconfont icon-github"></i></a><!-- Ins--><a title="instagram" target="_blank" rel="noopener nofollow" href="//www.instagram.com//"><i class="iconfont icon-ins"></i></a><!-- RSS--><!-- çŸ¥ä¹--><!-- é¢†è‹±--><!-- è„¸ä¹¦--><a title="facebook" target="_blank" rel="noopener nofollow" href="//www.facebook.com//"><i class="iconfont icon-tian7_facebook"></i></a></section><p><span>Copyright Â©</span><span> user 2026</span></p><div class="is-flex is-justify-content-center is-flex-wrap-wrap"><p>Powered by Hexo &verbar;&nbsp;</p><p class="is-flex is-justify-content-center"><a title="Hexo theme author" target="_blank" rel="noopener" href="//github.com/haojen">Theme by Haojen&nbsp;</a></p><div style="margin-top: 2px"><a class="github-button" title="github-button" target="_blank" rel="noopener" href="https://github.com/haojen/hexo-theme-Claudia" data-color-scheme="no-preference: light; light: light; dark: dark;" data-show-count="true"></a></div></div><div><span></span></div></footer><script src="/js/lang-switch.js"></script><script async defer src="https://buttons.github.io/buttons.js"></script><script>$claudia.fadeInImage(null, $claudia.blurBackdropImg)

window.addEventListener('resize', $claudia.throttle(function () {
    var images = document.querySelectorAll('.js-img-fadeIn')

    images.forEach($claudia.blurBackdropImg)
}, 150))</script></body></html>