<!DOCTYPE html><html class="appearance-auto" lang="en"><head><meta charset="UTF-8"><title>Differences in Padding Strategies Between Decoder-only and Encoder-only Models</title><meta name="description" content="May the Force be with you"><meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no, initial-scale=1"><!-- Google Analytics --><!-- End Google Analytics -->
<!-- Baidu Analytics --><!-- End Baidu Analytics --><link rel="icon" href="/images/favicon.ico"><link rel="stylesheet" href="/style/common/bulma.css"><link rel="stylesheet" href="/style/base.css"><link rel="stylesheet" href="/style/common/helper.css"><script src="/js/common.js"></script><link rel="stylesheet" href="/style/post.css"><link rel="stylesheet" href="/style/themes/highlight-theme-light.css"><link rel="stylesheet" href="/style/common/jquery.fancybox.min.css"><script src="/js/highlight.pack.js"></script><meta name="description" content="ğŸ“Œ What is Padding?In Large Language Models (LLMs), padding is a method used to standardize sequence lengths for batch processing.
For example:
12Sentence 1: &quot;I love NLP&quot;Sentence 2: &quot;Padding is useful in LLM training&quot;

Using the &amp;lt;pad&amp;gt; token for alignment:
12&quot;I love NLP &amp;lt;pad&amp;gt; &amp;lt;pad&amp;gt; &amp;lt;pad&amp;gt;&quot;&quot;Padding is useful in LLM training&quot;


ğŸ“Œ Paddi.."><meta name="generator" content="Hexo 6.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="é»‘å¤´å‘†é±¼è¿›åŒ–ä¹‹æ—…" type="application/atom+xml">
</head><body class="is-flex is-flex-direction-column"><header class="header-widget is-flex-shrink-0 is-hidden-mobile"><div class="container is-fullhd is-flex is-justify-content-space-between is-align-items-center is-full-height"><section class="is-hidden-mobile is-flex-shrink-0"><h2><a href="/">user's blog</a></h2></section><h3 class="is-hidden-mobile is-family-serif is-full-height is-flex is-align-items-center is-flex-shrink-0"><div class="is-full-height" id="postTopic"><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">Differences in Padding Strategies Between Decoder-only and Enco..</p><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">Click back to the top</p></div></h3><aside class="is-flex-shrink-0"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></aside></div></header><header class="is-flex header-widget is-flex-shrink-0 is-align-items-center is-justify-content-center is-hidden-tablet"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></header><main><main class="container is-max-widescreen content section post-page pt-4 px-4"><div class="columns is-flex-desktop is-justify-content-center is-flex-direction-row-reverse"><div class="column is-3 is-hidden-mobile"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%93%8C-What-is-Padding"><span class="toc-text">ğŸ“Œ What is Padding?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%93%8C-Padding-Positioning-Left-vs-Right"><span class="toc-text">ğŸ“Œ Padding Positioning: Left vs Right</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%93%8C-Why-Do-Encoder-only-Models-e-g-BERT-Use-Right-Padding"><span class="toc-text">ğŸ“Œ Why Do Encoder-only Models (e.g., BERT) Use Right Padding?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%93%8C-Why-Do-Decoder-only-LLMs-Use-Left-Padding"><span class="toc-text">ğŸ“Œ Why Do Decoder-only LLMs Use Left Padding?</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%93%8C-Padding-Differences-in-Training-vs-Inference"><span class="toc-text">ğŸ“Œ Padding Differences in Training vs Inference</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%93%8C-Summary-amp-Key-Takeaways-TL-DR"><span class="toc-text">ğŸ“Œ Summary &amp; Key Takeaways (TL;DR)</span></a></li></ol></div><div class="column is-9"><header class="my-4"><a href="/tags/LLM"><i class="tag post-item-tag">LLM</i></a></header><h1 class="mt-0 mb-1 is-family-serif" id="postTitle">Differences in Padding Strategies Between Decoder-only and Encoder-only Models</h1><time class="has-text-grey" datetime="2025-03-06T09:43:10.000Z">2025-03-06</time><article class="mt-2 post-content"><h2 id="ğŸ“Œ-What-is-Padding"><a href="#ğŸ“Œ-What-is-Padding" class="headerlink" title="ğŸ“Œ What is Padding?"></a>ğŸ“Œ <strong>What is Padding?</strong></h2><p>In <strong>Large Language Models (LLMs)</strong>, <strong>padding</strong> is a method used to standardize sequence lengths for <strong>batch processing</strong>.</p>
<p>For example:</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Sentence 1: "I love NLP"</span><br><span class="line">Sentence 2: "Padding is useful in LLM training"</span><br></pre></td></tr></tbody></table></figure>

<p>Using the <code>&lt;pad&gt;</code> token for alignment:</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">"I love NLP &lt;pad&gt; &lt;pad&gt; &lt;pad&gt;"</span><br><span class="line">"Padding is useful in LLM training"</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h2 id="ğŸ“Œ-Padding-Positioning-Left-vs-Right"><a href="#ğŸ“Œ-Padding-Positioning-Left-vs-Right" class="headerlink" title="ğŸ“Œ Padding Positioning: Left vs Right"></a>ğŸ“Œ <strong>Padding Positioning: Left vs Right</strong></h2><p>There are two common padding strategies:</p>
<ul>
<li><p><strong>Right padding</strong>:</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">"I love NLP &lt;pad&gt; &lt;pad&gt;"</span><br></pre></td></tr></tbody></table></figure>
</li>
<li><p><strong>Left padding</strong>:</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">"&lt;pad&gt; &lt;pad&gt; I love NLP"</span><br></pre></td></tr></tbody></table></figure></li>
</ul>
<p>Typically:</p>
<ul>
<li><strong>Decoder-only models</strong> (e.g., GPT, Llama): Use <strong>Left padding</strong>.</li>
<li><strong>Encoder-only models</strong> (e.g., BERT): Use <strong>Right padding</strong>.</li>
</ul>
<p>Transformers can be categorized into three main architectures:</p>
<table>
<thead>
<tr>
<th>Model Type</th>
<th>Representative Models</th>
<th>Characteristics</th>
<th>Common Applications</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Encoder-only</strong></td>
<td><strong>BERT</strong>, RoBERTa, ALBERT, ELECTRA</td>
<td><strong>Bidirectional attention</strong></td>
<td>NLP tasks like text classification, named entity recognition</td>
</tr>
<tr>
<td><strong>Decoder-only</strong></td>
<td>GPT, GPT-2, GPT-3, GPT-4, LLaMA, Mistral</td>
<td><strong>Causal attention (Autoregressive)</strong></td>
<td>Text generation, chatbots, writing assistance</td>
</tr>
<tr>
<td><strong>Encoder-Decoder</strong></td>
<td>Transformer (original), T5, BART, mT5, PEGASUS</td>
<td><strong>Encoder: bidirectional, Decoder: autoregressive</strong></td>
<td>Machine translation, summarization, dialogue systems</td>
</tr>
</tbody></table>
<hr>
<h2 id="ğŸ“Œ-Why-Do-Encoder-only-Models-e-g-BERT-Use-Right-Padding"><a href="#ğŸ“Œ-Why-Do-Encoder-only-Models-e-g-BERT-Use-Right-Padding" class="headerlink" title="ğŸ“Œ Why Do Encoder-only Models (e.g., BERT) Use Right Padding?"></a>ğŸ“Œ <strong>Why Do Encoder-only Models (e.g., BERT) Use Right Padding?</strong></h2><ul>
<li><strong>Encoder-only models</strong> (like BERT) aim to obtain <strong>representations for each token</strong>.</li>
<li>These models use <strong>bidirectional attention</strong>, meaning each token attends to <strong>both past and future tokens</strong>.</li>
<li><strong>Slight shifts in position do not significantly impact model performance</strong>.</li>
<li>Special tokens (e.g., <code>[CLS]</code>) in BERT maintain a <strong>fixed position</strong> for tasks like classification, making <strong>right padding more natural</strong>.</li>
</ul>
<p>Example:</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[CLS] Hello I love NLP [SEP] &lt;pad&gt; &lt;pad&gt;</span><br></pre></td></tr></tbody></table></figure>

<ul>
<li>Right padding keeps <code>[CLS]</code> and <code>[SEP]</code> in stable positions, allowing the model to focus on meaningful tokens.</li>
</ul>
<hr>
<h2 id="ğŸ“Œ-Why-Do-Decoder-only-LLMs-Use-Left-Padding"><a href="#ğŸ“Œ-Why-Do-Decoder-only-LLMs-Use-Left-Padding" class="headerlink" title="ğŸ“Œ Why Do Decoder-only LLMs Use Left Padding?"></a>ğŸ“Œ <strong>Why Do Decoder-only LLMs Use Left Padding?</strong></h2><p><strong>Decoder-only models</strong> (like GPT) are <strong>autoregressive</strong>, meaning each token is generated based only on <strong>previous tokens</strong>, and future tokens are <strong>masked</strong>.</p>
<ul>
<li><strong>Positional Encoding Stability</strong>:<br>Left padding ensures that meaningful tokens have a <strong>consistent relative position</strong>, preventing <strong>position encoding misalignment</strong>.<ul>
<li>When using <strong>absolute positional encoding</strong>, every token (including <code>&lt;pad&gt;</code>) gets a unique position index.</li>
<li>Padding tokens at the beginning <strong>do not affect the modelâ€™s attention mechanism</strong> due to <strong>masking</strong>.</li>
</ul>
</li>
</ul>
<p>Example:</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Position Index: [ 1      2      3      4      5      6 ]</span><br><span class="line">Token:         [ &lt;pad&gt;, &lt;pad&gt;, Hello,  I,   love,  NLP ]</span><br><span class="line">Mask:          [  0,     0,     1,     1,     1,    1 ]</span><br></pre></td></tr></tbody></table></figure>

<ul>
<li><p>The model <strong>only attends to tokens where the mask is 1</strong>, ignoring padding tokens.</p>
</li>
<li><p><strong>Attention Masking</strong>:<br>Left padding ensures that <code>&lt;pad&gt;</code> tokens <strong>do not interfere with token position encoding</strong>.</p>
</li>
</ul>
<p>Illustration:</p>
<table>
<thead>
<tr>
<th>Token</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
</tr>
</thead>
<tbody><tr>
<td>Left</td>
<td><code>&lt;pad&gt;</code></td>
<td><code>&lt;pad&gt;</code></td>
<td>Hello</td>
<td>I</td>
<td>love</td>
<td>NLP</td>
</tr>
<tr>
<td>Right</td>
<td>Hello</td>
<td>I</td>
<td>love</td>
<td>NLP</td>
<td><code>&lt;pad&gt;</code></td>
<td><code>&lt;pad&gt;</code></td>
</tr>
</tbody></table>
<ul>
<li><strong>With Left padding</strong>, the last valid token <strong>always remains in the same position</strong>.</li>
<li><strong>With Right padding</strong>, token positions shift, affecting positional encoding stability.</li>
</ul>
<hr>
<h2 id="ğŸ“Œ-Padding-Differences-in-Training-vs-Inference"><a href="#ğŸ“Œ-Padding-Differences-in-Training-vs-Inference" class="headerlink" title="ğŸ“Œ Padding Differences in Training vs Inference"></a>ğŸ“Œ <strong>Padding Differences in Training vs Inference</strong></h2><table>
<thead>
<tr>
<th>Phase</th>
<th>Padding Strategy</th>
<th>Reason</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Training</strong></td>
<td>Left padding for decoder-only; Right padding for encoder-only</td>
<td>Optimized for batch processing efficiency</td>
</tr>
<tr>
<td><strong>Inference</strong></td>
<td>Typically, no padding for single sequences; Left padding for batched inference</td>
<td>Ensures stable positional encoding</td>
</tr>
</tbody></table>
<hr>
<h2 id="ğŸ“Œ-Summary-amp-Key-Takeaways-TL-DR"><a href="#ğŸ“Œ-Summary-amp-Key-Takeaways-TL-DR" class="headerlink" title="ğŸ“Œ Summary &amp; Key Takeaways (TL;DR)"></a>ğŸ“Œ <strong>Summary &amp; Key Takeaways (TL;DR)</strong></h2><ul>
<li><strong>Padding</strong> standardizes sequence lengths for batch processing.</li>
<li><strong>Decoder-only models (GPT, Llama)</strong> use <strong>Left padding</strong> to <strong>stabilize positional encoding and prevent future token leakage</strong>. Left padding tokens are masked out.</li>
<li><strong>Encoder-only models (BERT, RoBERTa)</strong> use <strong>Right padding</strong> since they employ <strong>bidirectional attention</strong> and rely on stable special token positions (e.g., <code>[CLS]</code>).</li>
<li>Although padding tokens occupy positions in <strong>positional encoding</strong>, <strong>attention masks</strong> effectively filter them out, ensuring they do not affect model predictions.</li>
</ul>
<hr>
</article><section class="jump-container is-flex is-justify-content-space-between my-6"><!-- em is empty placeholder--><a class="button is-default" href="/2025/03/06/NLP%20Insights/Decoder%E6%A8%A1%E5%9E%8B%E5%92%8CEncoder%E6%A8%A1%E5%9E%8B%E5%9C%A8Padding%E4%B8%AD%E7%9A%84%E4%B8%8D%E5%90%8C.en/" title="Decoder-onlyä¸Encoder-onlyæ¨¡å‹Paddingç­–ç•¥çš„å·®å¼‚"><i class="iconfont icon-prev mr-2 has-text-grey"></i><span class="has-text-weight-semibold">Previous: Decoder-onlyä¸Encoder-onlyæ¨¡å‹Paddingç­–ç•¥çš„å·®å¼‚</span></a><a class="button is-default" href="/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2.en/" title="MoEæ¨¡å‹çš„çš„å¯è§†åŒ–æŒ‡å—ï¼šæ­ç§˜ MoE åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„è§’è‰²"><span class="has-text-weight-semibold">Next: MoEæ¨¡å‹çš„çš„å¯è§†åŒ–æŒ‡å—ï¼šæ­ç§˜ MoE åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„è§’è‰²</span><i class="iconfont icon-next ml-2 has-text-grey"></i></a></section><article class="mt-6 comment-container"><script async repo="Haojen/Claudia-theme-blog" src="https://utteranc.es/client.js" issue-term="pathname" theme="preferred-color-scheme"></script></article></div></div></main></main><footer class="is-flex is-flex-direction-column is-align-items-center is-flex-shrink-0 is-family-serif"><section class="sns-container"><a title="twitter" target="_blank" rel="noopener nofollow" href="//twitter.com//"><i class="iconfont icon-twitter"></i></a><!-- Github--><a title="github" target="_blank" rel="noopener nofollow" href="//github.com/haojen"><i class="iconfont icon-github"></i></a><!-- Ins--><a title="instagram" target="_blank" rel="noopener nofollow" href="//www.instagram.com//"><i class="iconfont icon-ins"></i></a><!-- RSS--><!-- çŸ¥ä¹--><!-- é¢†è‹±--><!-- è„¸ä¹¦--><a title="facebook" target="_blank" rel="noopener nofollow" href="//www.facebook.com//"><i class="iconfont icon-tian7_facebook"></i></a></section><p><span>Copyright Â©</span><span> user 2026</span></p><div class="is-flex is-justify-content-center is-flex-wrap-wrap"><p>Powered by Hexo &verbar;&nbsp;</p><p class="is-flex is-justify-content-center"><a title="Hexo theme author" target="_blank" rel="noopener" href="//github.com/haojen">Theme by Haojen&nbsp;</a></p><div style="margin-top: 2px"><a class="github-button" title="github-button" target="_blank" rel="noopener" href="https://github.com/haojen/hexo-theme-Claudia" data-color-scheme="no-preference: light; light: light; dark: dark;" data-show-count="true"></a></div></div><div><span></span></div></footer><script src="/js/lang-switch.js"></script><script async defer src="https://buttons.github.io/buttons.js"></script><script src="/js/jquery-3.6.1.min.js"></script><script src="/js/jquery-fancybox.min.js"></script><script src="/js/img_zoom.js"></script><script src="/js/post.js"></script></body></html>