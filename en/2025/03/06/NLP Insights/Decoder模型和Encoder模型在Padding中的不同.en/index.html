<!DOCTYPE html><html lang="[&quot;zh-CN&quot;,&quot;en&quot;]" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Decoder-onlyä¸Encoder-onlyæ¨¡å‹Paddingç­–ç•¥çš„å·®å¼‚ | é»‘å¤´å‘†é±¼è¿›åŒ–ä¹‹æ—…</title><meta name="author" content="Huiyu Chen"><meta name="copyright" content="Huiyu Chen"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="ğŸ“Œ Padding çš„å«ä¹‰åœ¨å¤§æ¨¡å‹ (LLM) ä¸­ï¼Œpadding æ˜¯ç”¨äºå°†ä¸åŒé•¿åº¦çš„åºåˆ—è°ƒæ•´ä¸ºåŒä¸€é•¿åº¦çš„æ–¹æ³•ï¼Œä»¥ä¾¿äºæ‰¹é‡ (batch) å¤„ç†ã€‚ ä¾‹å¦‚ï¼š 12å¥å­1: &quot;I love NLP&quot;å¥å­2: &quot;Padding is useful in LLM training&quot;  ä½¿ç”¨ &lt;pad&gt; token è¿›è¡Œå¯¹é½ï¼š 12&quot;I love NLP &lt;pad&gt; &lt;pad">
<meta property="og:type" content="article">
<meta property="og:title" content="Decoder-onlyä¸Encoder-onlyæ¨¡å‹Paddingç­–ç•¥çš„å·®å¼‚">
<meta property="og:url" content="https://chenhuiyu.github.io/en/2025/03/06/NLP%20Insights/Decoder%E6%A8%A1%E5%9E%8B%E5%92%8CEncoder%E6%A8%A1%E5%9E%8B%E5%9C%A8Padding%E4%B8%AD%E7%9A%84%E4%B8%8D%E5%90%8C.en/index.html">
<meta property="og:site_name" content="é»‘å¤´å‘†é±¼è¿›åŒ–ä¹‹æ—…">
<meta property="og:description" content="ğŸ“Œ Padding çš„å«ä¹‰åœ¨å¤§æ¨¡å‹ (LLM) ä¸­ï¼Œpadding æ˜¯ç”¨äºå°†ä¸åŒé•¿åº¦çš„åºåˆ—è°ƒæ•´ä¸ºåŒä¸€é•¿åº¦çš„æ–¹æ³•ï¼Œä»¥ä¾¿äºæ‰¹é‡ (batch) å¤„ç†ã€‚ ä¾‹å¦‚ï¼š 12å¥å­1: &quot;I love NLP&quot;å¥å­2: &quot;Padding is useful in LLM training&quot;  ä½¿ç”¨ &lt;pad&gt; token è¿›è¡Œå¯¹é½ï¼š 12&quot;I love NLP &lt;pad&gt; &lt;pad">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://chenhuiyu.github.io/img/butterfly-icon.png">
<meta property="article:published_time" content="2025-03-06T09:43:10.000Z">
<meta property="article:modified_time" content="2026-02-20T22:13:37.939Z">
<meta property="article:author" content="Huiyu Chen">
<meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://chenhuiyu.github.io/img/butterfly-icon.png"><script type="application/ld+json">{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Decoder-onlyä¸Encoder-onlyæ¨¡å‹Paddingç­–ç•¥çš„å·®å¼‚",
  "url": "https://chenhuiyu.github.io/2025/03/06/NLP%20Insights/Decoder%E6%A8%A1%E5%9E%8B%E5%92%8CEncoder%E6%A8%A1%E5%9E%8B%E5%9C%A8Padding%E4%B8%AD%E7%9A%84%E4%B8%8D%E5%90%8C.en/",
  "image": "https://chenhuiyu.github.io/img/butterfly-icon.png",
  "datePublished": "2025-03-06T09:43:10.000Z",
  "dateModified": "2026-02-20T22:13:37.939Z",
  "author": [
    {
      "@type": "Person",
      "name": "Huiyu Chen",
      "url": "https://chenhuiyu.github.io"
    }
  ]
}</script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://chenhuiyu.github.io/en/2025/03/06/NLP%20Insights/Decoder%E6%A8%A1%E5%9E%8B%E5%92%8CEncoder%E6%A8%A1%E5%9E%8B%E5%9C%A8Padding%E4%B8%AD%E7%9A%84%E4%B8%8D%E5%90%8C.en/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=5.5.4"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: 'å¤åˆ¶æˆåŠŸ',
    error: 'å¤åˆ¶å¤±è´¥',
    noSupport: 'æµè§ˆå™¨ä¸æ”¯æŒ'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: 'åˆšåˆš',
    min: 'åˆ†é’Ÿå‰',
    hour: 'å°æ—¶å‰',
    day: 'å¤©å‰',
    month: 'ä¸ªæœˆå‰'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.13.0/dist/infinitegrid.min.js',
    buttonText: 'åŠ è½½æ›´å¤š'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Decoder-onlyä¸Encoder-onlyæ¨¡å‹Paddingç­–ç•¥çš„å·®å¼‚',
  isHighlightShrink: false,
  isToc: true,
  pageType: 'post'
}</script><meta name="generator" content="Hexo 6.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="é»‘å¤´å‘†é±¼è¿›åŒ–ä¹‹æ—…" type="application/atom+xml">
</head><body><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">é»‘å¤´å‘†é±¼è¿›åŒ–ä¹‹æ—…</span></a><a class="nav-page-title" href="/"><span class="site-name">Decoder-onlyä¸Encoder-onlyæ¨¡å‹Paddingç­–ç•¥çš„å·®å¼‚</span><span class="site-name"><i class="fa-solid fa-circle-arrow-left"></i><span>  Back to Home</span></span></a></span><div id="menus"></div></nav><div id="post-info"><h1 class="post-title">Decoder-onlyä¸Encoder-onlyæ¨¡å‹Paddingç­–ç•¥çš„å·®å¼‚</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2025-03-06T09:43:10.000Z" title="Created 2025-03-06 17:43:10">2025-03-06</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2026-02-20T22:13:37.939Z" title="Updated 2026-02-21 06:13:37">2026-02-21</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/NLP-Insights/">NLP Insights</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title=""><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post Views:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="container post-content" id="article-container"><h2 id="ğŸ“Œ-Padding-çš„å«ä¹‰"><a href="#ğŸ“Œ-Padding-çš„å«ä¹‰" class="headerlink" title="ğŸ“Œ Padding çš„å«ä¹‰"></a>ğŸ“Œ <strong>Padding çš„å«ä¹‰</strong></h2><p>åœ¨å¤§æ¨¡å‹ (<strong>LLM</strong>) ä¸­ï¼Œ<strong>padding</strong> æ˜¯ç”¨äºå°†ä¸åŒé•¿åº¦çš„åºåˆ—è°ƒæ•´ä¸ºåŒä¸€é•¿åº¦çš„æ–¹æ³•ï¼Œä»¥ä¾¿äºæ‰¹é‡ (<strong>batch</strong>) å¤„ç†ã€‚</p>
<p>ä¾‹å¦‚ï¼š</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">å¥å­1: "I love NLP"</span><br><span class="line">å¥å­2: "Padding is useful in LLM training"</span><br></pre></td></tr></tbody></table></figure>

<p>ä½¿ç”¨ <code>&lt;pad&gt;</code> token è¿›è¡Œå¯¹é½ï¼š</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">"I love NLP &lt;pad&gt; &lt;pad&gt; &lt;pad&gt;"</span><br><span class="line">"Padding is useful in LLM training"</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h2 id="ğŸ“Œ-Padding-ä½ç½®çš„é€‰æ‹©ï¼šLeft-vs-Right"><a href="#ğŸ“Œ-Padding-ä½ç½®çš„é€‰æ‹©ï¼šLeft-vs-Right" class="headerlink" title="ğŸ“Œ Padding ä½ç½®çš„é€‰æ‹©ï¼šLeft vs Right"></a>ğŸ“Œ <strong>Padding ä½ç½®çš„é€‰æ‹©ï¼šLeft vs Right</strong></h2><p>Padding æœ‰ä¸¤ç§å¸¸è§æ–¹å¼ï¼š</p>
<ul>
<li><p><strong>Right padding</strong>ï¼ˆå³å¡«å……ï¼‰ï¼š</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">"I love NLP &lt;pad&gt; &lt;pad&gt;"</span><br></pre></td></tr></tbody></table></figure>
</li>
<li><p><strong>Left padding</strong>ï¼ˆå·¦å¡«å……ï¼‰ï¼š</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">"&lt;pad&gt; &lt;pad&gt; I love NLP"</span><br></pre></td></tr></tbody></table></figure></li>
</ul>
<p>é€šå¸¸ï¼š</p>
<ul>
<li><strong>Decoder-only æ¨¡å‹</strong>ï¼ˆå¦‚ GPT, Llamaï¼‰ï¼šé‡‡ç”¨ <strong>Left padding</strong></li>
<li><strong>Encoder-only æ¨¡å‹</strong>ï¼ˆå¦‚ BERTï¼‰ï¼šé‡‡ç”¨ <strong>Right padding</strong></li>
</ul>
<p>å…·ä½“è€Œè¨€ï¼ŒTransformer æ¨¡å‹é€šå¸¸åˆ†ä¸ºä¸‰ç±»ç»“æ„ï¼š</p>
<table>
<thead>
<tr>
<th>æ¨¡å‹ç±»å‹</th>
<th>ä»£è¡¨æ¨¡å‹</th>
<th>ç‰¹å¾</th>
<th>å¸¸è§ç”¨é€”</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Encoder-only</strong></td>
<td><strong>BERT</strong>ã€RoBERTaã€ALBERTã€ELECTRA</td>
<td>åŒå‘æ³¨æ„åŠ›ï¼ˆBidirectional Attentionï¼‰</td>
<td>è‡ªç„¶è¯­è¨€ç†è§£ï¼ˆNLUï¼‰ï¼Œå¦‚æ–‡æœ¬åˆ†ç±»ã€åºåˆ—æ ‡æ³¨</td>
</tr>
<tr>
<td><strong>Decoder-only</strong></td>
<td>GPTã€GPT-2ã€GPT-3ã€GPT-4ã€LLaMAã€Mistral</td>
<td>å•å‘è‡ªå›å½’æ³¨æ„åŠ›ï¼ˆCausal Attentionï¼‰</td>
<td>æ–‡æœ¬ç”Ÿæˆã€èŠå¤©ã€å†™ä½œ</td>
</tr>
<tr>
<td><strong>Encoder-Decoder</strong></td>
<td>TransformeråŸå§‹è®ºæ–‡ä¸­çš„æ¨¡å‹ã€T5ã€BARTã€mT5ã€PEGASUS</td>
<td>Encoderä¸ºåŒå‘æ³¨æ„åŠ›ï¼ŒDecoderä¸ºå•å‘è‡ªå›å½’æ³¨æ„åŠ›</td>
<td>æœºå™¨ç¿»è¯‘ã€æ‘˜è¦ç”Ÿæˆã€å¯¹è¯</td>
</tr>
</tbody></table>
<hr>
<h2 id="ğŸ“Œ-ä¸ºä»€ä¹ˆ-Encoder-only-æ¨¡å‹ï¼ˆå¦‚BERTï¼‰é‡‡ç”¨-Right-paddingï¼Ÿ"><a href="#ğŸ“Œ-ä¸ºä»€ä¹ˆ-Encoder-only-æ¨¡å‹ï¼ˆå¦‚BERTï¼‰é‡‡ç”¨-Right-paddingï¼Ÿ" class="headerlink" title="ğŸ“Œ ä¸ºä»€ä¹ˆ Encoder-only æ¨¡å‹ï¼ˆå¦‚BERTï¼‰é‡‡ç”¨ Right paddingï¼Ÿ"></a>ğŸ“Œ ä¸ºä»€ä¹ˆ Encoder-only æ¨¡å‹ï¼ˆå¦‚BERTï¼‰é‡‡ç”¨ Right paddingï¼Ÿ</h2><ul>
<li><strong>Encoder-only æ¨¡å‹</strong>ï¼ˆå¦‚ BERTï¼‰çš„æ ¸å¿ƒç›®æ ‡æ˜¯è·å¾—<strong>æ¯ä¸ª token çš„åµŒå…¥è¡¨ç¤º</strong>ï¼ˆEmbedding representationï¼‰ã€‚</li>
<li>æ­¤ç±»æ¨¡å‹ä¸º<strong>åŒå‘æ³¨æ„åŠ›ï¼ˆBidirectional Attentionï¼‰</strong>ï¼Œæ¯ä¸ª token å¯åŒæ—¶å…³æ³¨ä¸Šä¸‹æ–‡ï¼Œå› æ­¤<strong>ä½ç½®çš„è½»å¾®å˜åŒ–ä¸ä¼šå¯¹ç»“æœé€ æˆä¸¥é‡å¹²æ‰°</strong>ã€‚</li>
<li>æ­¤å¤–ï¼Œencoder-only æ¨¡å‹ä¸­é€šå¸¸æœ‰ç‰¹æ®Š tokenï¼ˆå¦‚ <code>[CLS]</code>ï¼‰ï¼Œä½ç½®ç›¸å¯¹ç¨³å®šï¼Œç”¨äºå¥å­åˆ†ç±»æˆ–è¡¨ç¤ºï¼Œå› æ­¤é‡‡ç”¨ <strong>right padding</strong> æ›´è‡ªç„¶ï¼Œä¹Ÿæ›´åˆç†ã€‚</li>
</ul>
<p>ç¤ºä¾‹è¯´æ˜ï¼š</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[CLS] Hello I love NLP [SEP] &lt;pad&gt; &lt;pad&gt;</span><br></pre></td></tr></tbody></table></figure>

<ul>
<li>å³å¡«å……åï¼Œ<code>[CLS]</code> å’Œ <code>[SEP]</code> token ä½ç½®ç¨³å®šï¼Œä¸”ä¾¿äºæ¨¡å‹ä¸“æ³¨äºå‰é¢çš„æœ‰æ•ˆä¿¡æ¯ã€‚</li>
</ul>
<hr>
<h2 id="ğŸ“Œ-ä¸ºä»€ä¹ˆ-Decoder-only-LLM-é‡‡ç”¨-Left-paddingï¼Ÿ"><a href="#ğŸ“Œ-ä¸ºä»€ä¹ˆ-Decoder-only-LLM-é‡‡ç”¨-Left-paddingï¼Ÿ" class="headerlink" title="ğŸ“Œ ä¸ºä»€ä¹ˆ Decoder-only LLM é‡‡ç”¨ Left paddingï¼Ÿ"></a>ğŸ“Œ ä¸ºä»€ä¹ˆ Decoder-only LLM é‡‡ç”¨ Left paddingï¼Ÿ</h2><p>ä»¥ GPT ä¸ºä»£è¡¨çš„ <strong>Decoder-only æ¨¡å‹</strong> æ˜¯è‡ªå›å½’ï¼ˆ<strong>Autoregressive</strong>ï¼‰æ¨¡å‹ï¼Œæ¯ä¸ªè¯çš„ç”Ÿæˆä»…ä¾èµ–äºå½“å‰åŠä¹‹å‰çš„è¯ï¼Œæœªæ¥è¯ä¸å¯è§ã€‚å› æ­¤ï¼š</p>
<ul>
<li><strong>ä½ç½®ç¼–ç çš„ç¨³å®šæ€§</strong>ï¼š<br>å·¦å¡«å……ç¡®ä¿çœŸå® token çš„ç›¸å¯¹ä½ç½®ç¨³å®šï¼Œæ¨¡å‹ç”Ÿæˆæ–° token æ—¶ä½ç½®ç¼–ç å§‹ç»ˆç¨³å®šäºåºåˆ—æœ«å°¾ã€‚<ul>
<li>å½“é‡‡ç”¨<strong>ç»å¯¹ä½ç½®ç¼–ç </strong>ï¼ˆAbsolute Positional Encodingï¼‰æ—¶ï¼Œæ¯ä¸ª tokenï¼ˆåŒ…æ‹¬ <code>&lt;pad&gt;</code>ï¼‰éƒ½æœ‰å¯¹åº”çš„ä½ç½®ç¼–å·ã€‚</li>
<li>å¯¹äºå·¦å¡«å……çš„ padding tokensï¼Œè™½ç„¶å®ƒä»¬å æ®äº†ä½ç½®ç¼–å·ï¼ˆå¦‚ 1ã€2ï¼‰ï¼Œä½†æ¨¡å‹é€šè¿‡<strong>æ©ç æœºåˆ¶</strong>å¿½ç•¥å…¶å¯¹æ³¨æ„åŠ›å’Œè¾“å‡ºç»“æœçš„å½±å“ã€‚<br>ç¤ºä¾‹ï¼š</li>
</ul>
</li>
</ul>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ä½ç½®ç¼–ç : [ 1      2      3      4      5      6 ]</span><br><span class="line">Token:   [ &lt;pad&gt;, &lt;pad&gt;, Hello,  I,   love,  NLP ]</span><br><span class="line">æ©ç :     [  0,     0,     1,     1,     1,    1 ]</span><br></pre></td></tr></tbody></table></figure>

<ul>
<li>æ¨¡å‹åªå…³æ³¨æ©ç ä¸º 1 çš„æœ‰æ•ˆ tokenï¼Œè€Œå¿½ç•¥æ©ç ä¸º 0 çš„ padding tokensã€‚</li>
<li><strong>æ³¨æ„åŠ›æ©ç ï¼ˆAttention Maskï¼‰</strong>ï¼š<br>å·¦ä¾§çš„ <code>&lt;pad&gt;</code> ä¼šè¢«<strong>æ³¨æ„åŠ›æ©ç ï¼ˆattention maskï¼‰å¿½ç•¥</strong>ï¼Œä»è€Œé¿å… padding token å¹²æ‰°æœ‰æ•ˆ token çš„ä½ç½®ç¼–ç å’Œæ³¨æ„åŠ›è®¡ç®—ã€‚</li>
</ul>
<p>ç¤ºä¾‹è¯´æ˜ï¼š</p>
<table>
<thead>
<tr>
<th>Token</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
</tr>
</thead>
<tbody><tr>
<td>Left</td>
<td><code>&lt;pad&gt;</code></td>
<td><code>&lt;pad&gt;</code></td>
<td>Hello</td>
<td>I</td>
<td>love</td>
<td>NLP</td>
</tr>
<tr>
<td>Right</td>
<td>Hello</td>
<td>I</td>
<td>love</td>
<td>NLP</td>
<td><code>&lt;pad&gt;</code></td>
<td><code>&lt;pad&gt;</code></td>
</tr>
</tbody></table>
<ul>
<li><strong>Left padding</strong> ä¸‹ï¼Œæœ€åæœ‰æ•ˆ token å§‹ç»ˆåœ¨åŒä¸€ä½ç½®ï¼ˆ6ï¼‰ã€‚</li>
<li><strong>Right padding</strong> ä¸‹ï¼Œtoken çš„ä½ç½®éšåºåˆ—é•¿åº¦å˜åŒ–ï¼Œå½±å“ä½ç½®ç¼–ç çš„ç¨³å®šæ€§ã€‚</li>
</ul>
<hr>
<h2 id="ğŸ“Œ-Padding-åœ¨è®­ç»ƒä¸æ¨ç†é˜¶æ®µçš„å·®å¼‚"><a href="#ğŸ“Œ-Padding-åœ¨è®­ç»ƒä¸æ¨ç†é˜¶æ®µçš„å·®å¼‚" class="headerlink" title="ğŸ“Œ Padding åœ¨è®­ç»ƒä¸æ¨ç†é˜¶æ®µçš„å·®å¼‚"></a>ğŸ“Œ <strong>Padding åœ¨è®­ç»ƒä¸æ¨ç†é˜¶æ®µçš„å·®å¼‚</strong></h2><table>
<thead>
<tr>
<th>é˜¶æ®µ (Phase)</th>
<th>Padding ç­–ç•¥</th>
<th>åŸå› </th>
</tr>
</thead>
<tbody><tr>
<td><strong>è®­ç»ƒ (Training)</strong></td>
<td>æ‰¹é‡å¤„ç†æ—¶ï¼ŒDecoder-only å¸¸ç”¨å·¦å¡«å……ï¼›Encoder-only æ¨¡å‹åˆ™å¸¸ç”¨å³å¡«å……</td>
<td>æ‰¹é‡å¤„ç†ï¼ŒåŠ å¿«è®¡ç®—æ•ˆç‡</td>
</tr>
<tr>
<td><strong>æ¨ç† (Inference)</strong></td>
<td>é€šå¸¸å•æ¡åºåˆ—ï¼Œæ— éœ€ paddingï¼›è‹¥éœ€è¦æ‰¹é‡æ¨ç†ï¼Œä»é‡‡ç”¨å·¦å¡«å……</td>
<td>ç¨³å®šä½ç½®ç¼–ç </td>
</tr>
</tbody></table>
<hr>
<h2 id="ğŸ“Œ-æ€»ç»“ä¸å…³é”®è¦ç‚¹ï¼ˆTL-DRï¼‰"><a href="#ğŸ“Œ-æ€»ç»“ä¸å…³é”®è¦ç‚¹ï¼ˆTL-DRï¼‰" class="headerlink" title="ğŸ“Œ æ€»ç»“ä¸å…³é”®è¦ç‚¹ï¼ˆTL;DRï¼‰"></a>ğŸ“Œ <strong>æ€»ç»“ä¸å…³é”®è¦ç‚¹ï¼ˆTL;DRï¼‰</strong></h2><ul>
<li><strong>Padding</strong> ç”¨äºåºåˆ—é•¿åº¦æ ‡å‡†åŒ–ã€‚</li>
<li><strong>Decoder-only LLMs (GPT, Llama)</strong> é€šå¸¸é‡‡ç”¨<strong>å·¦å¡«å……ï¼ˆLeft paddingï¼‰</strong>ï¼Œç›®çš„æ˜¯<strong>ç¨³å®šä½ç½®ç¼–ç å¹¶é¿å…æœªæ¥ä¿¡æ¯æ³„æ¼</strong>ï¼›å·¦ä¾§ padding ä¼šè¢«æ©ç å¿½ç•¥ï¼Œä¸å¹²æ‰°æ¨¡å‹é¢„æµ‹ã€‚</li>
<li><strong>Encoder-only æ¨¡å‹ï¼ˆå¦‚BERTç³»åˆ—ï¼‰</strong>é€šå¸¸é‡‡ç”¨<strong>å³å¡«å……ï¼ˆRight paddingï¼‰</strong>ï¼Œå› ä¸ºæ¨¡å‹ä¸ºåŒå‘æ³¨æ„åŠ›ï¼Œä¸”ç‰¹æ®Štokenï¼ˆå¦‚<code>[CLS]</code>ï¼‰ä½ç½®éœ€è¦ä¿æŒç¨³å®šã€‚</li>
<li>ä½ç½®ç¼–ç ä¸­è™½ç„¶ padding token å ä½ï¼Œä½†ä¼šè¢«<strong>æ³¨æ„åŠ›æ©ç </strong>æœ‰æ•ˆå±è”½ï¼Œä¸å½±å“æ¨¡å‹çš„æœ€ç»ˆè¾“å‡ºã€‚</li>
</ul>
<hr>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>Author: </span><span class="post-copyright-info"><a href="https://chenhuiyu.github.io">Huiyu Chen</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>Link: </span><span class="post-copyright-info"><a href="https://chenhuiyu.github.io/2025/03/06/NLP%20Insights/Decoder%E6%A8%A1%E5%9E%8B%E5%92%8CEncoder%E6%A8%A1%E5%9E%8B%E5%9C%A8Padding%E4%B8%AD%E7%9A%84%E4%B8%8D%E5%90%8C.en/">https://chenhuiyu.github.io/2025/03/06/NLP%20Insights/Decoder%E6%A8%A1%E5%9E%8B%E5%92%8CEncoder%E6%A8%A1%E5%9E%8B%E5%9C%A8Padding%E4%B8%AD%E7%9A%84%E4%B8%8D%E5%90%8C.en/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>Copyright Notice: </span><span class="post-copyright-info">All articles on this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless otherwise stated.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/LLM/">LLM</a></div><div class="post-share"><div class="social-share" data-image="/img/butterfly-icon.png" data-sites="facebook,x,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.6/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><a class="pagination-related" href="/2025/03/06/NLP%20Insights/Differences%20in%20Padding%20Strategies%20Between%20Decoder-only%20and%20Encoder-only%20Models.en/" title="Differences in Padding Strategies Between Decoder-only and Encoder-only Models"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info"><div class="info-1"><div class="info-item-1">Previous</div><div class="info-item-2">Differences in Padding Strategies Between Decoder-only and Encoder-only Models</div></div><div class="info-2"><div class="info-item-1">ğŸ“Œ What is Padding?In Large Language Models (LLMs), padding is a method used to standardize sequence lengths for batch processing. For example: 12Sentence 1: "I love NLP"Sentence 2: "Padding is useful in LLM training"  Using the &lt;pad&gt; token for alignment: 12"I love NLP &lt;pad&gt; &lt;pad&gt; &lt;pad&gt;""Padding is useful in LLM training"   ğŸ“Œ Padding Positioning: Left vs RightThere are two common padding strategies:  Right padding: 1"I love NLP &lt;pad&gt; &lt;pad&gt;"  Left padding: ...</div></div></div></a><a class="pagination-related" href="/2025/02/11/NLP%20Insights/%E6%8E%A8%E7%90%86%20LLM%20%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8E%A2%E7%B4%A2%E6%8E%A8%E7%90%86%E6%97%B6%E8%AE%A1%E7%AE%97%E6%8A%80%E6%9C%AF%E4%B8%8E%20DeepSeek-R1.zh-CN/" title="æ¨ç† LLM çš„å¯è§†åŒ–æŒ‡å—ï¼šæ¢ç´¢æ¨ç†æ—¶è®¡ç®—æŠ€æœ¯ä¸ DeepSeek-R1"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-right"><div class="info-1"><div class="info-item-1">Next</div><div class="info-item-2">æ¨ç† LLM çš„å¯è§†åŒ–æŒ‡å—ï¼šæ¢ç´¢æ¨ç†æ—¶è®¡ç®—æŠ€æœ¯ä¸ DeepSeek-R1</div></div><div class="info-2"><div class="info-item-1">æ¨ç† LLM çš„å¯è§†åŒ–æŒ‡å—ï¼šæ¢ç´¢æ¨ç†æ—¶è®¡ç®—æŠ€æœ¯ä¸ DeepSeek-R1åŸæ–‡åœ°å€ï¼šA Visual Guide to Reasoning LLMs ğŸ“… ä½œè€…ï¼šMaarten Grootendorst ğŸ“† æ—¥æœŸï¼š2025 å¹´ 2 æœˆ 3 æ—¥  ğŸ“Œ å¼•è¨€DeepSeek-R1ã€OpenAI o3-mini å’Œ Google Gemini 2.0 Flash Thinking æ˜¯å¦‚ä½•é€šè¿‡â€œæ¨ç†â€æ¡†æ¶å°† LLMï¼ˆå¤§å‹è¯­è¨€æ¨¡å‹, Large Language Modelsï¼‰ æ‰©å±•åˆ°æ–°é«˜åº¦çš„å…¸å‹ç¤ºä¾‹ã€‚ å®ƒä»¬æ ‡å¿—ç€ä» æ‰©å±•è®­ç»ƒæ—¶è®¡ç®—ï¼ˆtrain-time computeï¼‰ åˆ° æ‰©å±•æ¨ç†æ—¶è®¡ç®—ï¼ˆtest-time computeï¼‰ çš„èŒƒå¼è½¬å˜ã€‚ åœ¨æœ¬ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬æä¾›äº† è¶…è¿‡ 40 å¼ å®šåˆ¶å¯è§†åŒ–å›¾è¡¨ï¼Œå¸¦ä½ æ·±å…¥æ¢ç´¢ï¼š  æ¨ç† LLMï¼ˆReasoning LLMsï¼‰ é¢†åŸŸ æ¨ç†æ—¶è®¡ç®—ï¼ˆTest-Time Computeï¼‰ æœºåˆ¶ DeepSeek-R1 çš„æ ¸å¿ƒæ€æƒ³  æˆ‘ä»¬å°†é€æ­¥ä»‹ç»ç›¸å…³æ¦‚å¿µï¼Œå¸®åŠ©ä½ å»ºç«‹å¯¹è¿™ä¸€æ–°èŒƒå¼çš„ç›´è§‰ç†è§£ã€‚    ğŸ“– ä»€ä¹ˆæ˜¯æ¨ç† LLMï¼Ÿä¸æ™®é€š LLMï¼ˆLarge Langu...</div></div></div></a></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><a class="pagination-related" href="/2025/02/11/NLP%20Insights/MoE%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96%E6%8C%87%E5%8D%97%EF%BC%9A%E6%8F%AD%E7%A7%98%20MoE%20%E5%9C%A8%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%B8%AD%E7%9A%84%E8%A7%92%E8%89%B2.en/" title="MoEæ¨¡å‹çš„çš„å¯è§†åŒ–æŒ‡å—ï¼šæ­ç§˜ MoE åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„è§’è‰²"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-02-11</div><div class="info-item-2">MoEæ¨¡å‹çš„çš„å¯è§†åŒ–æŒ‡å—ï¼šæ­ç§˜ MoE åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„è§’è‰²</div></div><div class="info-2"><div class="info-item-1">MoE æ¨¡å‹çš„çš„å¯è§†åŒ–æŒ‡å—ï¼šæ­ç§˜ MoE åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„è§’è‰²åŸæ–‡åœ°å€ï¼šA Visual Guide to Mixture of Experts (MoE) ğŸ“… ä½œè€…ï¼šMaarten Grootendorst ğŸ“† æ—¥æœŸï¼š2024 å¹´ 10 æœˆ 7 æ—¥  æ¢ç´¢è¯­è¨€æ¨¡å‹ï¼šæ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMoEï¼‰å¯è§†åŒ–æŒ‡å—ç›®å½• MoE æ¨¡å‹çš„çš„å¯è§†åŒ–æŒ‡å—ï¼šæ­ç§˜ MoE åœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„è§’è‰² æ¢ç´¢è¯­è¨€æ¨¡å‹ï¼šæ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMoEï¼‰å¯è§†åŒ–æŒ‡å— ç›®å½• ä»€ä¹ˆæ˜¯æ··åˆä¸“å®¶ï¼ˆMoEï¼‰æ¨¡å‹ï¼Ÿ Experts Dense Layers Sparse Layers What does an Expert Learn? ä¸“å®¶çš„æ¶æ„ï¼ˆArchitecture of Expertsï¼‰      å½“æˆ‘ä»¬æŸ¥çœ‹æœ€æ–°å‘å¸ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼ŒLarge Language Modelsï¼‰æ—¶ï¼Œå¸¸å¸¸ä¼šåœ¨æ ‡é¢˜ä¸­çœ‹åˆ° â€œMoEâ€ã€‚è¿™ä¸ª â€œMoEâ€ ä»£è¡¨ä»€ä¹ˆï¼Ÿä¸ºä»€ä¹ˆè¿™ä¹ˆå¤š LLM éƒ½åœ¨ä½¿ç”¨å®ƒï¼Ÿ åœ¨è¿™ä»½å¯è§†åŒ–æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬ä¼šé€šè¿‡ 50 å¤šä¸ªå¯è§†åŒ–å›¾ç¤ºï¼Œé€æ­¥æ¢ç´¢è¿™ä¸€å…³é”®ç»„ä»¶ï¼š**Mixture of Experts (MoE)**ã€‚   å›¾ç¤ºå†…...</div></div></div></a><a class="pagination-related" href="/2024/02/27/NLP%20Insights/FastChat%20%E8%AE%AD%E7%BB%83%E8%84%9A%E6%9C%AC%E4%BB%A3%E7%A0%81%E9%80%90%E8%A1%8C%E8%A7%A3%E6%9E%90-Train.py%20%E3%80%90FastChat%20%E7%B3%BB%E5%88%97%E7%AC%AC%201%20%E7%AF%87%E3%80%91.zh-CN/" title="FastChat è®­ç»ƒè„šæœ¬ä»£ç é€è¡Œè§£æ-Train.py ã€FastChat ç³»åˆ—ç¬¬ 1 ç¯‡ã€‘"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-27</div><div class="info-item-2">FastChat è®­ç»ƒè„šæœ¬ä»£ç é€è¡Œè§£æ-Train.py ã€FastChat ç³»åˆ—ç¬¬ 1 ç¯‡ã€‘</div></div><div class="info-2"><div class="info-item-1">FastChat è®­ç»ƒè„šæœ¬ä»£ç é€è¡Œè§£æ-Train.py ã€FastChat ç³»åˆ—ç¬¬ 1 ç¯‡ã€‘åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†æ·±å…¥æ¢è®¨ FastChat çš„ train.py è„šæœ¬ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºè®­ç»ƒå’Œä¼˜åŒ–å¤§å‹è¯­è¨€æ¨¡å‹çš„å…³é”®ç»„ä»¶ã€‚FastChat æ˜¯ä¸€ä¸ªå…ˆè¿›çš„å¼€æºå¹³å°ï¼Œä¸“æ³¨äºå¼€å‘ã€éƒ¨ç½²å’Œè¯„ä¼°åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„èŠå¤©æœºå™¨äººã€‚è¯¥å¹³å°ä¸ä»…æä¾›å¯¹é¡¶å°–æ¨¡å‹å¦‚ Vicuna å’Œ MT-Bench çš„æ”¯æŒï¼Œè¿˜åŒ…æ‹¬ä¸€ä¸ªåˆ†å¸ƒå¼çš„å¤šæ¨¡å‹æœåŠ¡ç³»ç»Ÿï¼Œé…å¤‡äº† Web UI å’Œä¸ OpenAI å…¼å®¹çš„ RESTful APIï¼Œä½¿ç”¨æˆ·èƒ½å¤Ÿé«˜æ•ˆåœ°è®­ç»ƒå’Œè¯„ä¼°ä»–ä»¬çš„æ¨¡å‹ã€‚ æœ¬æ–‡çš„æ·±å…¥åˆ†æå°†èšç„¦äº train.py è„šæœ¬çš„æºä»£ç ã€‚è¿™ä¸ªè„šæœ¬æ˜¯åŸºäº transformers åº“çš„è‡ªç„¶è¯­è¨€å¤„ç†æ¨¡å‹è®­ç»ƒè„šæœ¬ï¼Œæ¶µç›–äº†æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹è®­ç»ƒå’Œä¿å­˜ç­‰å…³é”®æ­¥éª¤ã€‚æˆ‘ä»¬æ—¨åœ¨æä¾›å¯¹ train.py ä¸­æ¯ä¸ªç±»å’Œå‡½æ•°çš„è¯¦ç»†è§£é‡Šï¼ŒåŒ…æ‹¬å®ƒä»¬çš„åŠŸèƒ½å’Œåœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­çš„ä½œç”¨ã€‚ 1. å¯¼å…¥æ¨¡å—1. å†…ç½®æ¨¡å—è¿™äº›æ˜¯ Python è‡ªå¸¦çš„æ ‡å‡†åº“æ¨¡å—ï¼Œæ— éœ€é¢å¤–å®‰è£…ã€‚ 1from dataclasses import dataclass, field  å¯¼å…¥ Pytho...</div></div></div></a><a class="pagination-related" href="/2024/02/19/NLP%20Insights/Understanding%20the%20Differences%20Between%20Fine-tuning%20and%20Further%20Pretraining%20in%20Large%20Language%20Models.zh-CN/" title="Understanding the Differences Between Fine-tuning and Further Pretraining in Large Language Models"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-19</div><div class="info-item-2">Understanding the Differences Between Fine-tuning and Further Pretraining in Large Language Models</div></div><div class="info-2"><div class="info-item-1">Understanding the Differences Between Fine-tuning and Further Pretraining in Large Language ModelsIn the world of Natural Language Processing (NLP), the advent of large language models like GPT and BERT has revolutionized how we approach tasks such as text classification, sentiment analysis, and question-answering. Two pivotal techniques in leveraging these models are Fine-tuning and Further Pretraining. While they may seem similar at a glance, they cater to different needs and scenarios in t...</div></div></div></a><a class="pagination-related" href="/2024/10/23/NLP%20Insights/Introduction%20to%20LLM%20Training%20Terminology:%20LoRA,%20DPO,%20KTO,%20and%20SFT%20Technologies.zh-CN/" title="Detailed Explanation of LoRA, DPO, KTO, and SFT Technologies"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-10-23</div><div class="info-item-2">Detailed Explanation of LoRA, DPO, KTO, and SFT Technologies</div></div><div class="info-2"><div class="info-item-1">Introduction to LLM Training Terminology:LoRA, DPO, KTO, and SFT TechnologiesThis document provides a detailed introduction to several important techniques used in fine-tuning and optimizing large language models (such as LLAMA3), including SFT (Supervised Fine-Tuning), LoRA (Low-Rank Adaptation), Alignment technologies, KTO (Kahneman-Tversky Optimization), and DPO (Direct Preference Optimization). The document also elaborates on the principles of each technique, specific implementation metho...</div></div></div></a><a class="pagination-related" href="/2025/03/06/NLP%20Insights/Differences%20in%20Padding%20Strategies%20Between%20Decoder-only%20and%20Encoder-only%20Models.en/" title="Differences in Padding Strategies Between Decoder-only and Encoder-only Models"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2025-03-06</div><div class="info-item-2">Differences in Padding Strategies Between Decoder-only and Encoder-only Models</div></div><div class="info-2"><div class="info-item-1">ğŸ“Œ What is Padding?In Large Language Models (LLMs), padding is a method used to standardize sequence lengths for batch processing. For example: 12Sentence 1: "I love NLP"Sentence 2: "Padding is useful in LLM training"  Using the &lt;pad&gt; token for alignment: 12"I love NLP &lt;pad&gt; &lt;pad&gt; &lt;pad&gt;""Padding is useful in LLM training"   ğŸ“Œ Padding Positioning: Left vs RightThere are two common padding strategies:  Right padding: 1"I love NLP &lt;pad&gt; &lt;pad&gt;"  Left padding: ...</div></div></div></a><a class="pagination-related" href="/2024/02/19/NLP%20Insights/Understanding%20the%20Differences%20Between%20Fine-tuning%20and%20Further%20Pretraining%20in%20Large%20Language%20Models.en/" title="Understanding the Differences Between Fine-tuning and Further Pretraining in Large Language Models"><div class="cover" style="background: var(--default-bg-color)"></div><div class="info text-center"><div class="info-1"><div class="info-item-1"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-19</div><div class="info-item-2">Understanding the Differences Between Fine-tuning and Further Pretraining in Large Language Models</div></div><div class="info-2"><div class="info-item-1">Understanding the Differences Between Fine-tuning and Further Pretraining in Large Language ModelsIn the world of Natural Language Processing (NLP), the advent of large language models like GPT and BERT has revolutionized how we approach tasks such as text classification, sentiment analysis, and question-answering. Two pivotal techniques in leveraging these models are Fine-tuning and Further Pretraining. While they may seem similar at a glance, they cater to different needs and scenarios in t...</div></div></div></a></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Huiyu Chen</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">æ–‡ç« </div><div class="length-num">125</div></a><a href="/tags/"><div class="headline">æ ‡ç­¾</div><div class="length-num">46</div></a><a href="/categories/"><div class="headline">åˆ†ç±»</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>å…¬å‘Š</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Contents</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%93%8C-Padding-%E7%9A%84%E5%90%AB%E4%B9%89"><span class="toc-number">1.</span> <span class="toc-text">ğŸ“Œ Padding çš„å«ä¹‰</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%93%8C-Padding-%E4%BD%8D%E7%BD%AE%E7%9A%84%E9%80%89%E6%8B%A9%EF%BC%9ALeft-vs-Right"><span class="toc-number">2.</span> <span class="toc-text">ğŸ“Œ Padding ä½ç½®çš„é€‰æ‹©ï¼šLeft vs Right</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%93%8C-%E4%B8%BA%E4%BB%80%E4%B9%88-Encoder-only-%E6%A8%A1%E5%9E%8B%EF%BC%88%E5%A6%82BERT%EF%BC%89%E9%87%87%E7%94%A8-Right-padding%EF%BC%9F"><span class="toc-number">3.</span> <span class="toc-text">ğŸ“Œ ä¸ºä»€ä¹ˆ Encoder-only æ¨¡å‹ï¼ˆå¦‚BERTï¼‰é‡‡ç”¨ Right paddingï¼Ÿ</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%93%8C-%E4%B8%BA%E4%BB%80%E4%B9%88-Decoder-only-LLM-%E9%87%87%E7%94%A8-Left-padding%EF%BC%9F"><span class="toc-number">4.</span> <span class="toc-text">ğŸ“Œ ä¸ºä»€ä¹ˆ Decoder-only LLM é‡‡ç”¨ Left paddingï¼Ÿ</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%93%8C-Padding-%E5%9C%A8%E8%AE%AD%E7%BB%83%E4%B8%8E%E6%8E%A8%E7%90%86%E9%98%B6%E6%AE%B5%E7%9A%84%E5%B7%AE%E5%BC%82"><span class="toc-number">5.</span> <span class="toc-text">ğŸ“Œ Padding åœ¨è®­ç»ƒä¸æ¨ç†é˜¶æ®µçš„å·®å¼‚</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%93%8C-%E6%80%BB%E7%BB%93%E4%B8%8E%E5%85%B3%E9%94%AE%E8%A6%81%E7%82%B9%EF%BC%88TL-DR%EF%BC%89"><span class="toc-number">6.</span> <span class="toc-text">ğŸ“Œ æ€»ç»“ä¸å…³é”®è¦ç‚¹ï¼ˆTL;DRï¼‰</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>æœ€æ–°æ–‡ç« </span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/21/NLP%20Insights/Evaluation%20of%20Generation-Based%20Large%20Language%20Models%20(LLMs):%20Opportunities%20and%20Challenges%20from%20Generation%20to%20Judgment.zh-CN/" title="Untitled">Untitled</a><time datetime="2026-02-20T22:15:00.000Z" title="å‘è¡¨äº 2026-02-21 06:15:00">2026-02-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/21/NLP%20Insights/Evaluation%20of%20Generation-Based%20Large%20Language%20Models%20(LLMs):%20Opportunities%20and%20Challenges%20from%20Generation%20to%20Judgment.en/" title="Untitled">Untitled</a><time datetime="2026-02-20T22:15:00.000Z" title="å‘è¡¨äº 2026-02-21 06:15:00">2026-02-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/24/NLP%20Insights/SeCom%20Redefining%20Memory%20Management%20in%20Conversational%20AI.en/" title="SeCom: Redefining Memory Management in Conversational AI">SeCom: Redefining Memory Management in Conversational AI</a><time datetime="2025-06-24T08:00:00.000Z" title="å‘è¡¨äº 2025-06-24 16:00:00">2025-06-24</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/24/NLP%20Insights/SeCom%20Redefining%20Memory%20Management%20in%20Conversational%20AI.zh-CN/" title="SeCom: Redefining Memory Management in Conversational AI">SeCom: Redefining Memory Management in Conversational AI</a><time datetime="2025-06-24T08:00:00.000Z" title="å‘è¡¨äº 2025-06-24 16:00:00">2025-06-24</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/24/NLP%20Insights/SeCom:%20%E9%87%8D%E6%96%B0%E5%AE%9A%E4%B9%89%E5%AF%B9%E8%AF%9DAI%E7%9A%84%E8%AE%B0%E5%BF%86%E7%AE%A1%E7%90%86.en/" title="SeCom: é‡æ–°å®šä¹‰å¯¹è¯AIçš„è®°å¿†ç®¡ç†">SeCom: é‡æ–°å®šä¹‰å¯¹è¯AIçš„è®°å¿†ç®¡ç†</a><time datetime="2025-06-24T08:00:00.000Z" title="å‘è¡¨äº 2025-06-24 16:00:00">2025-06-24</time></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 - 2026 By Huiyu Chen</span><span class="framework-info"><span>æ¡†æ¶ </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 6.3.0</a><span class="footer-separator">|</span><span>ä¸»é¢˜ </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.5.4</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Reading Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Toggle Between Light and Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle Between Single-column and Double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="Settings"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back to Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.4"></script><script src="/js/main.js?v=5.5.4"></script><div class="js-pjax"></div><script src="/js/lang-switch.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>