<!DOCTYPE html><html class="appearance-auto" lang="en"><head><meta charset="UTF-8"><title>SeCom: Redefining Memory Management in Conversational AI</title><meta name="description" content="May the Force be with you"><meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no, initial-scale=1"><!-- Google Analytics --><!-- End Google Analytics -->
<!-- Baidu Analytics --><!-- End Baidu Analytics --><link rel="icon" href="/images/favicon.ico"><link rel="stylesheet" href="/style/common/bulma.css"><link rel="stylesheet" href="/style/base.css"><link rel="stylesheet" href="/style/common/helper.css"><script src="/js/common.js"></script><link rel="stylesheet" href="/style/post.css"><link rel="stylesheet" href="/style/themes/highlight-theme-light.css"><link rel="stylesheet" href="/style/common/jquery.fancybox.min.css"><script src="/js/highlight.pack.js"></script><meta name="description" content="SeCom: Redefining Memory Management in Conversational AIForewordI’ve recently been diving into memory management for dialog-based AI, especially how to construct and retrieve memories in long-term conversations. During my exploration I came across an eye-opening ICLR 2025 paper—**”SeCom: On Memory Construction and Retrieval for Personalized Conversational .."><meta name="generator" content="Hexo 6.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="黑头呆鱼进化之旅" type="application/atom+xml">
</head><body class="is-flex is-flex-direction-column"><header class="header-widget is-flex-shrink-0 is-hidden-mobile"><div class="container is-fullhd is-flex is-justify-content-space-between is-align-items-center is-full-height"><section class="is-hidden-mobile is-flex-shrink-0"><h2><a href="/">user's blog</a></h2></section><h3 class="is-hidden-mobile is-family-serif is-full-height is-flex is-align-items-center is-flex-shrink-0"><div class="is-full-height" id="postTopic"><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">SeCom: Redefining Memory Management in Conversational AI</p><p class="is-full-height is-flex-shrink-0 is-flex is-align-items-center is-justify-content-center">Click back to the top</p></div></h3><aside class="is-flex-shrink-0"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></aside></div></header><header class="is-flex header-widget is-flex-shrink-0 is-align-items-center is-justify-content-center is-hidden-tablet"><h3 class="is-inline-block"><a href="/">Home</a></h3><h3 class="is-inline-block"><a href="/about">About</a></h3><h3 class="is-inline-block"><a href="/archives">Archives</a></h3></header><main><main class="container is-max-widescreen content section post-page pt-4 px-4"><div class="columns is-flex-desktop is-justify-content-center is-flex-direction-row-reverse"><div class="column is-3 is-hidden-mobile"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#SeCom-Redefining-Memory-Management-in-Conversational-AI"><span class="toc-text">SeCom: Redefining Memory Management in Conversational AI</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Foreword"><span class="toc-text">Foreword</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Why-Should-We-Care-About-Dialog-Memory-Management"><span class="toc-text">1. Why Should We Care About Dialog Memory Management?</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-Real-World-Challenges-in-Long-Conversations"><span class="toc-text">1.1 Real-World Challenges in Long Conversations</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-A-Quick-Landscape-of-Existing-Approaches"><span class="toc-text">1.2 A Quick Landscape of Existing Approaches</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-3-Retrieval-Augmented-Generation-RAG-in-Dialog"><span class="toc-text">1.2.3 Retrieval-Augmented Generation (RAG) in Dialog</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-The-Granularity-Dilemma"><span class="toc-text">1.3 The Granularity Dilemma</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Inside-SeCom"><span class="toc-text">2. Inside SeCom</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-Two-Key-Insights"><span class="toc-text">2.1 Two Key Insights</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-System-Pipeline"><span class="toc-text">2.2 System Pipeline</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-How-to-Segment-Without-Labels"><span class="toc-text">2.3 How to Segment Without Labels</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4-Denoising-via-LLMLingua-2"><span class="toc-text">2.4 Denoising via LLMLingua-2</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-5-Hybrid-Retrieval"><span class="toc-text">2.5 Hybrid Retrieval</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Final-Thoughts"><span class="toc-text">3. Final Thoughts</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-What-SeCom-Teaches-Us"><span class="toc-text">3.1 What SeCom Teaches Us</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#References"><span class="toc-text">References</span></a></li></ol></li></ol></div><div class="column is-9"><header class="my-4"><a href="/tags/Conversational%20AI"><i class="tag post-item-tag">Conversational AI</i></a><a href="/tags/Memory%20Management"><i class="tag post-item-tag">Memory Management</i></a><a href="/tags/SeCom"><i class="tag post-item-tag">SeCom</i></a><a href="/tags/RAG"><i class="tag post-item-tag">RAG</i></a></header><h1 class="mt-0 mb-1 is-family-serif" id="postTitle">SeCom: Redefining Memory Management in Conversational AI</h1><time class="has-text-grey" datetime="2025-06-24T08:00:00.000Z">2025-06-24</time><article class="mt-2 post-content"><h1 id="SeCom-Redefining-Memory-Management-in-Conversational-AI"><a href="#SeCom-Redefining-Memory-Management-in-Conversational-AI" class="headerlink" title="SeCom: Redefining Memory Management in Conversational AI"></a>SeCom: Redefining Memory Management in Conversational AI</h1><h2 id="Foreword"><a href="#Foreword" class="headerlink" title="Foreword"></a>Foreword</h2><p>I’ve recently been diving into memory management for dialog-based AI, especially how to construct and retrieve memories in long-term conversations. During my exploration I came across an eye-opening ICLR 2025 paper—**”SeCom: On Memory Construction and Retrieval for Personalized Conversational Agents”**—a collaboration between Microsoft and Tsinghua University.</p>
<p>SeCom solves a core problem: <strong>How can an agent effectively manage and retrieve historical information in prolonged conversations?</strong> In this post I’ll unpack the method’s key ideas and technical innovations, hoping to spark inspiration for researchers working in this arena.</p>
<h2 id="1-Why-Should-We-Care-About-Dialog-Memory-Management"><a href="#1-Why-Should-We-Care-About-Dialog-Memory-Management" class="headerlink" title="1. Why Should We Care About Dialog Memory Management?"></a>1. Why Should We Care About Dialog Memory Management?</h2><h3 id="1-1-Real-World-Challenges-in-Long-Conversations"><a href="#1-1-Real-World-Challenges-in-Long-Conversations" class="headerlink" title="1.1 Real-World Challenges in Long Conversations"></a>1.1 Real-World Challenges in Long Conversations</h3><p>Anyone who chats with LLMs regularly has probably experienced this: once a conversation grows long, the agent seems to “forget” earlier context or respond incoherently. That’s the memory problem in action.</p>
<p>Even with long-context models, super-long dialogs increase compute cost and often degrade quality. Key challenges include:</p>
<ul>
<li><strong>Context length limits</strong>: Token budgets remain finite.</li>
<li><strong>Information relevance</strong>: History contains plenty of facts irrelevant to the current query.</li>
<li><strong>Semantic coherence</strong>: Related information may be scattered across non-contiguous turns.</li>
<li><strong>Personalization</strong>: The agent must remember user preferences and interaction patterns.</li>
</ul>
<h3 id="1-2-A-Quick-Landscape-of-Existing-Approaches"><a href="#1-2-A-Quick-Landscape-of-Existing-Approaches" class="headerlink" title="1.2 A Quick Landscape of Existing Approaches"></a>1.2 A Quick Landscape of Existing Approaches</h3><p>The community’s strategies roughly split into three camps:</p>
<ol>
<li><strong>“Give Me Everything” (full history)</strong><ul>
<li>Complete information, zero recall loss.</li>
<li>But like moving an entire library just to find one book—computational overkill.</li>
</ul>
</li>
<li><strong>“Bullet-Point Digest” (summaries)</strong><ul>
<li>Compact and efficient.</li>
<li>Risk of omitting crucial details during abstraction.</li>
</ul>
</li>
<li><strong>“Precision Strike” (retrieval-based)</strong><ul>
<li>Fetch only what you need, exactly when you need it.</li>
<li>Success hinges on choosing the right retrieval granularity—precisely the issue SeCom addresses.</li>
</ul>
</li>
</ol>
<h4 id="1-2-3-Retrieval-Augmented-Generation-RAG-in-Dialog"><a href="#1-2-3-Retrieval-Augmented-Generation-RAG-in-Dialog" class="headerlink" title="1.2.3 Retrieval-Augmented Generation (RAG) in Dialog"></a>1.2.3 Retrieval-Augmented Generation (RAG) in Dialog</h4><p>RAG faces dialog-specific hurdles:</p>
<ul>
<li><strong>Chunking strategy</strong>: How to segment a dialog into retrievable units.</li>
<li><strong>Relevance estimation</strong>: Harder than in static docs due to dialog dynamics.</li>
<li><strong>Temporal dependency</strong>: Order matters; turns refer to earlier context.</li>
</ul>
<h3 id="1-3-The-Granularity-Dilemma"><a href="#1-3-The-Granularity-Dilemma" class="headerlink" title="1.3 The Granularity Dilemma"></a>1.3 The Granularity Dilemma</h3><p>We often index memories at the turn-level or at the whole-conversation level. Both extremes break down:</p>
<ul>
<li><strong>Turn-level</strong> → fragments context, loses dependencies, retrieval recall suffers.</li>
<li><strong>Conversation-level</strong> → topic mixture, lots of noise, retrieval becomes coarse.</li>
<li><strong>Summaries</strong> → irreversible information loss.</li>
</ul>
<p>SeCom’s insight: dialog naturally contains <strong>paragraph-level thematic boundaries</strong>. Segmenting at this “just-right” granularity preserves coherence without exploding memory size.</p>
<h2 id="2-Inside-SeCom"><a href="#2-Inside-SeCom" class="headerlink" title="2. Inside SeCom"></a>2. Inside SeCom</h2><h3 id="2-1-Two-Key-Insights"><a href="#2-1-Two-Key-Insights" class="headerlink" title="2.1 Two Key Insights"></a>2.1 Two Key Insights</h3><ol>
<li><strong>Paragraph-like Topic Shifts</strong> exist in dialog just as in essays.</li>
<li><strong>Natural Language Is Redundant</strong>—filler words, confirmations, small talk, etc. Removing them boosts retrieval precision.</li>
</ol>
<p>Hence <strong>SeCom = Segmentation + Compression</strong>.</p>
<h3 id="2-2-System-Pipeline"><a href="#2-2-System-Pipeline" class="headerlink" title="2.2 System Pipeline"></a>2.2 System Pipeline</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">History → [Segmenter] → Paragraph-level units → [Compressor] → Denoised memories → [Retriever] → Relevant context → [Generator] → Final reply</span><br></pre></td></tr></tbody></table></figure>

<p>Technically:</p>
<ol>
<li>Segmenter $f_{\mathcal I}$ splits the dialog.</li>
<li>Compressor $f_{comp}$ denoises each segment.</li>
<li>Retriever $f_R$ ranks memories for the current user utterance $u^*$.</li>
<li>LLM $f_{LLM}$ produces the answer based on top-N memories.</li>
</ol>
<h3 id="2-3-How-to-Segment-Without-Labels"><a href="#2-3-How-to-Segment-Without-Labels" class="headerlink" title="2.3 How to Segment Without Labels"></a>2.3 How to Segment Without Labels</h3><p>SeCom leverages GPT-4 in a <strong>zero-shot</strong> fashion: craft a prompt asking the model to mark topic boundaries and output span indices. No training data required.</p>
<p>When limited gold data are available, a <strong>reflection-based</strong> loop iteratively refines the guidelines using WindowDiff scores and GPT-4 reasoning.</p>
<p>An <strong>incremental segmenter</strong> decides on-the-fly whether a new turn merges into the previous segment or starts a fresh one.</p>
<h3 id="2-4-Denoising-via-LLMLingua-2"><a href="#2-4-Denoising-via-LLMLingua-2" class="headerlink" title="2.4 Denoising via LLMLingua-2"></a>2.4 Denoising via LLMLingua-2</h3><p>LLMLingua-2 scores token importance and keeps the top $(1-r)$ fraction (e.g., 25 %) accordingly. Empirically, retaining just 25 % tokens preserves <strong>&gt;95 %</strong> key information, lifts retrieval GPT4Score by <strong>+9.46</strong>, and yields 4 × speed-up.</p>
<h3 id="2-5-Hybrid-Retrieval"><a href="#2-5-Hybrid-Retrieval" class="headerlink" title="2.5 Hybrid Retrieval"></a>2.5 Hybrid Retrieval</h3><p>BM25 (sparse) and MPNet (dense) scores are linearly combined:</p>
<p>$$\text{score}_{hybrid}=\alpha,\text{BM25}+(1-\alpha),\text{MPNet}, \quad \alpha=0.6$$</p>
<h2 id="3-Final-Thoughts"><a href="#3-Final-Thoughts" class="headerlink" title="3. Final Thoughts"></a>3. Final Thoughts</h2><h3 id="3-1-What-SeCom-Teaches-Us"><a href="#3-1-What-SeCom-Teaches-Us" class="headerlink" title="3.1 What SeCom Teaches Us"></a>3.1 What SeCom Teaches Us</h3><ul>
<li><strong>Simplicity Wins</strong>: Segment + Compress, nothing fancy, yet highly effective.</li>
<li><strong>Understand the Problem First</strong>: The authors nailed the granularity pain-point before designing a solution.</li>
</ul>
<p>Future directions:</p>
<ul>
<li><strong>Personalized segmentation</strong> tuned to each user’s dialog style.</li>
<li><strong>Real-time adaptation</strong> of compression and segmentation based on quality metrics.</li>
</ul>
<hr>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><strong>Paper</strong>: <a target="_blank" rel="noopener" href="https://www.arxiv.org/abs/2502.05589">SeCom: On Memory Construction and Retrieval for Personalized Conversational Agents (ICLR 2025)</a></li>
<li><strong>Project Page</strong>: <a target="_blank" rel="noopener" href="https://llmlingua.com/secom.html">https://llmlingua.com/secom.html</a></li>
<li><strong>Code</strong>: SeCom-main</li>
<li><strong>Datasets</strong>: LOCOMO, Long-MT-Bench+, DialSeg711, TIAGE, SuperDialSeg</li>
</ul>
<p><em>This post is based on Microsoft &amp; Tsinghua University’s ICLR 2025 paper. Please refer to the original publication and open-source repo for implementation details.</em> </p>
</article><section class="jump-container is-flex is-justify-content-space-between my-6"><!-- em is empty placeholder--><a class="button is-default" href="/2026/02/21/NLP%20Insights/Evaluation%20of%20Generation-Based%20Large%20Language%20Models%20(LLMs):%20Opportunities%20and%20Challenges%20from%20Generation%20to%20Judgment.en/" title="Untitled"><i class="iconfont icon-prev mr-2 has-text-grey"></i><span class="has-text-weight-semibold">Previous: Untitled</span></a><a class="button is-default" href="/2025/06/24/NLP%20Insights/SeCom%20Redefining%20Memory%20Management%20in%20Conversational%20AI.zh-CN/" title="SeCom: Redefining Memory Management in Conversational AI"><span class="has-text-weight-semibold">Next: SeCom: Redefining Memory Management in Conversational AI</span><i class="iconfont icon-next ml-2 has-text-grey"></i></a></section><article class="mt-6 comment-container"><script async repo="Haojen/Claudia-theme-blog" src="https://utteranc.es/client.js" issue-term="pathname" theme="preferred-color-scheme"></script></article></div></div></main></main><footer class="is-flex is-flex-direction-column is-align-items-center is-flex-shrink-0 is-family-serif"><section class="sns-container"><a title="twitter" target="_blank" rel="noopener nofollow" href="//twitter.com//"><i class="iconfont icon-twitter"></i></a><!-- Github--><a title="github" target="_blank" rel="noopener nofollow" href="//github.com/haojen"><i class="iconfont icon-github"></i></a><!-- Ins--><a title="instagram" target="_blank" rel="noopener nofollow" href="//www.instagram.com//"><i class="iconfont icon-ins"></i></a><!-- RSS--><!-- 知乎--><!-- 领英--><!-- 脸书--><a title="facebook" target="_blank" rel="noopener nofollow" href="//www.facebook.com//"><i class="iconfont icon-tian7_facebook"></i></a></section><p><span>Copyright ©</span><span> user 2026</span></p><div class="is-flex is-justify-content-center is-flex-wrap-wrap"><p>Powered by Hexo &verbar;&nbsp;</p><p class="is-flex is-justify-content-center"><a title="Hexo theme author" target="_blank" rel="noopener" href="//github.com/haojen">Theme by Haojen&nbsp;</a></p><div style="margin-top: 2px"><a class="github-button" title="github-button" target="_blank" rel="noopener" href="https://github.com/haojen/hexo-theme-Claudia" data-color-scheme="no-preference: light; light: light; dark: dark;" data-show-count="true"></a></div></div><div><span></span></div></footer><script src="/js/lang-switch.js"></script><script async defer src="https://buttons.github.io/buttons.js"></script><script src="/js/jquery-3.6.1.min.js"></script><script src="/js/jquery-fancybox.min.js"></script><script src="/js/img_zoom.js"></script><script src="/js/post.js"></script></body></html>