<!DOCTYPE html><html lang="[&quot;zh-CN&quot;,&quot;en&quot;]" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>黑头呆鱼进化之旅 - 只身打码过草原</title><meta name="author" content="Huiyu Chen"><meta name="copyright" content="Huiyu Chen"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta property="og:type" content="website">
<meta property="og:title" content="黑头呆鱼进化之旅">
<meta property="og:url" content="https://chenhuiyu.github.io/page/4/index.html">
<meta property="og:site_name" content="黑头呆鱼进化之旅">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://chenhuiyu.github.io/img/butterfly-icon.png">
<meta property="article:author" content="Huiyu Chen">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://chenhuiyu.github.io/img/butterfly-icon.png"><script type="application/ld+json"></script><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://chenhuiyu.github.io/page/4/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=5.5.4"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><script>
    (() => {
      
    const saveToLocal = {
      set: (key, value, ttl) => {
        if (!ttl) return
        const expiry = Date.now() + ttl * 86400000
        localStorage.setItem(key, JSON.stringify({ value, expiry }))
      },
      get: key => {
        const itemStr = localStorage.getItem(key)
        if (!itemStr) return undefined
        const { value, expiry } = JSON.parse(itemStr)
        if (Date.now() > expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return value
      }
    }

    window.btf = {
      saveToLocal,
      getScript: (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        Object.entries(attr).forEach(([key, val]) => script.setAttribute(key, val))
        script.onload = script.onreadystatechange = () => {
          if (!script.readyState || /loaded|complete/.test(script.readyState)) resolve()
        }
        script.onerror = reject
        document.head.appendChild(script)
      }),
      getCSS: (url, id) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onload = link.onreadystatechange = () => {
          if (!link.readyState || /loaded|complete/.test(link.readyState)) resolve()
        }
        link.onerror = reject
        document.head.appendChild(link)
      }),
      addGlobalFn: (key, fn, name = false, parent = window) => {
        if (!false && key.startsWith('pjax')) return
        const globalFn = parent.globalFn || {}
        globalFn[key] = globalFn[key] || {}
        globalFn[key][name || Object.keys(globalFn[key]).length] = fn
        parent.globalFn = globalFn
      }
    }
  
      
      const activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      const activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }

      btf.activateDarkMode = activateDarkMode
      btf.activateLightMode = activateLightMode

      const theme = saveToLocal.get('theme')
    
          theme === 'dark' ? activateDarkMode() : theme === 'light' ? activateLightMode() : null
        
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        document.documentElement.classList.toggle('hide-aside', asideStatus === 'hide')
      }
    
      
    const detectApple = () => {
      if (/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)) {
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
  
    })()
  </script><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false,"highlightFullpage":false,"highlightMacStyle":false},
  copy: {
    success: '复制成功',
    error: '复制失败',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.13.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyloadPlugin: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '黑头呆鱼进化之旅',
  isHighlightShrink: false,
  isToc: false,
  pageType: 'home'
}</script><meta name="generator" content="Hexo 6.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="黑头呆鱼进化之旅" type="application/atom+xml">
</head><body><div class="bg-animation" id="web_bg" style="background-image: url(https://i.redd.it/569yxksicxmc1.jpg);"></div><div class="page" id="body-wrap"><header class="full_page" id="page-header"><nav id="nav"><span id="blog-info"><a class="nav-site-title" href="/"><span class="site-name">黑头呆鱼进化之旅</span></a></span><div id="menus"></div></nav><div id="site-info"><h1 id="site-title">黑头呆鱼进化之旅</h1></div><div id="scroll-down"><i class="fas fa-angle-down scroll-down-effects"></i></div></header><main class="layout" id="content-inner"><div class="recent-posts nc" id="recent-posts"><div class="recent-post-items"><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/10/23/NLP%20Insights/LLM%E8%AE%AD%E7%BB%83%E6%9C%AF%E8%AF%AD%E4%BB%8B%E7%BB%8D%EF%BC%9ASFT%E3%80%81LoRA%E3%80%81Alignment%E3%80%81KTO%E5%92%8CDPO.en/" title="LoRA, DPO, KTO 与 SFT 技术详解">LoRA, DPO, KTO 与 SFT 技术详解</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-10-23T08:26:29.000Z" title="发表于 2024-10-23 16:26:29">2024-10-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/NLP-Insights/">NLP Insights</a></span></div><div class="content">LoRA, DPO, KTO 与 SFT 技术详解本篇文档将详细介绍几种在大型语言模型（如 LLAMA3）微调和优化中的重要技术，包括 SFT（Supervised Fine-Tuning）、LoRA（Low-Rank Adaptation）、Alignment 技术、KTO（Kahneman-Tversky Optimization） 和 DPO（Direct Preference Optimization）。文中还将详细阐述每种技术的原理、具体实现方法以及相应的损失函数与优化器选择。  1. SFT（Supervised Fine-Tuning）1.1 原理SFT 是一种传统的微调方法，通过监督学习对预训练模型进行微调，调整模型的参数使其在特定任务上表现更好。SFT 通常用于针对特定的标注数据进行模型微调，训练的过程类似于常规的监督学习。 1.2 实现方法 选择预训练模型：如 GPT、BERT 等语言模型。 准备标注数据集：数据集包含输入和输出对。 训练模型：使用标准的交叉熵损失函数对模型进行训练，通过梯度下降优化参数。  1.3 核心代码使用 Hugging Face 的 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/10/23/NLP%20Insights/LLM%E8%AE%AD%E7%BB%83%E6%9C%AF%E8%AF%AD%E4%BB%8B%E7%BB%8D%EF%BC%9ASFT%E3%80%81LoRA%E3%80%81Alignment%E3%80%81KTO%E5%92%8CDPO/" title="LoRA, DPO, KTO 与 SFT 技术详解">LoRA, DPO, KTO 与 SFT 技术详解</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-10-23T08:26:29.000Z" title="发表于 2024-10-23 16:26:29">2024-10-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/NLP-Insights/">NLP Insights</a></span></div><div class="content">LoRA, DPO, KTO 与 SFT 技术详解本篇文档将详细介绍几种在大型语言模型（如 LLAMA3）微调和优化中的重要技术，包括 SFT（Supervised Fine-Tuning）、LoRA（Low-Rank Adaptation）、Alignment 技术、KTO（Kahneman-Tversky Optimization） 和 DPO（Direct Preference Optimization）。文中还将详细阐述每种技术的原理、具体实现方法以及相应的损失函数与优化器选择。  1. SFT（Supervised Fine-Tuning）1.1 原理SFT 是一种传统的微调方法，通过监督学习对预训练模型进行微调，调整模型的参数使其在特定任务上表现更好。SFT 通常用于针对特定的标注数据进行模型微调，训练的过程类似于常规的监督学习。 1.2 实现方法 选择预训练模型：如 GPT、BERT 等语言模型。 准备标注数据集：数据集包含输入和输出对。 训练模型：使用标准的交叉熵损失函数对模型进行训练，通过梯度下降优化参数。  1.3 核心代码使用 Hugging Face 的 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/08/13/NLP%20Insights/%E4%BD%BF%E7%94%A8%E5%8E%8B%E7%BC%A9%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA%E8%BF%9B%E8%A1%8C%E6%9C%AC%E5%9C%B0%20LLM%20%E7%9A%84%E5%BF%AB%E9%80%9F%20JSON%20%E8%A7%A3%E7%A0%81.en/" title="使用压缩有限状态机进行本地 LLM 的快速 JSON 解码">使用压缩有限状态机进行本地 LLM 的快速 JSON 解码</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-08-13T08:12:10.000Z" title="发表于 2024-08-13 16:12:10">2024-08-13</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/NLP-Insights/">NLP Insights</a></span></div><div class="content">使用压缩有限状态机进行本地 LLM 的快速 JSON 解码作者: Liangsheng Yin, Ying Sheng, Lianmin Zheng日期: 2024 年 2 月 5 日  本文内容基于 LMSYS Org 发布的一篇博客文章，原文链接：LMSYS Org 博客。相关的代码库可以在以下链接找到：SGLang 代码库。 让一个 LLM 始终生成符合特定模式的有效 JSON 或 YAML，对于许多应用来说是一个关键特性。在这篇博客文章中，我们介绍了一种显著加速这种约束解码的优化方法。我们的方法利用了压缩的有限状态机，并且兼容任何正则表达式，因此可以适用于任何 JSON 或 YAML 模式。与现有系统逐步解码一个标记的方式不同，我们的方法分析了正则表达式的有限状态机，压缩了单一的转换路径，并在可能的情况下一次性解码多个标记。与最先进的系统（guidance + llama.cpp，outlines + vLLM）相比，我们的方法可以将延迟减少最多 2 倍，并提高吞吐量最多 2.5 倍。这一优化还使得约束解码比普通解码更快。你可以在 SGLang 上试用它。  图一展示了 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/08/13/NLP%20Insights/%E4%BD%BF%E7%94%A8%E5%8E%8B%E7%BC%A9%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E6%9C%BA%E8%BF%9B%E8%A1%8C%E6%9C%AC%E5%9C%B0%20LLM%20%E7%9A%84%E5%BF%AB%E9%80%9F%20JSON%20%E8%A7%A3%E7%A0%81/" title="使用压缩有限状态机进行本地 LLM 的快速 JSON 解码">使用压缩有限状态机进行本地 LLM 的快速 JSON 解码</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-08-13T08:12:10.000Z" title="发表于 2024-08-13 16:12:10">2024-08-13</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/NLP-Insights/">NLP Insights</a></span></div><div class="content">使用压缩有限状态机进行本地 LLM 的快速 JSON 解码作者: Liangsheng Yin, Ying Sheng, Lianmin Zheng日期: 2024 年 2 月 5 日  本文内容基于 LMSYS Org 发布的一篇博客文章，原文链接：LMSYS Org 博客。相关的代码库可以在以下链接找到：SGLang 代码库。 让一个 LLM 始终生成符合特定模式的有效 JSON 或 YAML，对于许多应用来说是一个关键特性。在这篇博客文章中，我们介绍了一种显著加速这种约束解码的优化方法。我们的方法利用了压缩的有限状态机，并且兼容任何正则表达式，因此可以适用于任何 JSON 或 YAML 模式。与现有系统逐步解码一个标记的方式不同，我们的方法分析了正则表达式的有限状态机，压缩了单一的转换路径，并在可能的情况下一次性解码多个标记。与最先进的系统（guidance + llama.cpp，outlines + vLLM）相比，我们的方法可以将延迟减少最多 2 倍，并提高吞吐量最多 2.5 倍。这一优化还使得约束解码比普通解码更快。你可以在 SGLang 上试用它。  图一展示了 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/08/07/NLP%20Insights/Running%20Fine-tuned%20Gemma-2-2b-it%20with%20vLLM.en/" title="Detailed Steps for Running Fine-tuned Gemma-2-2b-it with vLLM">Detailed Steps for Running Fine-tuned Gemma-2-2b-it with vLLM</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-08-07T02:30:00.000Z" title="发表于 2024-08-07 10:30:00">2024-08-07</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/NLP-Insights/">NLP Insights</a></span></div><div class="content">In this post, I will share the steps to run the fine-tuned Gemma-2-2b-it model using vLLM. This guide will cover the installation process, environment configuration, and common troubleshooting tips. Installation and Verification of vLLMFirst, ensure that you have installed and verified vLLM version 0.5.3.  Install vLLM: 1!pip install vllm==0.5.3  Verify the installation: 123import vllmprint(vllm.__version__)# Output: 0.5.3  Installing FlashinferFollow these steps to install Flashinfer, ensuri...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/08/07/NLP%20Insights/Running%20Fine-tuned%20Gemma-2-2b-it%20with%20vLLM/" title="Detailed Steps for Running Fine-tuned Gemma-2-2b-it with vLLM">Detailed Steps for Running Fine-tuned Gemma-2-2b-it with vLLM</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-08-07T02:30:00.000Z" title="发表于 2024-08-07 10:30:00">2024-08-07</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/NLP-Insights/">NLP Insights</a></span></div><div class="content">In this post, I will share the steps to run the fine-tuned Gemma-2-2b-it model using vLLM. This guide will cover the installation process, environment configuration, and common troubleshooting tips. Installation and Verification of vLLMFirst, ensure that you have installed and verified vLLM version 0.5.3.  Install vLLM: 1!pip install vllm==0.5.3  Verify the installation: 123import vllmprint(vllm.__version__)# Output: 0.5.3  Installing FlashinferFollow these steps to install Flashinfer, ensuri...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/08/07/NLP%20Insights/%E4%BD%BF%E7%94%A8vLLM%E8%BF%90%E8%A1%8C%E5%BE%AE%E8%B0%83%E5%90%8E%E7%9A%84Gemma-2.en/" title="使用vLLM运行微调后的Gemma-2">使用vLLM运行微调后的Gemma-2</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-08-07T02:30:00.000Z" title="发表于 2024-08-07 10:30:00">2024-08-07</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/NLP-Insights/">NLP Insights</a></span></div><div class="content">使用vLLM运行微调后的Gemma-2-2b-it的详细步骤在这里分享一下我运行微调后的Gemma-2-2b-it模型并使用vLLM的步骤，希望对其他人有所帮助。本文将详细介绍安装过程、环境配置以及常见问题的解决方法。 安装和验证vLLM首先，确保安装并验证vLLM的版本是0.5.3。  安装vLLM： 1pip install vllm==0.5.3  验证安装： 123import vllmprint(vllm.__version__)# 输出: 0.5.3  安装Flashinfer按照以下步骤安装Flashinfer，并确保您的torch版本和CUDA兼容性。  检查torch版本和CUDA兼容性： 123import torchprint(torch.__version__)  # 应输出: 2.3.1+cu121print(torch.version.cuda) # 应输出: 12.1  安装Flashinfer：根据文档，Gemma运行在版本0.08。vLLM需要FlashInfer v0.0.8（请参阅vLLM版本和Flashinfer文档中关于Gemma 2的部...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/08/07/NLP%20Insights/%E4%BD%BF%E7%94%A8vLLM%E8%BF%90%E8%A1%8C%E5%BE%AE%E8%B0%83%E5%90%8E%E7%9A%84Gemma-2/" title="使用vLLM运行微调后的Gemma-2">使用vLLM运行微调后的Gemma-2</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-08-07T02:30:00.000Z" title="发表于 2024-08-07 10:30:00">2024-08-07</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/NLP-Insights/">NLP Insights</a></span></div><div class="content">使用vLLM运行微调后的Gemma-2-2b-it的详细步骤在这里分享一下我运行微调后的Gemma-2-2b-it模型并使用vLLM的步骤，希望对其他人有所帮助。本文将详细介绍安装过程、环境配置以及常见问题的解决方法。 安装和验证vLLM首先，确保安装并验证vLLM的版本是0.5.3。  安装vLLM： 1pip install vllm==0.5.3  验证安装： 123import vllmprint(vllm.__version__)# 输出: 0.5.3  安装Flashinfer按照以下步骤安装Flashinfer，并确保您的torch版本和CUDA兼容性。  检查torch版本和CUDA兼容性： 123import torchprint(torch.__version__)  # 应输出: 2.3.1+cu121print(torch.version.cuda) # 应输出: 12.1  安装Flashinfer：根据文档，Gemma运行在版本0.08。vLLM需要FlashInfer v0.0.8（请参阅vLLM版本和Flashinfer文档中关于Gemma 2的部...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/04/17/NLP%20Insights/%20%E5%9B%B0%E6%83%91%E5%BA%A6(PPL).en/" title="如何准确计算固定长度模型的困惑度（PPL）">如何准确计算固定长度模型的困惑度（PPL）</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-04-17T04:00:00.000Z" title="发表于 2024-04-17 12:00:00">2024-04-17</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/NLP-Insights/">NLP Insights</a></span></div><div class="content">如何计算固定长度模型的困惑度（PPL）困惑度（PPL）是评估语言模型最常用的指标之一。在深入探讨之前，我们应该注意这个指标特别适用于传统语言模型（有时被称为自回归或因果语言模型），而对于像 BERT 这样的 masked language models 则没有明确定义（见模型总结）。 困惑度被定义为序列的指数化平均负对数似然。如果我们有一个标记化序列 $X = (x_0, x_1, \dots, x_t)$，那么 $X$ 的困惑度为， $$\text{PPL}(X) = \exp \left{ -\frac{1}{t}\sum*{i=1}^t \log p\theta (xi|x*{&lt;i}) \right}$$ 其中 $\log p\theta (x_i|x{&lt;i})$ 是第 i 个标记的对数似然，条件是根据我们的模型前面的标记 $x_{&lt;i}$。直观上，它可以被认为是评估模型在语料库中指定标记集合上预测均匀性的能力。重要的是，这意味着标记化程序直接影响模型的困惑度，这在比较不同模型时应始终考虑。 这也相当于数据和模型预测之间的交叉熵的指数化。想要了解更多关于困...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2024/04/17/NLP%20Insights/%20%E5%9B%B0%E6%83%91%E5%BA%A6(PPL)/" title="如何准确计算固定长度模型的困惑度（PPL）">如何准确计算固定长度模型的困惑度（PPL）</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time datetime="2024-04-17T04:00:00.000Z" title="发表于 2024-04-17 12:00:00">2024-04-17</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/NLP-Insights/">NLP Insights</a></span></div><div class="content">如何计算固定长度模型的困惑度（PPL）困惑度（PPL）是评估语言模型最常用的指标之一。在深入探讨之前，我们应该注意这个指标特别适用于传统语言模型（有时被称为自回归或因果语言模型），而对于像 BERT 这样的 masked language models 则没有明确定义（见模型总结）。 困惑度被定义为序列的指数化平均负对数似然。如果我们有一个标记化序列 $X = (x_0, x_1, \dots, x_t)$，那么 $X$ 的困惑度为， $$\text{PPL}(X) = \exp \left{ -\frac{1}{t}\sum*{i=1}^t \log p\theta (xi|x*{&lt;i}) \right}$$ 其中 $\log p\theta (x_i|x{&lt;i})$ 是第 i 个标记的对数似然，条件是根据我们的模型前面的标记 $x_{&lt;i}$。直观上，它可以被认为是评估模型在语料库中指定标记集合上预测均匀性的能力。重要的是，这意味着标记化程序直接影响模型的困惑度，这在比较不同模型时应始终考虑。 这也相当于数据和模型预测之间的交叉熵的指数化。想要了解更多关于困...</div></div></div></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/page/3/#content-inner"><i class="fas fa-chevron-left fa-fw"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/#content-inner">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/#content-inner">5</a><span class="space">&hellip;</span><a class="page-number" href="/page/13/#content-inner">13</a><a class="extend next" rel="next" href="/page/5/#content-inner"><i class="fas fa-chevron-right fa-fw"></i></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info text-center"><div class="avatar-img"><img src="/img/butterfly-icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info-name">Huiyu Chen</div><div class="author-info-description"></div><div class="site-data"><a href="/archives/"><div class="headline">文章</div><div class="length-num">124</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">46</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xxxxxx"><i class="fab fa-github"></i><span>Follow Me</span></a></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is my Blog</div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/21/NLP%20Insights/Evaluation%20of%20Generation-Based%20Large%20Language%20Models%20(LLMs):%20Opportunities%20and%20Challenges%20from%20Generation%20to%20Judgment/" title="Evaluation of Generation-Based Large Language Models (LLMs): Opportunities and Challenges from Generation to Judgment">Evaluation of Generation-Based Large Language Models (LLMs): Opportunities and Challenges from Generation to Judgment</a><time datetime="2026-02-20T22:00:00.000Z" title="发表于 2026-02-21 06:00:00">2026-02-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/02/21/NLP%20Insights/Evaluation%20of%20Generation-Based%20Large%20Language%20Models%20(LLMs):%20Opportunities%20and%20Challenges%20from%20Generation%20to%20Judgment.en/" title="Evaluation of Generation-Based Large Language Models (LLMs): Opportunities and Challenges from Generation to Judgment">Evaluation of Generation-Based Large Language Models (LLMs): Opportunities and Challenges from Generation to Judgment</a><time datetime="2026-02-20T22:00:00.000Z" title="发表于 2026-02-21 06:00:00">2026-02-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/24/NLP%20Insights/SeCom%20Redefining%20Memory%20Management%20in%20Conversational%20AI.en/" title="SeCom: Redefining Memory Management in Conversational AI">SeCom: Redefining Memory Management in Conversational AI</a><time datetime="2025-06-24T08:00:00.000Z" title="发表于 2025-06-24 16:00:00">2025-06-24</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/24/NLP%20Insights/SeCom%20Redefining%20Memory%20Management%20in%20Conversational%20AI/" title="SeCom: Redefining Memory Management in Conversational AI">SeCom: Redefining Memory Management in Conversational AI</a><time datetime="2025-06-24T08:00:00.000Z" title="发表于 2025-06-24 16:00:00">2025-06-24</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/06/24/NLP%20Insights/SeCom:%20%E9%87%8D%E6%96%B0%E5%AE%9A%E4%B9%89%E5%AF%B9%E8%AF%9DAI%E7%9A%84%E8%AE%B0%E5%BF%86%E7%AE%A1%E7%90%86.en/" title="SeCom: 重新定义对话AI的记忆管理">SeCom: 重新定义对话AI的记忆管理</a><time datetime="2025-06-24T08:00:00.000Z" title="发表于 2025-06-24 16:00:00">2025-06-24</time></div></div></div></div><div class="card-widget card-categories"><div class="item-headline">
            <i class="fas fa-folder-open"></i>
            <span>分类</span>
            
          </div>
          <ul class="card-category-list" id="aside-cat-list">
            <li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Code-Chronicles/"><span class="card-category-list-name">Code Chronicles</span><span class="card-category-list-count">34</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Debugging-Diaries/"><span class="card-category-list-name">Debugging Diaries</span><span class="card-category-list-count">10</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Life-Reflections/"><span class="card-category-list-name">Life Reflections</span><span class="card-category-list-count">18</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/NLP-Insights/"><span class="card-category-list-name">NLP Insights</span><span class="card-category-list-count">52</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Tech-Toolbox/"><span class="card-category-list-name">Tech Toolbox</span><span class="card-category-list-count">8</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Wanderlust-Adventures/"><span class="card-category-list-name">Wanderlust Adventures</span><span class="card-category-list-count">2</span></a></li>
          </ul></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>标签</span></div><div class="card-tag-cloud"><a href="/tags/Language-Modeling/" style="font-size: 1.14em; color: #999b9d">Language Modeling</a> <a href="/tags/Gemma-2/" style="font-size: 1.14em; color: #999b9d">Gemma-2</a> <a href="/tags/Embedding/" style="font-size: 1.14em; color: #999b9d">Embedding</a> <a href="/tags/Deep-Learning/" style="font-size: 1.14em; color: #999b9d">Deep Learning</a> <a href="/tags/Recommendation/" style="font-size: 1.23em; color: #999ea6">Recommendation</a> <a href="/tags/SeCom/" style="font-size: 1.23em; color: #999ea6">SeCom</a> <a href="/tags/Leetcode/" style="font-size: 1.41em; color: #99a5b7">Leetcode</a> <a href="/tags/Chatbot/" style="font-size: 1.23em; color: #999ea6">Chatbot</a> <a href="/tags/Prompt/" style="font-size: 1.14em; color: #999b9d">Prompt</a> <a href="/tags/IssueFix/" style="font-size: 1.37em; color: #99a4b2">IssueFix</a> <a href="/tags/LLM/" style="font-size: 1.5em; color: #99a9bf">LLM</a> <a href="/tags/Conversational-AI/" style="font-size: 1.23em; color: #999ea6">Conversational AI</a> <a href="/tags/Structured-LLM/" style="font-size: 1.14em; color: #999b9d">Structured LLM</a> <a href="/tags/Git/" style="font-size: 1.14em; color: #999b9d">Git</a> <a href="/tags/Gorilla/" style="font-size: 1.14em; color: #999b9d">Gorilla</a> <a href="/tags/Gradient-Descent/" style="font-size: 1.14em; color: #999b9d">Gradient Descent</a> <a href="/tags/Python/" style="font-size: 1.46em; color: #99a7bb">Python</a> <a href="/tags/%E5%8F%8C%E5%91%A8%E8%B5%9B/" style="font-size: 1.28em; color: #99a0aa">双周赛</a> <a href="/tags/Daily-Challenge/" style="font-size: 1.14em; color: #999b9d">Daily Challenge</a> <a href="/tags/Perplexity/" style="font-size: 1.14em; color: #999b9d">Perplexity</a> <a href="/tags/Tool-Use/" style="font-size: 1.14em; color: #999b9d">Tool Use</a> <a href="/tags/Deployment/" style="font-size: 1.23em; color: #999ea6">Deployment</a> <a href="/tags/Blog/" style="font-size: 1.23em; color: #999ea6">Blog</a> <a href="/tags/DSSM/" style="font-size: 1.14em; color: #999b9d">DSSM</a> <a href="/tags/%E6%9D%82%E8%B0%88/" style="font-size: 1.14em; color: #999b9d">杂谈</a> <a href="/tags/SGLang/" style="font-size: 1.14em; color: #999b9d">SGLang</a> <a href="/tags/Gemma-2-2b-it/" style="font-size: 1.14em; color: #999b9d">Gemma-2-2b-it</a> <a href="/tags/Memory-Management/" style="font-size: 1.23em; color: #999ea6">Memory Management</a> <a href="/tags/Guide-to-Living-on-Singapore-Island/" style="font-size: 1.1em; color: #999">Guide to Living on Singapore Island</a> <a href="/tags/Language-Learning/" style="font-size: 1.37em; color: #99a4b2">Language Learning</a> <a href="/tags/Python-Basic/" style="font-size: 1.37em; color: #99a4b2">Python Basic</a> <a href="/tags/Onnx/" style="font-size: 1.23em; color: #999ea6">Onnx</a> <a href="/tags/FastChat/" style="font-size: 1.23em; color: #999ea6">FastChat</a> <a href="/tags/Train/" style="font-size: 1.23em; color: #999ea6">Train</a> <a href="/tags/FAISS/" style="font-size: 1.14em; color: #999b9d">FAISS</a> <a href="/tags/%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98/" style="font-size: 1.32em; color: #99a2ae">每日一题</a> <a href="/tags/RAG/" style="font-size: 1.23em; color: #999ea6">RAG</a> <a href="/tags/vLLM/" style="font-size: 1.23em; color: #999ea6">vLLM</a> <a href="/tags/Sports/" style="font-size: 1.14em; color: #999b9d">Sports</a> <a href="/tags/%E5%9D%A1%E5%B2%9B%E7%94%9F%E6%B4%BB%E6%8C%87%E5%8C%97/" style="font-size: 1.19em; color: #999da1">坡岛生活指北</a></div></div><div class="card-widget card-archives">
    <div class="item-headline">
      <i class="fas fa-archive"></i>
      <span>归档</span>
      <a class="card-more-btn" href="/archives/"
            title="查看更多">
            <i class="fas fa-angle-right"></i>
          </a>
    </div>
  
    <ul class="card-archive-list">
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2026/02/">
            <span class="card-archive-list-date">
              二月 2026
            </span>
            <span class="card-archive-list-count">2</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/06/">
            <span class="card-archive-list-date">
              六月 2025
            </span>
            <span class="card-archive-list-count">4</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/03/">
            <span class="card-archive-list-date">
              三月 2025
            </span>
            <span class="card-archive-list-count">4</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2025/02/">
            <span class="card-archive-list-date">
              二月 2025
            </span>
            <span class="card-archive-list-count">4</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2024/12/">
            <span class="card-archive-list-date">
              十二月 2024
            </span>
            <span class="card-archive-list-count">14</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2024/10/">
            <span class="card-archive-list-date">
              十月 2024
            </span>
            <span class="card-archive-list-count">4</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2024/08/">
            <span class="card-archive-list-date">
              八月 2024
            </span>
            <span class="card-archive-list-count">6</span>
          </a>
        </li>
      
        <li class="card-archive-list-item">
          <a class="card-archive-list-link" href="/archives/2024/04/">
            <span class="card-archive-list-date">
              四月 2024
            </span>
            <span class="card-archive-list-count">2</span>
          </a>
        </li>
      
    </ul>
  </div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>网站信息</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">文章数目 :</div><div class="item-count">124</div></div><div class="webinfo-item"><div class="item-name">本站访客数 :</div><div class="item-count" id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">本站总浏览量 :</div><div class="item-count" id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">最后更新时间 :</div><div class="item-count" id="last-push-date" data-lastPushDate="2026-02-20T21:54:23.531Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div></div></div></div></div></main><footer id="footer"><div class="footer-other"><div class="footer-copyright"><span class="copyright">&copy;&nbsp;2025 - 2026 By Huiyu Chen</span><span class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo 6.3.0</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly 5.5.4</a></span></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="日间和夜间模式切换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=5.5.4"></script><script src="/js/main.js?v=5.5.4"></script><div class="js-pjax"></div><script src="/js/lang-switch.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>