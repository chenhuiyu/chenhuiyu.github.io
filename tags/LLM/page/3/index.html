<!DOCTYPE html><html class="appearance-auto" lang="zh-CN"><head><meta charset="UTF-8"><title>user's blog</title><meta name="description" content="May the Force be with you"><meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no, initial-scale=1"><!-- Google Analytics --><!-- End Google Analytics -->
<!-- Baidu Analytics --><!-- End Baidu Analytics --><link rel="icon" href="/images/favicon.ico"><link rel="stylesheet" href="/style/common/bulma.css"><link rel="stylesheet" href="/style/base.css"><link rel="stylesheet" href="/style/common/helper.css"><script src="/js/common.js"></script><link rel="stylesheet" href="/style/widget-post-list.css"><meta name="generator" content="Hexo 6.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="黑头呆鱼进化之旅" type="application/atom+xml">
</head><body class="is-flex is-flex-direction-column"><header class="header-widget is-flex-shrink-0 is-hidden-mobile"><div class="container is-fullhd is-flex is-justify-content-space-between is-align-items-center is-full-height"><section class="is-hidden-mobile is-flex-shrink-0"><h2><a href="/">user's blog</a></h2></section><h3 class="is-hidden-mobile is-family-serif is-full-height is-flex is-align-items-center is-flex-shrink-0"><span>标签 · LLM</span></h3><aside class="is-flex-shrink-0"><h3 class="is-inline-block"><a href="/">首页</a></h3><h3 class="is-inline-block"><a href="/about">关于</a></h3><h3 class="is-inline-block"><a href="/archives">归档</a></h3></aside></div></header><header class="is-flex header-widget is-flex-shrink-0 is-align-items-center is-justify-content-center is-hidden-tablet"><h3 class="is-inline-block"><a href="/">首页</a></h3><h3 class="is-inline-block"><a href="/about">关于</a></h3><h3 class="is-inline-block"><a href="/archives">归档</a></h3></header><main><article class="post-container is-flex is-justify-content-center section container is-max-widescreen pt-4 px-2"><div class="columns is-variable is-1-tablet is-3-desktop-only is-2-widescreen is-full-width"><section class="column"><article class="post-item-card"><section class="content post-card-content p-4 pb-5"><header><a href="/tags/LLM"><i class="tag post-item-tag">LLM</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2024/02/19/NLP%20Insights/Understanding%20the%20Differences%20Between%20Fine-tuning%20and%20Further%20Pretraining%20in%20Large%20Language%20Models.en/">Understanding the Differences Between Fine-tuning and Further Pretraining in Large Language Models</a></h2><time class="has-text-grey" datetime="2024-02-19T07:06:29.000Z">2024-02-19</time><p class="is-flex-grow-2 mt-2">Understanding the Differences Between Fine-tuning and Further Pretraining in Large Language ModelsIn the world of Natural Language Processing (NLP), the advent of large language models like GPT and BERT has revolutionized how we approach tasks such as text classification, sentiment analysis, and question-answering. Two pivotal techniques in leveraging thes..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2024/02/19/NLP%20Insights/Understanding%20the%20Differences%20Between%20Fine-tuning%20and%20Further%20Pretraining%20in%20Large%20Language%20Models.en/">更多</a></section></article><article class="post-item-card"><section class="content post-card-content p-4 pb-5"><header><a href="/tags/LLM"><i class="tag post-item-tag">LLM</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2024/02/19/NLP%20Insights/Understanding%20the%20Differences%20Between%20Fine-tuning%20and%20Further%20Pretraining%20in%20Large%20Language%20Models.zh-CN/">Understanding the Differences Between Fine-tuning and Further Pretraining in Large Language Models</a></h2><time class="has-text-grey" datetime="2024-02-19T07:06:29.000Z">2024-02-19</time><p class="is-flex-grow-2 mt-2">Understanding the Differences Between Fine-tuning and Further Pretraining in Large Language ModelsIn the world of Natural Language Processing (NLP), the advent of large language models like GPT and BERT has revolutionized how we approach tasks such as text classification, sentiment analysis, and question-answering. Two pivotal techniques in leveraging thes..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2024/02/19/NLP%20Insights/Understanding%20the%20Differences%20Between%20Fine-tuning%20and%20Further%20Pretraining%20in%20Large%20Language%20Models.zh-CN/">更多</a></section></article><article class="post-item-card"><section class="content post-card-content p-4 pb-5"><header><a href="/tags/LLM"><i class="tag post-item-tag">LLM</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2023/08/02/NLP%20Insights/LLAMA2.en/">Training Llama 2 Model on Single GPU with int8 Quantization and LoRA</a></h2><time class="has-text-grey" datetime="2023-08-02T07:38:29.000Z">2023-08-02</time><p class="is-flex-grow-2 mt-2">Training Llama 2 Model on Single GPU with int8 Quantization and LoRALlama 2概述Llama 2 是一个包含预训练和微调的生成式文本模型的集合，其规模从 70 亿到 700 亿个参数不等。Llama2模型是由Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert等人在Llama 2: Open Foundation and Fine-Tuned Chat Models中提出的。
该论文的摘要如下：
在这项工作中，我们开发并发布了Llama 2，这是一组从70亿到700亿参数的预训练和微调的大型语言模型（LLMs）。我们的微调LLMs，称为Llama 2-Chat，针对对话用例进行了优化..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2023/08/02/NLP%20Insights/LLAMA2.en/">更多</a></section></article><article class="post-item-card"><section class="content post-card-content p-4 pb-5"><header><a href="/tags/LLM"><i class="tag post-item-tag">LLM</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2023/08/02/NLP%20Insights/LLAMA2.zh-CN/">Training Llama 2 Model on Single GPU with int8 Quantization and LoRA</a></h2><time class="has-text-grey" datetime="2023-08-02T07:38:29.000Z">2023-08-02</time><p class="is-flex-grow-2 mt-2">Training Llama 2 Model on Single GPU with int8 Quantization and LoRALlama 2概述Llama 2 是一个包含预训练和微调的生成式文本模型的集合，其规模从 70 亿到 700 亿个参数不等。Llama2模型是由Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert等人在Llama 2: Open Foundation and Fine-Tuned Chat Models中提出的。
该论文的摘要如下：
在这项工作中，我们开发并发布了Llama 2，这是一组从70亿到700亿参数的预训练和微调的大型语言模型（LLMs）。我们的微调LLMs，称为Llama 2-Chat，针对对话用例进行了优化..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2023/08/02/NLP%20Insights/LLAMA2.zh-CN/">更多</a></section></article><article class="post-item-card"><section class="content post-card-content p-4 pb-5"><header><a href="/tags/LLM"><i class="tag post-item-tag">LLM</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2023/07/28/NLP%20Insights/LONGNET.en/">LONGNET - Scaling Transformers to 1,000,000,000 Tokens</a></h2><time class="has-text-grey" datetime="2023-07-28T06:43:10.000Z">2023-07-28</time><p class="is-flex-grow-2 mt-2">LONGNET：将Transformer扩展到10亿个标记在本篇文章中，我们将详细讨论一个近期发布的先进模型——“LongNet”。该模型由微软亚洲研究院研发，于大约两周前正式公布。LongNet基于Transformer模型构建，其核心理念在于拓展Transformer的应用规模。值得一提的是，研究团队成功地将其扩展至处理10亿个令牌的规模。对于熟悉语言模型的人来说，会明白序列长度对模型性能的影响，因为序列长度决定了在执行注意力机制时，能够关联的令牌数量，从而影响模型可以获取的上下文信息长度。例如，我们希望像GPT这样的模型能拥有更长的上下文，使得模型可以参考更久之前的单词来预测下一个令牌。而LongNet就成功地将这个能力扩展到了10亿个令牌。以下图为例，可以清晰看出，GPT的序列长度仅为512，而Po..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2023/07/28/NLP%20Insights/LONGNET.en/">更多</a></section></article><article class="post-item-card"><section class="content post-card-content p-4 pb-5"><header><a href="/tags/LLM"><i class="tag post-item-tag">LLM</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2023/07/28/NLP%20Insights/LONGNET.zh-CN/">LONGNET - Scaling Transformers to 1,000,000,000 Tokens</a></h2><time class="has-text-grey" datetime="2023-07-28T06:43:10.000Z">2023-07-28</time><p class="is-flex-grow-2 mt-2">LONGNET：将Transformer扩展到10亿个标记在本篇文章中，我们将详细讨论一个近期发布的先进模型——“LongNet”。该模型由微软亚洲研究院研发，于大约两周前正式公布。LongNet基于Transformer模型构建，其核心理念在于拓展Transformer的应用规模。值得一提的是，研究团队成功地将其扩展至处理10亿个令牌的规模。对于熟悉语言模型的人来说，会明白序列长度对模型性能的影响，因为序列长度决定了在执行注意力机制时，能够关联的令牌数量，从而影响模型可以获取的上下文信息长度。例如，我们希望像GPT这样的模型能拥有更长的上下文，使得模型可以参考更久之前的单词来预测下一个令牌。而LongNet就成功地将这个能力扩展到了10亿个令牌。以下图为例，可以清晰看出，GPT的序列长度仅为512，而Po..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2023/07/28/NLP%20Insights/LONGNET.zh-CN/">更多</a></section></article><article class="post-item-card"><section class="content post-card-content p-4 pb-5"><header><a href="/tags/LLM"><i class="tag post-item-tag">LLM</i></a><a href="/tags/Prompt"><i class="tag post-item-tag">Prompt</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2023/07/27/NLP%20Insights/Prompt%20Engineering.en/">Prompt Engineering</a></h2><time class="has-text-grey" datetime="2023-07-27T07:44:10.000Z">2023-07-27</time><p class="is-flex-grow-2 mt-2">Prompt EngineeringPrompt Engineering, 也被称为上下文提示，是指在不更新模型权重的情况下，与LLM（语言模型）进行交互以引导其产生期望输出的方法。它是一门实证科学，提示工程方法的效果在不同模型之间可能会有很大的差异，因此需要进行大量的实验和试探。
本文仅关注自回归语言模型的提示工程，不涉及填空测试、图像生成或多模态模型。在本质上，提示工程的目标是实现模型的对齐和可操控性。您可以查阅我之前关于可控文本生成的帖子。
基本提示方法zero-shot学习和few-shot学习是两种最基本的提示模型方法，这些方法由许多LLM论文首创，并且通常用于评估LLM性能。
zero-shot学习zero-shot学习是将任务文本直接输入模型并要求获得结果。
（所有情感分析示例来自于SST-2..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2023/07/27/NLP%20Insights/Prompt%20Engineering.en/">更多</a></section></article><article class="post-item-card"><section class="content post-card-content p-4 pb-5"><header><a href="/tags/LLM"><i class="tag post-item-tag">LLM</i></a><a href="/tags/Prompt"><i class="tag post-item-tag">Prompt</i></a></header><h2 class="mt-4 mb-0 is-family-serif"><a href="/2023/07/27/NLP%20Insights/Prompt%20Engineering.zh-CN/">Prompt Engineering</a></h2><time class="has-text-grey" datetime="2023-07-27T07:44:10.000Z">2023-07-27</time><p class="is-flex-grow-2 mt-2">Prompt EngineeringPrompt Engineering, 也被称为上下文提示，是指在不更新模型权重的情况下，与LLM（语言模型）进行交互以引导其产生期望输出的方法。它是一门实证科学，提示工程方法的效果在不同模型之间可能会有很大的差异，因此需要进行大量的实验和试探。
本文仅关注自回归语言模型的提示工程，不涉及填空测试、图像生成或多模态模型。在本质上，提示工程的目标是实现模型的对齐和可操控性。您可以查阅我之前关于可控文本生成的帖子。
基本提示方法zero-shot学习和few-shot学习是两种最基本的提示模型方法，这些方法由许多LLM论文首创，并且通常用于评估LLM性能。
zero-shot学习zero-shot学习是将任务文本直接输入模型并要求获得结果。
（所有情感分析示例来自于SST-2..</p><a class="button is-default mt-2 has-text-weight-semibold" href="/2023/07/27/NLP%20Insights/Prompt%20Engineering.zh-CN/">更多</a></section></article><section class="paginator is-flex is-justify-content-flex-end is-flex-wrap-wrap mt-5"><a class="extend prev" rel="prev" href="/tags/LLM/page/2/"><i class="iconfont icon-prev has-text-grey"></i></a><a class="page-number" href="/tags/LLM/">1</a><a class="page-number" href="/tags/LLM/page/2/">2</a><span class="page-number current">3</span></section></section><aside class="column is-hidden-mobile is-4-tablet is-3-widescreen"><div style="position: sticky; top: 50px;"><main class="aside-card-container tag-widget is-in-tag-page"><h3>标签</h3><section><a href="/tags/IssueFix"><span class="tag post-item-tag" style="margin-bottom: 5px;">IssueFix</span></a><a href="/tags/Blog"><span class="tag post-item-tag" style="margin-bottom: 5px;">Blog</span></a><a href="/tags/Git"><span class="tag post-item-tag" style="margin-bottom: 5px;">Git</span></a><a href="/tags/K8s"><span class="tag post-item-tag" style="margin-bottom: 5px;">K8s</span></a><a href="/tags/Language%20Modeling"><span class="tag post-item-tag" style="margin-bottom: 5px;">Language Modeling</span></a><a href="/tags/Perplexity"><span class="tag post-item-tag" style="margin-bottom: 5px;">Perplexity</span></a><a href="/tags/Onnx"><span class="tag post-item-tag" style="margin-bottom: 5px;">Onnx</span></a><a href="/tags/Deployment"><span class="tag post-item-tag" style="margin-bottom: 5px;">Deployment</span></a><a href="/tags/Chatbot"><span class="tag post-item-tag" style="margin-bottom: 5px;">Chatbot</span></a><a href="/tags/DSSM"><span class="tag post-item-tag" style="margin-bottom: 5px;">DSSM</span></a><a href="/tags/Recommendation"><span class="tag post-item-tag" style="margin-bottom: 5px;">Recommendation</span></a><a href="/tags/LLM"><span class="tag post-item-tag" style="margin-bottom: 5px;">LLM</span></a><a href="/tags/FastChat"><span class="tag post-item-tag" style="margin-bottom: 5px;">FastChat</span></a><a href="/tags/Train"><span class="tag post-item-tag" style="margin-bottom: 5px;">Train</span></a><a href="/tags/Agent"><span class="tag post-item-tag" style="margin-bottom: 5px;">Agent</span></a><a href="/tags/Tool%20Use"><span class="tag post-item-tag" style="margin-bottom: 5px;">Tool Use</span></a><a href="/tags/Gorilla"><span class="tag post-item-tag" style="margin-bottom: 5px;">Gorilla</span></a><a href="/tags/Prompt"><span class="tag post-item-tag" style="margin-bottom: 5px;">Prompt</span></a><a href="/tags/vLLM"><span class="tag post-item-tag" style="margin-bottom: 5px;">vLLM</span></a><a href="/tags/Gemma-2-2b-it"><span class="tag post-item-tag" style="margin-bottom: 5px;">Gemma-2-2b-it</span></a><a href="/tags/Conversational%20AI"><span class="tag post-item-tag" style="margin-bottom: 5px;">Conversational AI</span></a><a href="/tags/Memory%20Management"><span class="tag post-item-tag" style="margin-bottom: 5px;">Memory Management</span></a><a href="/tags/SeCom"><span class="tag post-item-tag" style="margin-bottom: 5px;">SeCom</span></a><a href="/tags/RAG"><span class="tag post-item-tag" style="margin-bottom: 5px;">RAG</span></a><a href="/tags/Gemma-2"><span class="tag post-item-tag" style="margin-bottom: 5px;">Gemma-2</span></a><a href="/tags/SGLang"><span class="tag post-item-tag" style="margin-bottom: 5px;">SGLang</span></a><a href="/tags/Structured%20LLM"><span class="tag post-item-tag" style="margin-bottom: 5px;">Structured LLM</span></a><a href="/tags/Language%20Learning"><span class="tag post-item-tag" style="margin-bottom: 5px;">Language Learning</span></a><a href="/tags/English%20Vocabulary"><span class="tag post-item-tag" style="margin-bottom: 5px;">English Vocabulary</span></a><a href="/tags/Deep%20Learning"><span class="tag post-item-tag" style="margin-bottom: 5px;">Deep Learning</span></a><a href="/tags/Gradient%20Descent"><span class="tag post-item-tag" style="margin-bottom: 5px;">Gradient Descent</span></a><a href="/tags/Living%20in%20Singapore"><span class="tag post-item-tag" style="margin-bottom: 5px;">Living in Singapore</span></a><a href="/tags/Guide%20to%20Living%20on%20Singapore%20Island"><span class="tag post-item-tag" style="margin-bottom: 5px;">Guide to Living on Singapore Island</span></a><a href="/tags/Sports"><span class="tag post-item-tag" style="margin-bottom: 5px;">Sports</span></a><a href="/tags/%E5%9D%A1%E5%B2%9B%E7%94%9F%E6%B4%BB%E6%8C%87%E5%8C%97"><span class="tag post-item-tag" style="margin-bottom: 5px;">坡岛生活指北</span></a><a href="/tags/Small%20Talk"><span class="tag post-item-tag" style="margin-bottom: 5px;">Small Talk</span></a><a href="/tags/%E6%9D%82%E8%B0%88"><span class="tag post-item-tag" style="margin-bottom: 5px;">杂谈</span></a><a href="/tags/Python"><span class="tag post-item-tag" style="margin-bottom: 5px;">Python</span></a><a href="/tags/Leetcode"><span class="tag post-item-tag" style="margin-bottom: 5px;">Leetcode</span></a><a href="/tags/%E6%AF%8F%E6%97%A5%E4%B8%80%E9%A2%98"><span class="tag post-item-tag" style="margin-bottom: 5px;">每日一题</span></a><a href="/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92"><span class="tag post-item-tag" style="margin-bottom: 5px;">动态规划</span></a><a href="/tags/FAISS"><span class="tag post-item-tag" style="margin-bottom: 5px;">FAISS</span></a><a href="/tags/Embedding"><span class="tag post-item-tag" style="margin-bottom: 5px;">Embedding</span></a><a href="/tags/Python%20Basic"><span class="tag post-item-tag" style="margin-bottom: 5px;">Python Basic</span></a><a href="/tags/Daily%20Challenge"><span class="tag post-item-tag" style="margin-bottom: 5px;">Daily Challenge</span></a><a href="/tags/%E5%8F%8C%E5%91%A8%E8%B5%9B"><span class="tag post-item-tag" style="margin-bottom: 5px;">双周赛</span></a></section></main></div></aside></div></article><script>$claudia.fadeInImage(null, $claudia.blurBackdropImg)

window.addEventListener('resize', $claudia.throttle(function () {
    var images = document.querySelectorAll('.js-img-fadeIn')

    images.forEach($claudia.blurBackdropImg)
}, 150))</script></main><footer class="is-flex is-flex-direction-column is-align-items-center is-flex-shrink-0 is-family-serif"><section class="sns-container"><a title="twitter" target="_blank" rel="noopener nofollow" href="//twitter.com//"><i class="iconfont icon-twitter"></i></a><!-- Github--><a title="github" target="_blank" rel="noopener nofollow" href="//github.com/haojen"><i class="iconfont icon-github"></i></a><!-- Ins--><a title="instagram" target="_blank" rel="noopener nofollow" href="//www.instagram.com//"><i class="iconfont icon-ins"></i></a><!-- RSS--><!-- 知乎--><!-- 领英--><!-- 脸书--><a title="facebook" target="_blank" rel="noopener nofollow" href="//www.facebook.com//"><i class="iconfont icon-tian7_facebook"></i></a></section><p><span>Copyright ©</span><span> user 2026</span></p><div class="is-flex is-justify-content-center is-flex-wrap-wrap"><p>Powered by Hexo &verbar;&nbsp;</p><p class="is-flex is-justify-content-center"><a title="Hexo theme author" target="_blank" rel="noopener" href="//github.com/haojen">Theme by Haojen&nbsp;</a></p><div style="margin-top: 2px"><a class="github-button" title="github-button" target="_blank" rel="noopener" href="https://github.com/haojen/hexo-theme-Claudia" data-color-scheme="no-preference: light; light: light; dark: dark;" data-show-count="true"></a></div></div><div><span></span></div></footer><script src="/js/lang-switch.js"></script><script async defer src="https://buttons.github.io/buttons.js"></script><script>$claudia.fadeInImage(null, $claudia.blurBackdropImg)

window.addEventListener('resize', $claudia.throttle(function () {
    var images = document.querySelectorAll('.js-img-fadeIn')

    images.forEach($claudia.blurBackdropImg)
}, 150))</script></body></html>